{
    "1504.07433": {
        "abstract": "ABSTRACTWe present a fast and user friendly exoplanet transit light curve modelling package PyTransit,implementing optimised versions of the Giménez (2006) and Mandel & Agol (2002) transitmodels. The package offers an object-oriented Python interface to access the two models im-plemented natively in Fortran with OpenMP parallelisation. A partial OpenCL version of thequadratic Mandel-Agol model is also included for GPU-accelerated computations. The aimof PyTransit is to facilitate the analysis of photometric time series of exoplanet transits con-sisting of hundreds of thousands of datapoints, and of multi-passband transit light curves fromspectrophotometric observations, as a part of a researchers programming toolkit for buildingcomplex, problem-specific, analyses.Key words: Methods: numerical–Techniques: photometric–Planets and satellites1 INTRODUCTIONThe rapid increase in computational power during the last decadeshas allowed for the adoption of increasingly robust statistical meth-ods in the analysis of astrophysical data. Specially, combining afully Bayesian approach to inference with Markov Chain MonteCarlo (MCMC) sampling for the posterior estimation has allowedfor improved characterisation of model parameter uncertainties inparameter estimation problems, while the adoption of Bayesianmodel selection has given us the tools to robustly judge betweenmany competing hypotheses aiming to explain our observationaldata. The increased flexibility and robustness come with a price.While the methods allow us to work with complex models with ahigh number of dimensions (free parameters), increasing dimen-sionality quickly increases the number of likelihood evaluationsrequired for the analysis. Further, while the computers keep get-ting faster, also the size of the observational datasets (and the re-serachers’ ambitions) keep growing thanks to the advancements ininstrumentation and observation techniques.In the analysis of photometric times series of exoplanet tran-sits (transit light curves), the size and complexity of the observa-tional datasets has increased due to the introduction of space-basedtelescopes CoRoT and Kepler, observing potentially hundreds ofindividual transits for a single transiting planet;1 due to introduc-tion of lucky-imaging techniques allowing for very high time reso-lution observations; and due to the maturing of spectrophotometryas a transit observation method.1 And, while the Kepler long-cadence (LC) mode produces a relativelysmall number of exposures per transit, the modelling of long cadence datarequires model supersampling to account for the extended integration time(Kipping 2010).The space-based telescopes and lucky-imaging cameras pro-duce light curves with tens to hundreds of thousands exposures,while the ground-based spectrophotometric observations of indi-vidual transits yield a smaller number of exposures, but with anadditional dimension (number of passbands extracted from the ob-served spectra) to our time series. Both the stellar limb darkeningand the planetary radius vary as a function of the wavelength cov-erage of the passband, and in typical cases the dimensionality ofthe parameter space increases by 3-5 free parameters per passband",
        "arxiv": "1504.07433",
        "doi": "10.1093/mnras/stv894",
        "file_name": "1504.07433.pdf",
        "file_path": "./sources/1504.07433/1504.07433.pdf",
        "title": "PyTransit: Fast and Easy Exoplanet Transit Modelling in Python",
        "urls": {
            "git": [
                {
                    "#_appearances": 5,
                    "url": "https://github.com/hpparvi/exo_tutorials"
                },
                {
                    "#_appearances": 4,
                    "url": "https://github.com/hpparvi/PyTransit"
                }
            ]
        }
    },
    "1508.02911": {
        "abstract": "ABSTRACTKMOS (K-Band Multi Object Spectrograph) is a novel integral field spectrograph installedin the VLT’s ANTU unit. The instrument offers an ability to observe 24 2.8′′×2.8′′ sub-fieldspositionable within a 7.2′ patrol field, each sub-field producing a spectrum with a 14×14-pixel spatial resolution. The main science drivers for KMOS are the study of galaxies, starformation, and molecular clouds, but its ability to simultaneously measure spectra of multi-ple stars makes KMOS an interesting instrument for exoplanet atmosphere characterizationvia transmission spectroscopy. We set to test whether transmission spectroscopy is practi-cal with KMOS, and what are the conditions required to achieve the photometric precisionneeded, based on observations of a partial transit of WASP-19b, and full transits of GJ 1214band HD 209458b. Our analysis uses the simultaneously observed comparison stars to reducethe effects from instrumental and atmospheric sources, and Gaussian processes to model theresidual systematics. We show that KMOS can, in theory, deliver the photometric precisionrequired for transmission spectroscopy. However, this is shown to require a) pre-imaging toensure accurate centering and b) a very stable night with optimal observing conditions (see-ing ∼0.8′′). Combining these two factors with the need to observe several transits, each witha sufficient out-of-transit baseline (and with the fact that similar or better precision can bereached with telescopes and instruments with smaller pressure,) we conclude that transmis-sion spectroscopy is not the optimal science case to take advantage of the abilities offered byKMOS and VLT.Key words: Instrumentation: spectrographs–Techniques: photometric–Techniques: spectro-scopic: Planets and satellites: atmospheres1 INTRODUCTIONTransmission spectroscopy, the measurement of a transit depth as afunction of wavelength, allows us to probe the existence and abun-dance of different atmospheric species–each with their wavelength-dependent extinction features–in planetary atmospheres (Brown2001). However, the variations in the transit depth are minute, andhigh-precision spectroscopic time series are required in the charac-terization of the planetary transmission spectra.Systematic trends from changing telluric and instrumentalconditions impair the ground-based measurements, and the highest-quality transmission spectroscopy observations have been carriedout using space-based HST until recently (Charbonneau et al. 2002;Berta et al. 2012; Sing et al. 2011, 2013; Knutson et al. 2014;Evans et al. 2013). However, the use of multi-object spectrographsin combination with modern data analysis methods has led to re-markable improvements in the precision that can be achieved from? hannu.parviainen@physics.ox.ac.ukthe ground. Simultaneous measurements of the target star and sev-eral comparison stars allow for the correction of common-modesystematics, in parallel to relative photometry (Bean et al. 2010;Jordán et al. 2013; Gibson et al. 2013a,b; Bean et al. 2013; Murgaset al. 2014). Further, the use of Gaussian processes has facilitatedthe correction of systematics by allowing for the robust modeling ofcorrelated noise—including time correlation and correlations withauxiliary measurements such as seeing and humidity—in model-independent fashion (Gibson et al. 2012; Roberts et al. 2013; Ras-mussen & Williams 2006).The K-band Multi Object Spectrograph (KMOS, Sharples",
        "arxiv": "1508.02911",
        "doi": null,
        "file_name": "1508.02911.pdf",
        "file_path": "./sources/1508.02911/1508.02911.pdf",
        "title": "Exoplanet Transmission Spectroscopy using KMOS",
        "urls": {
            "git": [
                {
                    "#_appearances": 1,
                    "url": "https://github.com/hpparvi/PyTransit"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/hpparvi/PyDE"
                }
            ]
        }
    },
    "1602.08793": {
        "abstract": "AbstractIn this paper, we study statistical inference in functional quantile regression for scalar response and a functionalcovariate. Specifically, we consider a functional linear quantile regression model where the effect of the covariate onthe quantile of the response is modeled through the inner product between the functional covariate and an unknownsmooth regression parameter function that varies with the level of quantile. The objective is to test that the regressionparameter is constant across several quantile levels of interest. The parameter function is estimated by combiningideas from functional principal component analysis and quantile regression. An adjusted Wald testing procedureis proposed for this hypothesis of interest, and its chi-square asymptotic null distribution is derived. The testingprocedure is investigated numerically in simulations involving sparse and noisy functional covariates and in a capitalbike share data application. The proposed approach is easy to implement and the R code is published online athttps://github.com/xylimeng/fQR-testing.Keywords: Composite quantile regression, Functional principal component analysis, Functional quantile regression,Measurement error, Wald test.2020 MSC: Primary 62G08, Secondary 62H151. IntroductionThe advance in computation and technology generated an explosion of data that have functional characteristics.The need to analyze this type of data triggered a rapid growth of the functional data analysis (FDA) field; see [9, 35]for two comprehensive treatments. Most research in functional data analysis has primarily focused on mean regression(see, for example, [10, 19, 20, 41, 46]); only a few works accommodate higher-order moment effects [29, 38]. Quantileregression is appealing in many applications as it allows us to describe the entire conditional distribution of theresponse at various quantile levels. For example, in our capital bike share data application, it is of interest to studyhow the bike rental behavior of casual users in the previous day affects the upper quantiles of total bike rentals in thecurrent day.Quantile regression models for scalar responses and functional covariates have been introduced in [2]. Functionalquantile regression (fQR) models essentially extend the standard quantile regression framework to account for func-tional covariates: the effect of the covariate on a particular quantile of the response is modeled through the innerproduct between the functional covariate and an unknown smooth regression parameter function that varies with thelevel of quantile. Cardot et al. [2] considered a smoothing splines-based approach to represent the functional co-variates and derived its convergence rate; Kato [22] studied principal component analysis (PCA)-based estimationand established a sharp convergence rate. In [5] and [4] Crambes et al. discussed nonparametric quantile regressionestimation and studied the theoretical properties of a support vector machine-based estimator, a method inspired from[30]. Yao et al. [47] considered regularized partially fQR model by additionally incorporating high-dimensional scalarcovariates. Shi et al. [37] developed a procedure to test the adequacy of fQR based on functional PCA. Unlike these∗Corresponding author. Email address: meng@rice.eduPreprint submitted to Journal of Multivariate AnalysisarXiv:1602.08793v3  [stat.ME",
        "arxiv": "1602.08793",
        "doi": "10.1016/j.jmva.2022.104985",
        "file_name": "1602.08793.pdf",
        "file_path": "./sources/1602.08793/1602.08793.pdf",
        "title": "Inference in Functional Linear Quantile Regression",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/xylimeng/fQR-testing"
                }
            ]
        }
    },
    "1603.05867": {
        "abstract": "AbstractIn this paper, we consider optimal low-rank regularized inverse matrix approxi-mations and their applications to inverse problems. We give an explicit solution to ageneralized rank-constrained regularized inverse approximation problem, where the keynovelties are that we allow for updates to existing approximations and we can incorpo-rate additional probability distribution information. Since computing optimal regular-ized inverse matrices under rank constraints can be challenging, especially for problemswhere matrices are large and sparse or are only accessable via function call, we pro-pose an efficient rank-update approach that decomposes the problem into a sequence ofsmaller rank problems. Using examples from image deblurring, we demonstrate thatmore accurate solutions to inverse problems can be achieved by using rank-updatesto existing regularized inverse approximations. Furthermore, we show the potentialbenefits of using optimal regularized inverse matrix updates for solving perturbed to-mographic reconstruction problems.Keywords: ill-posed inverse problems, low-rank matrix approximation, regularization, BayesriskAMS: 65F22, 15A09, 15A291 IntroductionOptimal low-rank inverse approximations play a critical role in many scientific applicationssuch as matrix completion, machine learning, and data analysis [38, 14, 26]. Recent theoret-ical and computational developments on regularized low-rank inverse matrices have enabled∗Department of Mathematics, Virginia Tech, Blacksburg, VAB jmchung@vt.edu m www.math.vt.edu/people/jmchung/†Department of Mathematics, Virginia Tech, Blacksburg, VAB mcchung@vt.edu m www.math.vt.edu/people/mcchung/1arXiv:1603.05867v1  [math.NA]  18 Mar 2016",
        "arxiv": "1603.05867",
        "doi": null,
        "file_name": "1603.05867.pdf",
        "file_path": "./sources/1603.05867/1603.05867.pdf",
        "title": "Optimal regularized inverse matrices for inverse problems",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/juliannechung/ORIM"
                }
            ]
        }
    },
    "1605.00361": {
        "abstract": "AbstractWe show that repulsive random variables can yield Monte Carlo methods withfaster convergence rates than the typical N−1/2, where N is the number of integrandevaluations. More precisely, we propose stochastic numerical quadratures involvingdeterminantal point processes associated with multivariate orthogonal polynomials,and we obtain root mean square errors that decrease as N−(1+1/d)/2, where d is thedimension of the ambient space. First, we prove a central limit theorem (CLT) for thelinear statistics of a class of determinantal point processes, when the reference mea-sure is a product measure supported on a hypercube, which satisfies the Nevai-classregularity condition; a result which may be of independent interest. Next, we intro-duce a Monte Carlo method based on these determinantal point processes, and provea CLT with explicit limiting variance for the quadrature error, when the referencemeasure satisfies a stronger regularity condition. As a corollary, by taking a specificreference measure and using a construction similar to importance sampling, we ob-tain a general Monte Carlo method, which applies to any measure with continuouslyderivable density. Loosely speaking, our method can be interpreted as a stochasticcounterpart to Gaussian quadrature, which, at the price of some convergence rate, iseasily generalizable to any dimension and has a more explicit error term.Contents1 Introduction 21.1 Gaussian quadrature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31.2 Monte Carlo methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41.3 Quasi-Monte Carlo methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 51.4 Bayesian quadrature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61.5 Our contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 Statement of the results 92.1 Determinantal point processes and multivariate OP Ensembles . . . . . . . 92.1.1 Point processes and determinantal correlation functions . . . . . . . 92.1.2 Multivariate OP Ensembles . . . . . . . . . . . . . . . . . . . . . . . 10∗Univ. Lille, CNRS, Centrale Lille, UMR 9189 – CRIStAL, 59651 Villeneuve dAscq Cedex, France.Email: remi.bardenet@gmail.com†Univ. Lille, CNRS, Inria, UMR 8524, Laboratoire Paul Painlevé, F-59000 Lille, France.Email: adrien.hardy@univ-lille.fr1arXiv:1605.00361v2  [math.PR",
        "arxiv": "1605.00361",
        "doi": null,
        "file_name": "1605.00361.pdf",
        "file_path": "./sources/1605.00361/1605.00361.pdf",
        "title": "Monte Carlo with Determinantal Point Processes",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/rbardenet/dppmc"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/guilgautier/DPPy"
                }
            ]
        }
    },
    "1611.06933": {
        "abstract": "AbstractIn lexicon-based classification, documents are assigned labelsby comparing the number of words that appear from two op-posed lexicons, such as positive and negative sentiment. Cre-ating such words lists is often easier than labeling instances,and they can be debugged by non-experts if classification per-formance is unsatisfactory. However, there is little analysisor justification of this classification heuristic. This paper de-scribes a set of assumptions that can be used to derive a prob-abilistic justification for lexicon-based classification, as wellas an analysis of its expected accuracy. One key assumptionbehind lexicon-based classification is that all words in eachlexicon are equally predictive. This is rarely true in practice,which is why lexicon-based approaches are usually outper-formed by supervised classifiers that learn distinct weights oneach word from labeled instances. This paper shows that it ispossible to learn such weights without labeled data, by lever-aging co-occurrence statistics across the lexicons. This offersthe best of both worlds: light supervision in the form of lexi-cons, and data-driven classification with higher accuracy thantraditional word-counting heuristics.IntroductionLexicon-based classification refers to a classification rulein which documents are assigned labels based on the countof words from lexicons associated with each label (Taboadaet al. 2011). For example, suppose that we have opposedlabels Y ∈ {0, 1}, and we have associated lexiconsW0 andW1. Then for a document with a vector of word counts x,the lexicon-based decision rule is,(1)∑i ∈W0xi ≷∑j ∈W1xj ,where the ≷ operator indicates a decision rule. Put simply,the rule is to select the label whose lexicon matches the mostword tokens.Lexicon-based classification is widely used in industryand academia, with applications ranging from sentimentclassification and opinion mining (Pang and Lee 2008;Liu 2015) to the psychological and ideological analysisof texts (Laver and Garry 2000; Tausczik and Pennebaker2010). The popularity of this approach can be explained byits relative simplicity and ease of use: for domain experts,creating lexicons is intuitive, and, in comparison with label-ing instances, it may offer a faster path towards a reasonablyaccurate classifier (Settles 2011). Furthermore, classificationerrors can be iteratively debugged by refining the lexicons.",
        "arxiv": "1611.06933",
        "doi": "10.1609/aaai.v31i1.10965",
        "file_name": "1611.06933.pdf",
        "file_path": "./sources/1611.06933/1611.06933.pdf",
        "title": "Unsupervised Learning for Lexicon-Based Classification",
        "urls": {}
    },
    "1706.03883": {
        "abstract": "AbstractWe propose a novel approach to the problem ofmultilevel clustering, which aims to simultane-ously partition data in each group and discovergrouping patterns among groups in a potentiallylarge hierarchically structured corpus of data.Our method involves a joint optimization formu-lation over several spaces of discrete probabilitymeasures, which are endowed with Wassersteindistance metrics. We propose a number of vari-ants of this problem, which admit fast optimiza-tion algorithms, by exploiting the connection tothe problem of finding Wasserstein barycenters.Consistency properties are established for the es-timates of both local and global clusters. Finally,experiment results with both synthetic and realdata are presented to demonstrate the flexibilityand scalability of the proposed approach. 11. IntroductionIn numerous applications in engineering and sciences, dataare often organized in a multilevel structure. For instance,a typical structural view of text data in machine learningis to have words grouped into documents, documents aregrouped into corpora. A prominent strand of modeling andalgorithmic works in the past couple decades has been todiscover latent multilevel structures from these hierarchi-cally structured data. For specific clustering tasks, one maybe interested in simultaneously partitioning the data in eachgroup (to obtain local clusters) and partitioning a collectionof data groups (to obtain global clusters). Another concreteexample is the problem of clustering images (i.e., globalclusters) where each image contains partions of multipleannotated regions (i.e., local clusters) (Oliva and Torralba,1Department of Statistics, University of Michigan, USA.2Adobe Research. 3Center for Pattern Recognition and Data An-alytics (PRaDA), Deakin University, Australia. Correspondenceto: Nhat Ho <minhnhat@umich.edu>.Proceedings of the 34 th International Conference on MachineLearning, Sydney, Australia, PMLR 70, 2017. Copyright 2017by the author(s).1Code is available at https://github.com/moonfolk/Multilevel-Wasserstein-Means2001). While hierachical clustering techniques may be em-ployed to find a tree-structed clustering given a collectionof data points, they are not applicable to discovering thenested structure of multilevel data. Bayesian hierarchicalmodels provide a powerful approach, exemplified by in-fluential works such as (Blei et al., 2003; Pritchard et al.,2000; Teh et al., 2006). More specific to the simultane-ous and multilevel clustering problem, we mention the pa-",
        "arxiv": "1706.03883",
        "doi": null,
        "file_name": "1706.03883.pdf",
        "file_path": "./sources/1706.03883/1706.03883.pdf",
        "title": "Multilevel Clustering via Wasserstein Means",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/moonfolk/Multilevel-Wasserstein-Means"
                }
            ]
        }
    },
    "1707.06588": {
        "abstract": "ABSTRACTWe present a new neural text to speech (TTS) method that is able to transform textto speech in voices that are sampled in the wild. Unlike other systems, our solutionis able to deal with unconstrained voice samples and without requiring alignedphonemes or linguistic features. The network architecture is simpler than those inthe existing literature and is based on a novel shifting buffer working memory. Thesame buffer is used for estimating the attention, computing the output audio, andfor updating the buffer itself. The input sentence is encoded using a context-freelookup table that contains one entry per character or phoneme. The speakers aresimilarly represented by a short vector that can also be fitted to new identities, evenwith only a few samples. Variability in the generated speech is achieved by primingthe buffer prior to generating the audio. Experimental results on several datasetsdemonstrate convincing capabilities, making TTS accessible to a wider range ofapplications. In order to promote reproducibility, we release our source code andmodels1.1 INTRODUCTIONWe study the task of mimicking a person’s voice based on samples that are captured in-the-wild. Asfar as we know, no other solution exists for this highly applicable learning problem. While the currentsystems are mostly based on carefully collected or curated audio samples, our method is able toemploy the audio of public speeches (from youtube), despite a large amount of background noise andclapping and even with an inaccurate automatic transcript. Moreover, almost all in-the-wild videoscontain multiple other speakers that become challenging voice sample outliers and, in some cases,the videos are shot with home equipment and are of reduced quality.Our method, called VoiceLoop, is inspired by a working-memory model known as the phonologicalloop (Baddeley, 1986). The loop holds verbal information for short periods of time. It comprisesboth a phonological store, where information is constantly being replaced, and a rehearsal process,which maintains longer-term representations in the phonological store.In our method, we construct a phonological store by employing a shifting buffer that is best seenas a matrix S ∈ Rd×k with columns S[1] . . . S[k]. At every time point, all columns shift to theright (S[i + 1] = S[i] for 1 ≤ i < k), column k is discarded, and a new representation vector uis placed in the first position (S[1] = u). u is a function of four parameters, among which are thelatest “spoken” output and the buffer S itself. The buffer is, therefore, constantly refreshed with newinformation, similar to the phonological store, and the mechanism that creates the representationsreuses the existing information in the buffer, thus creating long term dependencies.The two other input parameters of the network that computes the new representation u are the identityof the speaker and the current attention-mediated context. The identity is captured by a learnedembedding and is stored in a lookup table (for the individuals in the training set) or fitted (for newindividuals). The usage of this embedding for the phonological store means that it influences thedynamic behavior of the store, the attention mechanism and the output process. Since the last processrequires heavy personalization, it also receives the identity embedding directly.1PyTorch code and sample audio files are available here: https://github.com/facebookresearch/loop1arXiv:1707.06",
        "arxiv": "1707.06588",
        "doi": null,
        "file_name": "1707.06588.pdf",
        "file_path": "./sources/1707.06588/1707.06588.pdf",
        "title": "VoiceLoop: Voice Fitting and Synthesis via a Phonological Loop",
        "urls": {
            "git": [
                {
                    "#_appearances": 1,
                    "url": "https://github.com/sotelo/parrot"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/keithito/tacotron"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/facebookresearch/loop"
                }
            ]
        }
    },
    "1707.08551": {
        "abstract": "ABSTRACTRecently we have observed emerging uses of deep learning tech-niques in multimedia systems. Developing a practical deep learningsystem is arduous and complex. It involves labor-intensive tasksfor constructing sophisticated neural networks, coordinating mul-tiple network models, and managing a large amount of training-related data. To facilitate such a development process, we proposeTensorLayer which is a Python-based versatile deep learning library.TensorLayer provides high-level modules that abstract sophisticatedoperations towards neuron layers, network models, training dataand dependent training jobs. In spite of offering simplicity, it hastransparent module interfaces that allows developers to flexiblyembed low-level controls within a backend engine, with the aim ofsupporting fine-grain tuning towards training. Real-world clusterexperiment results show that TensorLayer is able to achieve com-petitive performance and scalability in critical deep learning tasks.TensorLayer was released in September 2016 on GitHub. Since after,it soon become one of the most popular open-sourced deep learninglibrary used by researchers and practitioners.KEYWORDSDeep Learning, Reinforcement Learning, Parallel Computation,Computer Vision, Natural Language Processing, Data ManagementACM Reference Format:Hao Dong, Akara Supratak, Luo Mai, Fangde Liu, Axel Oehmichen, SimiaoYu, Yike Guo . 2017. TensorLayer: A Versatile Library for Efficient DeepLearning Development. In Proceedings of MM ’17, Mountain View, CA, USA,October 23–27, 2017, 4 pages.https://doi.org/10.1145/3123266.31293911 INTRODUCTIONRecently we have observed the prosperity of applying deep learninginto multimedia systems. Important applications include achievingvisual recognition using convolution neural networks (CNN) (e.g.,object recognition [23] and image generation [29]), natural lan-guage understanding using recurrent neural networks (RNN) [26]and machine strategic thinking using deep reinforcement learningPermission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from permissions@acm.org.MM ’17, October 23–27, 2017, Mountain View, CA, USA© 2017 Copyright held by the owner/author(s). Publication rights licensed to Associa-tion for Computing Machinery.ACM ISBN 978-1-4503-4906-2/17/10. . . $15.00https://doi.org/10.1145/3123266.3129391(DRL) [27]. Such a prosperity has led to a booming of deep learn-ing frameworks including TensorFlow [1], MXNet [7], Torch [9]and CNTK [30]. Developing a deep learning system typically starts",
        "arxiv": "1707.08551",
        "doi": null,
        "file_name": "1707.08551.pdf",
        "file_path": "./sources/1707.08551/1707.08551.pdf",
        "title": "TensorLayer: A Versatile Library for Efficient Deep Learning Development",
        "urls": {
            "git": [
                {
                    "#_appearances": 1,
                    "url": "https://github.com/zsdonghao/tensorlayer"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/tflearn/tflearn"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/fchollet/keras"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/deepmind/sonnet"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/akaraspt/tl_paper"
                }
            ]
        }
    },
    "1708.02286": {
        "abstract": "AbstractPerson Re-Identification (person re-id) is a crucial taskas its applications in visual surveillance and human-computer interaction. In this work, we present a novel jointSpatial and Temporal Attention Pooling Network (ASTPN)for video-based person re-identification, which enables thefeature extractor to be aware of the current input video se-quences, in a way that interdependency from the matchingitems can directly influence the computation of each other’srepresentation. Specifically, the spatial pooling layer isable to select regions from each frame, while the attentiontemporal pooling performed can select informative framesover the sequence, both pooling guided by the informationfrom distance matching. Experiments are conduced on theiLIDS-VID, PRID-2011 and MARS datasets and the resultsdemonstrate that this approach outperforms existing state-of-art methods. We also analyze how the joint pooling inboth dimensions can boost the person re-id performancemore effectively than using either of them separately 1.1. IntroductionPerson Re-Identification has been viewed as one of thekey subproblems of the generic object recognition task. Itis also important due to its applications in surveillance, andhuman-computer interaction communities. Given a queryimage, the task is to identify a set of matching person im-ages from a pool, usually captured from the same/differentcameras, from different viewpoints, at the same/differenttime points. It is a very challenging task due to thelarge variations of lighting conditions, viewing angles, bodyposes and occlusions.Methods for re-identification in still images setting havebeen extensively investigated, including feature represen-∗indicates equal contributions.1the code is available at https://github.com/shuangjiexu/Spatial-Temporal-Pooling-Networks-ReIDVideo a Video aVideo b Video cDistance DistanceFigure 1. Sample video frames from one person captured by threecameras a, b and c, simulating how human compare different videopairs. The regions under the cycles are the parts which visual at-tentions are drawn on.tation learning [11, 19, 15, 38], distance metric learning[14, 41, 30, 36, 37, 21] and CNN-based schemes [33, 32,24, 25]. Very recently, researchers began to explore solvingthis problem in video-based setting, which is a more naturalway to perform re-identification. The intuition of this kindof methods is that temporal information related to personmotion can be captured from video. Moreover, sequences ofimages provide rich samples of persons’ appearances, help-",
        "arxiv": "1708.02286",
        "doi": null,
        "file_name": "1708.02286.pdf",
        "file_path": "./sources/1708.02286/1708.02286.pdf",
        "title": "Jointly Attentive Spatial-Temporal Pooling Networks for Video-based Person Re-Identification",
        "urls": {
            "git": [
                {
                    "#_appearances": 1,
                    "url": "https://github.com/shuangjiexu/Spatial-"
                }
            ]
        }
    },
    "1708.07747": {
        "abstract": "AbstractWe present Fashion-MNIST, a new dataset comprising of 28 × 28 grayscaleimages of 70, 000 fashion products from 10 categories, with 7, 000 imagesper category. The training set has 60, 000 images and the test set has10, 000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machinelearning algorithms, as it shares the same image size, data format and thestructure of training and testing splits. The dataset is freely available athttps://github.com/zalandoresearch/fashion-mnist.1 IntroductionThe MNIST dataset comprising of 10-class handwritten digits, was first introduced by LeCun et al.[1998] in 1998. At that time one could not have foreseen the stellar rise of deep learning tech-niques and their performance. Despite the fact that today deep learning can do so much the sim-ple MNIST dataset has become the most widely used testbed in deep learning, surpassing CIFAR-10 [Krizhevsky and Hinton, 2009] and ImageNet [Deng et al., 2009] in its popularity via Googletrends1. Despite its simplicity its usage does not seem to be decreasing despite calls for it in thedeep learning community.The reason MNIST is so popular has to do with its size, allowing deep learning researchers to quicklycheck and prototype their algorithms. This is also complemented by the fact that all machine learninglibraries (e.g. scikit-learn) and deep learning frameworks (e.g. Tensorflow, Pytorch) provide helperfunctions and convenient examples that use MNIST out of the box.Our aim with this work is to create a good benchmark dataset which has all the accessibility ofMNIST, namely its small size, straightforward encoding and permissive license. We took the ap-proach of sticking to the 10 classes 70, 000 grayscale images in the size of 28× 28 as in the originalMNIST. In fact, the only change one needs to use this dataset is to change the URL from where theMNIST dataset is fetched. Moreover, Fashion-MNIST poses a more challenging classification taskthan the simple MNIST digits data, whereas the latter has been trained to accuracies above 99.7%as reported in Wan et al. [2013], Ciregan et al. [2012].We also looked at the EMNIST dataset provided by Cohen et al. [2017], an extended version ofMNIST that extends the number of classes by introducing uppercase and lowercase characters. How-1https://trends.google.com/trends/explore?date=all&q=mnist,CIFAR,ImageNethttp://arxiv.org/abs/1708.07747v2https://github.com/zalandoresearch/fashion-mnisthttps://trends.google.com/trends/explore?date=all&q=mnist,CIFAR,ImageNetever, to be able to use it seamlessly one needs to not only extend the deep learning framework’sMNIST helpers, but also change the underlying deep neural network to classify these extra classes.2 Fashion-MNIST DatasetFashion-MNIST is based on the assortment on Zalando’s website2. Every fashion product on Za-lando has a set of pictures shot by professional photographers, demonstrating different aspects ofthe product, i.e. front and back looks, details, looks with model and in an outfit. The original picturehas a light-gray background (hexadecimal color: #fdfdfd) and stored in 762× 1000 JPEG format.For efficiently serving different frontend components, the original picture is resampled with multipleresolutions, e.g. large, medium, small, thumbnail and tiny.We use the front look thumbnail images of 70, 000 unique products to build Fashion-MNIST. Thoseproducts come from different gender groups: men, women, kids and neutral. In particular, white-color products are not included in the dataset as they have low contrast to the background. Thethumbnails (51 × 73) are then fed into the following conversion pipeline, which is visualized inFigure 1.1. Converting the input to a PNG image.",
        "arxiv": "1708.07747",
        "doi": null,
        "file_name": "1708.07747.pdf",
        "file_path": "./sources/1708.07747/1708.07747.pdf",
        "title": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms",
        "urls": {
            "git": [
                {
                    "#_appearances": 4,
                    "url": "https://github.com/zalandoresearch/fashion-mnist"
                }
            ]
        }
    },
    "1709.06691": {
        "abstract": "AbstractWe present a general method to compute a presentation for any cusped arithmetic hyperbolic latticeΓ, applying a classical result of Macbeath to a suitable Γ-invariant horoball cover of the correspondingsymmetric space. As applications we compute presentations for the Picard modular groups PU(2, 1,Od) ford = 1, 3, 7 and the quaternion hyperbolic lattice PU(2, 1,H) with entries in the Hurwitz integer ring H. Theimplementation of the method for these groups is computer-assisted.1 IntroductionDiscrete subgroups and lattices in semisimple Lie groups form a rich and well-studied class of finitely gen-erated groups acting on non-positively curved metric spaces. The case of real rank one, where the associatedsymmetric space is negatively curved, is of special interest. There are essentially two main families of con-structions of such lattices, arithmetic on one hand and geometric on the other. Arithmetic lattices are roughlyspeaking obtained by taking matrices with entries lying in the integer ring of some number field; the generaldefinition is more complicated and we will not give it here, as the arithmetic lattices that we consider in this pa-per are of this simplest type. By Margulis’ celebrated superrigidity and arithmeticity theorems, all (irreducible)lattices in G are of this arithmetic type when G is a semisimple Lie group of real rank at least 2.The other family involves geometric constructions such as polyhedra, reflections or other types of involutionsor other finite-order isometries. A prototype of this type of construction is given by Coxeter groups in theconstant curvature geometries En, Sn and Hn, which are generated by reflections across hyperplanes. Thesegroups are classical and were classified by Coxeter in the spaces En and Sn, whereas their hyperbolic counterparts(studied by Vinberg and others) are still not completely understood. However by construction these groupscome equipped with data including a presentation (as an abtract Coxeter group) and a fundamental domain fortheir action on the symmetric space.Arithmetic lattices are given by a global description and their global structure is in some sense well under-stood by work of Siegel, Borel, Tits, Prasad and others. However concrete information such as a presentationand a fundamental domain are not readily accessible from the arithmetic construction. One can obtain geo-metric information such as volume by Prasad’s celebrated volume formula ([Pr]) but computing the constantsappearing in this formula usually involves some non-trivial work (see for example [Be] and [Sto]).Very few presentations of arithmetic lattices, and of lattices in general, are known. Presentations canprovide useful geometric and algebraic information about groups, such as explicit index of torsion-free subgroups(effective Selberg lemma, as used for example in [Sto]), cohomology of the group Γ or quotient space X/Γ, seefor instance [Y] (for the Picard modular groups with d = 1, 3) and of course representations of Γ, for instanceif one is interested in deformations of Γ into a larger Lie group.Presentations for SL(n,Z) with n > 3 were given by Steinberg ([Ste], following Magnus); the case of SL(2,Z)is classical and possibly dates to Gauss; see also Siegel [Si]. In rank one, Swan gave in [Sw] presentations forthe Bianchi groups PGL(2,Od) (where Od denotes the ring of integers of Q[i√d] for d a positive square-freeinteger), following Bianchi’s original construction in [Bi]. These act as isometries of (real) hyperbolic 3-space,as they are lattices in PGL(2,C) ' Isom+(H3R).Presentations for the related Picard modular groups PU(2, 1,Od) were found only recently in the simplestcases of d = 3 ([FP]) and d = 1 ([FFP]). One of the reasons for this is that the associated symmetric space,∗Second author partially supported by National Science Foundation Grant DMS-1708463.1arXiv:1709.06",
        "arxiv": "1709.06691",
        "doi": "10.2140/agt.2022.22.3577",
        "file_name": "1709.06691.pdf",
        "file_path": "./sources/1709.06691/1709.06691.pdf",
        "title": "Presentations for cusped arithmetic hyperbolic lattices",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/alice-mark/LatticePresentations"
                }
            ]
        }
    },
    "1711.05101": {
        "abstract": "ABSTRACTL2 regularization and weight decay regularization are equivalent for standardstochastic gradient descent (when rescaled by the learning rate), but as we demon-strate this is not the case for adaptive gradient algorithms, such as Adam. Whilecommon implementations of these algorithms employ L2 regularization (oftencalling it “weight decay” in what may be misleading due to the inequivalence weexpose), we propose a simple modification to recover the original formulation ofweight decay regularization by decoupling the weight decay from the optimizationsteps taken w.r.t. the loss function. We provide empirical evidence that our pro-posed modification (i) decouples the optimal choice of weight decay factor fromthe setting of the learning rate for both standard SGD and Adam and (ii) substan-tially improves Adam’s generalization performance, allowing it to compete withSGD with momentum on image classification datasets (on which it was previouslytypically outperformed by the latter). Our proposed decoupled weight decay hasalready been adopted by many researchers, and the community has implementedit in TensorFlow and PyTorch; the complete source code for our experiments isavailable at https://github.com/loshchil/AdamW-and-SGDW1 INTRODUCTIONAdaptive gradient methods, such as AdaGrad (Duchi et al., 2011), RMSProp (Tieleman & Hinton,2012), Adam (Kingma & Ba, 2014) and most recently AMSGrad (Reddi et al., 2018) have becomea default method of choice for training feed-forward and recurrent neural networks (Xu et al., 2015;Radford et al., 2015). Nevertheless, state-of-the-art results for popular image classification datasets,such as CIFAR-10 and CIFAR-100 Krizhevsky (2009), are still obtained by applying SGD withmomentum (Gastaldi, 2017; Cubuk et al., 2018). Furthermore, Wilson et al. (2017) suggested thatadaptive gradient methods do not generalize as well as SGD with momentum when tested on adiverse set of deep learning tasks, such as image classification, character-level language modelingand constituency parsing. Different hypotheses about the origins of this worse generalization havebeen investigated, such as the presence of sharp local minima (Keskar et al., 2016; Dinh et al.,2017) and inherent problems of adaptive gradient methods (Wilson et al., 2017). In this paper, weinvestigate whether it is better to use L2 regularization or weight decay regularization to train deepneural networks with SGD and Adam. We show that a major factor of the poor generalization of themost popular adaptive gradient method, Adam, is due to the fact that L2 regularization is not nearlyas effective for it as for SGD. Specifically, our analysis of Adam leads to the following observations:L2 regularization and weight decay are not identical. The two techniques can be made equiv-alent for SGD by a reparameterization of the weight decay factor based on the learningrate; however, as is often overlooked, this is not the case for Adam. In particular, whencombined with adaptive gradients, L2 regularization leads to weights with large historicparameter and/or gradient amplitudes being regularized less than they would be when us-ing weight decay.L2 regularization is not effective in Adam. One possible explanation why Adam and otheradaptive gradient methods might be outperformed by SGD with momentum is that commondeep learning libraries only implement L2 regularization, not the original weight decay.Therefore, on tasks/datasets where the use of L2 regularization is beneficial for SGD (e.g.,1arXiv:1711.",
        "arxiv": "1711.05101",
        "doi": null,
        "file_name": "1711.05101.pdf",
        "file_path": "./sources/1711.05101/1711.05101.pdf",
        "title": "Decoupled Weight Decay Regularization",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/sgugger/Adam-experiments"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/pytorch/pytorch"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/loshchil/AdamW-and-SGDW"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/Yagami123/Caffe-AdamW-AdamWR"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/GLambard/AdamW_Keras"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/xgastaldi/shake-shake"
                }
            ]
        }
    },
    "1711.082": {
        "abstract": "AbstractThe work in this paper is driven by the question how toexploit the temporal cues available in videos for their ac-curate classification, and for human action recognition inparticular? Thus far, the vision community has focused onspatio-temporal approaches with fixed temporal convolutionkernel depths. We introduce a new temporal layer that mod-els variable temporal convolution kernel depths. We embedthis new temporal layer in our proposed 3D CNN. We extendthe DenseNet architecture - which normally is 2D - with 3Dfilters and pooling kernels. We name our proposed videoconvolutional network ‘Temporal 3D ConvNet’ (T3D) andits new temporal layer ‘Temporal Transition Layer’ (TTL).Our experiments show that T3D outperforms the currentstate-of-the-art methods on the HMDB51, UCF101 and Ki-netics datasets.The other issue in training 3D ConvNets is about trainingthem from scratch with a huge labeled dataset to get a rea-sonable performance. So the knowledge learned in 2D Con-vNets is completely ignored. Another contribution in thiswork is a simple and effective technique to transfer knowl-edge from a pre-trained 2D CNN to a randomly initialized3D CNN for a stable weight initialization. This allows us tosignificantly reduce the number of training samples for 3DCNNs. Thus, by finetuning this network, we beat the perfor-mance of generic and recent methods in 3D CNNs, whichwere trained on large video datasets, e.g. Sports-1M, andfinetuned on the target datasets, e.g. HMDB51/UCF101.The T3D codes will be released soon1.1. IntroductionCompelling advantages of exploiting temporal ratherthan merely spatial cues for video classification have beenshown lately [8, 29, 36]. Such insights are all the more im-portant given the surge in multimedia videos on the Internet.?Ali Diba and Mohsen Fayyaz contributed equally to this work.Mohsen Fayyaz contributed to this work while he was at Sensifai.1https://github.com/MohsenFayyaz89/T3DEven if considerable progress in exploiting temporal cueswas made [4, 10, 29, 30], the corresponding systems are stillwanting. Recently, several variants of Convolutional NeuralNetworks (ConvNets) have been proposed that use 3D con-volutions, but they fail to exploit long-range temporal infor-mation, thus limiting the performance of these architectures.Complicating aspects include: (i) these video architectureshave many more parameters than 2D ConvNets; (ii) trainingthe video architectures calls for extra large labeled datasets;and (iii) extraction and usage of optical-flow maps which isvery demanding, and also difficult to obtain for large scaledataset, e.g. Sports-1M. All of these issues negatively influ-ence their computational cost and performance. Two ways",
        "arxiv": null,
        "doi": null,
        "file_name": "1711.082.pdf",
        "file_path": "./sources/1711.082/1711.082.pdf",
        "title": "Temporal 3D ConvNets: New Architecture and Transfer Learning for Video Classification",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/MohsenFayyaz89/T3D"
                }
            ]
        }
    },
    "1712.05976": {
        "abstract": "\tAbstract\tI Introduction\tII Numerical implementation of the equation of state approach\tIII A brief reminder on the designer f(R) gravity models\tIV Evolution of perturbations in the dark energy fluid of f(R) gravity\tV Impact of f(R) gravity on observables and constraints\tVI Comparison with wCDM models\tVII Discussion and conclusion\tVIII Acknowledgements\t Comparison between the EoS and EFT approaches for dark energy perturbations\t References",
        "arxiv": "1712.05976",
        "doi": null,
        "file_name": "1712.05976.pdf",
        "file_path": "./sources/1712.05976/1712.05976.pdf",
        "title": "Do cosmological data rule out $f(\\mathcal{R})$ with $w\\neq-1$?",
        "urls": {
            "git": [
                {
                    "#_appearances": 1,
                    "url": "https://github.com/borisbolliet/class_eos_fr_public"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/borisbolliet/class"
                }
            ]
        }
    },
    "1803.07268": {
        "abstract": "Abstract. Template-matching methods for visual tracking have gainedpopularity recently due to their comparable performance and fast speed.However, they lack effective ways to adapt to changes in the target ob-ject’s appearance, making their tracking accuracy still far from state-of-the-art. In this paper, we propose a dynamic memory network to adaptthe template to the target’s appearance variations during tracking. AnLSTM is used as a memory controller, where the input is the searchfeature map and the outputs are the control signals for the reading andwriting process of the memory block. As the location of the target is atfirst unknown in the search feature map, an attention mechanism is ap-plied to concentrate the LSTM input on the potential target. To preventaggressive model adaptivity, we apply gated residual template learningto control the amount of retrieved memory that is used to combine withthe initial template. Unlike tracking-by-detection methods where the ob-ject’s information is maintained by the weight parameters of neural net-works, which requires expensive online fine-tuning to be adaptable, ourtracker runs completely feed-forward and adapts to the target’s appear-ance changes by updating the external memory. Moreover, unlike othertracking methods where the model capacity is fixed after offline training –the capacity of our tracker can be easily enlarged as the memory require-ments of a task increase, which is favorable for memorizing long-term ob-ject information. Extensive experiments on OTB and VOT demonstratesthat our tracker MemTrack performs favorably against state-of-the-arttracking methods while retaining real-time speed of 50 fps. 1Keywords: Addressable Memory, Gated Residual Template Learning1 IntroductionAlong with the success of convolution neural networks in object recognitionand detection, an increasing number of trackers [4, 13, 22, 26, 31] have adopteddeep learning models for visual object tracking. Among them are two dominanttracking strategies. One is the tracking-by-detection scheme that online trainsan object appearance classifier [22, 26] to distinguish the target from the back-ground. The model is first learned using the initial frame, and then fine-tunedusing the training samples generated in the subsequent frames based on the1 Code is available at https://github.com/skyoung/MemTrackarXiv:1803.07268v2  [cs.CV]  2",
        "arxiv": "1803.07268",
        "doi": "10.1007/978-3-030-01240-3_10",
        "file_name": "1803.07268.pdf",
        "file_path": "./sources/1803.07268/1803.07268.pdf",
        "title": "Learning Dynamic Memory Networks for Object Tracking",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/skyoung/MemTrack"
                }
            ]
        }
    },
    "1803.0787": {
        "abstract": "Abstract—Classification of multivariate time series (MTS) hasbeen tackled with a large variety of methodologies and applied toa wide range of scenarios. Reservoir Computing (RC) providesefficient tools to generate a vectorial, fixed-size representation ofthe MTS that can be further processed by standard classifiers.Despite their unrivaled training speed, MTS classifiers based ona standard RC architecture fail to achieve the same accuracy offully trainable neural networks. In this paper we introduce thereservoir model space, an unsupervised approach based on RCto learn vectorial representations of MTS. Each MTS is encodedwithin the parameters of a linear model trained to predict a low-dimensional embedding of the reservoir dynamics. Compared toother RC methods, our model space yields better representationsand attains comparable computational performance, thanks to anintermediate dimensionality reduction procedure. As a secondcontribution we propose a modular RC framework for MTSclassification, with an associated open-source Python library. Theframework provides different modules to seamlessly implementadvanced RC architectures. The architectures are compared toother MTS classifiers, including deep learning models and timeseries kernels. Results obtained on benchmark and real-worldMTS datasets show that RC classifiers are dramatically fasterand, when implemented using our proposed representation, alsoachieve superior classification accuracy.Index Terms—Reservoir computing, model space, time seriesclassification, recurrent neural networksI. INTRODUCTIONThe problem of classifying multivariate time series (MTS)consists in assigning each MTS to one of a fixed numberof classes. This is a fundamental task in many applications,including (but not limited to) health monitoring [1], civilengineering [2], action recognition [3], and speech analysis[4]. The problem has been tackled by approaches spanningfrom the definition of tailored distance measures over MTSto the identification of patterns in the form of dictionariesor shapelets [5], [6], [7], [8]. In this paper we focus onclassifiers based on recurrent neural networks (RNNs), whichfirst process sequentially the MTS with a dynamic model, andthen exploit the sequence of the model states generated overtime to perform classification [9].Reservoir computing (RC) is a family of RNN modelswhose recurrent part is generated randomly and then keptfixed [10], [11]. Despite this strong simplification, the recur-rent part of the model (the reservoir) provides a rich pool ofdynamic features which are suitable for solving a large varietyof tasks. Indeed, RC models achieved excellent performance in*filippombianchi@gmail.comF. M. Bianchi is with NORCE, The Norwegian Research CentreS. Scardapane is with Sapienza University of RomeS. Løkse and R. Jenssen are with UiT, the Arctic University of Tromsø.",
        "arxiv": "1803.0787",
        "doi": null,
        "file_name": "1803.0787.pdf",
        "file_path": "./sources/1803.0787/1803.0787.pdf",
        "title": "Reservoir computing approaches for representation and classification of multivariate time series",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/hfawaz/cd-diagram"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/FilippoMB/Reservoir-Computing-framework-for-multivariate-time-series-classification"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/FilippoMB/Reservoir-Computing-framework-for-"
                }
            ]
        }
    },
    "1804.00247": {
        "abstract": "AbstractThis article describes our experiments in neural machine translation using the recent Ten-sor2Tensor framework and the Transformer sequence-to-sequencemodel (Vaswani et al., 2017).We examine some of the critical parameters that affect the final translation quality, memoryusage, training stability and training time, concluding each experiment with a set of recom-mendations for fellow researchers. In addition to confirming the general mantra “more dataand larger models”, we address scaling to multiple GPUs and provide practical tips for im-proved training regarding batch size, learning rate, warmup steps, maximum sentence lengthand checkpoint averaging. We hope that our observations will allow others to get better resultsgiven their particular hardware and data constraints.1. IntroductionIt has been already clearly established that neural machine translation (NMT) isthe new state of the art in machine translation, see e.g. the most recent evaluationcampaigns (Bojar et al., 2017a; Cettolo et al., 2017). Many fundamental changes of theunderlying neural network architecture are nevertheless still frequent and it is verydifficult to predict which of the architectures has the best combination of propertiesto win in the long term, considering all relevant criteria like translation quality, modelsize, stability and speed of training, interpretability but also practical availability ofgood implementations. A considerable part of amodel’s success in translation qualityconsists in the training data, the model’s sensitivity to noise in the data but also on awide range of hyper-parameters that affect the training. Having the right setting ofthem turns out to be often a critical component for the success.1arXiv:1804.00247v2  [cs.CL]  2 May 2018In this article, we experimentwith a relatively newNMTmodel, calledTransformer(Vaswani et al., 2017) as implemented in the Tensor2Tensor1 (abbreviated T2T) toolkit,version 1.2.9. Themodel and the toolkit have been released shortly after the evaluationcampaign atWMT20172 and its behavior on large-data news translation is not yet fullyexplored. We want to empirically explore some of the important hyper-parameters.Hopefully, our observations will be useful also for other researchers considering this",
        "arxiv": "1804.00247",
        "doi": "10.2478/pralin-2018-0002",
        "file_name": "1804.00247.pdf",
        "file_path": "./sources/1804.00247/1804.00247.pdf",
        "title": "Training Tips for the Transformer Model",
        "urls": {
            "git": [
                {
                    "#_appearances": 10,
                    "url": "https://github.com/tensorflow/tensor2tensor"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/awslabs/sockeye"
                }
            ]
        }
    },
    "1804.02747": {
        "abstract": "AbstractWe present and evaluate the Fast (conditional) Independence Test (FIT) – a non-parametric conditional independence test. The test is based on the idea that whenP (X | Y,Z) = P (X | Y ), Z is not useful as a feature to predict X , as long as Yis also a regressor. On the contrary, if P (X | Y, Z) 6= P (X | Y ), Z might improveprediction results. FIT applies to thousand-dimensional random variables with ahundred thousand samples in a fraction of the time required by alternative methods.We provide an extensive evaluation that compares FIT to six extant nonparametricindependence tests. The evaluation shows that FIT has low probability of makingboth Type I and Type II errors compared to other tests, especially as the number ofavailable samples grows. Our implementation of FIT is publicly available1.1 IntroductionTwo random variables X and Y are conditionally independent given a third variable Z if and only ifP (X,Y | Z) = P (X | Z)P (Y | Z). We denote this relationship by X ⊥⊥Y | Z, and its negation,the case of conditional dependence, by X 6⊥⊥Y | Z. This article develops and evaluates the Fast(conditional) Independence Test (FIT), a nonparametric conditional or unconditional independencetest. Given a finite sample from the joint distribution P (X,Y, Z), FIT returns the p-value underthe null hypothesis that X ⊥⊥Y | Z. FIT applies to datasets with large sample sizes on scalar- orvector-valued variables, and returns in short time. Extensive empirical comparison with existingalternatives shows that FIT is currently the only conditional independence test to achieve this goal.1.1 MotivationIndependence tests are ubiquitous in scientific inference. They are the main work horse of classicalhypothesis testing (see Fisher [1925], Chapter 21), which underlies most experimental and manyobservational techniques of data analysis. A conditional independence test (CIT) extends this idea bytesting for independence between two variables given the value of a third, or more generally, giventhe values of a set of further variables [Dawid, 1979]. Probabilistic dependence – conditional ornot – is often taken as an indicator of informational exchange (see e.g. Cover and Thomas [2012],Chapter 2), causal connection [Dawid, 1979, Spirtes et al., 2000] or simply as tool for prediction and1https://github.com/kjchalup/fcitarXiv:1804.02747v1  [stat.ML]  8 Apr 201",
        "arxiv": "1804.02747",
        "doi": null,
        "file_name": "1804.02747.pdf",
        "file_path": "./sources/1804.02747/1804.02747.pdf",
        "title": "Fast Conditional Independence Test for Vector Variables with Large Sample Sizes",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/kjchalup/fcit"
                }
            ]
        }
    },
    "1804.04128": {
        "abstract": "Abstract. This paper proposes a novel approach to generate multiplecolor palettes that reflect the semantics of input text and then colorizea given grayscale image according to the generated color palette. In con-trast to existing approaches, our model can understand rich text, whetherit is a single word, a phrase, or a sentence, and generate multiple possiblepalettes from it. For this task, we introduce our manually curated datasetcalled Palette-and-Text (PAT). Our proposed model called Text2Colorsconsists of two conditional generative adversarial networks: the text-to-palette generation networks and the palette-based colorization networks.The former captures the semantics of the text input and produce rele-vant color palettes. The latter colorizes a grayscale image using the gen-erated color palette. Our evaluation results show that people preferredour generated palettes over ground truth palettes and that our modelcan effectively reflect the given palette when colorizing an image.Keywords: Color Palette Generation · Image Colorization · ConditionalGenerative Adversarial Networks.1 IntroductionHumans can associate certain words with certain colors. The real question is, canmachines effectively learn the relationship between color and text? Using textto express colors can allow ample room for creativity, and it would be usefulto visualize the colors of a certain semantic concept. For instance, since colorscan leave a strong impression on people [19], corporations often decide upon theseason‘s color theme from marketing concepts such as ‘passion.’ Through textinput, even people without artistic backgrounds can easily create color palettes* These authors contributed equally.arXiv:1804.04128v2  [cs.CV]  7 Aug 20182 Hyojin Bahng, Seungjoo Yoo, Wonwoong ChoFig. 1. Colorization results of Text2Colors given text inputs. The text inputis shown above the input grayscale image, and the generated palettes are on the rightof the grayscale image. The color palette is well-reflected in the colorized image when",
        "arxiv": "1804.04128",
        "doi": null,
        "file_name": "1804.04128.pdf",
        "file_path": "./sources/1804.04128/1804.04128.pdf",
        "title": "Coloring with Words: Guiding Image Colorization Through Text-based Palette Generation",
        "urls": {}
    },
    "1805.03383": {
        "abstract": "AbstractThis work identifies and addresses two important techni-cal challenges in single-image super-resolution: (1) how toupsample an image without magnifying noise and (2) how topreserve large scale structure when upsampling. We sum-marize the techniques we developed for our second placeentry in Track 1 (Bicubic Downsampling), seventh place en-try in Track 2 (Realistic Adverse Conditions), and seventhplace entry in Track 3 (Realistic difficult) in the 2018 NTIRESuper-Resolution Challenge. Furthermore, we present newneural network architectures that specifically address thetwo challenges listed above: denoising and preservation oflarge-scale structure.1. IntroductionSuper-resolution (SR) is a classic problem in image pro-cessing where the goal is to generate a high resolution im-age from one or more low resolution images. Applica-tions of super-resolution are wide-ranging. For instance,SR is important for allowing modern high-definition dis-plays to function properly when showing video recorded atlower resolutions. SR also has many applications in med-ical imaging, such as reducing noise in images stemmingfrom uncontrollable patient motions [11]. This work fo-cuses on single image super-resolution, which is useful forphotographic enhancement, license plate recognition, satel-lite imaging, and other remote sensing applications such asrecognition of a military target [16].Deep learning techniques can learn a mapping directlyfrom low resolution to high resolution images, where allfeature construction is automated. This makes some typesof complex preprocessing much easier than previous ap-proaches, for example, we no longer need to explicitlychoose a dictionary of low-level features (e.g., edge detec-tors) to convolve with the image. The fact that trainingdeep neural networks has become much easier within the∗All authors contributed equally. Thanks to other members of DukeData Science Teampast few years has led to more reliable automated training.On the other hand, the fact that these deep learning meth-ods use recursive mathematical formulas that are now muchmore complicated than before makes it more difficult to de-termine how to best troubleshoot them to achieve higher-quality performance.In this work we discuss several insights into the prob-lem of single-image super-resolution – many of which haveled to higher quality performance beyond entries from lastyear’s NTIRE single-image SR competition. These insightsconcern the amplification of noise when upsampling andthe preservation of large scale structure in enhanced im-ages. We introduce neural network architectures for both",
        "arxiv": "1805.03383",
        "doi": "10.1109/cvprw.2018.00132",
        "file_name": "1805.03383.pdf",
        "file_path": "./sources/1805.03383/1805.03383.pdf",
        "title": "New Techniques for Preserving Global Structure and Denoising with Low Information Loss in Single-Image Super-Resolution",
        "urls": {
            "git": [
                {
                    "#_appearances": 3,
                    "url": "https://github.com/nikhilvravi/DukeSR"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/websterbei/EDSR_tensorflow"
                }
            ]
        }
    },
    "1806.02199": {
        "abstract": "ABSTRACTHigh-dimensional time series are common in many domains. Since human cognition isnot optimized to work well in high-dimensional spaces, these areas could benefit frominterpretable low-dimensional representations. However, most representation learningalgorithms for time series data are difficult to interpret. This is due to non-intuitive mappingsfrom data features to salient properties of the representation and non-smoothness over time.To address this problem, we propose a new representation learning framework building onideas from interpretable discrete dimensionality reduction and deep generative modeling.This framework allows us to learn discrete representations of time series, which give rise tosmooth and interpretable embeddings with superior clustering performance. We introducea new way to overcome the non-differentiability in discrete representation learning andpresent a gradient-based version of the traditional self-organizing map algorithm that ismore performant than the original. Furthermore, to allow for a probabilistic interpretation ofour method, we integrate a Markov model in the representation space. This model uncoversthe temporal transition structure, improves clustering performance even further and providesadditional explanatory insights as well as a natural representation of uncertainty.We evaluate our model in terms of clustering performance and interpretability on static(Fashion-)MNIST data, a time series of linearly interpolated (Fashion-)MNIST images, achaotic Lorenz attractor system with two macro states, as well as on a challenging real worldmedical time series application on the eICU data set. Our learned representations comparefavorably with competitor methods and facilitate downstream tasks on the real world data.1 INTRODUCTIONInterpretable representation learning on time series is a seminal problem for uncovering the latent structurein complex systems, such as chaotic dynamical systems or medical time series. In areas where humanshave to make decisions based on large amounts of data, interpretability is fundamental to ease the humantask. Especially when decisions have to be made in a timely manner and rely on observing some chaoticexternal process over time, such as in finance or medicine, the need for intuitive interpretations is even stronger.However, many unsupervised methods, such as clustering, make misleading i.i.d. assumptions about the data,neglecting their rich temporal structure and smooth behaviour over time. This poses the need for a methodof clustering, where the clusters assume a topological structure in a lower dimensional space, such that therepresentations of the time series retain their smoothness in that space. In this work, we present a method withthese properties.1arXiv:1806.02199v7  [cs.LG]  4 J",
        "arxiv": "1806.02199",
        "doi": null,
        "file_name": "1806.02199.pdf",
        "file_path": "./sources/1806.02199/1806.02199.pdf",
        "title": "SOM-VAE: Interpretable Discrete Representation Learning on Time Series",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/ratschlab/SOM-VAE"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/JustGlowing/minisom"
                }
            ]
        }
    },
    "1809.09307": {
        "abstract": "AbstractStatistical characteristics of deep network representations,such as sparsity and correlation, are known to be relevant tothe performance and interpretability of deep learning. When astatistical characteristic is desired, often an adequate regular-izer can be designed and applied during the training phase.Typically, such a regularizer aims to manipulate a statisti-cal characteristic over all classes together. For classificationtasks, however, it might be advantageous to enforce the de-sired characteristic per class such that different classes canbe better distinguished. Motivated by the idea, we design twoclass-wise regularizers that explicitly utilize class informa-tion: class-wise Covariance Regularizer (cw-CR) and class-wise Variance Regularizer (cw-VR). cw-CR targets to reducethe covariance of representations calculated from the sameclass samples for encouraging feature independence. cw-VRis similar, but variance instead of covariance is targeted toimprove feature compactness. For the sake of completeness,their counterparts without using class information, Covari-ance Regularizer (CR) and Variance Regularizer (VR), areconsidered together. The four regularizers are conceptuallysimple and computationally very efficient, and the visualiza-tion shows that the regularizers indeed perform distinct rep-resentation shaping. In terms of classification performance,significant improvements over the baseline and L1/L2 weightregularization methods were found for 21 out of 22 tasks overpopular benchmark datasets. In particular, cw-VR achievedthe best performance for 13 tasks including ResNet-32/110.IntroductionFor deep learning, a variety of regularization techniqueshave been developed by focusing on the weight parame-ters. A classic example is the use of L2 (Hoerl and Kennard1970) and L1 (Tibshirani 1996) weight regularizers. Theyhave been popular because they are easy to use, computa-tionally light, and often result in performance enhancements.Another example is the parameter sharing technique that en-forces the same weight values as in the Convolutional NeuralNetworks (CNNs). Regularization techniques that focus onthe representation (the activations of the units in a deep net-work), however, have been less popular even though the per-formance of deep learning is known to depend on the learnedrepresentation heavily.∗Authors contributed equally.Figure 1: A single unit’s activation histogram (upper threeplots) and two randomly chosen units’ activation scatterplots (lower three plots) for MNIST. For a 6-layer Multi-layer Perceptron (MLP), the fifth layer’s representation vec-tors calculated using 10,000 test samples were used to gen-erate the plots. For the baseline model, a substantial overlapamong different classes can be observed at the time of ini-",
        "arxiv": "1809.09307",
        "doi": "10.1609/aaai.v33i01.33013396",
        "file_name": "1809.09307.pdf",
        "file_path": "./sources/1809.09307/1809.09307.pdf",
        "title": "Utilizing Class Information for Deep Network Representation Shaping",
        "urls": {
            "git": [
                {
                    "#_appearances": 1,
                    "url": "https://github.com/snu-adsl/class_"
                }
            ]
        }
    },
    "1901.05674": {
        "abstract": "AbstractMalware scanners try to protect users from opening malicious docu-ments by statically or dynamically analyzing documents. However,malware developers may apply evasions that conceal the malicious-ness of a document. Given the variety of existing evasions, systemat-ically assessing the impact of evasions onmalware scanners remainsan open challenge. This paper presents a novel methodology fortesting the capability of malware scanners to cope with evasions.We apply the methodology to malicious Portable Document Format(PDF) documents and present an in-depth study of how current PDFevasions affect 41 state-of-the-art malware scanners. The study isbased on a framework for creating malicious PDF documents thatuse one or more evasions. Based on such documents, we measurehow effective different evasions are at concealing the maliciousnessof a document. We find that many static and dynamic scanners canbe easily fooled by relatively simple evasions and that the effec-tiveness of different evasions varies drastically. Our work not onlyis a call to arms for improving current malware scanners, but byproviding a large-scale corpus of malicious PDF documents withevasions, we directly support the development of improved toolsto detect document-based malware. Moreover, our methodologypaves the way for a quantitative evaluation of evasions in otherkinds of malware.1 INTRODUCTIONMalware scanners, or shortly scanners, are software tools thatdetect malicious files, or in brief, malware. Two common types ofscanners are static and dynamic scanners. Static scanners reasonabout a file by examining its content without actually running it.In contrast, dynamic scanners examine the behavior of a file atrun-time, either by executing it (e.g. Windows executable), or byopening it in the appropriate application (e.g. Adobe Reader forPDF files) or an emulator of such an application.Perhaps as old as the emergence of scanners [60] are evasions,which are used by attackers to circumvent scanners. Also knownas “logic bombs” in earlier work [28], evasions try to fool scannersthrough a variety of static techniques, such as code obfuscation, anddynamic techniques, such as checking the run-time environmentto behave benignly when the environment appears to be a scanner.The ultimate goal is the same across all evasions: bypass the scanner,while preserving the infection capabilities of the file to compromisethe victim’s security.As scanners are constantly improving their abilities to detectmalware, evasion techniques are evolving aswell. To bypassmoderndefenses that deploy both static and dynamic analysis, attackersmay combine evasions, which can lead to side-effects that have tobe assessed. Vendors of malware scanners must keep fighting newevasion techniques and their combinations, just like new attacks.It is therefore crucial for vendors to understand which evasions toaddress first and how evasions and their combinations impact theirscanners.",
        "arxiv": "1901.05674",
        "doi": null,
        "file_name": "1901.05674.pdf",
        "file_path": "./sources/1901.05674/1901.05674.pdf",
        "title": "Easy to Fool? Testing the Anti-evasion Capabilities of PDF Malware Scanners",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/sola-da/Chameleon"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/rapid7/metasploit-framework"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/gdelugre/origami"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/sslab-gatech/platpal"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/cuckoosandbox/community"
                }
            ]
        }
    },
    "1902.0311": {
        "abstract": "ABSTRACTInterest surrounding cryptocurrencies, digital or virtual currenciesthat are used as a medium for financial transactions, has growntremendously in recent years. The anonymity surrounding thesecurrencies makes investors particularly susceptible to fraud—suchas “pump and dump” scams—where the goal is to artificially inflatethe perceived worth of a currency, luring victims into investingbefore the scammers can sell their holdings. Because of the speedand relative anonymity offered by social platforms such as Twitterand Telegram, social media has become a preferred platform forscammers who wish to spread false hype about the cryptocurrencythey are trying to pump. In this work we propose and evaluatea computational approach that can automatically identify pumpand dump scams as they unfold by combining information acrosssocial media platforms. We also develop a multi-modal approachfor predicting whether a particular pump attempt will succeed ornot. Finally, we analyze the prevalence of bots in cryptocurrencyrelated tweets, and observe a significant significant presence of botsduring the pump attempts.KEYWORDScryptocurrency, pump and dump, social media data mining, anom-aly detection1 INTRODUCTIONThe inception of blockchain technology [14] gave birth to the pop-ular cryptocurrency Bitcoin (symbol BTC). Since then, thousandsof cryptocurrencies have emerged, and their hype has caused mas-sive price swings on the trading markets. In December 2017, BTCquadrupled in market value in just over a month, then within a fewdays started a gradual decline until it reached half of its peak value.These price changes allowed some investors to realize huge profits,contributing to the allure of cryptocurrencies. Even though mostinvestments are made in relatively established cryptocurrencies,including Bitcoin (BTC) and Ethereum (ETH), there are thousands ofother smaller cryptocurrencies. These currencies are prime targetsfor manipulation by scammers, as evidenced by the proliferation ofpump and dump schemes.Pump and dump schemes are those in which a security priceinflates due to deliberately deceptive activities. Those fraudulentschemes originated in the early days of the stock market and arenow growing rapidly in the cryptocurrency market. The fact that07:0009:0011:0013:0015:00",
        "arxiv": "1902.0311",
        "doi": "10.31219/osf.io/dqz89",
        "file_name": "1902.0311.pdf",
        "file_path": "./sources/1902.0311/1902.0311.pdf",
        "title": "Identifying and Analyzing Cryptocurrency Manipulations in Social Media",
        "urls": {
            "git": [
                {
                    "#_appearances": 1,
                    "url": "https://github.com/Mehrnoom/Cryptocurrency-Pump-Dump"
                }
            ]
        }
    },
    "1902.03984": {
        "abstract": "ABSTRACTGenerative Adversarial Networks (GANs) are one of the most popular tools forlearning complex high dimensional distributions. However, generalization prop-erties of GANs have not been well understood. In this paper, we analyze thegeneralization of GANs in practical settings. We show that discriminators trainedon discrete datasets with the original GAN loss have poor generalization capabil-ity and do not approximate the theoretically optimal discriminator. We proposea zero-centered gradient penalty for improving the generalization of the discrimi-nator by pushing it toward the optimal discriminator. The penalty guarantees thegeneralization and convergence of GANs. Experiments on synthetic and largescale datasets verify our theoretical analysis.1 INTRODUCTIONGANs (Goodfellow et al., 2014) are one of the most popular tools for modeling high dimensionaldata. The original GAN is, however, highly unstable and often suffers from mode collapse. Much ofrecent researches has focused on improving the stability of GANs (Radford et al., 2015; Arjovskyet al., 2017; Heusel et al., 2017; Miyato et al., 2018; Karras et al., 2018). On the theoretical aspect,Nagarajan & Kolter (2017) proved that gradient based training of the original GAN is locally stable.Heusel et al. (2017) further proved that GANs trained with Two Timescale Update Rule (TTUR)converge to local equilibria. However, the generalization of GANs at local equilibria is not discussedin depth in these papers.Arora et al. (2017) showed that the generator can win by remembering a polynomial number oftraining examples. The result implies that a low capacity discriminator cannot detect the lack of di-versity. Therefore, it cannot teach the generator to approximate the target distribution. In section 4,we discuss the generalization capability of high capacity discriminators. We show that high capacitydiscriminators trained with the original GAN loss tends to overfit to the mislabeled samples in train-ing dataset, guiding the generator toward collapsed equilibria (i.e. equilibria where the generatorhas mode collapse).Arora et al. (2018) proposed to measure the generalization capability of GAN by estimating thenumber of modes in the model distribution using the birthday paradox. Experiments on severaldatasets showed that the number of modes in the model distribution is several times greater thanthe number of training examples. The author concluded that although GANs might not be able tolearn distributions, they do exhibit some level of generalization. Our analysis shows that poor gen-eralization comes from the mismatch between discriminators trained on discrete finite datasets andthe theoretically optimal discriminator. We propose a zero-centered gradient penalty for improvingthe generalization capability of (high capacity) discriminators. Our zero-centered gradient penaltypushes the discriminator toward the optimal one, making GAN to converge to equilibrium with goodgeneralization capability.Our contributions are as follow:1arXiv:1902.03984v1  [",
        "arxiv": "1902.03984",
        "doi": null,
        "file_name": "1902.03984.pdf",
        "file_path": "./sources/1902.03984/1902.03984.pdf",
        "title": "Improving Generalization and Stability of Generative Adversarial Networks",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/htt210/GeneralizationAndStabilityInGANs"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/LMescheder/GAN_stability"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/LMescheder/GAN"
                }
            ]
        }
    },
    "1902.07153": {
        "abstract": "AbstractGraph Convolutional Networks (GCNs) and theirvariants have experienced significant attention andhave become the de facto methods for learninggraph representations. GCNs derive inspirationprimarily from recent deep learning approaches,and as a result, may inherit unnecessary complex-ity and redundant computation. In this paper,we reduce this excess complexity through suc-cessively removing nonlinearities and collapsingweight matrices between consecutive layers. Wetheoretically analyze the resulting linear modeland show that it corresponds to a fixed low-passfilter followed by a linear classifier. Notably, ourexperimental evaluation demonstrates that thesesimplifications do not negatively impact accuracyin many downstream applications. Moreover, theresulting model scales to larger datasets, is natu-rally interpretable, and yields up to two orders ofmagnitude speedup over FastGCN.1. IntroductionGraph Convolutional Networks (GCNs) (Kipf & Welling,2017) are an efficient variant of Convolutional Neural Net-works (CNNs) on graphs. GCNs stack layers of learnedfirst-order spectral filters followed by a nonlinear activationfunction to learn graph representations. Recently, GCNs andsubsequent variants have achieved state-of-the-art resultsin various application areas, including but not limited tocitation networks (Kipf & Welling, 2017), social networks(Chen et al., 2018), applied chemistry (Liao et al., 2019),natural language processing (Yao et al., 2019; Han et al.,2012; Zhang et al., 2018c), and computer vision (Wanget al., 2018; Kampffmeyer et al., 2018).Historically, the development of machine learning algo-*Equal contribution 1Cornell University 2Federal Insti-tute of Ceara (Brazil). Correspondence to: Felix Wu<fw245@cornell.edu>, Tianyi Zhang <tz58@cornell.edu>.Proceedings of the 36 th International Conference on MachineLearning, Long Beach, California, PMLR 97, 2019. Copyright2019 by the author(s).rithms has followed a clear trend from initial simplicity toneed-driven complexity. For instance, limitations of thelinear Perceptron (Rosenblatt, 1958) motivated the develop-ment of the more complex but also more expressive neuralnetwork (or multi-layer Perceptrons, MLPs) (Rosenblatt,1961). Similarly, simple pre-defined linear image filters (So-bel & Feldman, 1968; Harris & Stephens, 1988) eventuallygave rise to nonlinear CNNs with learned convolutionalkernels (Waibel et al., 1989; LeCun et al., 1989). As ad-ditional algorithmic complexity tends to complicate theo-",
        "arxiv": "1902.07153",
        "doi": null,
        "file_name": "1902.07153.pdf",
        "file_path": "./sources/1902.07153/1902.07153.pdf",
        "title": "Simplifying Graph Convolutional Networks",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/Tiiiger/SGC"
                }
            ]
        }
    },
    "1903.01899": {
        "abstract": "AbstractAnti-patterns are poor solutions to recurring design problems. Several empirical studies have highlighted their negative impact onprogram comprehension, maintainability, as well as fault-proneness. A variety of detection approaches have been proposed to identifytheir occurrences in source code. However, these approaches can identify only a subset of the occurrences and report large numbers offalse positives and misses. Furthermore, a low agreement is generally observed among different approaches. Recent studies have shownthe potential of machine-learning models to improve this situation. However, such algorithms require large sets of manually-producedtraining-data, which often limits their application in practice.In this paper, we present SMAD (SMart Aggregation of Anti-patterns Detectors), a machine-learning based ensemble method toaggregate various anti-patterns detection approaches on the basis of their internal detection rules. Thus, our method uses several detectiontools to produce an improved prediction from a reasonable number of training examples. We implemented SMAD for the detection oftwo well known anti-patterns: God Class and Feature Envy. With the results of our experiments conducted on eight java projects, weshow that: (1) our method clearly improves the so aggregated tools; (2) SMAD significantly outperforms other ensemble methods.Keywords: Software Quality, Anti-patterns, Machine Learning, Ensemble Methods1. IntroductionSource code refactoring, which consists in improving the in-ternal structure of the code while keeping its external behaviourunchanged, represents a substantial portion of software mainte-nance activities. To help practitioners in identifying where – andwhat kind of – refactoring should be applied in software systems,the concept of design smells has been introduced and defined byFowler [13] as symptoms of poor solutions to recurring designproblems. These symptoms, also called anti-patterns, are typi-cally introduced in object-oriented systems when developers im-plement sub-optimal design solutions due to lack of knowledgeand–or time constraints. For example, the God Class anti-patternrefers to the situation in which a class grows rapidly by the addi-tion of new functionalities, when developers break the principle ofsingle responsibility. Prior empirical studies highlighted the nega-tive impact of anti-patterns on a variety of quality characteristics,such as program comprehension [1], maintainability [45], and cor-rectness (increase of fault-proneness) [19]. Thus, it is importantto identify their occurrences in systems and apply refactoring op-erations to remove them.Several approaches have been proposed to detect the occur-Email addresses: antoine.barbez@polymtl.ca (Antoine Barbez),foutse.khomh@polymtl.ca (Foutse Khomh),yann-gael.gueheneuc@concordia.ca (Yann-Gaël Guéhéneuc)rences of anti-patterns in source code. Most of these approachesattempt to identify bad motifs in models of source code usingmanually-defined heuristics that rely on some metrics (e.g., cy-clomatic complexity). For example, Moha et al. [33] proposed adomain-specific language to describe and generate detection algo-rithms for anti-patterns using structural and lexical metrics whilePalomba et al. [36, 34] proposed a rule-based approach to detectanti-patterns from change history information.Even though these approaches have shown acceptable perfor-mances, none of them can claim high accuracy on any systemsand for any anti-patterns. Besides, each approach relies on its owndefinitions of anti-patterns and only focuses on specific aspects ofsystems. Thus, we observe a complementarity among the differ-",
        "arxiv": "1903.01899",
        "doi": null,
        "file_name": "1903.01899.pdf",
        "file_path": "./sources/1903.01899/1903.01899.pdf",
        "title": "A Machine-learning Based Ensemble Method For Anti-patterns Detection",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/ptidejteam/v5.2"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/antoineBarbez/SMAD"
                }
            ]
        }
    },
    "1903.10583": {
        "abstract": "AbstractThe Burrows-Wheeler transform (BWT) is a well studied text transformationwidely used in data compression and text indexing. The BWT of two stringscan also provide similarity measures between them, based on the observationthat the more their symbols are intermixed in the transformation, the more thestrings are similar. In this article we present two new algorithms to computesimilarity measures based on the BWT for string collections. In particular,we present practical and theoretical improvements to the computation of theBurrows-Wheeler similarity distribution for all pairs of strings in a collection.Our algorithms take advantage of the BWT computed for the concatenation ofall strings, and use compressed data structures that allow reducing the runningtime with a small memory footprint, as shown by a set of experiments with realand artificial datasets.Keywords: Burrows-Wheeler transform, string similarity, string collections,compressed data structures, parallel algorithms1. IntroductionComparing strings is one of the most fundamental tasks in Bioinformat-ics and Information Retrieval [27, 14, 2]. While there exist many measures ofsimilarity between strings, alignment-based measures are widely used in Bioin-formatics because they are very good in capturing the conservation of blocks ofDNA and protein sequences. They are, however, computationally intensive toevaluate. With current databases of biological sequences at the order of hun-dreds of gigabytes, alternatives have been proposed both as faster, heuristicalgorithms and as easier to compute similarity measures [32, 33, 3, 25, 11].IA preliminary version of this work appeared in SPIRE 2018 [13].∗Corresponding authorEmail addresses: louza@usp.br (Felipe A. Louza), gpt@ic.unicamp.br (Guilherme P.Telles), sgog@ebay.com (Simon Gog), zhao@usp.br (Liang Zhao)Preprint submitted to Elsevier March 27, 2019arXiv:1903.10583v1  [cs.DS]  25 Mar 201",
        "arxiv": "1903.10583",
        "doi": "10.1016/j.tcs.2019.03.012",
        "file_name": "1903.10583.pdf",
        "file_path": "./sources/1903.10583/1903.10583.pdf",
        "title": "Algorithms to compute the Burrows-Wheeler Similarity Distribution",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/simongog/sdsl-lite"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/felipelouza/gsa-is"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/felipelouza/bwsd"
                }
            ]
        }
    },
    "1903.11056": {
        "abstract": "Abstract. We will discuss the RowHammer problem in DRAM, whichis a prime (and likely the first) example of how a circuit-level failuremechanism in Dynamic Random Access Memory (DRAM) can causea practical and widespread system security vulnerability. RowHammeris the phenomenon that repeatedly accessing a row in a modern DRAMchip predictably causes errors in physically-adjacent rows. It is caused bya hardware failure mechanism called read disturb errors. Building on ourinitial fundamental work that appeared at ISCA 2014, Google ProjectZero demonstrated that this hardware phenomenon can be exploited byuser-level programs to gain kernel privileges. Many other recent worksdemonstrated other attacks exploiting RowHammer, including remotetakeover of a server vulnerable to RowHammer. We will analyze theroot causes of the problem and examine solution directions. We will alsodiscuss what other problems may be lurking in DRAM and other typesof memories, e.g., NAND flash and Phase Change Memory, which canpotentially threaten the foundations of reliable and secure systems, asthe memory technologies scale to higher densities.1 SummaryAs memory scales down to smaller technology nodes, new failure mechanismsemerge that threaten its correct operation [79, 80]. If such failures are not antici-pated and corrected, they can not only degrade system reliability and availabilitybut also, even more importantly, open up new security vulnerabilities: a mali-cious attacker can exploit the exposed failure mechanism to take over an entiresystem. As such, new failure mechanisms in memory can become practical andsignificant threats to system security.In this keynote talk, based on our ISCA 2014 paper [55], we introduce theRowHammer problem in DRAM, which is a prime (and likely the first) exampleof a real circuit-level failure mechanism that causes a practical and widespreadsystem security vulnerability. RowHammer, as it is now popularly referred to,is the phenomenon that repeatedly accessing a row in a modern DRAM chipcauses bit flips in physically-adjacent rows at consistently predictable bit loca-tions. It is caused by a hardware failure mechanism called DRAM disturbanceerrors, which is a manifestation of circuit-level cell-to-cell interference in a scaledmemory technology. Specifically, when a DRAM row is opened (i.e., activated)and closed (i.e., precharged) repeatedly (i.e., hammered), enough times withinhttp://arxiv.org/abs/1903.11056v12 Onur Mutlua DRAM refresh interval, one or more bits in physically-adjacent DRAM rowscan be flipped to the wrong value. Using an FPGA-based DRAM testing infras-tructure [70, 42], we tested 129 DRAM modules manufactured by three majormanufacturers in seven recent years (2008–2014) and found that 110 of themexhibited RowHammer errors, the earliest of which dates back to 2010. OurISCA 2014 paper [55] provides a detailed and rigorous analysis of various char-acteristics of RowHammer, including its data pattern dependence, repeatabilityof errors, relationship with leaky cells, and various circuit-level causes of thephenomenon.We demonstrate that a very simple user-level program [55, 3] can reliablyand consistently induce RowHammer errors in commodity AMD and Intel sys-tems using vulnerable DRAM modules. We released the source code of thisprogram [3], which Google Project Zero later enhanced [4]. Using our user-level",
        "arxiv": "1903.11056",
        "doi": "10.1007/978-3-030-16350-1_1",
        "file_name": "1903.11056.pdf",
        "file_path": "./sources/1903.11056/1903.11056.pdf",
        "title": "RowHammer and Beyond",
        "urls": {
            "git": [
                {
                    "#_appearances": 1,
                    "url": "https://github.com/tadfisher/x210-bios"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/google/rowhammer-test"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/CMU-SAFARI/rowhammer"
                }
            ]
        }
    },
    "1903.11991": {
        "abstract": "AbstractA major challenge in current optimization research for deep learning is automat-ically finding optimal step sizes for each update step. The optimal step size isclosely related to the shape of the loss in the update step direction. However, thisshape has not yet been examined in detail. This work shows empirically that themini-batch loss along lines in negative gradient direction is locally mostly convexand well suited for one-dimensional parabolic approximations. We introduce a sim-ple and robust line search approach by exploiting this parabolic observation, whichperforms loss-shape-dependent update steps. Our approach combines well-knownmethods such as parabolic approximation, line search, and conjugate gradient toperform efficiently. It surpasses other step size estimating methods and competeswith standard optimization methods on a large variety of experiments without theneed for hand-designed step size schedules. Thus, it is of interest for objectiveswhere step-size schedules are unknown or do not perform well. Our extensive eval-uation includes multiple comprehensive hyperparameter grid searches on severaldatasets and architectures. Finally, we provide a general investigation of exact linesearches in the context of batch losses and exact losses, including their relation toour line search approach.1 IntroductionAutomatic determination of optimal step sizes for each update step of stochastic gradient descent is amajor challenge in current optimization research for deep learning [3,5,12,29,38,45,48,52,60]. Onedefault approach to tackle this challenge is to apply line search methods. Several of these have beenintroduced for Deep Learning [12, 29, 38, 45, 60]. However, these approaches have not analyzed theshape of the loss functions in update step direction in detail, which is important since the optimal stepsize stands in strong relation to this shape. To shed light on this, our work empirically analyses theshape of the loss function along update step direction for deep learning scenarios often considered inoptimization. We further elaborate on the properties found to define a simple and empirically justifiedoptimizer.Our contributions are as follows: 1: Our empirical analysis suggests that the mini-batch loss inthe negative gradient direction mostly shows locally convex shapes. Furthermore, we show thatparabolic approximations are well suited to estimate the minima in these directions (Section 3). 2:Exploiting this parabolic observation, we build a simple line search optimizer that constructs itsloss function-dependent learning rate schedule. The performance of our optimization method isextensively analyzed, including a comprehensive comparison to other optimization methods (Sections4,5). 3: We provide a convergence analysis that backs our empirical results under strong assumptions(Section 4.4). 4: We provide a general investigation of exact line searches on batch losses and theirrelation to line searches on the exact loss as well as their relation to our line search approach (Section6) and, finally, analyze the relation of our approach to interpolation (Section 7).34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.arXiv:1903.11991v5  [",
        "arxiv": "1903.11991",
        "doi": null,
        "file_name": "1903.11991.pdf",
        "file_path": "./sources/1903.11991/1903.11991.pdf",
        "title": "Parabolic Approximation Line Search for DNNs",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/cogsys-tuebingen/PAL"
                }
            ]
        }
    },
    "1904.04174": {
        "abstract": "ABSTRACTOver the past few years machine learning has seen a renewedexplosion of interest, following a number of studies showing theeffectiveness of neural networks in a range of tasks which hadpreviously been considered incredibly hard. Neural networks’ ef-fectiveness in the fields of image recognition and natural languageprocessing stems primarily from the vast amounts of data availableto companies and researchers, coupled with the huge amounts ofcompute power available in modern accelerators such as GPUs,FPGAs and ASICs. There are a number of approaches available todevelopers for utilizing GPGPU technologies such as SYCL, OpenCLand CUDA, however many applications require the same low levelmathematical routines. Libraries dedicated to accelerating thesecommon routines allow developers to easily make full use of theavailable hardware without requiring low level knowledge of thehardware themselves, however such libraries are often provided byhardware manufacturers for specific hardware such as cuDNN [9]for Nvidia hardware or MIOpen [5] for AMD hardware.SYCL-DNN is a new open-source library dedicated to provid-ing accelerated routines for neural network operations which arehardware and vendor agnostic. Built on top of the SYCL open stan-dard and written entirely in standard C++, SYCL-DNN allows auser to easily accelerate neural network code for a wide range ofhardware using a modern C++ interface. The library is tested onAMD’s OpenCL for GPU, Intel’s OpenCL for CPU and GPU, ARM’sOpenCL for Mali GPUs as well as ComputeAorta’s OpenCL forR-Car CV engine and host CPU. In this talk we will present per-formance figures for SYCL-DNN on this range of hardware, anddiscuss how high performance was achieved on such a varied setof accelerators with such different hardware features.CCS CONCEPTS• Computing methodologies → Neural networks; Massivelyparallel algorithms; Parallel programming languages; Computervision problems.KEYWORDSSYCL, OpenCL, neural networks, GPGPU, machine learning∗Authors listed alphabeticallyPermission to make digital or hard copies of part or all of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).IWOCL’19, May 13–15, 2019, Boston, MA, USA© 2019 Copyright held by the owner/author(s).ACM ISBN 978-1-4503-6230-6/19/05.https://doi.org/10.1145/3318170.3318183ACM Reference Format:Rod Burns, John Lawson, Duncan McBain and Daniel Soutar. 2019. Acceler-ated Neural Networks on OpenCL Devices Using SYCL-DNN. In Interna-tional Workshop on OpenCL (IWOCL’19), May 13–15, 2019, Boston, MA, USA.",
        "arxiv": "1904.04174",
        "doi": "10.1145/3318170.3318183",
        "file_name": "1904.04174.pdf",
        "file_path": "./sources/1904.04174/1904.04174.pdf",
        "title": "Accelerated Neural Networks on OpenCL Devices Using SYCL-DNN",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/triSYCL/triSYCL"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/intel/mkl-dnn"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/illuhad/hipSYCL"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/ROCmSoftwarePlatform/MIOpen"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/CodeplaySoftware/SYCL-DNN"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/ARM-software/ComputeLibrary"
                }
            ]
        }
    },
    "1904.08129": {
        "abstract": "Abstract—In this paper, we propose Rogue-Gym, a simple andclassic style roguelike game built for evaluating generalization inreinforcement learning (RL). Combined with the recent progressof deep neural networks, RL has successfully trained human-level agents without human knowledge in many games such asthose for Atari 2600. However, it has been pointed out that agentstrained with RL methods often overfit the training environment,and they work poorly in slightly different environments. To inves-tigate this problem, some research environments with proceduralcontent generation have been proposed. Following these studies,we propose the use of roguelikes as a benchmark for evaluatingthe generalization ability of RL agents. In our Rogue-Gym,agents need to explore dungeons that are structured differentlyeach time they start a new game. Thanks to the very diversestructures of the dungeons, we believe that the generalizationbenchmark of Rogue-Gym is sufficiently fair. In our experiments,we evaluate a standard reinforcement learning method, PPO,with and without enhancements for generalization. The resultsshow that some enhancements believed to be effective fail tomitigate the overfitting in Rogue-Gym, although others slightlyimprove the generalization ability.Index Terms—roguelike games, reinforcement learning, gener-alization, domain adaptation, neural networksI. INTRODUCTIONReinforcement learning (RL) is a key method for trainingAI agents without human knowledge. Recent advances in deepreinforcement learning have created human-level agents inmany games, such as those for Atari 2600 [1] and the gameDOOM [2], using only pixels as inputs. This method could beapplied to many domains, from robotics to the game industry.However, it is still difficult to generalize learned policiesbetween tasks even for current state of the art RL algorithms.Recent studies (e.g., by Zhang et al. [3] and by Cobbe etal. [4]) have shown that agents trained by RL methods oftenoverfit the training environment and perform poorly in a testenvironment, when the test environment is not exactly the sameas the training environment. This is an important problembecause test environments are often differ somewhat fromtraining environments in many applications of reinforcementlearning. For example, in real world applications includingself-driving cars [5], agents are often trained via simulatorsA part of this work was supported by JSPS KAKENHI Grant Number18K19832 and by JST, PRESTO.or designated areas but need to perform safely in real worldsituations that are similar to but different from their trainingenvironments. For agents to act appropriately in unknownsituations, they need to properly generalize their policies thatthey learned from the training environment. Generalization isalso important in transfer learning, where the goal is to transfera policy learned in a training environment to another similar",
        "arxiv": "1904.08129",
        "doi": "10.1109/cig.2019.8848075",
        "file_name": "1904.08129.pdf",
        "file_path": "./sources/1904.08129/1904.08129.pdf",
        "title": "Rogue-Gym: A New Challenge for Generalization in Reinforcement Learning",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/kngwyu/rogue-gym-agents-cog19"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/kngwyu/rogue-gym"
                }
            ]
        }
    },
    "1905.06982": {
        "abstract": "Abstract—Deep Gaussian processes (DGP) have appealingBayesian properties, can handle variable-sized data, and learndeep features. Their limitation is that they do not scale wellwith the size of the data. Existing approaches address this usinga deep random feature (DRF) expansion model, which makesinference tractable by approximating DGPs. However, DRF isnot suitable for variable-sized input data such as trees, graphs,and sequences. We introduce the GP-DRF, a novel Bayesianmodel with an input layer of GPs, followed by DRF layers.The key advantage is that the combination of GP and DRFleads to a tractable model that can both handle a variable-sizedinput as well as learn deep long-range dependency structuresof the data. We provide a novel efficient method to simultane-ously infer the posterior of GP’s latent vectors and infer theposterior of DRF’s internal weights and random frequencies.Our experiments show that GP-DRF outperforms the standardGP model and DRF model across many datasets. Furthermore,they demonstrate that GP-DRF enables improved uncertaintyquantification compared to GP and DRF alone, with respect toa Bhattacharyya distance assessment. Source code is available athttps://github.com/IssamLaradji/GP_DRF.I. INTRODUCTIONDeep neural network (DNN) models have achieved ground-breaking performance in many real-life domains such ascomputer vision and natural language processing [1]. Thisis mainly due to their ability to model long-range dependencystructures that may reside in the data. However, they do notprovide uncertainty quantification, which can be useful indecision making and high risk applications such as medicalinformatics or autonomous driving [2]. A recent methodaddresses this limitation using random feature expansion [3],but is unable to efficiently handle variable-sized data, suchas trees [4], protein sequences [5], audio sequences [6], orgraphs [7], in an end-to-end fashion. For instance, to predictthe chemical properties of variable-sized molecular data [8],a separate feature extraction stage was required in order toconstruct fixed-sized fingerprint vectors. Such sophisticatedfeature extraction schemes often require human expertise. Inthis work, we propose Gaussian Process Deep Random Feature(GP-DRF), a scalable Bayesian method, that addresses theaforementioned limitations.Bayesian models have received significant attention over thelast decade. Gaussian processes (GP) are a family of flexibleVariable-sized input Output...GP Layer RF Layer 1 RF Layer 2 RF Layer kx F yFig. 1: GP-DRF. A layer of Gaussian processes (GP) first mapsa possibly variable-sized input x to a fixed-sized latent feature",
        "arxiv": "1905.06982",
        "doi": null,
        "file_name": "1905.06982.pdf",
        "file_path": "./sources/1905.06982/1905.06982.pdf",
        "title": "Efficient Deep Gaussian Process Models for Variable-Sized Inputs",
        "urls": {
            "git": [
                {
                    "#_appearances": 1,
                    "url": "https://github.com/IssamLaradji/GP_DRF"
                }
            ]
        }
    },
    "1905.07808": {
        "abstract": "Abstract— The diversity of SLAM benchmarks affords ex-tensive testing of SLAM algorithms to understand their perfor-mance, individually or in relative terms. The ad-hoc creation ofthese benchmarks does not necessarily illuminate the particularweak points of a SLAM algorithm when performance isevaluated. In this paper, we propose to use a decision tree toidentify challenging benchmark properties for state-of-the-artSLAM algorithms and important components within the SLAMpipeline regarding their ability to handle these challenges.Establishing what factors of a particular sequence lead totrack failure or degradation relative to these characteristicsis important if we are to arrive at a strong understanding forthe core computational needs of a robust SLAM algorithm.Likewise, we argue that it is important to profile the com-putational performance of the individual SLAM componentsfor use when benchmarking. In particular, we advocate theuse of time-dilation during ROS bag playback, or what werefer to as slo-mo playback. Using slo-mo to benchmark SLAMinstantiations can provide clues to how SLAM implementationsshould be improved at the computational component level.Three prevalent VO/SLAM algorithms and two low-latencyalgorithms of our own are tested on selected typical sequences,which are generated from benchmark characterization, tofurther demonstrate the benefits achieved from computationallyefficient components.I. INTRODUCTIONSimultaneous localization and mapping (SLAM) is a corecomputational component supporting several application sce-narios in augmented/virtual reality and autonomous robotics.As such, benchmarks for assessing the performance ofSLAM reflect the diverse deployment profiles of potentialapplication scenarios. They reflect a wide variety of sensors[1]–[3], platforms [4], [5], motion patterns [6]–[8], sceneproperties [9], [10], and other meaningful characteristics.Given that a large portion of benchmarks are empirical,specialized (to use case) scenarios recorded for evaluationthrough replay, there can be a lack of control over impor-tant configuration variables related to performance. Further-more, the diversity of software interfaces for the differentdatasets and algorithms complicates comprehensive evalua-tion. SLAMBench [11] addresses this last issue through theuse of a common API for evaluating algorithms with anemphasis on analysis of different computational platformsand run-time parameters. SLAMBench performance metricsinclude energy consumption, accuracy, and computational1Wenkai Ye, Yipu Zhao, and Patricio A. Vela are with School of Elec-trical and Computer Engineering, Georgia Institute of Technology, Atlanta,Georgia, USA. {wye35,yzhao347,pvela}@gatech.edu.This work was supported in part by the China Scholarship Council (CSCStudent No: 201606260089) and the National Science Foundation (Award",
        "arxiv": "1905.07808",
        "doi": null,
        "file_name": "1905.07808.pdf",
        "file_path": "./sources/1905.07808/1905.07808.pdf",
        "title": "Characterizing SLAM Benchmarks and Methods for the Robust Perception Age",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/ivalab/Benchmarking_SLAM"
                }
            ]
        }
    },
    "1906.02152": {
        "abstract": "could be abstracted to other cryptocurrencies or outside of a cryptocurrency setting.• Ether: high risk asset whose USD market prices 𝑝𝐸𝑡 are exogenous• DStablecoin: a ‘stable’ asset collateralized in Ether whose USD price 𝑝𝐷𝑡 is endogenousNotably, a large DStablecoin system may have endogenous amplification effects on Ether price,similarly to how CDOs affected underlying assets in the 2008 financial crisis. We discuss this furtherin Section 7 but leave formal modeling of this to future work.There are several barriers for trading between crypto and fiat, which motivate our choice of assets.Most crypto-fiat pairs are through Bitcoin or Ether, which act as a gateway to other cryptoassets.Trading to fiat can involve moving assets between a number of exchanges and can take considerabletime to confirm on the blockchain. Trading to a stablecoin is comparatively simple. Trading to fiatcan also trigger more clear tax incidence. Additionally, some countries have imposed strict capitalcontrols on trading between fiat and crypto.Model outline. At 𝑡 = 0, the agents have endowments and prior beliefs. In each period 𝑡 :(1) New Ether price is revealed(2) Ether expectations are updated(3) Stablecoin holder decides portfolio weights(4) Speculator, seeing demand, decides leverage(5) DStablecoin market is cleared2.1 Stablecoin holderThe stablecoin holder starts with an initial endowment and decides portfolio weights to attain thedesired stability. The following table defines the agent’s state variables.Variable Definition𝑛𝑡 Ether held at time 𝑡�̄�𝑡 DStablecoin held at time 𝑡wt Portfolio weights chosen at time 𝑡The stablecoin holder weights its portfolio by wt. We denote the components as𝑤𝐸𝑡 and𝑤𝐷𝑡 forEther and DStablecoin weights respectively. The stablecoin holder’s portfolio value at time 𝑡 isA𝑡 = 𝑛𝑡𝑝𝐸𝑡 + �̄�𝑡𝑝𝐷𝑡 = 𝑛𝑡−1𝑝𝐸𝑡 + �̄�𝑡−1𝑝𝐷𝑡 .Given weights, 𝑛𝑡 and �̄�𝑡 will be determined based on the stablecoin clearing price 𝑝𝐷𝑡 .Ariah Klages-Mundt and Andreea Minca 7The basic results in Section 3 hold generally for any wt ≥ 0 (i.e., there is no shorting). In thiscase, wt could be chosen, e.g., from Sharpe ratio optimization, mean-variance optimization, orKelly criterion (among others). In Sections 4 & 5, in order to focus on the effects of speculatordecisions, we simplify the stablecoin holder as exogenous with unit price-elastic demand. In thiscase, DStablecoin demand is constant in dollar terms.2.2 SpeculatorThe speculator starts with an endowment of Ether and initial beliefs about Ether’s returns andvariance and decides leverage to maximize expected returns subject to protocol and self-imposedconstraints. The following tables define variables and parameters for the speculator.Variable Definition",
        "arxiv": "1906.02152",
        "doi": "10.21428/58320208.e46b7b81",
        "file_name": "1906.02152.pdf",
        "file_path": "./sources/1906.02152/1906.02152.pdf",
        "title": "(In)Stability for the Blockchain: Deleveraging Spirals and Stablecoin Attacks",
        "urls": {
            "git": [
                {
                    "#_appearances": 3,
                    "url": "https://github.com/runtimeverification/verified-smart-contracts"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/makerdao/awesome-makerdao"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/aklamun/Stablecoin_Deleveraging"
                }
            ]
        }
    },
    "1906.0372": {
        "abstract": "AbstractRecent analysis identified distinct genomic subtypes of lower-grade glioma tumors which areassociated with shape features. In this study, we propose a fully automatic way to quantifytumor imaging characteristics using deep learning-based segmentation and test whether thesecharacteristics are predictive of tumor genomic subtypes.We used preoperative imaging and genomic data of 110 patients from 5 institutions withlower-grade gliomas from The Cancer Genome Atlas. Based on automatic deep learning seg-mentations, we extracted three features which quantify two-dimensional and three-dimensionalcharacteristics of the tumors. Genomic data for the analyzed cohort of patients consisted ofpreviously identified genomic clusters based on IDH mutation and 1p/19q co-deletion, DNAmethylation, gene expression, DNA copy number, and microRNA expression. To analyze therelationship between the imaging features and genomic clusters, we conducted the Fisher exacttest for 10 hypotheses for each pair of imaging feature and genomic subtype. To account formultiple hypothesis testing, we applied a Bonferroni correction. P-values lower than 0.005 wereconsidered statistically significant.We found the strongest association between RNASeq clusters and the bounding ellipsoidvolume ratio (p < 0.0002) and between RNASeq clusters and margin fluctuation (p < 0.005).In addition, we identified associations between bounding ellipsoid volume ratio and all testedmolecular subtypes (p < 0.02) as well as between angular standard deviation and RNASeqcluster (p < 0.02). In terms of automatic tumor segmentation that was used to generate thequantitative image characteristics, our deep learning algorithm achieved a mean Dice coefficientof 82% which is comparable to human performance.Keywords: deep learning; brain segmentation; radiogenomics; MRI; LGG1arXiv:1906.03720v1  [eess.IV]  9 Jun 20191 IntroductionLower-grade gliomas (LGG) are a group of WHO grade II and grade III brain tumors. As opposedto grade I which are often curable by surgical resection, grade II and III are infiltrative and tendto recur and evolve to higher-grade lesion. Predicting patient outcomes based on histopathologi-",
        "arxiv": "1906.0372",
        "doi": "10.1016/j.compbiomed.2019.05.002",
        "file_name": "1906.0372.pdf",
        "file_path": "./sources/1906.0372/1906.0372.pdf",
        "title": "Association of genomic subtypes of lower-grade gliomas with shape features automatically extracted by a deep learning algorithm",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/mateuszbuda/brain-segmentation"
                }
            ]
        }
    },
    "1906.11148": {
        "abstract": "AbstractWe derive generalization and excess risk bounds for neural nets using a family ofcomplexity measures based on a multilevel relative entropy. The bounds are obtainedby introducing the notion of generated hierarchical coverings of neural nets and by usingthe technique of chaining mutual information introduced in Asadi et al. NeurIPS’18.The resulting bounds are algorithm-dependent and exploit the multilevel structure ofneural nets. This, in turn, leads to an empirical risk minimization problem with amultilevel entropic regularization. The minimization problem is resolved by introducinga multi-scale generalization of the celebrated Gibbs posterior distribution, proving thatthe derived distribution achieves the unique minimum. This leads to a new trainingprocedure for neural nets with performance guarantees, which exploits the chain ruleof relative entropy rather than the chain rule of derivatives (as in backpropagation).To obtain an efficient implementation of the latter, we further develop a multilevelMetropolis algorithm simulating the multi-scale Gibbs distribution, with an experimentfor a two-layer neural net on the MNIST data set.1 IntroductionWe introduce a family of complexity measures for the hypotheses of neural nets, based ona multilevel relative entropy. These complexity measures take into account the multilevelstructure of neural nets, as opposed to the classical relative entropy (KL-divergence) termderived from PAC-Bayesian bounds [1] or mutual information bounds [2, 3]. We derive thesecomplexity measures by combining the technique of chaining mutual information (CMI) [4],an algorithm-dependent extension of the classical chaining technique paired with the mutualinformation bound [2], with the multilevel architecture of neural nets. It is observed in thispaper that if a neural net is regularized in a multilevel manner as defined in Section 4, thenone can readily construct hierarchical coverings with controlled diameters for its hypothesisset, and exploit this to obtain new multi-scale and algorithm-dependent generalizationbounds and, in turn, new regularizers and training algorithms. The effect of such multilevelregularizations on the representation ability of neural nets has also been recently studiedin [5, 6] for the special case where layers are nearly-identity functions as for ResNets [7].Here, we demonstrate the advantage of multilevel architectures by showing how one can1arXiv:1906.11148v1  [cs.LG]  26 Jun ",
        "arxiv": "1906.11148",
        "doi": null,
        "file_name": "1906.11148.pdf",
        "file_path": "./sources/1906.11148/1906.11148.pdf",
        "title": "Chaining Meets Chain Rule: Multilevel Entropic Regularization and Training of Neural Nets",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/ARAsadi/Multilevel-Metropolis"
                }
            ]
        }
    },
    "1907.11111": {
        "abstract": null,
        "arxiv": "1907.11111",
        "doi": "10.1109/itsc.2019.8917177",
        "file_name": "1907.11111.pdf",
        "file_path": "./sources/1907.11111/1907.11111.pdf",
        "title": "MultiDepth: Single-Image Depth Estimation via Multi-Task Regression and Classification",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/lukasliebel/MultiDepth"
                }
            ]
        }
    },
    "1907.13316": {
        "abstract": "AbstractRecently a non-empirical stochastic walker algorithm has been developed tosearch for the minimum-energy escape paths (MEP) from the minima of thepotential surface [J. Phys. Soc. Jpn. 87, 063801 (2018); Physica A, 528,121481 (2019)]. This method is based on the Master equation for the dis-tribution function of the atomic configuration which has a nature to seekthe MEP up along the valley of the potential surface. This paper introducesAtomREM (Atomistic Rare Event Manager), which is an MPI parallelizedsolver program package for executing this method, which yields minimumenergy reaction pathways in terms of the microscopic evolution of atomicpositions. It is open-source and released under the GNU General Public Li-cense (GPL). AtomREM interfaces with the LAMMPS Molecular DynamicsSimulator as a library of versatile potential functions for application to var-ious systems. Examples of the applications to molecular and solid systemsare presented.Keywords: Potential landscape, Reaction paths, Minimum-energy escapepaths, Langevin mechanics, Non-empirical scheme, Stochastic algorithm,Parallel MPI implementation.PROGRAM SUMMARYProgram Title: AtomREM (Atomistic Rare-Event Manager)Program Files: https://github.com/ryosuke-akashi/AtomREM∗Corresponding author.E-mail address: iurii@cms.phys.s.u-tokyo.ac.jpPreprint submitted to Computer Physics Communications July 5, 2021arXiv:1907.13316v1  [physics.comp-ph]  31 Jul 2019",
        "arxiv": "1907.13316",
        "doi": "10.1016/j.cpc.2020.107260",
        "file_name": "1907.13316.pdf",
        "file_path": "./sources/1907.13316/1907.13316.pdf",
        "title": "AtomREM: Non-empirical seeker of the minimum energy escape paths on many-dimensional potential landscapes without coarse graining",
        "urls": {
            "git": [
                {
                    "#_appearances": 4,
                    "url": "https://github.com/ryosuke-akashi/AtomREM"
                }
            ]
        }
    },
    "1909.04965": {
        "abstract": "ABSTRACTAims. In this paper, we aim to characterise the surface magnetic fields of a sample of eight T Tauri stars from high-resolution near-infrared spectroscopy. Some stars in our sample are known to be magnetic from previous spectroscopic or spectropolarimetric studies.Our goals are firstly to apply Zeeman broadening modelling to T Tauri stars with high-resolution data, secondly to expand the sampleof stars with measured surface magnetic field strengths, thirdly to investigate possible rotational or long-term magnetic variability bycomparing spectral time series of given targets, and fourthly to compare the magnetic field modulus 〈B〉 tracing small-scale magneticfields to those of large-scale magnetic fields derived by Stokes V Zeeman Doppler Imaging (ZDI) studies.Methods. We modelled the Zeeman broadening of magnetically sensitive spectral lines in the near-infrared K-band from high-resolution spectra by using magnetic spectrum synthesis based on realistic model atmospheres and by using different descriptionsof the surface magnetic field. We developped a Bayesian framework that selects the complexity of the magnetic field prescriptionbased on the information contained in the data.Results. We obtain individual magnetic field measurements for each star in our sample using four different models. We find that theBayesian Model 4 performs best in the range of magnetic fields measured on the sample (from 1.5 kG to 4.4 kG). We do not detecta strong rotational variation of 〈B〉 with a mean peak-to-peak variation of 0.3 kG. Our confidence intervals are of the same order ofmagnitude, which suggests that the Zeeman broadening is produced by a small-scale magnetic field homogeneously distributed overstellar surfaces. A comparison of our results with mean large-scale magnetic field measurements from Stokes V ZDI show differentfractions of mean field strength being recovered, from 25–42% for relatively simple poloidal axisymmetric field topologies to 2–11%for more complex fields.Key words. stars: pre-main sequence – stars: magnetic field – line: profiles1. IntroductionWith an ever improving understanding of stellar magnetism,we now realise that magnetic fields have a paramount impactthroughout the entire life of a star. Particularly on the pre main-sequence (PMS), where stars are relatively cool and magneticfields seem ubiquitous, magnetism has a significant impact onstars themselves, their formation, accretion properties, rotationrate, flares, and wind characteristics among others. It also in-fluences potential orbiting exoplanets, especially their chemicalevolution and habitability.The origin of stellar magnetic fields and their evolutionalong the PMS, from stars that evolve from a fully-convectiveregime to a partially radiative and then eventually to a fullyradiative regime, is not fully understood. While cool PMS TTauri stars exhibit ubiquitous magnetic fields, it seems that moststars, that evolve into PMS Herbig Ae/Be stars and eventuallyto A/B type main-sequence stars, lose their fields at some point(Alecian et al. 2013; Sikora et al. 2019). The 5–10% of stars re-maining magnetic on the main sequence display simple, fossilfields that are not maintained by an active dynamo. Studying theevolution of magnetic fields in the T Tauri star regime in moredetail can allow us to understand when and how fast this transi-tion occurs, and perhaps identify different populations with dis-tinct magnetic properties. This could improve our understandingof the mechanisms and timescales at play regarding the evolu-tion and survival of the magnetic fields in the later evolutionarystages (Alecian et al. 2019).There are two direct methods widely used to measure stel-lar magnetic fields that both rely on the Zeeman effect. Thefirst is the measurement of Zeeman broadening, or splitting ofspectral lines in intensity spectra, and the second is the anal-",
        "arxiv": "1909.04965",
        "doi": "10.1051/0004-6361/2019.35695",
        "file_name": "1909.04965.pdf",
        "file_path": "./sources/1909.04965/1909.04965.pdf",
        "title": "Characterising the surface magnetic fields of T Tauri stars with high-resolution near-infrared spectroscopy",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/pkgw/mcmc-reporting"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/astro-alexis/magnotron-tts"
                }
            ]
        }
    },
    "1910.02369": {
        "abstract": "AbstractThe K-index is an easily computable centrality index in complex networks, such as a scientific citations network. Aresearcher has a K-index equal to K if he/she is cited by K articles that have at least K citations. The K-index hasseveral advantages over Hirsh’s h-index and, in previous studies, has shown better correlation with Nobel prizes thanany other index given by the Web of Science, including the h-index. However, correlation is not causation. Here weperform an experiment using the K-index, producing a shortlist to predict future Physics Nobel candidates. Our listhas been built before the 2019 Nobel Prizes announcements and should be compared to the actual results of the currentand following years.Keywords: complex networks, node centrality, Hirsch index, Nobel prizes.∗ Corresponding author.Preprint submitted to Physica A April 30, 2022arXiv:1910.02369v1  [cs.DL]  6 Oct 2019Highlights• We propose that the K-index could be a good network centrality index for the physics community and relevantto predict the likelihood of scientific prizes;• We propose an experiment where a list of highly cited candidates is refined to predict Physics Nobel Prizes inthe near future;• We present a list with twelve candidates with highest K-index from an initial list of 138 physicists from Clari-vate Highly Cited Researchers 2019 (HCR).• We present and discuss the K versus h plane for the candidates.1. IntroductionStatistical physicists have made important contributions to the interdisciplinary area of complex networks [1, 2].In particular, physicists have intensively studied scientometric networks thanks to the availability of large and reliabledata banks [3, 4, 5, 6, 7, 8]. Indeed, an important advancement for the area came with the introduction of the h-index by physicist Jorge E. Hirsch [9]. A researcher has h-index h if he/she has published h papers each one with atleast h citations. Centrality indexes proposals for citation networks experienced a boom after the introduction of theh-index [10, 11, 12, 13, 14].A decisive advantage of the h-index over its competitors is its ease of calculation. However, it also is knownthat the h-index has several drawbacks. For example, if a researcher has published a small or moderate number N ofpapers, then necessarily h ≤ N, even if every paper is of very high quality and has received thousands of citations.",
        "arxiv": "1910.02369",
        "doi": null,
        "file_name": "1910.02369.pdf",
        "file_path": "./sources/1910.02369/1910.02369.pdf",
        "title": "Citation network centrality: a scientific awards predictor?",
        "urls": {
            "git": [
                {
                    "#_appearances": 1,
                    "url": "https://github.com/ajholanda/k-nobel"
                }
            ]
        }
    },
    "1912.10375": {
        "abstract": "AbstractAdversarial attacks against natural languageprocessing systems, which perform seeminglyinnocuous modifications to inputs, can in-duce arbitrary mistakes to the target models.Though raised great concerns, such adversar-ial attacks can be leveraged to estimate the ro-bustness of NLP models. Compared with theadversarial example generation in continuousdata domain (e.g., image), generating adver-sarial text that preserves the original meaningis challenging since the text space is discreteand non-differentiable. To handle these chal-lenges, we propose a target-controllable adver-sarial attack framework T3, which is applica-ble to a range of NLP tasks. In particular, wepropose a tree-based autoencoder to embed thediscrete text data into a continuous represen-tation space, upon which we optimize the ad-versarial perturbation. A novel tree-based de-coder is then applied to regularize the syntac-tic correctness of the generated text and ma-nipulate it on either sentence (T3(SENT)) orword (T3(WORD)) level. We consider twomost representative NLP tasks: sentiment anal-ysis and question answering (QA). Extensiveexperimental results and human studies showthat T3 generated adversarial texts can suc-cessfully manipulate the NLP models to outputthe targeted incorrect answer without mislead-ing the human. Moreover, we show that thegenerated adversarial texts have high transfer-ability which enables the black-box attacks inpractice. Our work sheds light on an effectiveand general way to examine the robustness ofNLP models. Our code is publicly available athttps://github.com/AI-secure/T3/.1 IntroductionRecent studies have demonstrated that deep neu-ral networks (DNNs) are vulnerable to carefullycrafted adversarial examples (Goodfellow et al.,2015; Papernot et al., 2016; Eykholt et al., 2017;Question: Who ended the series in 1989?Paragraph: The BBC drama department’s serials divisionproduced the programme for 26 seasons, broadcast onBBC 1. Falling viewing numbers, a decline in the publicperception of the show and a less-prominent transmissionslot saw production suspended in 1989 by Jonathan Powell,controller of BBC 1. ... the BBC repeatedly affirmed thatthe series would return. Donald Trump ends a program on",
        "arxiv": "1912.10375",
        "doi": "10.18653/v1/2020.emnlp-main.495",
        "file_name": "1912.10375.pdf",
        "file_path": "./sources/1912.10375/1912.10375.pdf",
        "title": "T3: Tree-Autoencoder Constrained Adversarial Text Generation for Targeted Attack",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/AI-secure/T3"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/huggingface/pytorch-pretrained-"
                }
            ]
        }
    },
    "1912.12055": {
        "abstract": "ABSTRACT In this paper, we present nnAudio, a new neural network-based audio processing frameworkwith graphics processing unit (GPU) support that leverages 1D convolutional neural networks to performtime domain to frequency domain conversion. It allows on-the-fly spectrogram extraction due to its fastspeed, without the need to store any spectrograms on the disk. Moreover, this approach also allows back-propagation on the waveforms-to-spectrograms transformation layer, and hence, the transformation processcan be made trainable, further optimizing the waveform-to-spectrogram transformation for the specifictask that the neural network is trained on. All spectrogram implementations scale as Big-O of linear timewith respect to the input length. nnAudio, however, leverages the compute unified device architecture(CUDA) of 1D convolutional neural network from PyTorch, its short-time Fourier transform (STFT), Melspectrogram, and constant-Q transform (CQT) implementations are an order of magnitude faster than otherimplementations using only the central processing unit (CPU). We tested our framework on three differentmachines with NVIDIA GPUs, and our framework significantly reduces the spectrogram extraction timefrom the order of seconds (using a popular python library librosa) to the order of milliseconds, giventhat the audio recordings are of the same length. When applying nnAudio to variable input audio lengths,an average of 11.5 hours are required to extract 34 spectrogram types with different parameters from theMusicNet dataset using librosa. An average of 2.8 hours is required for nnAudio, which is still fourtimes faster than librosa. Our proposed framework also outperforms existing GPU processing librariessuch as Kapre and torchaudio in terms of processing speed.INDEX TERMS Convolution, Discrete Fourier transform, Short time Fourier transform, Spectrogram,CQT, Constant Q Transform, Mel Spectrogram, Signal processing, Library, PyTorch, GPUI. INTRODUCTIONSPECTROGRAMS, as time-frequency representations ofaudio signals, have been used as input for neural networkmodels since the 1980s [1–3]. Different types of spectro-grams are tailored to different applications. For example,Mel spectrograms and Mel frequency cepstral coefficients(MFCCs) are designed for speech-related applications [4, 5],and the constant-Q transformation is best for music relatedapplications [6, 7]. Despite recent advances in end-to-endlearning in the audio domain, such as WaveNet [8] and Sam-pleCNN [9], which make model training on raw audio datapossible, many recent publications still use spectrograms asthe input to their models for various applications [10]. Theseapplications include speech recognition [11, 12], speechemotion detection [13], speech-to-speech translation [14],speech enhancement [15], voice separation [16], singingvoice conversion [17], music tagging [18], cover detec-tion [19], melody extraction [20], and polyphonic music tran-scription [21]. One drawback of training an end-to-end modelon raw audio data is the longer training time. As pointed outVOLUME 4, 2016 1arXiv:1912.12055v3 ",
        "arxiv": "1912.12055",
        "doi": "10.1109/ACCESS.2020.3019084",
        "file_name": "1912.12055.pdf",
        "file_path": "./sources/1912.12055/1912.12055.pdf",
        "title": "nnAudio: An on-the-fly GPU Audio to Spectrogram Conversion Toolbox Using 1D Convolutional Neural Networks",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/KinWaiCheuk/nnAudio"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/pseeth/torch-stft"
                }
            ]
        }
    },
    "2002.08233": {
        "abstract": "ABSTRACTForce-directed algorithms are widely used to generate aesthetically-pleasing layouts of graphs or networks arisen in many scientificdisciplines. To visualize large-scale graphs, several parallel al-gorithms have been discussed in the literature. However, exist-ing parallel algorithms do not utilize memory hierarchy efficientlyand often offer limited parallelism. This paper addresses theselimitations with BatchLayout, an algorithm that groups verticesinto minibatches and processes them in parallel. BatchLayoutalso employs cache blocking techniques to utilize memory hier-archy efficiently. More parallelism and improved memory ac-cesses coupled with force approximating techniques, better initial-ization, and optimized learning rate make BatchLayout significantlyfaster than other state-of-the-art algorithms such as ForceAtlas2 andOpenOrd. The visualization quality of layouts from BatchLayoutis comparable or better than similar visualization tools. All of oursource code, links to datasets, results and log files are available athttps://github.com/khaled-rahman/BatchLayout.Index Terms: Human-centered computing—Visualization—Visualization techniques—Graph drawings; Human-centeredcomputing—Visualization—Visualization systems and tools—Visualization toolkits1 INTRODUCTIONNetworks or graphs are common representations of scientific, socialand business data. In a graph, a set of vertices represents entities(e.g., persons, brain neurons, atoms) and a set of edges indicatesrelationships among entities (friendship, neuron synapses, chemicalbonds). A key aspect of data-driven graph analytics is to visuallystudy large-scale networks, such as biological and social networks,with millions or even billions of vertices and edges. In networkvisualization, the first step is to generate a layout in a 2D or 3Dcoordinate system which can be fed into visualization tools such asGephi [4] and Cytoscape [29]. Therefore, the quality and computa-tional complexity of network visualization are often dominated bythe graph layout algorithms.Force-directed layout algorithms are among the most popularlyused techniques to generate the layout of a graph. Following thephilosophy of the spring energy model, these algorithms calculate at-tractive and repulsive forces among vertices in a graph and iterativelyminimize an energy function. Classical force-directed algorithms,such as the Fruchterman and Reingold (FR) algorithm [9], requireO(n2) time per iteration where n is the number of vertices in a graph.By approximating the repulsive force between non-adjacent nodes,we can get a faster O(n logn) algorithm. In this paper, we used theBarnes-Hut approximation [3] based on the quad-tree data structure.*e-mail: morahma@iu.edu, Department of Computer Science†e-mail: msujon@iu.edu, Department of Intelligent Systems Engineering‡Corresponding author.§e-mail: azad@iu.edu, Department of Intelligent Systems EngineeringEven though the layout quality (measured by stress, neighborhood",
        "arxiv": "2002.08233",
        "doi": "10.1109/pacificvis48177.2020.3756",
        "file_name": "2002.08233.pdf",
        "file_path": "./sources/2002.08233/2002.08233.pdf",
        "title": "BatchLayout: A Batch-Parallel Force-Directed Graph Layout Algorithm in Shared Memory",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/khaled-rahman/BatchLayout"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/SciTechStrategies/OpenOrd"
                }
            ]
        }
    },
    "2003.03033": {
        "abstract": "ABSTRACTNeural network pruning—the task of reducing the size of a network by removing parameters—has been thesubject of a great deal of work in recent years. We provide a meta-analysis of the literature, including an overviewof approaches to pruning and consistent findings in the literature. After aggregating results across 81 papersand pruning hundreds of models in controlled conditions, our clearest finding is that the community suffersfrom a lack of standardized benchmarks and metrics. This deficiency is substantial enough that it is hard tocompare pruning techniques to one another or determine how much progress the field has made over the pastthree decades. To address this situation, we identify issues with current practices, suggest concrete remedies, andintroduce ShrinkBench, an open-source framework to facilitate standardized evaluations of pruning methods. Weuse ShrinkBench to compare various pruning techniques and show that its comprehensive evaluation can preventcommon pitfalls when comparing pruning methods.1 INTRODUCTIONMuch of the progress in machine learning in the pastdecade has been a result of deep neural networks. Manyof these networks, particularly those that perform the best(Huang et al., 2018), require enormous amounts of compu-tation and memory. These requirements not only increaseinfrastructure costs, but also make deployment of net-works to resource-constrained environments such as mo-bile phones or smart devices challenging (Han et al., 2015;Sze et al., 2017; Yang et al., 2017).One popular approach for reducing these resource require-ments at test time is neural network pruning, which entailssystematically removing parameters from an existing net-work. Typically, the initial network is large and accurate,and the goal is to produce a smaller network with simi-lar accuracy. Pruning has been used since the late 1980s(Janowsky, 1989; Mozer & Smolensky, 1989a;b; Karnin,1990), but has seen an explosion of interest in the pastdecade thanks to the rise of deep neural networks.For this study, we surveyed 81 recent papers on pruningin the hopes of extracting practical lessons for the broadercommunity. For example: which technique achieves thebest accuracy/efficiency tradeoff? Are there strategies thatwork best on specific architectures or datasets? Whichhigh-level design choices are most effective?There are indeed several consistent results: pruning param-eters based on their magnitudes substantially compresses*Equal contribution 1MIT CSAIL, Cambridge, MA, USA.Correspondence to: Davis Blalock <dblalock@mit.edu>.Proceedings of the 3 rd MLSys Conference, Austin, TX, USA,2020. Copyright 2020 by the author(s).networks without reducing accuracy, and many pruningmethods outperform random pruning. However, our cen-tral finding is that the state of the literature is such that ourmotivating questions are impossible to answer. Few paperscompare to one another, and methodologies are so inconsis-tent between papers that we could not make these compar-isons ourselves. For example, a quarter of papers compareto no other pruning method, half of papers compare to at",
        "arxiv": "2003.03033",
        "doi": null,
        "file_name": "2003.03033.pdf",
        "file_path": "./sources/2003.03033/2003.03033.pdf",
        "title": "What is the State of Neural Network Pruning?",
        "urls": {
            "git": [
                {
                    "#_appearances": 9,
                    "url": "https://github.com/BVLC/caffe"
                },
                {
                    "#_appearances": 8,
                    "url": "https://github.com/keras-team/keras"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/kuangliu/pytorch-cifar"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/jjgo/shrinkbench"
                }
            ]
        }
    },
    "2003.08361": {
        "abstract": "Abstract—With the massive increase in the number of IoTdevices being deployed in smart cities, it becomes paramount formiddlewares to be able to handle very high loads and support de-manding use-cases. In order to do so, middlewares must scale hor-izontally while providing a commensurate increase in availabilityand throughput. Currently, most open-source IoT middlewaresdo not provide out-of-the-box support for scaling horizontally.In this paper, we present “Vermillion”, a scalable, secure andopen-source IoT middleware for smart cities which provides in-built support for scaling-out. We make three contributions inthis paper. Firstly, the middleware platform itself along with aformal process for data exchange between data producers andconsumers. Secondly, we propose the use of hash-based federationto distribute and manage load across various message brokernodes while eliminating inter-node synchronisation overheads.Thirdly, we discuss a case study where Vermillion was deployedin a city and briefly discuss about deployment considerationsusing the obtained results.Index Terms—Smart Cities, Middleware, Vermillion, Scalable,High-PerformanceI. INTRODUCTIONMetropolitan cities face increasing pressure on their alreadylimited resources due to unplanned allowance of rapid urban-isation. This leads to urban sprawls and unsustainable loadson critical resources in cities [1]. A promising solution to theaforementioned problems is the use of IoT technologies toregulate unchecked resource consumption. Although the useof IoT does not completely solve the problem, it significantlyameliorates it, thus paving the way for reasonably sustainablegrowth of cities [2]The use of IoT in cities comes with immense benefits suchas being able to actively monitor services, resource usage,potential dangers etc. thus enabling city administrators to takeprophylactic steps in preventing problems from occurring [3].Therefore, it is no surprise that the number of IoT deviceswill reach 18 billion by 2022 [4]. To be able to handlesuch rapid growth, it is very essential for IoT middlewares totechnologically adapt to such demanding circumstances. Therewill, inevitably, be very large loads on these middlewares andthey will be expected to perform with near-zero downtimewhile at the same time ensuring a high throughput.To address this problem, we have developed Vermillion [5],a scalable, secure and open-source IoT middleware, built forperformance. Vermillion comprises of numerous containerisedmicroservices all of which work together to provide a viewof a single logical entity. It uses Vertx.io [6] for the HTTPSproxy, RabbitMQ [7] as the message broker, PostgreSQL [8] asthe authentication database and MongoDB [9] as the historicaldata store. All of these microservices have been containerisedusing Docker [10] and are orchestrated either using docker-",
        "arxiv": "2003.08361",
        "doi": null,
        "file_name": "2003.08361.pdf",
        "file_path": "./sources/2003.08361/2003.08361.pdf",
        "title": "Vermillion: A High-Performance Scalable IoT Middleware for Smart Cities",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/rbccps-iisc/iudx-resource-server"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/processone/tsung"
                }
            ]
        }
    },
    "2004.11675": {
        "abstract": "ABSTRACTSimulation of fluid flow within complex geometries of porous media has many applications, from themicro-scale (cell membranes, filters, rocks) to macro-scale (groundwater, hydrocarbon reservoirs, andgeothermal) and beyond. Direct simulation of flow in porous media requires significant computationalresources to solve within reasonable timeframes. An integrated method combining predictions offluid flow (fast, limited accuracy) with direct flow simulation (slow, high accuracy) is outlined. Inthe tortuous flow paths of porous media, Deep Learning techniques based on Convolutional NeuralNetworks (CNNs) are shown to give an accurate estimate of the steady state velocity fields (in allaxes), and by extension, the macro-scale permeability. This estimate can be used as-is, or as initialconditions in direct simulation to reach a fully accurate result in a fraction of the compute time. AGated U-Net Convolutional Neural Network is trained on a datasets of 2D and 3D porous mediagenerated by correlated fields, with their steady state velocity fields calculated from direct LBMsimulation. Sensitivity analysis indicates that network accuracy is dependent on (1) the tortuosity ofthe domain, (2) the size of convolution filters, (3) the use of distance maps as input, (4) the use ofmass conservation loss functions. Permeability estimation from these predicted fields reaches over90% accuracy for 80% of cases. It is further shown that these velocity fields are error prone whenused for solute transport simulation. Using the predicted velocity fields as initial conditions is shownto accelerate direct flow simulation to physically true steady state conditions an order of magnitudeless compute time. Using Deep Learning predictions (or potentially any other approximation method)to accelerate flow simulation to steady state in complex pore structures shows promise as a techniquepush the boundaries fluid flow modelling.Keywords Porous Media, Convolutional Neural Networks, Lattice BoltzmannarXiv:2004.11675v1  [physics.flu-dyn]  22 Apr 2020A PREPRINT - APRIL 27, 20201 Introduction",
        "arxiv": "2004.11675",
        "doi": null,
        "file_name": "2004.11675.pdf",
        "file_path": "./sources/2004.11675/2004.11675.pdf",
        "title": "ML-LBM: Machine Learning Aided Flow Simulation in Porous Media",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/yingDaWang-UNSW/VelCNNs"
                }
            ]
        }
    },
    "2004.14191": {
        "abstract": "ABSTRACTCode coverage analysis plays an important role in the software test-ing process. More recently, the remarkable effectiveness of coveragefeedback has triggered a broad interest in feedback-guided fuzzing.In this work, we introduce bcov, a tool for binary-level coverageanalysis. Our tool statically instruments x86-64 binaries in the ELFformat without compiler support. We implement several techniquesto improve efficiency and scale to large real-world software. First,we bring Agrawal’s probe pruning technique to binary-level in-strumentation and effectively leverage its superblocks to reduceoverhead. Second, we introduce sliced microexecution, a robust tech-nique for jump table analysis which improves CFG precision andenables us to instrument jump table entries. Additionally, smallerinstructions in x86-64 pose a challenge for inserting detours. Toaddress this challenge, we aggressively exploit padding bytes andsystematically host detours in neighboring basic blocks.We evaluate bcov on a corpus of 95 binaries compiled from eightpopular and well-tested packages like FFmpeg and LLVM. Twoinstrumentation policies, with different edge-level precision, areused to patch all functions in this corpus - over 1.6 million functions.Our precise policy has average performance andmemory overheadsof 14% and 22% respectively. Instrumented binaries do not introduceany test regressions. The reported coverage is highly accurate withan average F-score of 99.86%. Finally, our jump table analysis iscomparable to that of IDA Pro on gcc binaries and outperforms iton clang binaries.CCS CONCEPTS• Software and its engineering → Software testing and de-bugging; • Security and privacy→ Software reverse engineering.KEYWORDScode coverage analysis, jump table analysis, binary instrumentationACM Reference Format:M. Ammar Ben Khadra, Dominik Stoffel, andWolfgang Kunz. 2020. EfficientBinary-Level Coverage Analysis. In Proceedings of the 28th ACM Joint Euro-pean Software Engineering Conference and Symposium on the Foundationsof Software Engineering (ESEC/FSE ’20), November 8–13, 2020, Virtual Event,USA. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3368089.3409694Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from permissions@acm.org.ESEC/FSE ’20, November 8–13, 2020, Virtual Event, USA© 2020 Association for Computing Machinery.ACM ISBN 978-1-4503-7043-1/20/11. . . $15.00https://doi.org/10.1145/3368089.34096941 INTRODUCTION",
        "arxiv": "2004.14191",
        "doi": null,
        "file_name": "2004.14191.pdf",
        "file_path": "./sources/2004.14191/2004.14191.pdf",
        "title": "Efficient Binary-Level Coverage Analysis",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/unicorn-engine/unicorn"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/torvalds/linux"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/microsoft/Detours"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/google/oss-fuzz"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/google/honggfuzz"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/dyninst/dyninst"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/aquynh/capstone"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/DynamoRIO/dynamorio"
                }
            ],
            "zenodo": [
                {
                    "#_appearances": 2,
                    "url": "https://doi.org/10.5281/zenodo.3876047"
                }
            ]
        }
    },
    "2004.14302": {
        "abstract": "AbstractExisting automatic evaluation metrics foropen-domain dialogue response generationsystems correlate poorly with human evalua-tion. We focus on evaluating response gener-ation systems via response selection. To eval-uate systems properly via response selection,we propose the method to construct responseselection test sets with well-chosen false can-didates. Specifically, we propose to constructtest sets filtering out some types of false candi-dates: (i) those unrelated to the ground-truthresponse and (ii) those acceptable as appro-priate responses. Through experiments, wedemonstrate that evaluating systems via re-sponse selection with the test sets developedby our method correlates more strongly withhuman evaluation, compared with widely usedautomatic evaluation metrics such as BLEU.1 IntroductionAutomatic evaluation for open-domain dialoguegeneration systems has a potential for driving theirresearch and development because of its high re-producibility and low cost. However, existing auto-matic evaluation metrics, such as BLEU (Papineniet al., 2002), correlate poorly with human evalua-tion (Liu et al., 2016). This poor correlation arisesfrom a nature of dialogue, that is, there are manyacceptable responses to an input context, known asthe one-to-many problem (Zhao et al., 2017).To tackle this problematic issue, we focus onevaluating response generation systems via re-sponse selection. In this task, systems select anappropriate response for a given context from aset of response candidates. Each candidate has thelabel that indicates whether the candidate is appro-priate for the given context. Traditionally, responseselection has been used to evaluate retrieval-baseddialogue systems (Lowe et al., 2015; Wu et al.,2017). Here, we consider applying response selec-tion to driving the research for dialogue generationRepositoryContext:  Do you have a car?Ground-Truth: Yes, I have a car.QueryNo, I have a car.I don’t know.I have a cold.No, I have a car. I don’t know.",
        "arxiv": "2004.14302",
        "doi": "10.18653/v1/2020.acl-main.55",
        "file_name": "2004.14302.pdf",
        "file_path": "./sources/2004.14302/2004.14302.pdf",
        "title": "Evaluating Dialogue Generation Systems via Response Selection",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/cl-tohoku/eval-via-selection"
                }
            ]
        }
    },
    "2006.09044": {
        "abstract": "AbstractFinding the ground state of a quantum mechanical system can be formulated as an optimal con-trol problem. In this formulation, the drift of the optimally controlled process is chosen to matchthe distribution of paths in the Feynman–Kac (FK) representation of the solution of the imaginarytime Schrödinger equation. This provides a variational principle that can be used for reinforcementlearning of a neural representation of the drift. Our approach is a drop-in replacement for path inte-gral Monte Carlo, learning an optimal importance sampler for the FK trajectories. We demonstratethe applicability of our approach to several problems of one-, two-, and many-particle physics.Keywords: Quantum Mechanics, Feynman–Kac Formula, Optimal Control, Reinforcement Learn-ing1. IntroductionQuantum mechanics takes place in infinite dimensional Hilbert space. Naturally, any numericalapproach to solving the equations of quantum mechanics – or any other physical system with a con-tinuum description – involves a finite truncation of this space. When we turn to many-body quantummechanics, the dimension of Hilbert space necessarily grows exponentially with the number of par-ticles relative to the finite truncation used for a single particle. To be specific, if a single particleis described by a wavefunction ψ(r) defined on a real-space grid of linear size L (with L3 pointsin three dimensions), the wavefunction of N particles Ψ(r1, . . . , rN ) is defined on a grid in 3Ndimensions of L3N points.Traditionally, this challenge has been dealt with by considering many body wavefunctions ofrestricted form. For example, the Hartree–Fock method employs factorized wavefunctions1Ψ(r1, . . . , rN ) = ψ1(r1) . . . ψN (rN ).This reduces the memory cost to linear in the number of particles,2 but represents a drastic simpli-fication that performs especially poorly when the interaction between particles is strong. Over theyears many post–Hartree–Fock hand-crafted improvements of the many-body wavefunction havebeen introduced, including Jastrow factors, and the coupled cluster and configuration interactionsmethods (Foulkes et al. (2001)).The exponential growth in the complexity of many-body quantum mechanics closely parallelsthe curse of dimensionality encountered in computer vision and other traditional applications of1. We leave aside the issue of the statistics of indistinguishable particles for the moment.2. The computational complexity of the Hartree–Fock method scales as the cube of the number of basis functions usedto represent the ψi(r).c© 2020 A. Barr, W. Gispen & A. Lamacraft.arXiv:2006.09044v1  [quant-ph]  1",
        "arxiv": "2006.09044",
        "doi": null,
        "file_name": "2006.09044.pdf",
        "file_path": "./sources/2006.09044/2006.09044.pdf",
        "title": "Quantum Ground States from Reinforcement Learning",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/AustenLamacraft/QuaRL"
                }
            ]
        }
    },
    "2006.09916": {
        "abstract": "AbstractActive learning is able to reduce the amount of la-belling effort by using a machine learning modelto query the user for specific inputs. While thereare many papers on new active learning tech-niques, these techniques rarely satisfy the con-straints of a real-world project. In this paper,we analyse the main drawbacks of current activelearning techniques and we present approachesto alleviate them. We do a systematic study onthe effects of the most common issues of real-world datasets on the deep active learning process:model convergence, annotation error, and datasetimbalance. We derive two techniques that canspeed up the active learning loop such as partialuncertainty sampling and larger query size. Fi-nally, we present our open-source Bayesian activelearning library, BaaL.*Equal contribution 1Element AI, Montréal, Canada. Corre-spondence to: Frédéric Branchaud-Charron <frederic.branchaud-charron@elementai.com>.Presented at the ICML 2020 Workshop on Uncertainty and Ro-bustness in Deep Learning. Copyright 2020 by the author(s).1. IntroductionThe amount of data readily available for machine learninghas exploded in recent years. However, for data to be usedfor deep learning models, labelling is often a required step.A common problem when labelling new datasets is the re-quired human effort to perform the annotation. In particular,tasks that require particular domain knowledge such as med-ical imaging, are expensive to annotate. To solve this, activelearning (AL) has been proposed to only label the core setof observations useful for training.While the active learning field includes many approaches(Kirsch et al., 2019; Tsymbalov et al., 2019; Beluch et al.,2018; Maddox et al., 2019), these methods are often notscalable to large datasets or too slow to be used in a morerealistic environment e.g. in a production setup. In partic-ular, active learning applied to images or text requires theusage of deep learning models which are slow to train andthemselves require a noticeable huge amount of data to beeffective (Deng et al., 2009; Abu-El-Haija et al., 2016).Furthermore, deep learning models require carefully tunedhyperparameters to be effective. In a research environment,one can fine-tune and perform hyperparameter search to findthe optimal combination that gives the biggest reduction inlabelling effort. In a real-world setting, the hyperparametersare set at the beginning with no guarantee for the outcome.Finally, in a real-world setup, the data is often not cleanednor balanced. In particular, studies have shown that humans",
        "arxiv": "2006.09916",
        "doi": null,
        "file_name": "2006.09916.pdf",
        "file_path": "./sources/2006.09916/2006.09916.pdf",
        "title": "Bayesian active learning for production, a systematic study and a reusable library",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/ElementAI/baal"
                }
            ]
        }
    },
    "2007.02824": {
        "abstract": "Abstract  The ability to understand and engineer molecular structures relies on having accurate descriptions of the energy as a function of atomic coordinates. Here we outline a new paradigm for deriving energy functions of hyperdimensional molecular systems, which involves generating data for low-dimensional systems in virtual reality (VR) to then efficiently train atomic neural networks (ANNs). This generates high-quality data for specific areas of interest within the hyperdimensional space that characterizes a molecule’s potential energy surface (PES). We demonstrate the utility of this approach by gathering data within VR to train ANNs on chemical reactions involving fewer than 8 heavy atoms. This strategy enables us to predict the energies of much higher-dimensional systems, e.g. containing nearly 100 atoms. Training on datasets containing only 15k geometries, this approach generates mean absolute errors around 2 kcal mol-1. This represents one of the first times that an ANN-PES for a large reactive radical has been generated using such a small dataset. Our results suggest VR enables the intelligent curation of high-quality data, which accelerates the learning process.              mailto:*glowacki@bristol.ac.uk1. Introduction  In the recent past, computations were mostly limited by the available processing power.  With the machine learning revolution, the issues related to generating and curating data have become equally as important as the algorithms used to process and learn the data1.  The molecular sciences have seen a surge in popularity of machine learning methods for a variety of applications, from designing new drug molecules2, 3 to planning synthetic chemistry strategies4. Multiple research groups have been applying machine learning to the prediction of molecular energies and forces5-8 , with the goal of accelerating molecular dynamics (MD) simulations. For small systems, ab initio calculations can be used to evaluate accurate energies and forces at each step of an MD simulation. However, this becomes too computationally expensive for larger systems and more approximate methods, such as force fields, are generally used. While much faster, these incur a trade-off in accuracy.  An alternative to force fields is to use accurate data, e.g. from electronic structure calculations, to fit potential energy surfaces (PES). Evaluating fitted PES should be faster than performing electronic structure calculations but should provide similar accuracy to the underlying data. A variety of machine learning techniques have been used for either fitting or interpolating PES, for example permutationally invariant fitting9, cubic splines10, modified ",
        "arxiv": "2007.02824",
        "doi": "10.1063/5.0015950",
        "file_name": "2007.02824.pdf",
        "file_path": "./sources/2007.02824/2007.02824.pdf",
        "title": "Training atomic neural networks using fragment-based data generated in virtual reality",
        "urls": {
            "git": [
                {
                    "#_appearances": 4,
                    "url": "https://github.com/SilviaAmAm/squalane_paper_si"
                }
            ]
        }
    },
    "2007.13145": {
        "abstract": "Abstract—This paper addresses the problem of photometric stereo, in both calibrated and uncalibrated scenarios, for non-Lambertiansurfaces based on deep learning. We first introduce a fully convolutional deep network for calibrated photometric stereo, which we callPS-FCN. Unlike traditional approaches that adopt simplified reflectance models to make the problem tractable, our method directlylearns the mapping from reflectance observations to surface normal, and is able to handle surfaces with general and unknown isotropicreflectance. At test time, PS-FCN takes an arbitrary number of images and their associated light directions as input and predicts asurface normal map of the scene in a fast feed-forward pass. To deal with the uncalibrated scenario where light directions areunknown, we introduce a new convolutional network, named LCNet, to estimate light directions from input images. The estimated lightdirections and the input images are then fed to PS-FCN to determine the surface normals. Our method does not require a pre-definedset of light directions and can handle multiple images in an order-agnostic manner. Thorough evaluation of our approach on bothsynthetic and real datasets shows that it outperforms state-of-the-art methods in both calibrated and uncalibrated scenarios.Index Terms—photometric stereo, non-Lambertian, uncalibrated, convolutional neural network.F1 INTRODUCTIONPHOTOMETRIC stereo aims at recovering the surface nor-mals of a static scene from a set of images capturedunder different light directions with a fixed camera [1], [2].Based on the availability of calibrated lighting conditions,photometric stereo can be categorized into calibrated and un-calibrated photometric stereo settings. Early calibrated pho-tometric stereo methods assumed a simplified reflectancemodel, such as the ideal Lambertian model [1], [2] or an-alytical reflectance models [3], [4], [5]. However, most ofthe real-world objects are non-Lambertian, and a specificanalytical model is only valid for a small set of materials.A bidirectional reflectance distribution function (BRDF) isa general form for describing the reflectance property of asurface, but it is difficult to directly use a non-parametricform of BRDFs for photometric stereo.Recently, with the great success of deep learning invarious computer vision tasks, deep learning based methodshave been introduced to calibrated photometric stereo tohandle surfaces with general and unknown isotropic re-flectance [6], [7], [8]. Instead of explicitly modeling complexsurface reflectances, they directly learn the mapping fromreflectance observations to surface normals given knownlight directions. However, the method in [6] depends ona pre-defined set of light directions during training andtesting. The methods in [6], [7] estimate the surface normals• G. Chen and K-Y. K. Wong are with The University of Hong Kong, HongKong, China.E-mail: {gychen,kykwong}@cs.hku.hk• K. Han is with University of Oxford, Oxford, United Kingdom.E-mail: khan@robots.ox.ac.uk• B. Shi is with Peking University, Beijing and Peng Cheng Laboratory,Shenzhen, China.E-mail: shiboxin@pku.edu.cn• Y. Matsushita is with Osaka University, Osaka, Japan.E-mail: yasumat@ist.osaka-u.ac.jpin a pixel-wise manner, making them not possible to accountfor the local context information of a surface point (e.g., sur-face smoothness prior). Taniai and Maehara [8] introduced",
        "arxiv": "2007.13145",
        "doi": "10.1109/tpami.2020.3005397",
        "file_name": "2007.13145.pdf",
        "file_path": "./sources/2007.13145/2007.13145.pdf",
        "title": "Deep Photometric Stereo for Non-Lambertian Surfaces",
        "urls": {}
    },
    "2007.13505": {
        "abstract": "AbstractA central mechanism in machine learning is to identify, store, and recognizepatterns. How to learn, access, and retrieve such patterns is crucial in Hopfieldnetworks and the more recent transformer architectures. We show that the attentionmechanism of transformer architectures is actually the update rule of modern Hop-field networks that can store exponentially many patterns. We exploit this high stor-age capacity of modern Hopfield networks to solve a challenging multiple instancelearning (MIL) problem in computational biology: immune repertoire classification.Accurate and interpretable machine learning methods solving this problem couldpave the way towards new vaccines and therapies, which is currently a very relevantresearch topic intensified by the COVID-19 crisis. Immune repertoire classificationbased on the vast number of immunosequences of an individual is a MIL problemwith an unprecedentedly massive number of instances, two orders of magnitudelarger than currently considered problems, and with an extremely low witness rate.In this work, we present our novel method DeepRC that integrates transformer-likeattention, or equivalently modern Hopfield networks, into deep learning architec-tures for massive MIL such as immune repertoire classification. We demonstratethat DeepRC outperforms all other methods with respect to predictive performanceon large-scale experiments, including simulated and real-world virus infection data,and enables the extraction of sequence motifs that are connected to a given diseaseclass. Source code and datasets: https://github.com/ml-jku/DeepRCIntroductionTransformer architectures (Vaswani et al., 2017) and their attention mechanisms are currently used inmany applications, such as natural language processing (NLP), imaging, and also in multiple instancelearning (MIL) problems (Lee et al., 2019). In MIL, a set or bag of objects is labelled rather thanobjects themselves as in standard supervised learning tasks (Dietterich et al., 1997). Examples for MILproblems are medical images, in which each sub-region of the image represents an instance, videoPreprint. Under review.arXiv:2007.13505v1  [cs.LG]  16 Jul 2020  ",
        "arxiv": "2007.13505",
        "doi": "10.1101/2020.04.12.038158",
        "file_name": "2007.13505.pdf",
        "file_path": "./sources/2007.13505/2007.13505.pdf",
        "title": "Modern Hopfield Networks and Attention for Immune Repertoire Classification",
        "urls": {
            "git": [
                {
                    "#_appearances": 3,
                    "url": "https://github.com/ml-jku/DeepRC"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/spro/practical-pytorch"
                }
            ]
        }
    },
    "2007.16196": {
        "abstract": "Abstract—Neural speaker embeddings trained using classifica-tion objectives have demonstrated state-of-the-art performancein multiple applications. Typically, such embeddings are trainedon an out-of-domain corpus on a single task e.g., speakerclassification, albeit with a large number of classes (speakers). Inthis work, we reformulate embedding training under the meta-learning paradigm. We redistribute the training corpus as anensemble of multiple related speaker classification tasks, andlearn a representation that generalizes better to unseen speakers.First, we develop an open source toolkit to train x-vectors thatis matched in performance with pre-trained Kaldi models forspeaker diarization and speaker verification applications. Wefind that different bottleneck layers in the architecture variedlyfavor different applications. Next, we use two meta-learningstrategies, namely prototypical networks and relation networks,to improve over the x-vector embeddings. Our best performingmodel achieves a relative improvement of 12.37% and 7.11%in speaker error on the DIHARD II development corpus andthe AMI meeting corpus, respectively. We analyze improvementsacross different domains in the DIHARD corpus. Notably, on thechallenging child speech domain, we study the relation betweenchild age and the diarization performance. Further, we showreductions in equal error rate for speaker verification on theSITW corpus (7.68%) and the VOiCES challenge corpus (8.78%).We observe that meta-learning particularly offers benefits inchallenging acoustic conditions and recording setups encounteredin these corpora. Our experiments illustrate the applicability ofmeta-learning as a generalized learning paradigm for trainingdeep neural speaker embeddings.I. INTRODUCTIONAudio speaker embeddings refer to fixed-dimensional vectorrepresentations extracted from variable duration audio utter-ances and assumed to contain information relevant to speakercharacteristics. In the last decade, speaker embeddings haveemerged as the most common representations used for speaker-identity relevant tasks such as speaker diarization (speakersegmentation followed by clustering: who spoke when?) [1]and speaker verification (does an utterance pair belong tosame speaker?) [2]. Such applications are relevant acrossa variety of domains such as voice bio-metrics [3], [4],automated meeting analysis [5], [6], and clinical interactionanalysis [7], [8]. Recent technology evaluation challenges [9]–[12] have drawn attention to these domains by incorporatingnatural and simulated in-the-wild speech corpora exemplifyingthe many diverse technical facets that need to be addressed.M. Kumar, T. J. Park and S. Narayanan are with Signal Analysis and Inter-pretation Laboratory, University of Southern California, Los Angeles, USAe-mail: (prabakar@usc.edu;taejinpa@usc.edu;shri@ee.usc.edu). S. Bishop iswith Department of Psychiatry, University of California, San Francisco, USAe-mail:(somer.bishop@ucsf.edu)",
        "arxiv": "2007.16196",
        "doi": null,
        "file_name": "2007.16196.pdf",
        "file_path": "./sources/2007.16196/2007.16196.pdf",
        "title": "Designing Neural Speaker Embeddings with Meta Learning",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/kaldi-asr/kaldi"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/manojpamk/pytorch"
                }
            ]
        }
    },
    "2008.06456": {
        "abstract": "Abstract: Recent automatic curriculum learning algorithms, and in particularTeacher-Student algorithms, rely on the notion of learning progress, making theassumption that the good next tasks are the ones on which the learner is mak-ing the fastest progress or digress. In this work, we first propose a simpler andimproved version of these algorithms. We then argue that the notion of learningprogress itself has several shortcomings that lead to a low sample efficiency forthe learner. We finally propose a new algorithm, based on the notion of masteringrate, that significantly outperforms learning progress-based algorithms.Keywords: curriculum learning, mastering rate, learning progress1 IntroductionRecently, deep reinforcement learning algorithms have been successfully applied to a wide range ofdomains ([1], [2], [3], [4]). However, their success relies heavily on dense rewards being given tothe agent; and learning in environments with sparse rewards is still a major limitation of RL due tothe low sample efficiency of the current algorithms in such scenarios.In sparse rewards settings, the sample inefficiency is essentially caused by the low likelihood of theagent obtaining a reward by random exploration. Recent attempts to tackle this issue revolve aroundproviding the agent an intrinsic reward that encourages exploring new states of the environment,thus increasing the likelihood of reaching the reward ([5], [6], [7]). An alternative way to improvethe sample efficiency is curriculum learning ([8]). It consists in first training the agent on an easyversion of the task at hand, where it can get reward more easily and learn, then training on increas-ingly difficult versions using the previously learned policy and finally, training on the task at hand.Its usage is not limited to reinforcement learning and robotics tasks, but also to supervised tasks.Curriculum learning may be decomposed into two parts:1. Defining the curriculum, i.e. the set of tasks the learner may be trained on.2. Defining the program, i.e. defining, at each training step, on which tasks to train the learner,given its learning state and the curriculum.The idea that using a curriculum of increasingly more difficult tasks speeds up neural networkstraining was put forward in [9]. [8] paved the way to a wider usage of curriculum learning in the field.[10] for example used hand-designed curricula to learn to perform memorization and addition withLSTMs ([11]). [12] used curriculum learning to train an actor-critic agent on Doom. [13] used smallcurricula to improve the sample efficiency of ground language learning with imitation learning. [14]used curriculum learning in the context of training a CNN for visualization tasks. These worksrely on hand-designed programs, where there is a performance threshold that allows the learner toadvance to the next task, or that increases the number of training examples of the harder tasks in thecase of supervised learning. Relying on hand-designed programs creates a significant bottleneck toa broader usage of curriculum learning because:• They are painful to design, requiring a lot of iterations.∗Equal contribution. Code: https://github.com/lcswillems/automatic-curriculumarXiv:2008.06456v1  [cs",
        "arxiv": "2008.06456",
        "doi": null,
        "file_name": "2008.06456.pdf",
        "file_path": "./sources/2008.06456/2008.06456.pdf",
        "title": "Mastering Rate based Curriculum Learning",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/lcswillems/automatic-curriculum"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/maximecb/gym-minigrid"
                }
            ]
        }
    },
    "2008.1188": {
        "abstract": "Abstract—This paper evaluates data stream classifiers fromthe perspective of connected devices, focusing on the use case ofHAR. We measure both classification performance and resourceconsumption (runtime, memory, and power) of five usual streamclassification algorithms, implemented in a consistent library,and applied to two real human activity datasets and to threesynthetic datasets. Regarding classification performance, resultsshow an overall superiority of the HT, the MF, and the NBclassifiers over the FNN and the Micro Cluster Nearest Neighbor(MCNN) classifiers on 4 datasets out of 6, including the real ones.In addition, the HT, and to some extent MCNN, are the onlyclassifiers that can recover from a concept drift. Overall, the threeleading classifiers still perform substantially lower than an offlineclassifier on the real datasets. Regarding resource consumption,the HT and the MF are the most memory intensive and have thelongest runtime, however, no difference in power consumptionis found between classifiers. We conclude that stream learningfor HAR on connected objects is challenged by two factorswhich could lead to interesting future work: a high memoryconsumption and low F1 scores overall.Index Terms—Application Platform, Data Management andAnalytics, Smart Environment, Data Streams, Classification,Power, Memory Footprint, Benchmark, Human Activity Recog-nition, MCNN, Mondrian, Hoeffding Tree.I. INTRODUCTIONInternet of Things applications may adopt a centralizedmodel, where connected objects transfer data to servers withadequate computing capabilities, or a decentralized model,where data is analyzed directly on the connected objects or onnearby devices. While the decentralized model limits networktransmission, increases battery life [2], [9], and reduces dataprivacy risks, it also raises important processing challengesdue to the modest computing capacity of connected objects.Indeed, it is not uncommon for wearable devices and othersmart objects to include a processing memory of less than100 KB, little to no storage memory, a slow CPU, and nooperating system. With multiple sensors producing data at ahigh frequency, typically 50 Hz to 800 Hz, processing speedand memory consumption become critical properties of dataanalyses.Data stream processing algorithms are precisely designed toanalyze virtually infinite sequences of data elements with re-duced amounts of working memory. Several classes of streamprocessing algorithms were developed in the past decades,such as filtering, counting, or sampling algorithms [16]. Thesealgorithms must follow multiple constraints such as a constantprocessing time per data element, or a constant space complex-ity [12]. Our study focuses on supervised classification, a keycomponent of contemporary data models.We evaluate supervised data stream classifiers from the",
        "arxiv": "2008.1188",
        "doi": null,
        "file_name": "2008.1188.pdf",
        "file_path": "./sources/2008.1188/2008.1188.pdf",
        "title": "A benchmark of data stream classification for human activity recognition on connected objects",
        "urls": {
            "git": [
                {
                    "#_appearances": 5,
                    "url": "https://github.com/azazel7/paper-benchmark"
                }
            ]
        }
    },
    "2008.11894": {
        "abstract": "Abstract. This paper focuses on webly supervised learning (WSL),where datasets are built by crawling samples from the Internet and di-rectly using search queries as web labels. Although WSL benefits fromfast and low-cost data collection, noises in web labels hinder better per-formance of the image classification model. To alleviate this problem,in recent works, self-label supervised loss Ls is utilized together withwebly supervised loss Lw. Ls relies on pseudo labels predicted by themodel itself. Since the correctness of the web label or pseudo label isusually on a case-by-case basis for each web sample, it is desirable toadjust the balance between Ls and Lw on sample level. Inspired by theability of Deep Neural Networks (DNNs) in confidence prediction, weintroduce Self-Contained Confidence (SCC) by adapting model uncer-tainty for WSL setting, and use it to sample-wisely balance Ls and Lw.Therefore, a simple yet effective WSL framework is proposed. A seriesof SCC-friendly regularization approaches are investigated, among whichthe proposed graph-enhanced mixup is the most effective method to pro-vide high-quality confidence to enhance our framework. The proposedWSL framework has achieved the state-of-the-art results on two large-scale WSL datasets, WebVision-1000 and Food101-N. Code is availableat https://github.com/bigvideoresearch/SCC.Keywords: Webly supervised learning, noisy labels, model uncertainty1 IntroductionLarge-scale human-labeled data plays a vital role in deep learning-based applica-tions such as image classification [3], scene recognition [41], face recognition [30],etc. However, high-quality human annotations require significant cost in laborand time. Webly supervised learning (WSL), therefore, has attracted more at-tention recently as a cost-effective approach for developing learning systems from? Work done during an internship at SenseTime EIG Research.arXiv:2008.11894v1  [cs.CV]  27 Aug 2020",
        "arxiv": "2008.11894",
        "doi": null,
        "file_name": "2008.11894.pdf",
        "file_path": "./sources/2008.11894/2008.11894.pdf",
        "title": "Webly Supervised Image Classification with Self-Contained Confidence",
        "urls": {
            "git": [
                {
                    "#_appearances": 1,
                    "url": "https://github.com/bigvideoresearch/SCC"
                }
            ]
        }
    },
    "2010.02793": {
        "abstract": "ABSTRACTGalaxy clustering is a standard cosmological probe that is commonly analysed through two-point statistics. In observations, the es-timation of the two-point correlation function crucially relies on counting pairs in a random catalogue. The latter contains a largenumber of randomly distributed points, which accounts for the survey window function. Random pair counts can also be advanta-geously used for modelling the window function in the observed power spectrum. Since pair counting scales as O(N2), where N isthe number of points, the computational time to measure random pair counts can be very expensive for large surveys. In this work,we present an alternative approach for estimating those counts that does not rely on the use of a random catalogue. We derived ananalytical expression for the anisotropic random-random pair counts that accounts for the galaxy radial distance distribution, surveygeometry, and possible galaxy weights. We show that a prerequisite is the estimation of the two-point correlation function of theangular selection function, which can be obtained efficiently using pixelated angular maps. Considering the cases of the VIPERS andSDSS-BOSS redshift surveys, we find that the analytical calculation is in excellent agreement with the pair counts obtained fromrandom catalogues. The main advantage of this approach is that the primary calculation only takes a few minutes on a single CPUand it does not depend on the number of random points. Furthermore, it allows for an accuracy on the monopole equivalent to whatwe would otherwise obtain when using a random catalogue with about 1500 times more points than in the data at hand. We alsodescribe and test an approximate expression for data-random pair counts that is less accurate than for random-random counts, but stillprovides subpercent accuracy on the monopole. The presented formalism should be very useful in accounting for the window functionin next-generation surveys, which will necessitate accurate two-point window function estimates over huge observed cosmologicalvolumes.Key words. Cosmology: miscellaneous – large-scale structure of Universe – Methods: numerical – Methods: statistical1. IntroductionThe spatial distribution of galaxies has a long history of provid-ing cosmological parameter constraints (e.g. Strauss et al. 1992;Vogeley et al. 1992; Maddox et al. 1996; Peacock et al. 2001;Cole et al. 2005; Tegmark et al. 2006; Percival et al. 2010; Blakeet al. 2012; de la Torre et al. 2013; Alam et al. 2017; eBOSSCollaboration et al. 2020, and references therein). This arisesfrom the fact that the statistical properties of galaxies, partic-ularly spatial ones, can be predicted by cosmological models.When analysing galaxy clustering, we usually compress the in-formation by using summary statistics, the most natural one be-ing the two-point correlation function or its Fourier counterpartin the power spectrum. This is due to the nearly Gaussian na-ture of primordial matter perturbations, which are almost fullydescribed by their two-point statistics. Although gravitationalevolution leads to non-Gaussianity, and in turn, non-vanishinghigher-order n-point statistics, two-point statistics continues tobe very informative.Despite the cosmological principle that implies that the cor-relation function is isotropic, meaning that it is only a function ofthe norm of the separation vector, because of the way the line-of-sight distance is measured in redshift surveys and the presenceof peculiar velocities, the observed correlation function becomesanisotropic. These velocities are induced on large scales by thecoherent convergence of matter towards overdensities as part ofthe general process of structure growth. This anisotropy makesobserved galaxy n-point statistics sensitive to the strength ofgravity acting on the large-scale structure (Kaiser 1987; Guzzoet al. 2008).Formally, the two-point correlation function is the excessprobability of finding a pair of objects at a given distance, with",
        "arxiv": "2010.02793",
        "doi": "10.1051/0004-6361/2020.39603",
        "file_name": "2010.02793.pdf",
        "file_path": "./sources/2010.02793/2010.02793.pdf",
        "title": "Fast analytical calculation of the random pair counts for realistic survey geometry",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "http://github.com/mianbreton/RR_code"
                }
            ]
        }
    },
    "2010.08138": {
        "abstract": "AbstractIn recent years, neural backdoor attack has been considered to be a potential securitythreat to deep learning systems. Such systems, while achieving the state-of-the-artperformance on clean data, perform abnormally on inputs with predefined triggers.Current backdoor techniques, however, rely on uniform trigger patterns, whichare easily detected and mitigated by current defense methods. In this work, wepropose a novel backdoor attack technique in which the triggers vary from inputto input. To achieve this goal, we implement an input-aware trigger generatordriven by diversity loss. A novel cross-trigger test is applied to enforce triggernonreusablity, making backdoor verification impossible. Experiments show thatour method is efficient in various attack scenarios as well as multiple datasets.We further demonstrate that our backdoor can bypass the state of the art defensemethods. An analysis with a famous neural network inspector again proves thestealthiness of the proposed attack. Our code is publicly available.1 IntroductionDue to their superior performance, deep neural networks have become essential in modern artificialintelligence systems. The state-of-the-art networks, however, require massive training data, expensivecomputing hardwares, and days or even weeks of training. Therefore, instead of training thesenetworks from scratch, many companies use pre-trained models provided by third-parties. This hascaused an emerging security threat of neural backdoor attacks, in which the provided networks lookgenuine but intentionally misbehave on a specific condition of the inputs.BadNets (1) is one of the first studies discussing this problem on the image classification task. Theauthors proposed to poison a part of the training data. More specifically, the poisoned images wereinjected with a fixed small pattern, called a trigger, at a specific location, and the correspondinglabels were changed to some pre-defined attack classes. The trained networks could classify the cleantesting images accurately but quickly switched to return attack labels when trigger patterns appeared.Liu et al. (2) extended it to different domains, including face recognition, speech recognition, andsentence attitude recognition. Since then, many variations of backdoor attacks have been proposed(3; 4).Though varying in mechanisms and scenarios, all these attacks rely on the same premise of using afixed trigger on all poisoned data. It became a crucial weakness that has been exploited by defensemethods (5; 6; 7; 8). These methods derive backdoor trigger candidates then verify by applying themto a set of clean test images.We argue that the fixed trigger premise is hindering the capability of backdoor attack methods. Adynamic backdoor that has the trigger pattern varying from input to input is much stealthier. Itbreaks the foundation assumption of all current defense methods, thus easily defeats them. Moreover,Preprint. Under review.arXiv:2010.08138v1  [cs.C",
        "arxiv": "2010.08138",
        "doi": null,
        "file_name": "2010.08138.pdf",
        "file_path": "./sources/2010.08138/2010.08138.pdf",
        "title": "Input-Aware Dynamic Backdoor Attack",
        "urls": {
            "git": [
                {
                    "#_appearances": 1,
                    "url": "https://github.com/VinAIResearch/input-aware-backdoor-attack-release"
                }
            ]
        }
    },
    "2010.1451": {
        "abstract": null,
        "arxiv": "2010.1451",
        "doi": null,
        "file_name": "2010.1451.pdf",
        "file_path": "./sources/2010.1451/2010.1451.pdf",
        "title": "Optimized Observable Readout from Single-shot Images of Ultracold Atoms via Machine Learning",
        "urls": {
            "git": [
                {
                    "#_appearances": 1,
                    "url": "https://gitlab.com/auj.lode/UNIQORN"
                }
            ]
        }
    },
    "2011.05013": {
        "abstract": "AbstractDe novo genome assembly focuses on finding connections between a vast amount ofshort sequences in order to reconstruct the original genome. The central problem ofgenome assembly could be described as finding a Hamiltonian path through a largedirected graph with a constraint that an unknown number of nodes and edges shouldbe avoided. However, due to local structures in the graph and biological features,the problem can be reduced to graph simplification, which includes removal ofredundant information. Motivated by recent advancements in graph representationlearning and neural execution of algorithms, in this work we train the MPNN modelwith max-aggregator to execute several algorithms for graph simplification. Weshow that the algorithms were learned successfully and can be scaled to graphs ofsizes up to 20 times larger than the ones used in training. We also test on graphsobtained from real-world genomic data—that of a lambda phage and E. coli.1 IntroductionRecently, we have witnessed a wide adoption of machine learning in graph representations andapproaches to problem-solving on graph-structured data. In the center of this research are usuallygraph neural networks (GNNs) [1, 2] which are applied to various problems on graphs, such as findingthe shortest path [3, 4], traveling salesman [5, 6], and bipartite matching [7]. Many of the earlierapproaches, however, give the model complete freedom to determine how to obtain the solution fromraw inputs, by using only the final algorithmic solutions as the supervision signal.This was addressed by Veličković et al. [8], where the authors have provided explicit step-by-stepguidance for learning algorithm execution. Moreover, they recognized that many of the classicalalgorithms share related subroutines, which enabled them not only to teach a model to execute severalalgorithms simultaneously, but also to demonstrate positive knowledge transfer between learningdifferent algorithms.While all the aforementioned research, and plenty of other work, made a significant breakthroughin the domain of machine learning on graph-structured problems, not a lot of work has yet beendone on real-world systems. In this work, motivated by the contribution of Veličković et al. [8],application to bipartite matching by Georgiev and Lió [7], and utilizing algorithmic reasoning inreinforcement learning [9], we take a step in that direction and apply the neural graph algorithmexecution approach on the problem of de novo genome assembly, which to this day remains as oneof the most difficult problems in bioinformatics. De novo genome assembly aims at reconstructingthe original genome sequence from hundreds of thousands of relatively short sequences called reads,without any knowledge of what the original sequence looks like. Ability to reconstruct genomes withhigh accuracy would have numerous applications in biology and precision medicine. In the idealcase, the problem is based on finding a Hamiltonian path through a complex directed graph, whichis known to be an NP-complete problem. Due to long repetitive regions in genomes and artifacts1st Workshop on Learning Meets Combinatorial Algorithms @ NeurIPS 2020, Vancouver, Canada.arXiv:2011.05013v1  [cs",
        "arxiv": "2011.05013",
        "doi": null,
        "file_name": "2011.05013.pdf",
        "file_path": "./sources/2011.05013/2011.05013.pdf",
        "title": "A step towards neural genome assembly",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/lvrcek/NeuralLayout"
                }
            ]
        }
    },
    "2011.05402": {
        "abstract": "AbstractThere is little to no data available to build nat-ural language processing models for most en-dangered languages. However, textual datain these languages often exists in formats thatare not machine-readable, such as paper booksand scanned images. In this work, we addressthe task of extracting text from these resources.We create a benchmark dataset of transcrip-tions for scanned books in three critically en-dangered languages and present a systematicanalysis of how general-purpose OCR toolsare not robust to the data-scarce setting of en-dangered languages. We develop an OCR post-correction method tailored to ease training inthis data-scarce setting, reducing the recogni-tion error rate by 34% on average across thethree languages.11 IntroductionNatural language processing (NLP) systems existfor a small fraction of the world’s over 6,000 liv-ing languages, the primary reason being the lackof resources required to train and evaluate models.Technological advances are concentrated on lan-guages that have readily available data, and mostother languages are left behind (Joshi et al., 2020).This is particularly notable in the case of endan-gered languages, i.e., languages that are in dangerof becoming extinct due to dwindling numbers ofnative speakers and the younger generations shift-ing to using other languages. For most endangeredlanguages, finding any data at all is challenging.In many cases, natural language text in theselanguages does exist. However, it is locked awayin formats that are not machine-readable — pa-per books, scanned images, and unstructured webpages. These include books from local publishing†: Work done at Carnegie Mellon University.1Code and data are available at https://shrutirij.github.io/ocr-el/.(a) Ainu (left) – Japanese (right)(b) Griko (top) – Italian (bottom)(c) Yakkha (top) – Nepali (middle) – English (bottom)(d) Handwritten Shangaji – typed English glossesText 31: cashew nuts Amina Sharaama explains how to make a sauce of green cashew nuts. Recorded on the 26th of April 2007. notebooks: p. 1230  31.1 ",
        "arxiv": "2011.05402",
        "doi": "10.18653/v1/2020.emnlp-main.478",
        "file_name": "2011.05402.pdf",
        "file_path": "./sources/2011.05402/2011.05402.pdf",
        "title": "OCR Post Correction for Endangered Language Texts",
        "urls": {}
    },
    "2102.0208": {
        "abstract": "AbstractWe introduce a top-down approach to dis-course parsing that is conceptually simplerthan its predecessors (Kobayashi et al., 2020;Zhang et al., 2020). By framing the task as asequence labelling problem where the goal isto iteratively segment a document into individ-ual discourse units, we are able to eliminatethe decoder and reduce the search space forsplitting points. We explore both traditionalrecurrent models and modern pre-trained trans-former models for the task, and additionally in-troduce a novel dynamic oracle for top-downparsing. Based on the Full metric, our pro-posed LSTM model sets a new state-of-the-artfor RST parsing.11 IntroductionDiscourse analysis involves the modelling of thestructure of text in a document. It provides a sys-tematic way to understand how texts are segmentedhierarchically into discourse units, and the relation-ships between them. Unlike syntax parsing whichmodels the relationship of words in a sentence, dis-course parsing operates at the document-level, andaims to explain the flow of writing. Studies havefound that discourse parsing is beneficial for down-stream NLP tasks including document-level senti-ment analysis (Bhatia et al., 2015) and abstractivesummarization (Koto et al., 2019).Rhetorical Structure Theory (RST; Mann andThompson (1988)) is one of the most widely useddiscourse theories in NLP (Hernault et al., 2010;Feng and Hirst, 2014; Ji and Eisenstein, 2014; Liet al., 2016; Yu et al., 2018). RST organizes textspans into a tree, where the leaves represent thebasic unit of discourse, known as elementary dis-course units (EDUs). EDUs are typically clauses1Code and trained models: https://github.com/fajri91/NeuralRST-TopDownEDU-1EDU-4EDU-2 EDU-3elabEDU-1:\tRoy\tE.\tParrott,\tthe\tcompany's\tpresident\tand\tchief\t\t\t\t\t\t\t\t\t\t\t\t\t\t\toperating\tofficer\tsince\tSept.\t1,\twas\tnamed\tto\tits\tboard.EDU-2:\tThe\tappointment\tincreased\tthe\tnumber\tof\tdirectors\tto\t10,EDU-3:\tthree\tof\twhom\tare\tcompany\temployees.EDU-4:\tSimpson\tis\tan\tauto\tparts\tmaker.elabelab",
        "arxiv": "2102.0208",
        "doi": "10.18653/v1/2021.eacl-main.60",
        "file_name": "2102.0208.pdf",
        "file_path": "./sources/2102.0208/2102.0208.pdf",
        "title": "Top-down Discourse Parsing via Sequence Labelling",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/yunan4nlp/NNDisParser"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/fajri91/NeuralRST-TopDown"
                }
            ]
        }
    },
    "2102.0401": {
        "abstract": "ABSTRACTSparsity in Deep Neural Networks (DNNs) has been widely studied to compressand accelerate the models on resource-constrained environments. It can be gen-erally categorized into unstructured fine-grained sparsity that zeroes out multipleindividual weights distributed across the neural network, and structured coarse-grained sparsity which prunes blocks of sub-networks of a neural network. Fine-grained sparsity can achieve a high compression ratio but is not hardware friendlyand hence receives limited speed gains. On the other hand, coarse-grained sparsitycannot concurrently achieve both apparent acceleration on modern GPUs and de-cent performance. In this paper, we are the first to study training from scratch anN:M fine-grained structured sparse network, which can maintain the advantages ofboth unstructured fine-grained sparsity and structured coarse-grained sparsity si-multaneously on specifically designed GPUs. Specifically, a 2 : 4 sparse networkcould achieve 2× speed-up without performance drop on Nvidia A100 GPUs.Furthermore, we propose a novel and effective ingredient, sparse-refined straight-through estimator (SR-STE), to alleviate the negative influence of the approxi-mated gradients computed by vanilla STE during optimization. We also define ametric, Sparse Architecture Divergence (SAD), to measure the sparse network’stopology change during the training process. Finally, We justify SR-STE’s ad-vantages with SAD and demonstrate the effectiveness of SR-STE by performingcomprehensive experiments on various tasks. Source codes and models are avail-able at https://github.com/NM-sparsity/NM-sparsity.1 INTRODUCTIONDeep neural networks (DNNs) have shown promising performances on various tasks including com-puter vision, natural language processing, speech recognition, etc. However, a DNN usually comeswith a large number of learnable parameters, ranging from millions of to even billions of (e.g.,GPT-3 (Brown et al., 2020)), making the DNN model burdensome and difficult to be applied toreal-world deployments. Therefore, researchers began to investigate how to speed up and compressDNNs via various methods such as knowledge distillation (Hinton et al., 2015), quantization (Jacobet al., 2018; Zhou et al., 2017), designing efficient model architectures (Howard et al., 2017), andstructured sparsity (Wen et al., 2016; Li et al., 2016).In this paper, we focus on the problem of sparsifying DNNs. Sparsity in DNNs can be categorizedinto unstructured sparsity and structured sparsity. Unstructured sparsity prunes individual weightsat any location, which is fine-grained and can achieve extremely high compression ratio (Han et al.,2015; Guo et al., 2016). However, unstructured sparsity struggles to take advantage of vector-processing architectures, which increases latency due to dependent sequences of reads (Nvidia,2020). Compared with unstructured sparsity, structured sparsity is more friendly to hardware, espe-cially for block pruning (Wang et al., 2019), kernel shape sparsity (Tan et al., 2020) or channel andfilter pruning (Li et al., 2016; Wen et al., 2016). Although structured sparsity can speed up DNNs oncommodity hardware, it hurts model performance more significantly than unstructured fine-grainedsparsity. For example, ResNet-50 network generated by unstructured pruning can achieve a 5.96×compression ratio, with the same accuracy as the original network, but it can only achieve 1× com-∗The first two authors equally contribute to this paper.1arXiv:2102.",
        "arxiv": "2102.0401",
        "doi": null,
        "file_name": "2102.0401.pdf",
        "file_path": "./sources/2102.0401/2102.0401.pdf",
        "title": "Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/princeton-vl/RAFT"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/NVIDIA/apex"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/anonymous-NM-sparsity/NM-sparsity"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/NM-sparsity/NM-sparsity"
                }
            ]
        }
    },
    "2102.07359": {
        "abstract": "ABSTRACTElectric Vehicle (EV) has become a preferable choice in the moderntransportation system due to its environmental and energy sus-tainability. However, in many large cities, EV drivers often fail tofind the proper spots for charging, because of the limited charg-ing infrastructures and the spatiotemporally unbalanced charg-ing demands. Indeed, the recent emergence of deep reinforcementlearning provides great potential to improve the charging expe-rience from various aspects over a long-term horizon. In this pa-per, we propose a framework, named Multi-Agent Spatio-TemporalReinforcement Learning (Master), for intelligently recommendingpublic accessible charging stations by jointly considering variouslong-term spatiotemporal factors. Specifically, by regarding eachcharging station as an individual agent, we formulate this prob-lem as a multi-objective multi-agent reinforcement learning task.We first develop a multi-agent actor-critic framework with thecentralized attentive critic to coordinate the recommendation be-tween geo-distributed agents. Moreover, to quantify the influenceof future potential charging competition, we introduce a delayedaccess strategy to exploit the knowledge of future charging compe-tition during training. After that, to effectively optimize multiplelearning objectives, we extend the centralized attentive critic tomulti-critics and develop a dynamic gradient re-weighting strategyto adaptively guide the optimization direction. Finally, extensiveexperiments on two real-world datasets demonstrate that Masterachieves the best comprehensive performance compared with ninebaseline approaches.KEYWORDSCharging station recommendation,multi-agent reinforcement learn-ing, multi-objective optimizationACM Reference Format:Weijia Zhang1†, Hao Liu2∗, Fan Wang3, Tong Xu1, Haoran Xin1, DejingDou2, Hui Xiong4∗. 2021. Intelligent Electric Vehicle Charging Recommen-dation Based on Multi-Agent Reinforcement Learning. In Proceedings of the∗ Corresponding author.† The research was done when the first author was an intern in Baidu Research underthe supervision of the second author.This paper is published under the Creative Commons Attribution 4.0 International(CC-BY 4.0) license. Authors reserve their rights to disseminate the work on theirpersonal and corporate Web sites with the appropriate attribution.WWW ’21, April 19–23, 2021, Ljubljana, Slovenia© 2021 IW3C2 (International World Wide Web Conference Committee), publishedunder Creative Commons CC-BY 4.0 License.ACM ISBN 978-1-4503-8312-7/21/04.https://doi.org/10.1145/3442381.3449934Web Conference 2021 (WWW ’21), April 19–23, 2021, Ljubljana, Slovenia.ACM,New York, NY, USA, 12 pages. https://doi.org/10.1145/3442381.34499341 INTRODUCTIONDue to the low-carbon emission and energy efficiency, Electricvehicles (EVs) are emerging as a favorable choice in the modern",
        "arxiv": "2102.07359",
        "doi": "10.1145/3442381.3449934",
        "file_name": "2102.07359.pdf",
        "file_path": "./sources/2102.07359/2102.07359.pdf",
        "title": "Intelligent Electric Vehicle Charging Recommendation Based on Multi-Agent Reinforcement Learning",
        "urls": {
            "git": [
                {
                    "#_appearances": 1,
                    "url": "https://github.com/Vvrep/MASTER-electric_vehicle_charging_recommendation"
                }
            ]
        }
    },
    "2102.13173": {
        "abstract": "Abstract:  The population of the elderly people has kept increasing rapidly over the world in the past decades. Solutions that are able to effectively support the elderly people to live independently at their home are thus urgently needed. Ambient assisted living (AAL) aims to provide products and services with ambient intelligence to build a safe environment around people in need. With the high prevalence of multiple chronic diseases, the elderly people often need different levels of care management to prolong independent living at home. An effective AAL system should provide the required clinical support as an extension to the services provided in hospitals. Following the rapid growth of available data, together with the wide application of machine learning technologies, we are now able to build intelligent ambient assisted systems to fulfil such a request. This paper discusses different levels of intelligence in AAL. We also introduce our solution for building an intelligent AAL system with the discussed technologies. Taking semantic web technology as its backbone, such an AAL system is able to aggregate information from different sources, solve the semantic gap between different data sources, and perform adaptive and personalized carepath management based on the ambient environment.  Keywords: Ambient assisted living, intelligent system, adaptive carepath management, semantic web    2 1. Introduction   As well known, the increase of the elderly population is a significant demographic and socioeconomic concern. Already very high in 2019 (9% of people are aged 65 or over), projections foresee a world-wide rate of about 16% in 2050 and of about 25% when restricting the focus on Europe and North America [58]. As a consequence of this threat, several actions have been planned, starting with the United Nation’s first World Assembly on Ageing in 1982. A major concept was introduced in 2005: Ambient Assisted Living (AAL) [4, 5]. AAL aims at assisting the aging people living well in their own home by increasing their autonomy and self-confidence. Following this concept, the AAL programme [6] was built up. Said program has funded over 220 projects since 2008 and is still active today. The introduction of AAL is a consequence of this state of affairs: rather than purely relying on human assistance, which is scarcer because of global aging, AAL capitalizes on the ever-increasing performance and pervasiveness of computer and networking systems in order to supply the aging population with some form of \"smart\" assistance so as to increase the autonomy, safety, and comfort of the aging ones. Already in our paper [7] we discussed the promises and challenges of this family of approaches, also highlighting the need for an evolution of the concept of AAL. This need was confirmed by the emergence of new and more advanced forms of intelligence in the past decade. Moreover, since the elderly populations are often suffering from multiple chronic diseases, it is important that an intelligent AAL system could provide clinical support as an extension of the care support provided in hospitals. The AAL systems are evolving to support more advanced application in meeting such a requirement. We consider that the evolution of AAL is in general following the three phases below: In the first phase, AAL was merely intended as the establishment of a ICT-backed \"safety net\" around the patient. This phase is characterized by the wide application of different types of sensors, and the AAL system detects dangerous situations around the person in assistance, such as a fall event. A second phase was characterised by the rapid adoption of Electronic Healthcare Records (EHR) in combination with sensor data from smart body devices such as smart watches and the availability of health data, in structured or unstructured format. This led to predictive AAL services, able to anticipate the need for intervention thanks to predictions backed by machine learning solutions. 3 The third phase, which we are currently experiencing, takes a step further and formulates AAL ",
        "arxiv": "2102.13173",
        "doi": null,
        "file_name": "2102.13173.pdf",
        "file_path": "./sources/2102.13173/2102.13173.pdf",
        "title": "Perspectives and solutions towards intelligent ambient assisted living systems",
        "urls": {
            "git": [
                {
                    "#_appearances": 3,
                    "url": "https://github.com/hongsun502/wstLogic"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/hongsun502/AALDemo"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/josd/eye"
                }
            ]
        }
    },
    "2103.04388": {
        "abstract": "Abstract—This paper presents a coverage-guided grammar-based fuzzing technique for automatically synthesizing a corpusof concise test inputs. We walk-through a case study of acompiler designed for education and the corresponding problemof generating meaningful test cases to provide to students.The prior state-of-the-art solution is a combination of fuzzingand test-case reduction techniques such as variants of delta-debugging. Our key insight is that instead of attempting tominimize convoluted fuzzer-generated test inputs, we can insteadgrow concise test inputs by construction using a form of iterativedeepening. We call this approach bonsai fuzzing. Experimentalresults show that bonsai fuzzing can generate test corpora havinginputs that are 16–45% smaller in size on average as comparedto a fuzz-then-reduce approach, while achieving approximatelythe same code coverage and fault-detection capability.Index Terms—test-case generation, grammar-based testing,fuzz testing, small scope hypothesis, test-case reductionI. INTRODUCTIONThis paper describes a new technique for automaticallygenerating a concise corpus of test inputs having a well-defined syntax and non-trivial semantics (e.g. for a compiler).This project originated when the authors were faced withthe task of generating a test corpus for use in an undergraduatecompilers course. The course project targets the ChocoPy pro-gramming language [1]. ChocoPy is a statically typed subsetof Python, designed specifically for education. In a ChocoPy-based course, students are expected to build a compiler in Javathat statically checks and then translates ChocoPy programsto RISC-V assembly. Student projects can be autograded bycomparing their compilers’ output at various stages—parser,type checker, and code generator—with the correspondingoutput produced by a reference implementation. When startingtheir project, students are provided with a suite of ChocoPy testprograms and the autograder, which together serve as a partialexecutable specification. This workflow simulates test-drivendevelopment, while also enabling students to continuously getfeedback about their progress. For instructors, writing testcases to validate every language feature is a tedious task; wewanted to automatically synthesize such a test corpus. Thispaper describes the technique we developed for this purpose.In particular, we focus on the problem of automatically gener-ating test cases that exercise the typechecker, since generatingwell-typed programs is known to be a difficult problem [2]–[5].This task presents two conflicting challenges: (1) the gen-erated test suite must be comprehensive in covering variousF2,2,2F2,2,1 F2,1,2 F1,2,2F2,1,1 F1,2,1 F1,1,2F1,1,1",
        "arxiv": "2103.04388",
        "doi": "10.1109/icse43902.2021.00072",
        "file_name": "2103.04388.pdf",
        "file_path": "./sources/2103.04388/2103.04388.pdf",
        "title": "Growing a Test Corpus with Bonsai Fuzzing",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/vasumv/bonsai-fuzzing"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/renatahodovan/picireny"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/renatahodovan/picire"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/google/closure-compiler"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/google/AFL"
                }
            ]
        }
    },
    "2103.06263": {
        "abstract": "AbstractSemi-discrete optimal transport problems, which evaluate the Wasserstein distance between a discreteand a generic (possibly non-discrete) probability measure, are believed to be computationally hard. Eventhough such problems are ubiquitous in statistics, machine learning and computer vision, however, thisperception has not yet received a theoretical justification. To fill this gap, we prove that computing theWasserstein distance between a discrete probability measure supported on two points and the Lebesguemeasure on the standard hypercube is already #P-hard. This insight prompts us to seek approximatesolutions for semi-discrete optimal transport problems. We thus perturb the underlying transportationcost with an additive disturbance governed by an ambiguous probability distribution, and we introducea distributionally robust dual optimal transport problem whose objective function is smoothed with themost adverse disturbance distributions from within a given ambiguity set. We further show that smooth-ing the dual objective function is equivalent to regularizing the primal objective function, and we identifyseveral ambiguity sets that give rise to several known and new regularization schemes. As a byproduct,we discover an intimate relation between semi-discrete optimal transport problems and discrete choicemodels traditionally studied in psychology and economics. To solve the regularized optimal transportproblems efficiently, we use a stochastic gradient descent algorithm with imprecise stochastic gradientoracles. A new convergence analysis reveals that this algorithm improves the best known convergenceguarantee for semi-discrete optimal transport problems with entropic regularizers.Keywords: optimal transport, Wasserstein distance, complexity, #P-hardness, discrete choice models,distributionally robust optimization, stochastic gradient descent algorithms1. IntroductionOptimal transport theory has a long and distinguished history in mathematics dating back to the seminalwork of Monge [1781] and Kantorovich [1942]. While originally envisaged for applications in civil engi-neering, logistics and economics, optimal transport problems provide a natural framework for comparingprobability measures and have therefore recently found numerous applications in statistics and machinelearning. Indeed, the minimum cost of transforming a probability measure 𝜇 on 𝒳 to some other proba-bility measure 𝜈 on 𝒴 with respect to a prescribed cost function on 𝒳 × 𝒴 can be viewed as a measure1arXiv:2103.06263v2  [cs.LG]  29 Apr 2022",
        "arxiv": "2103.06263",
        "doi": null,
        "file_name": "2103.06263.pdf",
        "file_path": "./sources/2103.06263/2103.06263.pdf",
        "title": "Semi-Discrete Optimal Transport: Hardness, Regularization and Numerical Solution",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/RAO-EPFL/Semi-Discrete-Smooth-OT"
                }
            ]
        }
    },
    "2103.11405": {
        "abstract": "AbstractNon-autoregressive Transformer is a promis-ing text generation model. However, cur-rent non-autoregressive models still fall be-hind their autoregressive counterparts in trans-lation quality. We attribute this accuracy gapto the lack of dependency modeling among de-coder inputs. In this paper, we propose CNAT,which learns implicitly categorical codes as la-tent variables into the non-autoregressive de-coding. The interaction among these categor-ical codes remedies the missing dependenciesand improves the model capacity. Experimentresults show that our model achieves compara-ble or better performance in machine transla-tion tasks, compared with several strong base-lines.1 IntroductionNon-autoregressive Transformer (NAT, Gu et al.,2018; Wang et al., 2019; Lee et al., 2018;Ghazvininejad et al., 2019) is a promising text gen-eration model for machine translation. It introducesthe conditional independent assumption among thetarget language outputs and simultaneously gener-ates the whole sentence, bringing in a remarkableefficiency improvement (more than 10× speed-up)versus the autoregressive model. However, the NATmodels still lay behind the autoregressive models interms of BLEU (Papineni et al., 2002) for machinetranslation. We attribute the low-quality of NATmodels to the lack of dependencies modeling forthe target outputs, making it harder to model thegeneration of the target side translation.A promising way is to model the dependenciesof the target language by the latent variables. A lineof research works (Kaiser et al., 2018; Roy et al.,2018; Shu et al., 2019; Ma et al., 2019) introducelatent variable modeling to the non-autoregressiveTransformer and improves translation quality. Thelatent variables could be regarded as the spring-board to bridge the modeling gap, introducingmore informative decoder inputs than the previ-ously copied inputs. More specifically, the latent-variable based model first predicts a latent vari-able sequence conditioned on the source represen-tation, where each variable represents a chunk ofwords. The model then simultaneously could gen-erate all the target tokens conditioning on the latentsequence and the source representation since thetarget dependencies have been modeled into the",
        "arxiv": "2103.11405",
        "doi": "10.18653/v1/2021.naacl-main.458",
        "file_name": "2103.11405.pdf",
        "file_path": "./sources/2103.11405/2103.11405.pdf",
        "title": "Non-Autoregressive Translation by Learning Target Categorical Codes",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/pytorch/fairseq"
                }
            ]
        }
    },
    "2104.06523": {
        "abstract": "AbstractMining health data can lead to faster medical decisions, improvement in the quality of treatment, diseaseprevention, reduced cost, and it drives innovative solutions within the healthcare sector. However, healthdata is highly sensitive and subject to regulations such as the General Data Protection Regulation (GDPR),which aims to ensure patient’s privacy. Anonymization or removal of patient identifiable information, thoughthe most conventional way, is the first important step to adhere to the regulations and incorporate privacyconcerns. In this paper, we review the existing anonymization techniques and their applicability to varioustypes (relational and graph-based) of health data. Besides, we provide an overview of possible attacks onanonymized data. We illustrate via a reconstruction attack that anonymization though necessary, is notsufficient to address patient privacy and discuss methods for protecting against such attacks. Finally, wediscuss tools that can be used to achieve anonymization.Availability: Code is available: https://github.com/iyempissy/anonymization-reconstruction-attack.Contact: iyiola@l3s.deKeywords: Privacy, Healthcare data, Anonymization, Attacks.1 IntroductionWith the increasing adoption of healthcare information technology (HIT)by medical institutions, the generation and capture of healthcare-relateddata have been increasing rapidly in the past years. The application ofartificial intelligence (AI) techniques already gives a glimpse of potentialimprovements ranging from lung cancer nodules detection in CT scansto disease prediction and treatment [61, 80, 97]. The challenge though isthat these AI models are usually data hungry and require large amountsof data for training. Health care data, on the other hand, contains highlysensitive patient information and cannot be easily shared. The reluctancebehind releasing data query/analysis tools build on health care data canbe further justified by the fundamental law of information recovery [23]which states that when a data source is queried multiple times and it returnsoverly accurate information for each query, the underlying data can bereconstructed partially or in full. Therefore, health data need to be protectedagainst such leakage to ensure patient’s privacy.Privacy can be applied to health data at different levels. For instance,at the data collection phase, randomization in the form of noise is usuallyadded. Federated learning, homomorphic encryption, and secure multi-party computation can be applied at the data distribution phase. In thiswork we focus on anonymization which is used to achieve privacy atthe data publication phase. Consequently, regulations such as the GDPR[81] require data anonymization or removal of personal or sensitiveidentification information before processing any knowledge extractiontask or query. We provide a comprehensive review of anonymization inhealthcare data focusing on the three main aspects: (i) anonymizationmodels and techniques (ii) attacks and defenses proposed for anonymizeddata (iii) available tools for anonymizing data.In the first part, we introduce basic concepts and discuss existinganonymization techniques and their applicability to various types of healthdata. In particular, we differentiate between two different health datatypes: (i) relational and (ii) graph-based. Relational (same-site) patient datarepresents patient visits and medical diagnosis from a single hospital, andgraph-based health data could include, for example, transmission networkor epidemiological graph where the nodes are the patients and the edgesare the interaction between the patients.",
        "arxiv": "2104.06523",
        "doi": "10.1089/big.2021.0169",
        "file_name": "2104.06523.pdf",
        "file_path": "./sources/2104.06523/2104.06523.pdf",
        "title": "A Review of Anonymization for Healthcare Data",
        "urls": {
            "git": [
                {
                    "#_appearances": 1,
                    "url": "https://github.com/iyempissy/anonymization-reconstruction-attack"
                }
            ]
        }
    },
    "2104.12087": {
        "abstract": "Abstract—For image inpainting, the convolutional neural networks (CNN) in previous methods often adopt standard convolutionaloperator, which treats valid pixels and holes indistinguishably. As a result, they are limited in handling irregular holes and tend toproduce color-discrepant and blurry inpainting result. Partial convolution (PConv) copes with this issue by conducting maskedconvolution and feature re-normalization conditioned only on valid pixels, but the mask-updating is handcrafted and independent withimage structural information. In this paper, we present an edge-guided learnable bidirectional attention map (Edge-LBAM) forimproving image inpainting of irregular holes with several distinct merits. Instead of using a hard 0-1 mask, a learnable attention mapmodule is introduced for learning feature re-normalization and mask-updating in an end-to-end manner. Learnable reverse attentionmaps are further proposed in the decoder for emphasizing on filling in unknown pixels instead of reconstructing all pixels. Motivated bythat the filling-in order is crucial to inpainting results and largely depends on image structures in exemplar-based methods, we furthersuggest a multi-scale edge completion network to predict coherent edges. Our Edge-LBAM method contains dual procedures,including structure-aware mask-updating guided by predict edges and attention maps generated by masks for feature re-normalization.Extensive experiments show that our Edge-LBAM is effective in generating coherent image structures and preventing colordiscrepancy and blurriness, and performs favorably against the state-of-the-art methods in terms of qualitative metrics and visualquality. The source code and pre-trained models are available at https://github.com/wds1998/Edge-LBAM.Index Terms—Image Inpainting, Convolutional Networks, Attention, Edge DetectionF1 INTRODUCTIONImage inpainting [7] filling in missing pixels (i.e., holes)with plausible hypotheses is a challenging low level visiontask with many real-world applications, e.g., distractingobject removal, occluded region completion, and imageediting. Traditional image inpainting methods [8], [9], [10]usually adopt exemplar-based techniques which progres-sively fill in holes by searching and copying similar patchesfrom known regions. However, exemplar-based inpaintingis limited in capturing high-level semantics and may failin hallucinating complex and non-repetitive structures. Incontrast, deep convolutional networks (CNNs) are powerfulin learning high-level semantics, and have greatly facilitatedthe advance of image inpainting in the recent few years [1],[11], [12], [13], [14], [15].Existing CNN-based approaches [1], [11], [12], [13], [14],[15] have achieved notable progress in the inpainting ofrectangular regions. However, most of them usually adoptstandard convolution operation which treat valid pixels andholes indistinguishably, making them limited in handlingirregular holes and leading to result with color discrepancyand blurriness (see Fig. 1(c)). The methods [13], [14], [15]exploit the mask of holes to guide the feature propaga-• D. Wang and C. Xie are with the School of Computer Science andTechnology, Harbin Institute of Technology, Harbin, China, e-mail:(wds02467@163.com; viousxie@outlook.com).*The two authors contributeequally.• W. Zuo and S. Liu are with the School of Computer Science and Technol-ogy, Harbin Institute of Technology, Harbin, 150001, China and are alsowith Peng Cheng Lab, Shenzhen, China, e-mail: (cswmzuo@gmail.com,shliu@hit.edu.cn).• Z. Niu is with the DAMO Academy at Alibaba Group, Hangzhou, China,e-mail: (zhenxing.nzx@alibaba-inc.com).Manuscript received xxx; revised xxx (Corresponding author: Wangmeng",
        "arxiv": "2104.12087",
        "doi": null,
        "file_name": "2104.12087.pdf",
        "file_path": "./sources/2104.12087/2104.12087.pdf",
        "title": "Image Inpainting with Edge-guided Learnable Bidirectional Attention Maps",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/wds1998/Edge-LBAM"
                }
            ]
        }
    },
    "2105.04459": {
        "abstract": "AbstractLearning maps between data samples is fundamental. Ap-plications range from representation learning, image trans-lation and generative modeling, to the estimation of spatialdeformations. Such maps relate feature vectors, or map be-tween feature spaces. Well-behaved maps should be regular,which can be imposed explicitly or may emanate from thedata itself. We explore what induces regularity for spatialtransformations, e.g., when computing image registrations.Classical optimization-based models compute maps betweenpairs of samples and rely on an appropriate regularizerfor well-posedness. Recent deep learning approaches haveattempted to avoid using such regularizers altogether byrelying on the sample population instead. We explore if it ispossible to obtain spatial regularity using an inverse consis-tency loss only and elucidate what explains map regularityin such a context. We find that deep networks combinedwith an inverse consistency loss and randomized off-gridinterpolation yield well behaved, approximately diffeomor-phic, spatial transformations. Despite the simplicity of thisapproach, our experiments present compelling evidence, onboth synthetic and real data, that regular maps can be ob-tained without carefully tuned explicit regularizers, whileachieving competitive registration performance.1. MotivationLearning maps between feature vectors or spaces is animportant task. Feature vector maps are used to improverepresentation learning [7], or to learn correspondences innatural language processing [4]. Maps between spaces areimportant for generative models when using normalizingflows [24] (to map between a simple and a complex probabil-ity distribution), or to determine spatial correspondences be-tween images, e.g., for optical flow [16] to determine motionfrom videos [12], depth estimation from stereo images [25],or medical image registration [40, 41].Regular maps are typically desired; e.g., diffeomorphicarXiv:2105.04459v3  [cs.CV",
        "arxiv": "2105.04459",
        "doi": "10.1109/iccv48922.2021.00338",
        "file_name": "2105.04459.pdf",
        "file_path": "./sources/2105.04459/2105.04459.pdf",
        "title": "ICON: Learning Regular Maps Through Inverse Consistency",
        "urls": {
            "git": [
                {
                    "#_appearances": 1,
                    "url": "https://github.com/uncbiag/ICON"
                }
            ]
        }
    },
    "2105.12108": {
        "abstract": "Abstract. In order to investigate the origin of the ongoing tension between the amplitudeof matter fluctuations measured by weak lensing experiments at low redshifts and the valueinferred from the cosmic microwave background anisotropies, we reconstruct the evolution ofthis amplitude from z ∼ 2 using existing large-scale structure data. To do so, we decouplethe linear growth of density inhomogeneities from the background expansion, and constrainits redshift dependence making use of a combination of 6 different data sets, including cos-mic shear, galaxy clustering and CMB lensing. We analyze these data under a consistentharmonic-space angular power spectrum-based pipeline. We show that current data con-strain the amplitude of fluctuations mostly in the range 0.2 < z < 0.7, where it is lower thanpredicted by Planck. This difference is mostly driven by current cosmic shear data, althoughthe growth histories reconstructed from different data combinations are consistent with eachother, and we find no evidence of systematic deviations in any particular experiment. Inspite of the tension with Planck, the data are well-described by the ΛCDM model, albeitwith a lower value of S8 ≡ σ8(Ωm/0.3)0.5. As part of our analysis, we find constraints onthis parameter of S8 = 0.7781± 0.0094 (68% confidence level), reaching almost percent-levelerrors comparable with CMB measurements, and 3.4σ away from the value found by Planck.arXiv:2105.12108v2  [astro-ph.CO]  9 Sep 2021mailto:carlos.garcia-garcia@physics.ox.ac.ukContents1 Introduction 12 Theory 32.1 Projected anisotropies 32.2 Growth reconstruction 53 Data 73.1 DES Y1 galaxy clustering and weak lensing 93.1.1 Galaxy clustering 93.1.2 Galaxy shear 10",
        "arxiv": "2105.12108",
        "doi": null,
        "file_name": "2105.12108.pdf",
        "file_path": "./sources/2105.12108/2105.12108.pdf",
        "title": "The growth of density perturbations in the last $\\sim$10 billion years from tomographic large-scale structure data",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://gitlab.com/qianjunhang/desi-legacy-survey-cross-correlations"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/xC-ell/growth-history"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/LSSTDESC/CCL"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/carlosggarcia/montepython_public"
                }
            ]
        }
    },
    "2105.13649": {
        "abstract": "Abstract— Deep neural networks (DNNs) play an increasinglyimportant role in various computer systems. In order to createthese networks, engineers typically specify a desired topology, andthen use an automated training algorithm to select the network’sweights. While training algorithms have been studied extensivelyand are well understood, the selection of topology remains a formof art, and can often result in networks that are unnecessarilylarge — and consequently are incompatible with end devices thathave limited memory, battery or computational power. Here, wepropose to address this challenge by harnessing recent advancesin DNN verification. We present a framework and a methodologyfor discovering redundancies in DNNs — i.e., for finding neuronsthat are not needed, and can be removed in order to reduce thesize of the DNN. By using sound verification techniques, we canformally guarantee that our simplified network is equivalent tothe original, either completely, or up to a prescribed tolerance.Further, we show how to combine our technique with slicing,which results in a family of very small DNNs, which are togetherequivalent to the original. Our approach can produce DNNsthat are significantly smaller than the original, rendering themsuitable for deployment on additional kinds of systems, and evenmore amenable to subsequent formal verification. We provide aproof-of-concept implementation of our approach, and use it toevaluate our techniques on several real-world DNNs.I. INTRODUCTIONThe wide-spread adoption of deep learning [17] has causeda significant leap forward in many domains within computerscience. Deep neural networks (DNNs) have now becomethe state of the art solution for a myriad of real-worldproblems, such as game playing [39], image recognition [40],and autonomous vehicles [5], [25]. This trend is likely tocontinue and intensify, thus creating an urgent need for toolsand techniques to analyze and manipulate DNNs.A part of the appeal of DNNs is that they are produced in amostly automated way. In order to create a DNN for a particu-lar task at hand, engineers first specify the network architecture— specifically, the number of layers in the network, the sizeand type of each layer, and the inter-layer connections. Then,they invoke an automated training algorithm for assigningweights to the network’s edges [17]. While the automatedtraining process has been extensively studied and is generallywell understood [17], the choice of network architecture isstill performed according to various rules of thumb, and isconsidered a form of art. This can often lead to a choice ofarchitecture that is wasteful — i.e., which results in a large[*] This is the extended version of a paper with the same title that is aboutto appear in FMCAD 2021. See https://fmcad.org/DNN, whereas a smaller DNN could have achieved similaraccuracy [15], [19], [23]. For DNNs intended to run on deviceswith limited resources (e.g., mobile phones, or embedded",
        "arxiv": "2105.13649",
        "doi": null,
        "file_name": "2105.13649.pdf",
        "file_path": "./sources/2105.13649/2105.13649.pdf",
        "title": "Pruning and Slicing Neural Networks using Formal Verification",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/vbcrlf/redy"
                }
            ]
        }
    },
    "2106.011": {
        "abstract": "Abstract Background and Objective: During lung can-cer radiotherapy, the position of infrared reflective ob-jects on the chest can be recorded to estimate the tu-mor location. However, radiotherapy systems have a la-tency inherent to robot control limitations that impedesthe radiation delivery precision. Prediction with onlinelearning of recurrent neural networks (RNN) allows foradaptation to non-stationary respiratory signals, butclassical methods such as real-time recurrent learning(RTRL) and truncated backpropagation through timeare respectively slow and biased. This study investi-gates the capabilities of unbiased online recurrent op-timization (UORO) to forecast respiratory motion andenhance safety in lung radiotherapy.Methods: We used nine observation records of the three-dimensional (3D) position of three external markers onthe chest and abdomen of healthy individuals breath-ing during intervals from 73s to 222s. The sampling fre-quency was 10Hz, and the amplitudes of the recordedtrajectories range from 6mm to 40mm in the superior-inferior direction. We forecast the 3D location of eachmarker simultaneously with a horizon value (the timeinterval in advance for which the prediction is made) be-tween 0.1s and 2.0s, using an RNN trained with UORO.We compare its performance with an RNN trained withRTRL, least mean squares (LMS), and offline linear re-Michel PohlThe University of Tokyo, 113-8654 Tokyo, JapanE-mail: michel.pohl@centrale-marseille.frMitsuru UesakaJapan Atomic Energy Commission, 100-8914 Tokyo, JapanHiroyuki Takahashi · Kazuyuki DemachiThe University of Tokyo, 113-8654 Tokyo, JapanRitu Bhusal ChhatkuliNational Institutes for Quantum and Radiological Scienceand Technology, 263-8555 Chiba, Japangression. We provide closed-form expressions for quanti-ties involved in the loss gradient calculation in UORO,thereby making its implementation efficient. Trainingand cross-validation were performed during the firstminute of each sequence.Results: On average over the horizon values consideredand the nine sequences, UORO achieves the lowest root-mean-square (RMS) error and maximum error amongthe compared algorithms. These errors are respectivelyequal to 1.3mm and 8.8mm, and the prediction timeper time step was lower than 2.8ms (Dell Intel corei9-9900K 3.60 GHz). Linear regression has the lowestRMS error for the horizon values 0.1s and 0.2s, followedby LMS for horizon values between 0.3s and 0.5s, and",
        "arxiv": null,
        "doi": null,
        "file_name": "2106.011.pdf",
        "file_path": "./sources/2106.011/2106.011.pdf",
        "title": "Prediction of the Position of External Markers Using a Recurrent Neural Network Trained With Unbiased Online Recurrent Optimization for Safe Lung Cancer Radiotherapy",
        "urls": {
            "zenodo": [
                {
                    "#_appearances": 2,
                    "url": "https://doi.org/10.5281/zenodo.5506964"
                }
            ]
        }
    },
    "2106.02713": {
        "abstract": "ABSTRACTThe generalization performance of a machine learning algorithm such as a neu-ral network depends in an intricate way on the structure of the data distribution.To analyze the influence of data structure on test loss dynamics, we study an ex-actly solveable model of stochastic gradient descent (SGD) which predicts testloss when training on features with arbitrary covariance structure. We solve thetheory exactly for both Gaussian features and arbitrary features and we show thatthe simpler Gaussian model accurately predicts test loss of nonlinear random-feature models and deep neural networks trained with SGD on real datasets suchas MNIST and CIFAR-10. We show that the optimal batch size at a fixed com-pute budget is typically small and depends on the feature correlation structure,demonstrating the computational benefits of SGD with small batch sizes. Lastly,we extend our theory to the more usual setting of stochastic gradient descent ona fixed subsampled training set, showing that both training and test error can beaccurately predicted in our framework on real data.1 INTRODUCTIONUnderstanding the dynamics of SGD on realistic learning problems is fundamental to learning the-ory. Due to the challenge of modeling the structure of realistic data, theoretical studies of general-ization often attempt to derive data-agnostic generalization bounds or study the typical performanceof the algorithm on high-dimensional, factorized data distributions (Engel & Van den Broeck, 2001).Realistic datasets, however, often lie on low dimensional structures embedded in high dimensionalambient spaces (Pope et al., 2021). For example, MNIST and CIFAR-10 lie on surfaces with intrin-sic dimension of ∼ 14 and ∼ 35 respectively (Spigler et al., 2020). To understand the average-caseperformance of SGD in more realistic learning problems and its dependence on data, model andhyperparameters, incorporating structural information about the learning problem is necessary.In this paper, we calculate the average case performance of SGD on models of the form f(x) =w ·ψ(x) for nonlinear feature mapψ trained with MSE loss. We express test loss dynamics in termsof the induced second and fourth moments ofψ. Under a regularity condition on the fourth moments,we show that the test error can be accurately predicted in terms of second moments alone. Wedemonstrate the accuracy of our theory on random feature models and wide neural networks trainedon MNIST and CIFAR-10 and accurately predict test loss scalings on these datasets. We explore indetail the effect of minibatch size, m, on learning dynamics. By varying m, we can interpolate ourtheory between single sample SGD (m = 1) and gradient descent on the population loss (m→∞).To explore the computational advantages SGD compared to standard full batch gradient descentwe analyze the loss achieved at a fixed compute budget C = tm for different minibatch size m andnumber of steps t, trading off the number of parameter update steps for denoising through averaging.We show that generally, the optimal batch size is small, with the precise optimum dependent on thelearning rate and structure of the features. Overall, our theory shows how learning rate, minibatchsize and data structure interact with the structure of the learning problem to determine generalizationdynamics. It provides a predictive account of training dynamics in wide neural networks.1arXiv:2106.02713v",
        "arxiv": "2106.02713",
        "doi": null,
        "file_name": "2106.02713.pdf",
        "file_path": "./sources/2106.02713/2106.02713.pdf",
        "title": "Learning Curves for SGD on Structured Features",
        "urls": {
            "git": [
                {
                    "#_appearances": 3,
                    "url": "https://github.com/Pehlevan-Group/sgd_structured_features"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/google/neural-tangents"
                }
            ]
        }
    },
    "2106.06158": {
        "abstract": "Abstract—This paper introduces PyGAD, an open-source easy-to-use Python library for building the genetic algorithm. PyGADsupports a wide range of parameters to give the user control overeverything in its life cycle. This includes, but is not limited to,population, gene value range, gene data type, parent selection,crossover, and mutation. PyGAD is designed as a general-purpose optimization library that allows the user to customizethe fitness function. Its usage consists of 3 main steps: buildthe fitness function, create an instance of the pygad.GA class,and calling the pygad.GA.run() method. The library supportstraining deep learning models created either with PyGAD itselfor with frameworks like Keras and PyTorch. Given its stablestate, PyGAD is also in active development to respond to theuser’s requested features and enhancement received on GitHubgithub.com/ahmedfgad/GeneticAlgorithmPython. PyGAD comeswith documentation pygad.readthedocs.io for further details andexamples.Keywords— genetic algorithm, evolutionary algorithm, optimiza-tion, deep learning, Python, NumPy, Keras, PyTorchI. INTRODUCTIONNature has inspired computer scientists to create algorithms forsolving computational problems. These naturally-inspired algorithmsare called evolutionary algorithms (EAs) [1] where the initial solution(or individual) to a given problem is evolved across multiple iterationsaiming to increase its quality. The EAs can be categorized by differentfactors like the number of solutions used. Some EAs evolve a singleinitial solution (e.g. hill-climbing) and others evolve a population ofsolutions (e.g. particle swarm or genetic algorithm).The genetic algorithm (GA) a biologically-inspired EA that solvesoptimization problems inspired by Darwin’s theory “survival of thefittest” [1], [2]. Originally, the GA was not designed for being acomputational algorithm rather understanding how natural selectionhappens. Later, it becomes one of the most popular computationalEAs.The core concepts of the GA are derived from the natural solution.There are several organisms called population. The members ofthe population may have the ability to mate and reproduce neworganisms. Because the good organisms can produce better offspring,then the good members of the population are selected for mating andothers die. When 2 good organisms mate, their offspring will likelybe better specially when some new, hopefully, better, changes areintroduced. These concepts form the core of the GA in computerscience.The GA steps are explained in Figure 1 [2]. The first step createsan initial population of solutions for the problems. These solutionsare not the best for the problem and thus evolved. The evolution startsby selecting the fittest solutions as parents based on a maximizationfitness function. Each pair of parents mate to produce one or moreFig. 1. Flowchart of the genetic algorithm.children. Each child shares genes from its 2 parents using the",
        "arxiv": "2106.06158",
        "doi": null,
        "file_name": "2106.06158.pdf",
        "file_path": "./sources/2106.06158/2106.06158.pdf",
        "title": "PyGAD: An Intuitive Genetic Algorithm Python Library",
        "urls": {
            "git": [
                {
                    "#_appearances": 5,
                    "url": "https://github.com/ahmedfgad/GeneticAlgorithmPython"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/danielwilczak101/EasyGA"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/ahmedfgad/GARI"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/ahmedfgad/FlappyBirdPyGAD"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/ahmedfgad/CoinTex"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/ahmedfgad/8QueensGenetic"
                }
            ]
        }
    },
    "2106.08808": {
        "abstract": "Abstract. Traditional supervised learning with deep neural networksrequires a tremendous amount of labelled data to converge to a goodsolution. For 3D medical images, it is often impractical to build a largehomogeneous annotated dataset for a specific pathology. Self-supervisedmethods offer a new way to learn a representation of the images in anunsupervised manner with a neural network. In particular, contrastivelearning has shown great promises by (almost) matching the performanceof fully-supervised CNN on vision tasks. Nonetheless, this method doesnot take advantage of available meta-data, such as participant’s age,viewed as prior knowledge. Here, we propose to leverage continuousproxy metadata, in the contrastive learning framework, by introducinga new loss called y-Aware InfoNCE loss. Specifically, we improve thepositive sampling during pre-training by adding more positive exam-ples with similar proxy meta-data with the anchor, assuming they sharesimilar discriminative semantic features.With our method, a 3D CNNmodel pre-trained on 104 multi-site healthy brain MRI scans can extractrelevant features for three classification tasks: schizophrenia, bipolar di-agnosis and Alzheimer’s detection. When fine-tuned, it also outperforms3D CNN trained from scratch on these tasks, as well as state-of-the-artself-supervised methods. Our code is made publicly available here.1 IntroductionRecently, self-supervised representation learning methods have shown great promises,surpassing traditional transfer learning from ImageNet to 3D medical images[29]. These models can be trained without costly annotations and they offer agreat initialization point for a wide set of downstream tasks, avoiding the domaingap between natural and medical images. They mainly rely on a pretext task thatis informative about the prior we have on the data. This proxy task essentiallyarXiv:2106.08808v1  [cs.CV]  16 Jun 2021https://github.com/Duplums/yAwareContrastiveLearning2 B. Dufumier et al.",
        "arxiv": "2106.08808",
        "doi": null,
        "file_name": "2106.08808.pdf",
        "file_path": "./sources/2106.08808/2106.08808.pdf",
        "title": "Contrastive Learning with Continuous Proxy Meta-Data for 3D MRI Classification",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/Duplums/yAwareContrastiveLearning"
                }
            ]
        }
    },
    "2106.10955": {
        "abstract": "ered: extractive and abstractive. Extractive summarization isbased on the identification of important sections of the textand producing a subset of the sentences from the originaltext, whereas abstractive summarization tries to reproducethe important content in a new way after interpretation andexamination of text using more advenced techniques.Supervised summarization models are built by treating theproblem as a classification task, and by classifying which fea-tures in a sentence are relevant for summarization.Those models aren’t very reliable because of the unpredictablenature of language, from which it is not easy to generate aclassification pattern. In addition, such models require train-ing data which worsen the data acquisition problem which isalready a challenge on its own.Task summarization problem concerns itself with representingdata in such way that the importance of each sentence andtheir terms is properly considered. Text should be representedin such ways that the inter-word and inter-sentence depen-dency is kept.The task is challenged by a sustainable data source for val-idation and subsequently, an efficient metric for evaluatingit and other text-understanding tasks. The challenges aretopics of Document Understanding Conference, which laterbecame Text Analaysis Conference. A domain-independentevaluation is also complex to be achieved. For example, amodel reporting a high accuracy in summarizing news articlesmight not perform with the same accuracy when summarizing,let’s say, Reddit posts.Generally, best performing models are deep learning based.The construction of such models is faced with challenges of itsown mainly regarded with computational resources. For thatreason, it’s important to consider more simpler approacheslike the ones that are presented in this paper.Fig. 1. Pipeline of our modelOur contribution includes a direct implementation of graph-based algorithms for computing relevant data that could sum-marize the test documents the best. The respective algorithmshelp us extract the most important sentences that will consti-tute the summary. We test two different metrics for computingsentence similiarity, and in one of them we employ a notion ofgraph similiarity - edit distance - Figure 1.Related workText summarization is an open problem. There hasn’tbeen report of an official model who can achieve a decentdata-independent accuracy. Current state-of-art modelsachieve accuracy of around 50% percent. Those models areusually deep learning models. (6) achieves state-of-the-artresults on the CNN/Daily mail dataset. The model presentedthere is the Reinforced Neural Extractive Summarization(RNES) model. (5) presents a general overview of two ranking",
        "arxiv": "2106.10955",
        "doi": null,
        "file_name": "2106.10955.pdf",
        "file_path": "./sources/2106.10955/2106.10955.pdf",
        "title": "Extractive approach for text summarization using graphs",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/tot98git/text-summ"
                }
            ]
        }
    },
    "2107.032": {
        "abstract": null,
        "arxiv": null,
        "doi": "10.1016/j.techfore.2022.121478",
        "file_name": "2107.032.pdf",
        "file_path": "./sources/2107.032/2107.032.pdf",
        "title": "The Geography of Open Source Software: Evidence from GitHub",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/n1tecki/Geography-of-Open-Source-Software"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/johanneswachs/OSS_Geography_Data"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/Hipo/university-domains-list"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/johanneswachs/OSS_"
                }
            ]
        }
    },
    "2107.09947": {
        "abstract": "AbstractMachine learning brings the hope of finding new biomarkersextracted from cohorts with rich biomedical measurements.A good biomarker is one that gives reliable detection ofthe corresponding condition. However, biomarkers are of-ten extracted from a cohort that differs from the targetpopulation. Such a mismatch, known as a dataset shift,can undermine the application of the biomarker to newindividuals. Dataset shifts are frequent in biomedical re-search, e.g. because of recruitment biases. When a datasetshift occurs, standard machine-learning techniques do notsuffice to extract and validate biomarkers. This articleprovides an overview of when and how dataset shifts breaksmachine-learning extracted biomarkers, as well as detectionand correction strategies.1 Introduction: dataset shiftbreaks learned biomarkersBiomarkers are measurements that provide informationabout a medical condition or physiological state [Strimbuand Tavel, 2010]. For example, the presence of an anti-body may indicate an infection; a complex combination offeatures extracted from a medical image can help assessthe evolution of a tumor. Biomarkers are important fordiagnosis, prognosis, and treatment or risk assessments.Complex biomedical measures may carry precious medi-cal information, as with histopathological images or genomesequencing of biopsy samples in oncology. Identifying quan-titative biomarkers from these requires sophisticated sta-tistical analysis. With large datasets becoming accessi-ble, supervised machine learning provides new promisesby optimizing the information extracted to relate to a spe-cific output variable of interest, such as a cancer diagnosis[Andreu-Perez et al., 2015, Faust et al., 2018, Deo, 2015].These methods, cornerstones of artificial intelligence, arestarting to appear in clinical practice: a machine-learningbased radiological tool for breast-cancer diagnosis has re-cently been approved by the FDA1.Can such predictive biomarkers, extracted through com-plex data processing, be safely used in clinical practice,beyond the initial research settings? One risk is the poten-tial mismatch, or dataset shift, between the distribution ofthe individuals used to estimate this statistical link andthat of the target population that should benefit from thebiomarker. In this case, the extracted associations may notapply to the target population [Kakarmath et al., 2020].Computer aided diagnostic of thoracic diseases from X-rayimages has indeed been shown to be unreliable for individ-uals of a given sex if built from a cohort over-representingthe other sex [Larrazabal et al., 2020]. More generally,machine-learning systems may fail on data from different",
        "arxiv": "2107.09947",
        "doi": "10.1093/gigascience/giab055",
        "file_name": "2107.09947.pdf",
        "file_path": "./sources/2107.09947/2107.09947.pdf",
        "title": "Preventing dataset shift from breaking machine-learning biomarkers",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/neurodatascience/dataset_shift_biomarkers"
                }
            ]
        }
    },
    "2108.00117": {
        "abstract": "Abstract.Purpose: Existing anomaly detection methods focus on detecting inter-class variations while medical image noveltyidentification is more challenging in the presence of intra-class variations. For example, a model trained with normalchest X-ray and common lung abnormalities, is expected to discover and flag idiopathic pulmonary fibrosis which isa rare lung disease and unseen during training. The nuances of intra-class variations and lack of relevant training datain medical image analysis pose great challenges for existing anomaly detection methods.Approach: We address the above challenges by proposing a hybrid model - Transformation-based Embedding learn-ing for Novelty Detection (TEND), which combines the merits of Classifier-based approach and AutoEncoder basedapproach. Training TEND consists of two stages. In the first stage, we learn in-distribution embeddings with an Au-toEncoder via the unsupervised reconstruction. In the second stage, we learn a discriminative classifier to distinguishin-distribution data and the transformed counterparts. Additionally, we propose a margin-aware objective to pull in-distribution data in a hypersphere while pushing away the transformed data. Eventually, the weighted sum of classprobability and the distance to margin constitutes the anomaly score.Results: Extensive experiments are performed on three public medical image datasets with the one-vs-rest setup(namely one class as in-distribution data and the left as intra-class out-of-distribution data) and the rest-vs-one setup.Additional experiments on generated intra-class out-of-distribution data with unused transformations are implementedon the datasets. The quantitative results show competitive performance as compared to the state-of-the-art approaches.Provided qualitative examples further demonstrate the effectiveness of TEND.Conclusion: Our anomaly detection model TEND can effectively identify the challenging intra-class out-of-distributionmedical images in an unsupervised fashion. It can be applied to discover unseen medical image classes and serveas the abnormal data screening for downstream medical tasks. The corresponding code is available at https://github.com/XiaoyuanGuo/TEND_MedicalNoveltyDetection.Keywords: anomaly detection, OOD detection, novelty identification, intra-class OOD, medical image.*Xiaoyuan Guo, xiaoyuan.guo@emory.edu1 IntroductionWith recent prominent developments of machine learning techniques in computer vision, integrat-ing machine learning tools to solve medical image problems is becoming more and more populardue to the powerful computation and efficiency.1 However, when deploying machine learningmodels in real-world applications, models trained on in-distribution (ID) data may fail to deal with1arXiv:2108.00117v2  [cs.CV]  22 Jan 20",
        "arxiv": "2108.00117",
        "doi": null,
        "file_name": "2108.00117.pdf",
        "file_path": "./sources/2108.00117/2108.00117.pdf",
        "title": "Margin-Aware Intra-Class Novelty Identification for Medical Images",
        "urls": {
            "git": [
                {
                    "#_appearances": 3,
                    "url": "https://github.com/XiaoyuanGuo/TEND_MedicalNoveltyDetection"
                }
            ]
        }
    },
    "2108.0058": {
        "abstract": "AbstractFeature pyramids have been proven powerful in imageunderstanding tasks that require multi-scale features.State-of-the-art methods for multi-scale feature learningfocus on performing feature interactions across space andscales using neural networks with a fixed topology. In thispaper, we propose graph feature pyramid networks that arecapable of adapting their topological structures to varyingintrinsic image structures, and supporting simultaneousfeature interactions across all scales. We first define animage specific superpixel hierarchy for each input image torepresent its intrinsic image structures. The graph featurepyramid network inherits its structure from this superpixelhierarchy. Contextual and hierarchical layers are designedto achieve feature interactions within the same scaleand across different scales. To make these layers morepowerful, we introduce two types of local channel attentionfor graph neural networks by generalizing global channelattention for convolutional neural networks. The proposedgraph feature pyramid network can enhance the multiscalefeatures from a convolutional feature pyramid network.We evaluate our graph feature pyramid network in theobject detection task by integrating it into the Faster R-CNN algorithm. The modified algorithm outperforms notonly previous state-of-the-art feature pyramid based meth-ods with a clear margin but also other popular detec-tion methods on both MS-COCO 2017 validation and testdatasets. Codes are available at https://github.com/GangmingZhao/GraphFPN-Graph-Feature-Pyramid-Network-for-Object-Detection.† This work is done when Gangming Zhao is a visiting studentat Fudan University. *Corresponding authors: wfge@fudan.edu.cn andyizhouy@acm.org1. IntroductionDeep convolutional neural networks exploit local con-nectivity and weights sharing, and have led to a series ofbreakthroughs in computer vision tasks, including imagerecognition [24, 48, 13, 49], object detection [10, 43, 34, 41,5, 31, 47], and semantic segmentation [33, 56, 29, 18, 54,50]. Since objects in an image may have varying scales, itis much desired to obtain multiscale feature maps that havefused high-level and low-level features with sufficient spa-tial resolution at every distinct scale. This motivated fea-ture pyramid networks (FPN [30]) and its improved ver-sions, such as path aggregation network (PANet [33]) andfeature pyramid transformer (FPT [54]), and other mteh-ods [22, 19, 9, 52, 12].Every image has multiscale intrinsic structures, includ-ing the grouping of pixels into object parts, the furthergrouping of parts into objects as well as the spatial layout ofobjects in the image space. Such multiscale intrinsic struc-",
        "arxiv": "2108.0058",
        "doi": "10.1109/iccv48922.2021.00276",
        "file_name": "2108.0058.pdf",
        "file_path": "./sources/2108.0058/2108.0058.pdf",
        "title": "GraphFPN: Graph Feature Pyramid Network for Object Detection",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/GangmingZhao/GraphFPN-Graph-Feature-Pyramid-Network-for-Object-Detection"
                }
            ]
        }
    },
    "2108.01368": {
        "abstract": "AbstractThe CSGM framework (Bora-Jalal-Price-Dimakis’17) has shown that deep gen-erative priors can be powerful tools for solving inverse problems. However, todate this framework has been empirically successful only on certain datasets (forexample, human faces and MNIST digits), and it is known to perform poorly onout-of-distribution samples. In this paper, we present the first successful applicationof the CSGM framework on clinical MRI data. We train a generative prior on brainscans from the fastMRI dataset, and show that posterior sampling via Langevindynamics achieves high quality reconstructions. Furthermore, our experimentsand theory show that posterior sampling is robust to changes in the ground-truthdistribution and measurement process. Our code and models are available at:https://github.com/utcsilab/csgm-mri-langevin.1 IntroductionCompressed sensing [23, 15] has enabled reductions to the number of measurements needed forsuccessful reconstruction in a variety of imaging inverse problems. In particular, it has led to shorterscan times for magnetic resonance imaging (MRI) [62, 90], and most MRI vendors have releasedproducts leveraging this framework to accelerate clinical workflows. Despite their successes, sparsity-based methods are limited by the achievable acceleration rates, as the sparsity assumptions are eitherhand-crafted or are limited to simple learned sparse codes [72, 73].More recently, deep learning techniques have been used as powerful data-driven reconstructionmethods for inverse problems [49, 68]. There are two broad families of deep learning inversiontechniques [68]: end-to-end supervised and distribution-learning approaches. End-to-end supervisedtechniques use a training set of measured images and deploy convolutional neural networks (CNNs)and other architectures to learn the inverse mapping from measurements to image. Network architec-tures that include both CNN blocks and the imaging forward model have grown in popularity, as theycombine deep learning with the compressed sensing optimization framework, see e.g. [32, 3, 64].End-to-end methods are trained for specific imaging anatomy and measurement models and showexcellent performance in these tasks. However, reconstruction quality is known to suffer when appliedout of distribution, and recently has been shown to severely degrade [4, 19] under certain types ofnatural measurement and anatomy perturbations.In this paper we study deep learning inversion techniques based on distribution learning. These modelsare trained without reference to measurements, and so easily adapt to changes in the measurement∗Ajil Jalal and Marius Arvinte contributed equally to this work.35th Conference on Neural Information Processing Systems (NeurIPS 2021).arXiv:2108.01368v2  [cs.LG]  6",
        "arxiv": "2108.01368",
        "doi": null,
        "file_name": "2108.01368.pdf",
        "file_path": "./sources/2108.01368/2108.01368.pdf",
        "title": "Robust Compressed Sensing MRI with Deep Generative Priors",
        "urls": {
            "git": [
                {
                    "#_appearances": 4,
                    "url": "https://github.com/utcsilab/csgm-mri-langevin"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/utcsilab/deep-jsense"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/facebookresearch/fastMRI"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/ermongroup/ncsnv2"
                }
            ]
        }
    },
    "2108.0609": {
        "abstract": "Abstract—This article presents SVC-onGoing1, an on-goingcompetition for on-line signature verification where researcherscan easily benchmark their systems against the state of the artin an open common platform using large-scale public databases,such as DeepSignDB2 and SVC2021 EvalDB3, and standardexperimental protocols. SVC-onGoing is based on the ICDAR2021 Competition on On-Line Signature Verification (SVC 2021),which has been extended to allow participants anytime. The goalof SVC-onGoing is to evaluate the limits of on-line signature ver-ification systems on popular scenarios (office/mobile) and writinginputs (stylus/finger) through large-scale public databases. Threedifferent tasks are considered in the competition, simulatingrealistic scenarios as both random and skilled forgeries aresimultaneously considered on each task. The results obtained inSVC-onGoing prove the high potential of deep learning methodsin comparison with traditional methods. In particular, the bestsignature verification system has obtained Equal Error Rate(EER) values of 3.33% (Task 1), 7.41% (Task 2), and 6.04% (Task3). Future studies in the field should be oriented to improve theperformance of signature verification systems on the challengingmobile scenarios of SVC-onGoing in which several mobile devicesand the finger are used during the signature acquisition.Index Terms—SVC-onGoing, SVC 2021, Biometrics, Hand-writing, Signature Verification, DeepSignDB, SVC2021 EvalDBI. INTRODUCTIONOn-line handwritten signature verification has always beena very active area of research due to its high popularity forauthentication scenarios [2] and the variety of open challengesthat are still under research nowadays [3], e.g., one/few-shotlearning [4]–[6], device interoperability [7], [8], aging [9],[10], types of impostors [11], signature complexity [12], [13],Ruben Tolosana is the corresponding author of the article. E-mail:ruben.tolosana@uam.es1https://competitions.codalab.org/competitions/272952https://github.com/BiDAlab/DeepSignDB3https://github.com/BiDAlab/SVC2021 EvalDBtemplate storage [14], etc. Despite all these challenges, theperformance of on-line signature verification systems has beenimproved in the last years due to several factors, especially: i)the evolution in the acquisition technology going from devicesspecifically designed to acquire handwriting and signaturein office-like scenarios through a pen stylus (e.g. Wacomdevices) to the current touch screens of mobile scenarios inwhich signatures can be captured anywhere using our ownpersonal smartphone through the finger [7], [15], and ii) theextended usage of deep learning technology in many differentareas, overcoming traditional handcrafted approaches and evenhuman performance [1], [16], [17]. So, with all these aspectsin mind, the question is: what are the current performancelimits of the on-line signature verification technology under",
        "arxiv": "2108.0609",
        "doi": null,
        "file_name": "2108.0609.pdf",
        "file_path": "./sources/2108.0609/2108.0609.pdf",
        "title": "SVC-onGoing: Signature Verification Competition",
        "urls": {
            "git": [
                {
                    "#_appearances": 6,
                    "url": "https://github.com/BiDAlab/DeepSignDB"
                },
                {
                    "#_appearances": 3,
                    "url": "https://github.com/BiDAlab/SVC2021_EvalDB"
                },
                {
                    "#_appearances": 3,
                    "url": "https://github.com/BiDAlab/SVC2021"
                }
            ]
        }
    },
    "2108.06771": {
        "abstract": "AbstractQuantification of uncertainty in deep-neural-networks (DNN) based image reg-istration algorithms plays a critical role in the deployment of image registra-tion algorithms for clinical applications such as surgical planning, intraopera-tive guidance, and longitudinal monitoring of disease progression or treatmentefficacy as well as in research-oriented processing pipelines. Currently availableapproaches for uncertainty estimation in DNN-based image registration algo-rithms may result in sub-optimal clinical decision making due to potentiallyinaccurate estimation of the uncertainty of the registration stems for the as-sumed parametric distribution of the registration latent space. We introduceNPBDREG, a fully non-parametric Bayesian framework for uncertainty estima-tion in DNN-based deformable image registration by combining an Adam opti-mizer with stochastic gradient Langevin dynamics (SGLD) to characterize theunderlying posterior distribution through posterior sampling. Thus, it has thepotential to provide uncertainty estimates that are highly correlated with thepresence of out of distribution data. We demonstrated the added-value of NPB-DREG, compared to the baseline probabilistic VoxelMorph model (PrVXM), onbrain MRI image registration using 390 image pairs from four publicly availabledatabases: MGH10, CMUC12, ISBR18 and LPBA40. The NPBDREG showsa better correlation of the predicted uncertainty with out-of-distribution data(r > 0.95 vs. r < 0.5) as well as a ∼ 7.3% improvement in the registrationaccuracy (Dice score, 0.74 vs. 0.69, p � 0.01), and a ∼ 18% improvement inregistration smoothness (percentage of folds in the deformation field, 0.014 vs.0.017, p � 0.01). Finally, NPBDREG demonstrated a better generalizationcapability for data corrupted by a mixed structure noise (Dice score of 0.73 vs.0.69, p� 0.01) compared to the baseline PrVXM approach.∗Corresponding author∗∗is a Taub fellow (supported by the Taub Family Foundation, Technion’s program forleaders in Science and Technology).Email addresses: ssamahkh@campus.technion.ac.il (Samah Khawaled),moti.freiman@technion.ac.il (Moti Freiman )To appear in Computerized Medical Imaging and Graphics, DOI: https: // doi. org/ 10. 1016/ j. compmedimag. 2022. 102087arXiv:2108.06771v4  [cs.CV]  9 Jun",
        "arxiv": "2108.06771",
        "doi": null,
        "file_name": "2108.06771.pdf",
        "file_path": "./sources/2108.06771/2108.06771.pdf",
        "title": "NPBDREG: Uncertainty Assessment in Diffeomorphic Brain MRI Registration using a Non-parametric Bayesian Deep-Learning Based Approach",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/samahkh/NPBDREG_prj"
                }
            ]
        }
    },
    "2109.011": {
        "abstract": "AbstractData-driven subword segmentation has be-come the default strategy for open-vocabularymachine translation and other NLP tasks,but may not be sufficiently generic for op-timal learning of non-concatenative morphol-ogy. We design a test suite to evaluate seg-mentation strategies on different types of mor-phological phenomena in a controlled, semi-synthetic setting. In our experiments, wecompare how well machine translation mod-els trained on subword- and character-levelcan translate these morphological phenomena.We find that learning to analyse and generatemorphologically complex surface representa-tions is still challenging, especially for non-concatenative morphological phenomena likereduplication or vowel harmony and for rareword stems. Based on our results, we recom-mend that novel text representation strategiesbe tested on a range of typologically diverselanguages to minimise the risk of adopting astrategy that inadvertently disadvantages cer-tain languages.11 IntroductionData-driven subword-level segmentation of text(Sennrich et al., 2016; Kudo, 2018) is a well-knownand widely used text representation strategy inthe natural language processing (NLP) commu-nity. While subword segmentation largely solvesthe open vocabulary problem, previous researchhas shown that models often break down in out-of-domain contexts (El Boukkouri et al., 2020),when encountering spelling errors (Belinkov andBisk, 2018; Pruthi et al., 2019), when translatingmorphologically-rich languages (Ataman and Fed-erico, 2018) and in multilingual scenarios (Chunget al., 2020; Wang et al., 2021). The reason forthis is that even slight deviations from the text seen1Test suite and code available at https://github.com/ZurichNLP/segtestwhen learning a segmentation model can result inentirely different segmentations and often aggres-sively over-segmented text.Given the rich morphological diversity acrossnatural languages, it is especially interesting to in-vestigate the suitability of subword segmentation torepresent different morphological phenomena. Forexample, reduplication is a non-concatenative mor-phological phenomenon2 that is common across",
        "arxiv": null,
        "doi": "10.18653/v1/2021.findings-emnlp.60",
        "file_name": "2109.011.pdf",
        "file_path": "./sources/2109.011/2109.011.pdf",
        "title": "How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/ZurichNLP/segtest"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/EdinburghNLP/nematus"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/BramVanroy/spacy_conll"
                }
            ],
            "zenodo": [
                {
                    "#_appearances": 3,
                    "url": "https://doi.org/10.5281/zenodo.1212303"
                }
            ]
        }
    },
    "2109.03939": {
        "abstract": "AbstractWe demonstrate that, hidden within one-layerrandomly weighted neural networks, thereexist subnetworks that can achieve impres-sive performance, without ever modifyingthe weight initializations, on machine trans-lation tasks. To find subnetworks for one-layer randomly weighted neural networks,we apply different binary masks to thesame weight matrix to generate different lay-ers. Hidden within a one-layer randomlyweighted Transformer, we find that subnet-works that can achieve 29.45/17.29 BLEUon IWSLT14/WMT14. Using a fixed pre-trained embedding layer, the previously foundsubnetworks are smaller than, but can match98%/92% (34.14/25.24 BLEU) of the perfor-mance of, a trained Transformersmall/base onIWSLT14/WMT14. Furthermore, we demon-strate the effectiveness of larger and deepertransformers in this setting, as well as the im-pact of different initialization methods.11 IntroductionModern deep learning often trains millions oreven billions of parameters (Devlin et al., 2018;Shoeybi et al., 2019; Raffel et al., 2019; Brownet al., 2020) to deliver good performance for amodel. Recently, Frankle and Carbin (2018);Frankle et al. (2020) demonstrated that theseover-parameterized networks contain sparse sub-networks, when trained in isolation, that canachieve similar or better performance than theoriginal model.Furthermore, recent studies revisit the initial-ization stage of finding these subnetworks in vi-sion models (Zhou et al., 2019; Ramanujan et al.,2020). Such a mask, which is used to mask outa part of the entire network to those subnetworks,1We released the source code at https://github.com/sIncerass/one_layer_lottery_ticket.∗Equal contribution.W1 W2 W3 W4M1 M2 M3 M4Input: I am happy.Output: Ich bin fröhlichM1 M2 M3 M4W1Normal Subnework",
        "arxiv": "2109.03939",
        "doi": null,
        "file_name": "2109.03939.pdf",
        "file_path": "./sources/2109.03939/2109.03939.pdf",
        "title": "What's Hidden in a One-layer Randomly Weighted Transformer?",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/sIncerass/one_layer_lottery_ticket"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/pytorch/fairseq"
                }
            ]
        }
    },
    "2109.09412": {
        "abstract": "AbstractWe present a novel method for injecting temporality into entailment graphs to address the prob-lem of spurious entailments, which may arise from similar but temporally distinct events involv-ing the same pair of entities. We focus on the sports domain in which the same pairs of teams playon different occasions, with different outcomes. We present an unsupervised model that aims tolearn entailments such as win/lose→ play, while avoiding the pitfall of learning non-entailmentssuch as win 6→ lose. We evaluate our model on a manually constructed dataset, showing that in-corporating time intervals and applying a temporal window around them, are effective strategies.1 IntroductionRecognising textual entailment and paraphrases is core to many downstream NLP applications such asquestion answering and semantic parsing. In the case of open-domain question answering over unstruc-tured data, the answer to a question may not be explicitly stated in the text, but may be recovered viaparaphrases and/or entailment rules.Entailment graphs (Berant et al., 2011; Berant et al., 2015; Hosseini et al., 2018), in which nodesrepresent predicates and edges are entailment relations, have been proposed as a means to answer suchquestions. They can be mined using unsupervised methods applied over large collections of text, bykeeping track of which entity pairs occur with which predicates. One common error made by thesegraphs, however, is that they assert spurious associations between similar but temporally distinct eventsthat occur with the same entity pairs. For example, both the predicates beat and lost against will applyto sports team entity pairs such as (Arsenal, Man United). This is likely to mislead the current methodsinto incorrectly assigning an entailment relation between these two predicates.In this paper we extend the framework of Hosseini et al. (2018) to incorporate the temporal locationof events, with the aim of mitigating these spurious entailments. Temporal information can be used todisentangle these groups of highly correlated predicates, because although they will share entity pairs,they will never occur at the same time. For example, in Figure 1 Arsenal and Man United played eachother three times in 2019, with three different outcomes: win (beat), lose (lost against), tie (tied with).In previous methods, the context in which the predicates occur appears to be the same, because theyonly consider entity pairs as context. Therefore they mistakenly take the examples in Figure 1 as evidenceof entailments or paraphrases between the three outcome predicates (win, lose, and tie), depending onthe distributions found in the data. Our method enriches this context to include time interval information,thereby filtering out combinations that are not temporally near each other. Thus we hope to avoid learningthat beat→ lost against, while still learning that beat→ play.As an initial test domain, we focus on the sports news genre, using extracted relations that involvetwo sports teams. We evaluate on a manually constructed dataset of 1,312 entailment pairs based onparaphrases of the predicates in the graph on the right hand side of Figure 1. Our goal is to recoverthe structure of this graph in an unsupervised way, separating each of the highly correlated outcomepredicates while predicting that they all entail play.This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/.*The first two authors contributed equally to this workarXiv:2109.09412v1 ",
        "arxiv": "2109.09412",
        "doi": "10.18653/v1/2020.textgraphs-1.7",
        "file_name": "2109.09412.pdf",
        "file_path": "./sources/2109.09412/2109.09412.pdf",
        "title": "Incorporating Temporal Information in Entailment Graph Mining",
        "urls": {
            "git": [
                {
                    "#_appearances": 1,
                    "url": "https://gitlab.com/lianeg/temporal-entailment-sports-dataset"
                }
            ]
        }
    },
    "2109.09968": {
        "abstract": "AbstractDeep reinforcement learning provides apromising approach for text-based games instudying natural language communicationbetween humans and artificial agents. How-ever, the generalization still remains a bigchallenge as the agents depend critically onthe complexity and variety of training tasks.In this paper, we address this problem byintroducing a hierarchical framework builtupon the knowledge graph-based RL agent.In the high level, a meta-policy is executedto decompose the whole game into a set ofsubtasks specified by textual goals, and selectone of them based on the KG. Then a sub-policy in the low level is executed to conductgoal-conditioned reinforcement learning. Wecarry out experiments on games with variousdifficulty levels and show that the proposedmethod enjoys favorable generalizability.1 IntroductionText-based games are simulated systems wherethe agent takes textual observations as the input,and interacts with the environment via text com-mands (Hausknecht et al., 2020). They are suitabletest-beds to study natural language understanding,commonsense reasoning and language-informeddecision making (Luketina et al., 2019). Reinforce-ment Learning (RL) based agents (Narasimhanet al., 2015; Zahavy et al., 2018) have been de-veloped to handle challenges such as language-based representation learning and combinatorialaction space. Among them, KG-based agents (Am-manabrolu and Hausknecht, 2020) yield promis-ing performance with the aid of Knowledge Graph(KG), which serves as a belief state to provide struc-tural information.To design intelligent RL-based agents for text-based games, it is necessary to build agents thatautomatically learn to solve different games. How-ever, generalization remains as one of the key chal-lenges of RL − the agent tends to overfit the train-ing environment and fails to generalize to new en-vironments (Cobbe et al., 2019). In the domainof text-based games, the TextWorld (Côté et al.,2018) makes it feasible to study generalizability bycreating non-overlapping game sets with customiz-able domain gaps (e.g., themes, vocabulary sets,difficulty levels and layouts). Most previous worksstudy generalizability either upon games with the",
        "arxiv": "2109.09968",
        "doi": "10.18653/v1/2021.findings-emnlp.116",
        "file_name": "2109.09968.pdf",
        "file_path": "./sources/2109.09968/2109.09968.pdf",
        "title": "Generalization in Text-based Games via Hierarchical Reinforcement Learning",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/xingdi-eric-yuan/GATA-public"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/YunqiuXu/H-KGA"
                }
            ]
        }
    },
    "2109.12599": {
        "abstract": "AbstractLearning sentence embeddings from dialogueshas drawn increasing attention due to its lowannotation cost and high domain adaptabil-ity. Conventional approaches employ thesiamese-network for this task, which obtainsthe sentence embeddings through modelingthe context-response semantic relevance by ap-plying a feed-forward network on top of thesentence encoders. However, as the seman-tic textual similarity is commonly measuredthrough the element-wise distance metrics (e.g.cosine and L2 distance), such architectureyields a large gap between training and evaluat-ing. In this paper, we propose DialogueCSE, adialogue-based contrastive learning approachto tackle this issue. DialogueCSE first in-troduces a novel matching-guided embedding(MGE) mechanism, which generates a context-aware embedding for each candidate responseembedding (i.e. the context-free embedding)according to the guidance of the multi-turncontext-response matching matrices. Then itpairs each context-aware embedding with itscorresponding context-free embedding and fi-nally minimizes the contrastive loss acrossall pairs. We evaluate our model on threemulti-turn dialogue datasets: the MicrosoftDialogue Corpus, the Jing Dong DialogueCorpus, and the E-commerce Dialogue Cor-pus. Evaluation results show that our ap-proach significantly outperforms the baselinesacross all three datasets in terms of MAP andSpearman’s correlation measures, demonstrat-ing its effectiveness. Further quantitative ex-periments show that our approach achievesbetter performance when leveraging more di-alogue context and remains robust when lesstraining data is provided.1 IntroductionSentence embeddings are used with success fora variety of NLP applications (Cer et al., 2018)and many prior methods have been proposed withdifferent learning schemes. Kiros et al. (2015); Lo-geswaran and Lee (2018); Hill et al. (2016) trainsentence encoders in a self-supervised manner withweb pages and books. Conneau et al. (2017); Ceret al. (2018); Reimers and Gurevych (2019) pro-pose to learn sentence embeddings on the super-vised datasets such as SNLI (Bowman et al., 2015)",
        "arxiv": "2109.12599",
        "doi": "10.18653/v1/2021.emnlp-main.185",
        "file_name": "2109.12599.pdf",
        "file_path": "./sources/2109.12599/2109.12599.pdf",
        "title": "DialogueCSE: Dialogue-based Contrastive Learning of Sentence Embeddings",
        "urls": {
            "git": [
                {
                    "#_appearances": 1,
                    "url": "https://github.com/wangruicn/DialogueCSE"
                }
            ]
        }
    },
    "2110.00428": {
        "abstract": "AbstractUnderstanding videos to localize moments with naturallanguage often requires large expensive annotated video re-gions paired with language queries. To eliminate the an-notation costs, we make a first attempt to train a natu-ral language video localization model in zero-shot man-ner. Inspired by unsupervised image captioning setup, wemerely require random text corpora, unlabeled video collec-tions, and an off-the-shelf object detector to train a model.With the unpaired data, we propose to generate pseudo-supervision of candidate temporal regions and correspond-ing query sentences, and develop a simple NLVL modelto train with the pseudo-supervision. Our empirical vali-dations show that the proposed pseudo-supervised methodoutperforms several baseline approaches and a number ofmethods using stronger supervision on Charades-STA andActivityNet-Captions.1. IntroductionOn increasing demands of understanding videos tosearch with natural language queries, natural languagevideo localization (NLVL) has been actively investigated inrecent literature [19,35,39,43,44,57]. The task targets to lo-calize a temporal moment in a video by a natural languagequery. In recent years, significant performance improve-ments on benchmark datasets has been made, facilitated bythe advances on deep learning methods [19, 39, 43, 56] andmassively annotated data [2, 19, 26, 37, 58].As illustrated in Fig. 1-(a), the annotations consist of atemporal region in a video (start time, end time) and a cor-responding query sentence. However, obtaining such pairedannotation is laborious and expensive. To alleviate the an-notation cost, a number of recent works addressed weakly-supervised setup of NLVL [12, 21, 33] which aims to lo-calize a moment without the temporal alignment of givenquery sentence. Although it eliminates the annotation cost∗: equal contribution. †: corresponding author. § now at U. of Minnesota,Twin Cities. Code: https://github.com/gistvision/PSVL(a) Fully-supervised NLVL (b) Weakly-supervised NLVL(c) Unsupervised Image Captioning (d) Zero-shot NLVLText CorporaImage collection Video collectionQueryVideo + Temporal region {ts, te} VideoObject detector Text Corpora Object detectorthe person pours some waterinto the glassQuerythe person pours some water",
        "arxiv": "2110.00428",
        "doi": "10.1109/iccv48922.2021.00150",
        "file_name": "2110.00428.pdf",
        "file_path": "./sources/2110.00428/2110.00428.pdf",
        "title": "Zero-shot Natural Language Video Localization",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/gistvision/PSVL"
                }
            ]
        }
    },
    "2110.01446": {
        "abstract": "Abstract—In this paper, we tackle the transductive semi-supervised learning problem that aims to obtain label predictionsfor the given unlabeled data points according to Vapnik’sprinciple. Our proposed approach is based on optimal trans-port, a mathematical theory that has been successfully used toaddress various machine learning problems, and is starting toattract renewed interest in semi-supervised learning community.The proposed approach, Optimal Transport Propagation (OTP),performs in an incremental process, label propagation throughthe edges of a complete bipartite edge-weighted graph, whoseaffinity matrix is constructed from the optimal transport planbetween empirical measures defined on labeled and unlabeleddata. OTP ensures a high degree of predictions certitude bycontrolling the propagation process using a certainty score basedon Shannon’s entropy. We also provide a convergence analysisof our algorithm. Experiments task show the superiority of theproposed approach over the state-of-the-art. We make our codepublicly available. 1Index Terms—Optimal Transport, Semi-supervised Learning,Label PropagationI. INTRODUCTIONDeep learning models have achieved state-of-the-artperformance on a broad spectrum of learning tasks, andare becoming increasingly popular in various applicationdomains [18], such as image classification and speechrecognition, where a large amount of labeled data is available.However, for many tasks, it is often prohibitively expensiveto collect a large high quality labeled dataset due to lackof time, resources, or other factors, while unlabeled data ischeap and abundant. Medicine is the best illustration of thisscenario, where measurement require expensive machineryand labels are the result of a labor and expensive expert-assisted time-consuming analysis.In conjunction with transfer learning (TL), semi-supervisedlearning (SSL) constitute an attractive approach towardsaddressing the lack of massive labeled datasets. It seeks tolargely alleviate the need for labeled samples by providing ameans to jointly leverage unlabeled instances. Graph-basedsemi-supervised approaches are one of the most widely usedclasses of semi-supervised learning methods, due to theirperformance and to more and more real graph datasets. Theproblem is to predict the label of all the unlabeled vertices inthe graph based only on a small subset of labeled vertices.1Code is available at: https://github.com/MouradElHamri/OTPA popular graph-based semi-supervised learning methodis to use label propagation, this latter has shown goodperformances in different machine learning applications overthe past few years, such as social network analysis [4] [35],natural language processing [2], and image segmentation [7].Most existing label propagation algorithms essentially estimate",
        "arxiv": "2110.01446",
        "doi": "10.1109/ijcnn52387.2021.9533521",
        "file_name": "2110.01446.pdf",
        "file_path": "./sources/2110.01446/2110.01446.pdf",
        "title": "Label Propagation Through Optimal Transport",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/MouradElHamri/OTP"
                }
            ]
        }
    },
    "2110.05966": {
        "abstract": "ABSTRACTThis paper addresses the problem of multi-channel multi-speech separation based on deep learning techniques. In theshort time Fourier transform domain, we propose an end-to-end narrow-band network that directly takes as input themulti-channel mixture signals of one frequency, and outputsthe separated signals of this frequency. In narrow-band, thespatial information (or inter-channel difference) can welldiscriminate between speakers at different positions. Thisinformation is intensively used in many narrow-band speechseparation methods, such as beamforming and clustering ofspatial vectors. The proposed network is trained to learn arule to automatically exploit this information and performspeech separation. Such a rule should be valid for any fre-quency, thence the network is shared by all frequencies. Inaddition, a full-band permutation invariant training crite-rion is proposed to solve the frequency permutation problemencountered by most narrow-band methods. Experimentsshow that, by focusing on deeply learning the narrow-bandinformation, the proposed method outperforms the oraclebeamforming method and the state-of-the-art deep learningbased method.Index Terms— Speech separation, deep learning, narrow-band, multi-channel, reverberant environments1. INTRODUCTIONThis work addresses the multi-channel speech source sepa-ration problem leveraging deep learning techniques. Tradi-tional speech source separation methods are often designedin the short-time Fourier transform (STFT) domain. In [1],the authors introduced the W-disjoint orthogonality assump-tion based on the time-frequency (TF) sparsity of speech,namely each TF bin of the mixed speech signals is assumedto be dominated by one speech source. Consequently, speechsources can be separated by clustering the TF bins. Theauthors of [2] proposed to estimate the mixing matrix us-ing hierarchical clustering of the TF-wise mixing vectors.Beamforming conducts speech separation by applying spatial* corresponding authorfiltering. The widely used minimum variance distortionlessresponse (MVDR) beamformer preserves the speech withdesired steering vector while suppresses others [3]. The re-cently proposed Guided Source Separation (GSS) [4] methodcombines the techniques of TF-bin clustering and MVDRbeamforming, which achieves excellent speech separationperformance, and thus is extensively adopted by the partici-pants of the CHiME-6 multispeaker speech recognition chal-lenge [5]. The traditional methods mentioned above [2, 3, 4]are all performed in narrow-band, namely processing theSTFT frequencies separately. This leads to the well-knownfrequency permutation problem that the separated signals at",
        "arxiv": "2110.05966",
        "doi": null,
        "file_name": "2110.05966.pdf",
        "file_path": "./sources/2110.05966/2110.05966.pdf",
        "title": "Multi-channel Narrow-band Deep Speech Separation with Full-band Permutation Invariant Training",
        "urls": {
            "git": [
                {
                    "#_appearances": 1,
                    "url": "https://github.com/Enny1991/beamformers"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/Audio-WestlakeU/NBSS"
                }
            ]
        }
    },
    "2110.08266": {
        "abstract": "Abstract—Predicting the next place to visit is a key in human mobility behavior modeling, which plays a significant role in variousfields, such as epidemic control, urban planning, traffic management, and travel recommendation. To achieve this, one typical solution isdesigning modules based on RNN to capture their preferences to various locations. Although these RNN-based methods can effectivelylearn individual’s hidden personalized preferences to her visited places, the interactions among users can only be weakly learnedthrough the representations of locations. Targeting this, we propose an end-to-end framework named personalized and group preferenceguided network (PG2Net), considering the users’ preferences to various places at both individual and collective levels. Specifically,PG2Net concatenates Bi-LSTM and attention mechanism to capture each user’s long-term mobility tendency. To learn population’sgroup preferences, we utilize spatial and temporal information of the visitations to construct a spatio-temporal dependency module. Weadopt a graph embedding method to map users’ trajectory into a hidden space, capturing their sequential relation. In addition, we devisean auxiliary loss to learn the vectorial representation of her next location. Experiment results on two Foursquare check-in datasets andone mobile phone dataset indicate the advantages of our model compared to the state-of-the-art baselines. Source codes are availableat https://github.com/urbanmobility/PG2Net.Index Terms—Next place prediction, human mobility, trajectory data, personalized and group preferences, attention mechanism.F1 INTRODUCTIONW ITH the rapid development of information and com-munication technologies, users can share their loca-tions almost anytime and anywhere to acquire the location-aware services, which forms an abundant amount of tra-jectory data. Location-based social networks (LBSNs), suchas Foursquare and Yelp, collect a huge amount of locationdata from millions of individuals [1, 2]. These trajectory dataprovide an unprecedented opportunity to study humanmobility behavior at scale [3, 4]. With massive mobile phonedata, González et al. found a high degree of temporal andspatial regularity in human trajectories [3]. Their radius ofgyrations clearly follow a power law distribution, indicat-ing the sample reproducible patterns of human mobility.In a following study, Song et al. measured the potentialpredictability of human mobility as 93% [5]. Due to the highpredictability of human mobility, researchers attempted tomodel the mobility behavior of the population at urban scaleand utilized it to tackle various urban challenges, includingtraffic congestion mitigation [6–8], air pollution exposure• H. Li, B. Wang, S. Zhu are with the College of Information, Mechan-ical, and Electrical Engineering, Shanghai Normal University, Shang-hai 201400, China.E-mail: lhfqy1995@gmail.com, binwang@shnu.edu.cn,suleizhu@163.com.• F. Xia is with the School of Information Science and Engineer-ing, Shandong University, Qingdao 266237, China. E-mail: xi-afan.1779@foxmail.com.• X. Zhai is with the Shanghai Municipal Country-rural Construction andTransportation Institute. E-mail: jessie zx28@163.com.• Y. Xu is with the MoE Key Laboratory of Artificial Intelligence and AIInstitute, Shanghai Jiao Tong University, Shanghai 200240, China. E-mail: yanyanxu@sjtu.edu.cn.• This work has been submitted to the IEEE for possible publication.Copyright may be transferred without notice, after which this version mayno longer be accessible.∗Corresponding authors.estimation [9], planning of electric vehicles charging behav-",
        "arxiv": "2110.08266",
        "doi": null,
        "file_name": "2110.08266.pdf",
        "file_path": "./sources/2110.08266/2110.08266.pdf",
        "title": "PG$^2$Net: Personalized and Group Preferences Guided Network for Next Place Prediction",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/urbanmobility/PG2Net"
                }
            ]
        }
    },
    "2110.14341": {
        "abstract": "AbstractThe Chow-Liu algorithm (IEEE Trans. Inform. Theory, 1968) has been a mainstay for thelearning of tree-structured graphical models from i.i.d. sampled data vectors. Its theoreticalproperties have been well-studied and are well-understood. In this paper, we focus on the classof trees that are arguably even more fundamental, namely homogeneous trees in which each pairof nodes that forms an edge has the same correlation ρ. We ask whether we are able to furtherreduce the error probability of learning the structure of the homogeneous tree model whenactive learning or active sampling of nodes or variables is allowed. Our figure of merit is theerror exponent, which quantifies the exponential rate of decay of the error probability with anincreasing number of data samples. At first sight, an improvement in the error exponent seemsimpossible, as all the edges are statistically identical. We design and analyze an algorithm ActiveLearning Algorithm for Trees with Homogeneous Edges (Active-LATHE), which surprisinglyboosts the error exponent by at least 40% when ρ is at least 0.8. For all other values of ρ, we alsoobserve commensurate, but more modest, improvements in the error exponent. Our analysishinges on judiciously exploiting the minute but detectable statistical variation of the samplesto allocate more data to parts of the graph in which we are less confident of being correct.1 IntroductionGraphical models provide a succinct representation of the conditional independence relationshipsamong random variables. Each node in the graph represents a random variable, and the inde-pendence relationships are captured by the edges in the graph. Tree-structured graphical modelsare a special class of graphical models whose underlying graphs are trees. Due to their simplic-ity in inference and learning tasks, they are widely employed in domains such as computationalbiology [SN87], natural language processing [PCX14], and signal processing [Mon12].The problem of learning the structure of graphical models is an important and fundamentaltask in model selection. With independent samples of random variables in the graph, the goal is toestimate the edges in the graph. The Chow–Liu (CL) algorithm proposed in [CL68] is a classicaltree structure learning algorithm, which finds the Maximum Weight Spanning Tree (MWST) ofthe graph whose weights are empirical estimates of mutual information between random variables.∗These authors contributed equally to this work.†Anshoo Tandon contributed to this paper when he was with the National University of Singapore. He is nowwith V-Labs, Bengaluru.1http://arxiv.org/abs/2110.14341v2fzzhang@u.nus.eduanshoo.tandon@gmail.comvtan@nus.edu.sgThe CL algorithm essentially finds the closest tree-structured graphical model to the empiricaldistribution in the sense of Kullback–Leibler (KL) divergence, and so the CL tree is the maximumlikelihood estimate of the underlying tree structure. Using large deviation techniques [DZ98],Tan, Anandkumar, Tong, and Willsky [TATW11] analyzed the exponential decay rate of the errorprobability, which is known as the error exponent. For the homogeneous Ising tree models, Tandon,Tan, and Zhu [TTZ20] showed that the exponent of the event that the CL algorithm estimates thewrong structure tends to 0 when the correlations between adjacent nodes approaches 1, whichimplies that the error probability decreases slowly with an increase in the number of independentsamples. Fortunately, this performance degradation can be greatly mitigated by active learning.Active learning is an umbrella term for algorithms that are allowed to actively select the datafrom which they learn the properties of the underlying distribution [Set09]. The freedom to choosedata to learn from allows the learner to home in on parts of the model that are deemed to be “mostuncertain”. This reduces the overall error probability or estimation error. Since all conventionalstructure learning algorithms, including the CL algorithm, rely on samples that are collected before",
        "arxiv": "2110.14341",
        "doi": "10.1109/tit.2022.3222046",
        "file_name": "2110.14341.pdf",
        "file_path": "./sources/2110.14341/2110.14341.pdf",
        "title": "Active-LATHE: An Active Learning Algorithm for Boosting the Error Exponent for Learning Homogeneous Ising Trees",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/Zhang-Fengzhuo/Active-LATHE"
                }
            ]
        }
    },
    "2111.1246": {
        "abstract": "AbstractRecent self-supervised models have demonstrated equal or better performance thansupervised methods, opening for AI systems to learn visual representations from prac-tically unlimited data. However, these methods are typically classification-based andthus ineffective for learning high-resolution feature maps that preserve precise spatialinformation. This work introduces superpixels to improve self-supervised learning ofdense semantically rich visual concept embeddings. Decomposing images into a smallset of visually coherent regions reduces the computational complexity by O(1000) whilepreserving detail. We experimentally show that contrasting over regions improves the ef-fectiveness of contrastive learning methods, extends their applicability to high-resolutionimages, improves overclustering performance, superpixels are better than grids, and re-gional masking improves performance. The expressiveness of our dense embeddings isdemonstrated by improving the SOTA unsupervised semantic segmentation benchmarkon Cityscapes, and for convolutional models on COCO. Code is available at https://github.com/robin-karlsson0/vice.1 IntroductionProgress in general computer vision tasks in the past decade has been based on supervisedlearning with large datasets annotated by human labelers [63]. Arguments are made thatgeneralizable and robust computer vision models have not yet been achieved, and further in-creasing the amount of labeled data is unsustainable [28, 74]. One hypothesis is that learning© 2022. The copyright of this document resides with its authors.It may be distributed unchanged freely in print or electronic forms.arXiv:2111.12460v3  [cs.CV]  7 Oct 2022CitationCitation{Krizhevsky, Sutskever, and Hinton} 2012CitationCitation{Cunn} 2020Citation",
        "arxiv": "2111.1246",
        "doi": null,
        "file_name": "2111.1246.pdf",
        "file_path": "./sources/2111.1246/2111.1246.pdf",
        "title": "ViCE: Improving Dense Representation Learning by Superpixelization and Contrasting Cluster Assignment",
        "urls": {
            "git": [
                {
                    "#_appearances": 3,
                    "url": "https://github.com/robin-karlsson0/vice"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/open-mmlab/mmsegmentation"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/facebookresearch/vissl"
                }
            ]
        }
    },
    "2111.1515": {
        "abstract": "AbstractObject encoding and identification are vital for robotictasks such as autonomous exploration, semantic scene un-derstanding, and re-localization. Previous approaches haveattempted to either track objects or generate descriptorsfor object identification. However, such systems are lim-ited to a “fixed” partial object representation from a singleviewpoint. In a robot exploration setup, there is a require-ment for a temporally “evolving” global object represen-tation built as the robot observes the object from multipleviewpoints. Furthermore, given the vast distribution of un-known novel objects in the real world, the object identifi-cation process must be class-agnostic. In this context, wepropose a novel temporal 3D object encoding approach,dubbed AirObject, to obtain global keypoint graph-basedembeddings of objects. Specifically, the global 3D objectembeddings are generated using a temporal convolutionalnetwork across structural information of multiple framesobtained from a graph attention-based encoding method.We demonstrate that AirObject achieves the state-of-the-artperformance for video object identification and is robust tosevere occlusion, perceptual aliasing, viewpoint shift, de-formation, and scale transform, outperforming the state-of-the-art single-frame and sequential descriptors. To thebest of our knowledge, AirObject is one of the first tempo-ral object encoding methods. Source code is available athttps://github.com/Nik-V9/AirObject.1. IntroductionObject encoding and identification are crucial for robotictasks such as autonomous exploration, semantic scene un-derstanding, and loop closure in simultaneous localizationand mapping (SLAM). For example, object-based seman-tic SLAM and identification of revisited objects require ro-bust and efficient object encodings [41, 45, 46]. Prior ap-proaches proposed in the literature have attempted to trackobject detections [47], use keypoint features [10], and gen-erate graph-based embeddings for object matching [50].However, such systems are limited to a “fixed” object rep-✓✓✓Figure 1. Temporally evolving topological graph representationsof objects within a video sequence. We propose a method, AirOb-ject, to match these temporally evolving representations and al-leviate problems caused by perceptually-aliased occluded singleframe representations.resentation from a single viewpoint and are not robust tosevere occlusion, viewpoint shift, perceptual aliasing, orscale transform. These single frame representations tend tolead to false correspondences amongst perceptually-aliased",
        "arxiv": "2111.1515",
        "doi": "10.1109/cvpr52688.2022.00822",
        "file_name": "2111.1515.pdf",
        "file_path": "./sources/2111.1515/2111.1515.pdf",
        "title": "AirObject: A Temporally Evolving Graph Embedding for Object Identification",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/Nik-V9/AirObject"
                }
            ]
        }
    },
    "2112.08777": {
        "abstract": "AbstractLong-context question answering (QA) tasksrequire reasoning over a long document ormultiple documents. Addressing these tasksoften benefits from identifying a set of evi-dence spans (e.g., sentences), which providesupporting evidence for answering the ques-tion. In this work, we propose a novel methodfor equipping long-context QA models with anadditional sequence-level objective for betteridentification of the supporting evidence. Weachieve this via an additional contrastive super-vision signal in finetuning, where the model isencouraged to explicitly discriminate support-ing evidence sentences from negative ones bymaximizing question-evidence similarity. Theproposed additional loss exhibits consistentimprovements on three different strong long-context transformer models, across two chal-lenging question answering benchmarks – Hot-potQA and QAsper.11 IntroductionAnswering questions that require reasoning overa long sequence, such over long documents ormultiple documents, is a challenging task (Panget al., 2021). Research in this domain mostly in-cludes tasks that involve multiple text segments,over benchmarks like HotpotQA (Yang et al., 2018)and QAsper (Dasigi et al., 2021). HotpotQA is amulti-hop QA benchmark over multiple paragraphsfrom Wikipedia, while QAsper involves readingcomprehension from long academic papers, whererelevant information on a question could be spreadacross the paper.Given the task complexity (Choi et al., 2017),benchmarks often provide an additional set of evi-dence spans, such as sentences or paragraphs, fora given question answer pair. This breaks downthe long-context QA task, adding a preliminary∗ Work partly done as an intern at AI2.1Our code is available at https://github.com/aviclu/long-context-qa-contrast.evidence span detection, which is crucial for suc-cessfully finding the correct answer, and also poten-tially helps in model interpretability. In this work,we propose a method for improving long-contextQA via leveraging such evidence spans, by maxi-mizing their similarity with the question.Since identifying the evidences provides rele-vant information for answering the question, prior",
        "arxiv": "2112.08777",
        "doi": "10.18653/v1/2022.naacl-main.207",
        "file_name": "2112.08777.pdf",
        "file_path": "./sources/2112.08777/2112.08777.pdf",
        "title": "Long Context Question Answering via Supervised Contrastive Learning",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/aviclu/long-context-qa-contrast"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/aviclu/CDLM"
                }
            ]
        }
    },
    "2112.08866": {
        "abstract": "AbstractRecent advances in probabilistic deep learning enable efficient amortized Bayesian inferencein settings where the likelihood function is only implicitly defined by a simulation program(simulation-based inference; SBI). But how faithful is such inference if the simulationrepresents reality somewhat inaccurately—that is, if the true system behavior at testtime deviates from the one seen during training? We conceptualize the types of modelmisspecification arising in SBI and systematically investigate how the performance ofneural posterior approximators gradually deteriorates under these misspecifications, makinginference results less and less trustworthy. To notify users about this problem, we propose anew misspecification measure that can be trained in an unsupervised fashion (i. e., withouttraining data from the true distribution) and reliably detects model misspecification attest time. Our experiments clearly demonstrate the utility of our new measure both ontoy examples with an analytical ground-truth and on representative scientific tasks incell biology, cognitive decision making, and disease outbreak dynamics. We show howthe proposed misspecification test warns users about suspicious outputs, raises an alarmwhen predictions are not trustworthy, and guides model designers in their search for bettersimulators.Keywords: deep learning, Bayesian inference, inverse problems, model misspecification,simulation based inference, invertible neural networks1. IntroductionComputer simulations play a fundamental role in many fields of science. However, theassociated inverse problems of finding simulation parameters that accurately reproduce orpredict real-world behavior are generally difficult and analytically intractable. Here, weconsider simulation-based inference (SBI; Cranmer et al., 2020) as a general approach toovercome this difficulty within a Bayesian inference framework. That is, given an assumedgenerative modelM (as represented by the simulation program, see Section 3.1 for details) andarXiv:2112.08866v5  [stat.ME]  8 Nov 2022Schmitt, Bürkner, Köthe, and RadevTraining Set",
        "arxiv": "2112.08866",
        "doi": null,
        "file_name": "2112.08866.pdf",
        "file_path": "./sources/2112.08866/2112.08866.pdf",
        "title": "Detecting Model Misspecification in Amortized Bayesian Inference with Neural Networks",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/marvinschmitt/ModelMisspecificationBF"
                }
            ]
        }
    },
    "2201.01357": {
        "abstract": "AbstractEstimation of heterogeneous treatment effects is an active area of researchin causal inference. Most of the existing methods, however, focus on estimatingthe conditional average treatment effects of a single, binary treatment given aset of pre-treatment covariates. In this paper, we propose a method to estimatethe heterogeneous causal effects of high-dimensional treatments, which posesunique challenges in terms of estimation and interpretation. The proposedapproach is based on a Bayesian mixture of regularized logistic regressions toidentify groups of units who exhibit similar patterns of treatment effects. Bydirectly modeling cluster membership with covariates, the proposed method-ology allows one to explore the unit characteristics that are associated withdifferent patterns of treatment effects. Our motivating application is conjointanalysis, which is a popular survey experiment in social science and market-ing research and is based on a high-dimensional factorial design. We applythe proposed methodology to the conjoint data, where survey respondents areasked to select one of two immigrant profiles with randomly selected attributes.∗The methods described in this paper can be implemented using open-source software FactorHetavailable at https://www.github.com/mgoplerud/FactorHet. We thank Jelena Bradic and partic-ipants at the 2021 Joint Statistical Meetings, the University of North Carolina Chapel Hill Methodsand Design Workshop, and the Bocconi Institute for Data Science and Analytics Seminar for helpfulfeedback on this paper. We also wish to thank two anonymous reviewers from the Alexander andDiviya Maguro Peer Pre-Review Program at Harvard’s Institute for Quantitative Social Science.Imai thanks the Alfred P. Sloan Foundation (Grant number 2020–13946) for partial support. Pash-ley was partially supported by the National Science Foundation Graduate Research Fellowship whileworking on this project under Grant No. DGE1745303. Any opinion, findings, and conclusions orrecommendations expressed in this material are those of the authors and do not necessarily reflectthe views of the National Science Foundation.†Assistant Professor, Department of Political Science, University of Pittsburgh. 4600 Wesley W.Posvar Hall, Pittsburgh, PA 15260. Email: mgoplerud@pitt.edu. URL: https://mgoplerud.com‡Professor, Department of Government and Department of Statistics, Harvard University.1737 Cambridge Street, Institute for Quantitative Social Science, Cambridge MA 02138. Email:imai@harvard.edu URL: https://imai.fas.harvard.edu§Assistant Professor, Department of Statistics, Rutgers University. 501 Hill Center, 110 Frel-inghuysen Road, Piscataway, NJ 08854. Email: nicole.pashley@rutgers.eduarXiv:2201.01357v2  [stat.ME] ",
        "arxiv": "2201.01357",
        "doi": null,
        "file_name": "2201.01357.pdf",
        "file_path": "./sources/2201.01357/2201.01357.pdf",
        "title": "Estimating Heterogeneous Causal Effects of High-Dimensional Treatments: Application to Conjoint Analysis",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/mgoplerud/FactorHet"
                }
            ]
        }
    },
    "2201.04768": {
        "abstract": "AbstractWe study the practical consequences of dataset sampling strategieson the ranking performance of recommendation algorithms. Rec-ommender systems are generally trained and evaluated on samplesof larger datasets. Samples are often taken in a naïve or ad-hoc fash-ion: e.g. by sampling a dataset randomly or by selecting users oritems with many interactions. As we demonstrate, commonly-useddata sampling schemes can have significant consequences on algo-rithm performance. Following this observation, this paper makesthree main contributions: (1) characterizing the effect of samplingon algorithm performance, in terms of algorithm and dataset char-acteristics (e.g. sparsity characteristics, sequential dynamics, etc.);(2) designing SVP-CF, which is a data-specific sampling strategy,that aims to preserve the relative performance of models after sam-pling, and is especially suited to long-tailed interaction data; and (3)developing an oracle, Data-genie, which can suggest the samplingscheme that is most likely to preserve model performance for agiven dataset. The main benefit of Data-genie is that it will allowrecommender system practitioners to quickly prototype and com-pare various approaches, while remaining confident that algorithmperformance will be preserved, once the algorithm is retrained anddeployed on the complete data. Detailed experiments show thatusing Data-genie, we can discard upto 5× more data than anysampling strategy with the same level of performance.CCS Concepts• Information systems→ Recommender systems; • Comput-ing methodologies → Feature selection.KeywordsSampling; Coreset Mining; Benchmarking; Large-scale LearningACM Reference Format:Noveen Sachdeva, Carole-Jean Wu, and Julian McAuley. 2022. On Sam-pling Collaborative Filtering Datasets. In Proceedings of the Fifteenth ACMInternational Conference on Web Search and Data Mining (WSDM ’22), Feb-ruary 21–25, 2022, Tempe, AZ, USA. ACM, New York, NY, USA, 9 pages.https://doi.org/10.1145/3488560.34984391 IntroductionRepresentative sampling of collaborative filtering (CF) data is acrucial problem from numerous stand-points and can be performedPermission to make digital or hard copies of part or all of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).WSDM ’22, February 21–25, 2022, Tempe, AZ, USA© 2022 Copyright held by the owner/author(s).ACM ISBN 978-1-4503-9132-0/22/02.https://doi.org/10.1145/3488560.3498439at various levels: (1) mining hard-negatives while training com-plex algorithms over massive datasets [5, 31]; (2) down-samplingthe item-space to estimate expensive ranking metrics [3, 22]; and",
        "arxiv": "2201.04768",
        "doi": "10.1145/3488560.3498439",
        "file_name": "2201.04768.pdf",
        "file_path": "./sources/2201.04768/2201.04768.pdf",
        "title": "On Sampling Collaborative Filtering Datasets",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/noveens/sampling_cf"
                }
            ]
        }
    },
    "2201.05757": {
        "abstract": "ABSTRACTSecurity incidents such as scams and hacks, have become a majorthreat to the health of the blockchain ecosystem, causing billionsof dollars in losses each year for blockchain users. To reveal thereal-world entities behind the pseudonymous blockchain accountand recover the stolen funds from the massive transaction data,much effort has been devoted to tracing the flow of illicit funds inblockchains recently. However, most current tracing approachesbased on heuristics and taint analysis have limitations in terms ofuniversality, effectiveness, and efficiency. This paper models theblockchain transaction records as a blockchain transaction graphand tackles blockchain transaction tracing as a graph searchingtask. We propose TRacer, a scalable transaction tracing tool foraccount-based blockchains. To infer the relevance between accountsduring graph searching, we develop a novel personalized PageRankmethod in TRacer based on the directed, weighted, temporal, andmulti-relationship blockchain transaction graphs. To the best of ourknowledge, TRacer is the first intelligent transaction tracing toolin account-based blockchains that can handle complex transactionactions in decentralized finance (DeFi). Experimental results andtheoretical analysis prove that TRacer can complete the transac-tion tracing task effectively at a low cost. All codes of TRacer areavailable at GitHub1.CCS CONCEPTS• Applied computing → Digital cash; • Mathematics of com-puting → Graph algorithms.KEYWORDSBlockchain, transaction tracing, personalized PageRank, local com-munity discovery∗Both authors contributed equally to this research.†Corresponding author.1https://github.com/wuzhy1ng/BlockchainSpiderPermission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from permissions@acm.org.KDD ’22, August 14–18, 2022, Washington DC, USA© 2022 Association for Computing Machinery.ACM ISBN 978-1-4503-XXXX-X/22/06. . . $15.00https://doi.org/xx.xxxx/xxxxxxx.xxxxxxxACM Reference Format:Zhiying Wu, Jieli Liu, Jiajing Wu, and Zibin Zheng. 2022. TRacer: Scalable",
        "arxiv": "2201.05757",
        "doi": null,
        "file_name": "2201.05757.pdf",
        "file_path": "./sources/2201.05757/2201.05757.pdf",
        "title": "TRacer: Scalable Graph-based Transaction Tracing for Account-based Blockchain Trading Systems",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/wuzhy1ng/BlockchainSpider"
                }
            ]
        }
    },
    "2201.12195": {
        "abstract": "AbstractThis paper considers the problem of measureestimation under the barycentric coding model(BCM), in which an unknown measure is assumedto belong to the set of Wasserstein-2 barycentersof a finite set of known measures. Estimating ameasure under this model is equivalent to esti-mating the unknown barycentric coordinates. Weprovide novel geometrical, statistical, and com-putational insights for measure estimation underthe BCM, consisting of three main results. Ourfirst main result leverages the Riemannian geome-try of Wasserstein-2 space to provide a procedurefor recovering the barycentric coordinates as thesolution to a quadratic optimization problem as-suming access to the true reference measures. Theessential geometric insight is that the parametersof this quadratic problem are determined by innerproducts between the optimal displacement mapsfrom the given measure to the reference measuresdefining the BCM. Our second main result thenestablishes an algorithm for solving for the co-ordinates in the BCM when all the measures areobserved empirically via i.i.d. samples. We proveprecise rates of convergence for this algorithm—determined by the smoothness of the underlyingmeasures and their dimensionality—thereby guar-anteeing its statistical consistency. Finally, wedemonstrate the utility of the BCM and associatedestimation procedures in three application areas:(i) covariance estimation for Gaussian measures;(ii) image processing; and (iii) natural languageprocessing.1Department of Computer Science, Tufts University2Department of Electrical and Computer Engineering, TuftsUniversity 3Department of Mathematics, Tufts University . Corre-spondence to: Matthew Werenski <matthew.werenski@tufts.edu>.Proceedings of the 39 th International Conference on MachineLearning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy-right 2022 by the author(s).1. IntroductionA number of recent machine learning applications includingcomputer vision (Schmitz et al., 2018; Bonneel et al., 2016),domain adaptation (Montesuma & Mboula, 2021; Redkoet al., 2019), natural language processing (NLP) (Singhet al., 2020; Colombo et al., 2021; Xu et al., 2018), andunsupervised segmentation of multivariate time series data(Cheng et al., 2021) have shown the utility of representingand modeling high-dimensional data as probability distri-butions. The essential insight in these applications is to",
        "arxiv": "2201.12195",
        "doi": null,
        "file_name": "2201.12195.pdf",
        "file_path": "./sources/2201.12195/2201.12195.pdf",
        "title": "Measure Estimation in the Barycentric Coding Model",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/MattWerenski/BCM"
                }
            ]
        }
    },
    "2202.10728": {
        "abstract": "Abstract—Recent studies in Learning to Rank have shown the possibility to effectively distill a neural network from an ensemble ofregression trees. This result leads neural networks to become a natural competitor of tree-based ensembles on the ranking task.Nevertheless, ensembles of regression trees outperform neural models both in terms of efficiency and effectiveness, particularly whenscoring on CPU. In this paper, we propose an approach for speeding up neural scoring time by applying a combination of Distillation,Pruning and Fast Matrix multiplication. We employ knowledge distillation to learn shallow neural networks from an ensemble ofregression trees. Then, we exploit an efficiency-oriented pruning technique that performs a sparsification of the mostcomputationally-intensive layers of the neural network that is then scored with optimized sparse matrix multiplication. Moreover, bystudying both dense and sparse high performance matrix multiplication, we develop a scoring time prediction model which helps indevising neural network architectures that match the desired efficiency requirements. Comprehensive experiments on two publiclearning-to-rank datasets show that neural networks produced with our novel approach are competitive at any point of theeffectiveness-efficiency trade-off when compared with tree-based ensembles, providing up to 4x scoring time speed-up withoutaffecting the ranking quality.Index Terms—Web search, learning-to-rank, neural networks, efficiency, distillation, pruning, matrix multiplication.F©2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or futuremedia, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale orredistribution to servers or lists, or reuse of any copyrighted component of this work in other works.1 INTRODUCTIONTHE estimation of relevance is a task of paramount im-portance in Web search. In fact, search engines providethe users with a list of relevant results answering a infor-mation need formulated as a textual query. In the last years,Learning to Rank (LtR) techniques have been successfullyapplied to solve this task. LtR is the field of machine learn-ing devoted to the development of supervised techniquesaddressing the ranking problem. LtR techniques have beenproficiently used in Web search, a scenario characterizedby tight latency bounds for query processing [11]. For thisreason, the investigation of new LtR techniques targets botheffectiveness and efficiency to provide accurate solutionsthat can be used in modern query processors. State-of-the-art approaches in learning to rank are ensembles of re-gression trees. Specifically, LambdaMART [9] is an effectivestate-of-the-art LtR algorithm that builds ensembles of re-gression trees by optimizing a loss function that depends ona listwise information retrieval metric, e.g., NDCG [30]. Thecounterpart of the retrieval accuracy guaranteed by tree-based models is the computational effort needed to traversehundreds or even thousands of trees. This computationaleffort hinders the application of this kind of models onlow-latency query processors. Furthermore, each tree in anensemble work by testing a sequence of boolean conditionson the input. The natural translation of this structure in if-then-else code conflicts with modern CPU architectures thatheavily rely on branch prediction and caching. A recent line• Cosimo Rulli and Rossano Venturini are with the University of Pisa, Italy.E-mail: cosimo.rulli@phd.unipi.it, rossano.venturini@di.unipi.it.• Franco Maria Nardini, Cosimo Rulli, Salvatore Trani and Rossano Ven-turini are with the ISTI–CNR, Pisa, Italy. E-mail: {cosimo.rulli, f.nardini,s.trani, rossano.venturini}@isti.cnr.it.of research investigates techniques for efficient traversal of",
        "arxiv": "2202.10728",
        "doi": "10.1109/tkde.2022.3152585",
        "file_name": "2202.10728.pdf",
        "file_path": "./sources/2202.10728/2202.10728.pdf",
        "title": "Distilled Neural Networks for Efficient Learning to Rank",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/oneapi-src/oneDNN"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/albanie/convnet-burden"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/hpclab/efficient_nn_for_ltr"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/hpclab/efficient"
                }
            ]
        }
    },
    "2203.01044": {
        "abstract": "ABSTRACTEntity alignment, aiming to identify equivalent entities across dif-ferent knowledge graphs (KGs), is a fundamental problem for con-structing Web-scale KGs. Over the course of its development, thelabel supervision has been considered necessary for accurate align-ments. Inspired by the recent progress of self-supervised learning,we explore the extent to which we can get rid of supervision forentity alignment. Commonly, the label information (positive entitypairs) is used to supervise the process of pulling the aligned enti-ties in each positive pair closer. However, our theoretical analysissuggests that the learning of entity alignment can actually benefitmore from pushing unlabeled negative pairs far away from eachother than pulling labeled positive pairs close. By leveraging thisdiscovery, we develop the self-supervised learning objective forentity alignment. We present SelfKG with efficient strategies tooptimize this objective for aligning entities without label supervi-sion. Extensive experiments on benchmark datasets demonstratethat SelfKG without supervision can match or achieve comparableresults with state-of-the-art supervised baselines. The performanceof SelfKG suggests that self-supervised learning offers great poten-tial for entity alignment in KGs. The code and data are available athttps://github.com/THUDM/SelfKG.CCS CONCEPTS• Computing methodologies→ Neural networks; • Informa-tion systems→ Information integration.KEYWORDSKnowledge Graphs, Self-Supervised Learning, Entity AlignmentACM Reference Format:Xiao Liu, Haoyun Hong, Xinghao Wang, Zeyi Chen, Evgeny Kharlamov,Yuxiao Dong, Jie Tang. 2022. SelfKG: Self-Supervised Entity Alignment inKnowledge Graphs. In Proceedings of the ACM Web Conference 2022 (WWW’22), April 25–29, 2022, Virtual Event, Lyon, France. ACM, New York, NY, USA,11 pages. https://doi.org/10.1145/3485447.3511945Xiao and Haoyun contributed equally to this work.Jie Tang is the corresponding author.Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from permissions@acm.org.WWW ’22, April 25–29, 2022, Virtual Event, Lyon, France.© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-9096-5/22/04. . . $15.00https://doi.org/10.1145/3485447.3511945DWY100K(dbp_wd) DWY100K(dbp_yg) DBP15K(fr_en)80859095",
        "arxiv": "2203.01044",
        "doi": "10.1145/3485447.3511945",
        "file_name": "2203.01044.pdf",
        "file_path": "./sources/2203.01044/2203.01044.pdf",
        "title": "SelfKG: Self-Supervised Entity Alignment in Knowledge Graphs",
        "urls": {
            "git": [
                {
                    "#_appearances": 4,
                    "url": "https://github.com/THUDM/SelfKG"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/nju-websoft/JAPE"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/facebookresearch/faiss"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/syxu828/Crosslingula-KG-Matching"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/nju-websoft/BootEA"
                }
            ]
        }
    },
    "2204.03792": {
        "abstract": null,
        "arxiv": "2204.03792",
        "doi": null,
        "file_name": "2204.03792.pdf",
        "file_path": "./sources/2204.03792/2204.03792.pdf",
        "title": "Deep water wave cloaking using a multi-layered buoyant plate",
        "urls": {}
    },
    "2204.08775": {
        "abstract": "AbstractThere are plenty of excellent plotting libraries. Each excels at a different usecase: one is good for printed 2D publication figures, the other at interactive3D graphics, a third has excellent LATEX integration or is good for creatingdashboards on the web.The aim of Plots.jl is to enable the user to use the same syntax tointeract with many different plotting libraries, such that it is possible to changethe library \"backend\" without needing to touch the code that creates the content– and without having to learn yet another application programming interface(API).This is achieved by the separation of the plot specification from the im-plementation of the actual graphical backend. These plot specifications maybe extended by a \"recipe\" system, which allows package authors and users todefine how to plot any new type (be it a statistical model, a map, a phyloge-netic tree or the solution to a system of differential equations) and create newtypes of plots - without depending on the Plots.jl package. This supportsa modular ecosystem structure for plotting and yields a high reuse potentialacross the entire julia package ecosystem. Plots.jl is publicly available athttps://github.com/JuliaPlots/Plots.jl.1arXiv:2204.08775v3  [cs.GR]  17 Jun 2022https://github.com/JuliaPlots/Plots.jlKeywordsvisualization; julia; plotting; julia-language; user-extendableIntroductionJulia[5] is a programming language that achieves high performance and stellarmodularity and composability by making use of multiple dispatch and just-in-time compilation. This comes at the cost of increased latency as the languagecompiles new machine-code the first time any function is called on new typesof arguments. This is notoriously an issue for packages that call a large part",
        "arxiv": "2204.08775",
        "doi": null,
        "file_name": "2204.08775.pdf",
        "file_path": "./sources/2204.08775/2204.08775.pdf",
        "title": "Plots.jl -- a user extendable plotting API for the julia programming language",
        "urls": {
            "git": [
                {
                    "#_appearances": 3,
                    "url": "https://github.com/ma-laforge/InspectDR.jl"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/src-d/hercules"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/JuliaPlots/UnicodePlots.jl"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/JuliaPlots/Plots.jl"
                }
            ],
            "zenodo": [
                {
                    "#_appearances": 2,
                    "url": "https://doi.org/10.5281/zenodo.4725318"
                },
                {
                    "#_appearances": 1,
                    "url": "https://doi.org/10.5281/zenodo.5837925"
                }
            ]
        }
    },
    "2204.0968": {
        "abstract": "Abstract: Although numerous reports using methods such as molecular dynamics, cellular au-tomata, and artificial chemistry have clarified the process connecting non-life and life on protocell simulations, none of the models could simultaneously explain the emergence of cell shape, con-tinuous self-replication, and replication control solely from molecular reactions and diffusion. Herein, we developed a model to generate all three conditions, except evolution ability, from hy-pothetical chains of chemical and molecular polymerization reactions. The present model considers a 2D lattice cell space, where virtual molecules are placed in each cell, and molecular reactions in each cell are based on a multiset rewriting rule, indicating stochastic transition of molecular species. The reaction paths of virtual molecules were implemented by replacing the rules of cellular au-tomata that generate Turing patterns with molecular reactions. The emergence of a cell-like form with all three conditions except evolution ability was modeled and demonstrated using only mo-lecular diffusion, reaction, and polymerization for modeling the chemical reactions of 15 types of molecules and two types of polymerized molecules. Furthermore, controlling self-replication is possible by changing the initial arrangement of a specific molecule. In summary, the present model is capable of investigating and refining existing hypotheses on the emergence of life. Keywords: origin of life; artificial life; artificial chemistry; multiset model; protocell; entropy  1. Introduction In science, one of the great challenges is clarifying when, where, why, and how life first arose as well as the form that the first life took. To this end, research related to the origin of life has long been conducted, and a vast number of papers and books, e.g. [1,2], have been published. “When” life came into being is a question that has been studied from two perspectives: (a) from a geological perspective, i.e., examining when the  2 of 29   Earth's environment became compatible with the survival of life and (b) from the perspective of studying the record of life using fossil sources. According to the geology of Earth, various hypotheses have been postulated, e.g., life arose 4.4 billion years ago when the environment and conditions were ripe for its emergence. When attempting to determine the emergence of first life from fossils, traces of life were found to exist 3.8 billion years ago; however, notable limitations exist to tracing fossils to ascertain the emergence of life. Several hypotheses have been postulated to explain “where” the first life arose, which may have included hydrothermal vents, the submarine subsurface, and land-based hot springs and geysers, which each demonstrate various advantages and disadvantages. For each hypothesis, many studies have been conducted to examine the materials and physical conditions necessary for the composition of life in the proposed location. As for “why” life emerged, no one hypothesis directly answers the question, but research on nonequilibrium thermodynamics, such as dissipative structure and entropy studies, may help find an answer. The concept of life as a physical phenomenon began with Schrödinger, who proposed “negative entropy” [3], and with the dissipative struc-ture proposed by Prigogine [4]. Considering life as dissipative structure phenomenon, approaching the “why” question of the origin of life is possible by exploring the condi-tions in which dissipative structures emerge. Further, this question might be answered by studying complex adaptive systems or self-organizing phenomena. Concerning the questions of “what” was the first life and “how” did it come into being, several hypotheses have been proposed, including the RNA world hypothesis [5]. Indeed, two major classifications exist: the “RNA world hypothesis,” suggesting that RNA with an autocatalytic function was the first life, and the “metabolic world (chemical evolution) hypothesis” [6], which points to the catalytic function of mineral surfaces etc. ",
        "arxiv": "2204.0968",
        "doi": null,
        "file_name": "2204.0968.pdf",
        "file_path": "./sources/2204.0968/2204.0968.pdf",
        "title": "Emergent simulation of cell-like shapes satisfying the conditions of life using lattice-type multiset chemical model",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/Takeshi-Ishida/Emergence-simulation-of-biological-cell-like-shapes"
                }
            ]
        }
    },
    "2204.12201": {
        "abstract": "ABSTRACTThe dramatic increase of data breaches in modern computing plat-forms has emphasized that access control is not sufficient to protectsensitive user data. Recent advances in cryptography allow end-to-end processing of encrypted data without the need for decryptionusing Fully Homomorphic Encryption (FHE). Such computationhowever, is still orders of magnitude slower than direct (unen-crypted) computation. Depending on the underlying cryptographicscheme, FHE schemes can work natively either at bit-level usingBoolean circuits, or over integers using modular arithmetic. Op-erations on integers are limited to addition/subtraction and mul-tiplication. On the other hand, bit-level arithmetic is much morecomprehensive allowing more operations, such as comparison anddivision. While modular arithmetic can emulate bit-level compu-tation, there is a significant cost in performance. In this work, wepropose a novel method, dubbed bridging, that blends faster andrestricted modular computation with slower and comprehensivebit-level computation, making them both usable within the sameapplication and with the same cryptographic scheme instantiation.We introduce and open source C++ types representing the twodistinct arithmetic modes, offering the possibility to convert fromone to the other. Experimental results show that bridging modularand bit-level arithmetic computation can lead to 1-2 orders of mag-nitude performance improvement for tested synthetic benchmarks,as well as one real-world FHE application: a genotype imputationcase study.CCS CONCEPTS• Security and privacy → Domain-specific security and pri-vacy architectures.KEYWORDSfully homomorphic encryption, privacy-preserving computationACM Reference Format:Eduardo Chielle, Oleg Mazonka, Homer Gamil, and Michail Maniatakos.2022. Accelerating Fully Homomorphic Encryption by BridgingModular andBit-Level Arithmetic. In IEEE/ACM International Conference on Computer-Aided Design (ICCAD ’22), October 30-November 3, 2022, San Diego, CA, USA.ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3508352.3549415Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from permissions@acm.org.ICCAD ’22, October 30-November 3, 2022, San Diego, CA, USA© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-9217-4/22/10. . . $15.00https://doi.org/10.1145/3508352.35494151 INTRODUCTIONWith the ever increasing rates of data generation, digital informa-",
        "arxiv": "2204.12201",
        "doi": "10.1145/3508352.3549415",
        "file_name": "2204.12201.pdf",
        "file_path": "./sources/2204.12201/2204.12201.pdf",
        "title": "Accelerating Fully Homomorphic Encryption by Bridging Modular and Bit-Level Arithmetic",
        "urls": {
            "git": [
                {
                    "#_appearances": 3,
                    "url": "https://github.com/momalab/e3"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/nucypher/nufhe"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/Microsoft/SEAL"
                }
            ]
        }
    },
    "2205.03018": {
        "abstract": "AbstractWe introduce Aksharantar, the largest pub-licly available transliteration dataset for21 Indic languages containing 26 milliontransliteration pairs. We build this datasetby mining transliteration pairs from largemonolingual and parallel corpora, as wellas collecting transliterations from humanannotators to ensure diversity of words andrepresentation of low-resource languages.We introduce a new, large, diverse test-set for Indic language transliteration con-taining 103k words pairs spanning 19 lan-guages that enables fine-grained analysis oftransliteration models.We train the IndicXlit model on the Ak-sharantar training set. IndicXlit is a singletransformer-based multilingual translitera-tion model for roman to Indic script con-version supporting 21 Indic languages. Itachieves state-of-the art results on the Dak-shina testset, and establishes strong base-lines on the Aksharantar testset releasedalong with this work.All the datasets and models are pub-licly available at https://indicnlp.ai4bharat.org/aksharantar. We hopethe availability of these large-scale, openresources will spur innovation for Indiclanguage transliteration and downstreamapplications.1 IntroductionThe Indian subcontinent is home to di-verse languages spanning four major lan-guage families (Indo-Aryan branch of Indo-European, Dravidian, Austro-Asiatic andTibeto-Burman) spoken by more than a bil-lion speakers. These languages are writ-ten in a variety of scripts: (a) Brahmi fam-ily of abugida scripts for most major Indiclanguages, (b) Arabic-derived abjad scriptsfor some languages like Urdu, Kashmiri andSindhi, and (c) Alphabetic Roman script formany languages with recent literary history.Some of these scripts are used by multiple lan-guages (e.g., Devanagari script is used to writeHindi, Marathi, Konkani, Maithili, and San-skrit among others; Bengali script is used towrite Bengali, Assamese, and Santali).These statistics highlight the scale and di-",
        "arxiv": "2205.03018",
        "doi": null,
        "file_name": "2205.03018.pdf",
        "file_path": "./sources/2205.03018/2205.03018.pdf",
        "title": "Aksharantar: Towards building open transliteration tools for the next billion users",
        "urls": {
            "git": [
                {
                    "#_appearances": 3,
                    "url": "https://github.com/notAI-tech/Anuvaad"
                },
                {
                    "#_appearances": 3,
                    "url": "https://github.com/anoopkunchukuttan/indic_nlp_library"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/AI4Bharat/IndianNLP-Transliteration"
                }
            ]
        }
    },
    "2205.03313": {
        "abstract": "AbstractParody is a figurative device used for mimick-ing entities for comedic or critical purposes.Parody is intentionally humorous and ofteninvolves sarcasm. This paper explores jointlymodelling these figurative tropes with the goalof improving performance of political parodydetection in tweets. To this end, we present amulti-encoder model that combines three par-allel encoders to enrich parody-specific repre-sentations with humor and sarcasm informa-tion. Experiments on a publicly available dataset of political parody tweets demonstrate thatour approach outperforms previous state-of-the-art methods.11 IntroductionParody is a figurative device which imitates enti-ties such as politicians and celebrities by copyingtheir particular style or a situation where the en-tity was involved (Rose, 1993). It is an intrinsicpart of social media as a relatively new comedicform (Vis, 2013). A very popular type of parodyis political parody, which is used to express politi-cal opposition and civic engagement (Davis et al.,2018).One of the hallmarks of parody expression is thedeployment of other figurative devices, such as hu-mor and sarcasm, as emphasized on studies of par-ody in linguistics (Haiman et al., 1998; Highfield,2016). For example, in Table 1 the text expressessarcasm about Myspace2 being a ‘winning tech-nology’, while mocking the fact that three morepopular social media sites were unavailable. Thisexample also highlights the similarities betweenparody and real tweets, which may pose issues tomisinformation classification systems (Mu and Ale-tras, 2020).1Code is available here https://github.com/iamoscar1/Multi_Encoder_Model_for_Political_Parody_Prediction2https://myspace.comTwitterHandle @Queen_UKParodytweetBoris Johnson on the phone. Verysmug that #myspace hasn’t gonedown. Says he’s always backedwinning technologies #whatsappdown#instagramdown #FacebookIsDown",
        "arxiv": "2205.03313",
        "doi": "10.18653/v1/2022.naacl-main.131",
        "file_name": "2205.03313.pdf",
        "file_path": "./sources/2205.03313/2205.03313.pdf",
        "title": "Combining Humor and Sarcasm for Improving Political Parody Detection",
        "urls": {
            "git": [
                {
                    "#_appearances": 3,
                    "url": "https://github.com/iamoscar1/Multi_Encoder_Model_for_Political_Parody_Prediction"
                }
            ]
        }
    },
    "2205.10986": {
        "abstract": "AbstractMulti-Label Image Classification (MLIC) ap-proaches usually exploit label correlations toachieve good performance. However, empha-sizing correlation like co-occurrence may over-look discriminative features of the target itselfand lead to model overfitting, thus undermin-ing the performance. In this study, we pro-pose a generic framework named Parallel Self-Distillation (PSD) for boosting MLIC models.PSD decomposes the original MLIC task intoseveral simpler MLIC sub-tasks via two elabo-rated complementary task decomposition strategiesnamed Co-occurrence Graph Partition (CGP) andDis-occurrence Graph Partition (DGP). Then, theMLIC models of fewer categories are trained withthese sub-tasks in parallel for respectively learn-ing the joint patterns and the category-specific pat-terns of labels. Finally, knowledge distillationis leveraged to learn a compact global ensem-ble of full categories with these learned patternsfor reconciling the label correlation exploitationand model overfitting. Extensive results on MS-COCO and NUS-WIDE datasets demonstrate thatour framework can be easily plugged into manyMLIC approaches and improve performances of re-cent state-of-the-art approaches. The explainablevisual study also further validates that our methodis able to learn both the category-specific and co-occurring features. The source code is released athttps://github.com/Robbie-Xu/CPSD.1 IntroductionNatural images often contain multiple visual objects, whichcan be characterized by a set of image labels. Multi-labelimage classification (MLIC) task is to recognize all these ob-jects, which is highly relevant to other vision tasks such asobject detection, image retrieval, and semantic segmentation.Most existing MLIC research works focus on exploitingthe label correlation property, which distinguishes it from thesingle-label image classification problem. Label correlationexploitation strategies, such as pair-wise and high-order la-∗Corresponding Authorlaptopmouse mouselaptoptruckcarboattruckcarboatEmphasizing dependency Decomposing dependency ",
        "arxiv": "2205.10986",
        "doi": "10.24963/ijcai.2022/208",
        "file_name": "2205.10986.pdf",
        "file_path": "./sources/2205.10986/2205.10986.pdf",
        "title": "Boosting Multi-Label Image Classification with Complementary Parallel Self-Distillation",
        "urls": {
            "git": [
                {
                    "#_appearances": 1,
                    "url": "https://github.com/Robbie-Xu/CPSD"
                }
            ]
        }
    },
    "2206.00831": {
        "abstract": "ABSTRACTLow-rank tensor models have been applied in accelerating dy-namic magnetic resonance imaging (dMRI). Recently, a newtensor nuclear norm based on t-SVD has been proposed andapplied to tensor completion. Inspired by the different proper-ties of the tensor nuclear norm (TNN) and the Casorati matrixnuclear norm (MNN), we introduce a combined TNN and Ca-sorati MNN regularizations framework to reconstruct dMRI,which we term as TMNN. The proposed method simultane-ously exploits the spatial structure and the temporal correla-tion of the dynamic MR data. The optimization problem canbe efficiently solved by the alternating direction method ofmultipliers (ADMM). In order to further improve the com-putational efficiency, we develop a fast algorithm under theCartesian sampling scenario. Numerical experiments basedon cardiac cine MRI and perfusion MRI data demonstrate theperformance improvement over the traditional Casorati nu-clear norm regularization method.Index Terms— Dynamic MRI reconstruction, tensor nu-clear norm, t-SVD1. INTRODUCTIONDynamic magnetic resonance imaging (dMRI) is one of themost important non-invasive imaging modalities, which hasfound a wide range of applications in clinical practice. How-ever, due to the physical limitations, it is usually challengingto obtain dynamic MR images with high spatiotemporal res-olution within clinically acceptable scan time. By exploitingthe low-rank structure of the spatiotemporal Casorati matrixformulated by extracting and unfolding each time frame as acolumn, the low-rank matrix recovery methods [1] have beensuccessfully applied to reconstruct dynamic MR images fromhighly undersampled k-space data to accelerate dMRI.Low-rank tensor priors have been introduced as power-ful alternatives due to their improvement in recovered imagequality [2, 3, 4]. Compared with matrix, tensor is a morenatural representation for multi-frame dynamic MR data.Recently, a new tensor decomposition called tensor singularThis work is supported by the National Natural Science Foundationof China under Grant 61871159 and Natural Science Foundation of Hei-longjiang YQ2021F005.value decomposition (t-SVD) [5] has been proposed. Com-pared with the traditional tensor decompositions (e.g., CP,TUCKER), t-SVD can be easily computed by solving thematrix SVDs in the Fourier domain. Based on t-SVD, Lu etal. proposed a new tensor nuclear norm (TNN) [6], which isthe convex envelope of the tensor average rank. In addition,unlike certain traditional tensor constraints, utilizing the TNNconstraint avoids explicit selection of tensor ranks. Someworks have adopted this framework to reconstruct dMRI.Banco et al. [7] have applied t-SVD in dMRI reconstruc-",
        "arxiv": "2206.00831",
        "doi": "10.1109/isbi52829.2022.9761409",
        "file_name": "2206.00831.pdf",
        "file_path": "./sources/2206.00831/2206.00831.pdf",
        "title": "Dynamic Cardiac MRI Reconstruction Using Combined Tensor Nuclear Norm and Casorati Matrix Nuclear Norm Regularizations",
        "urls": {}
    },
    "2206.11022": {
        "abstract": "AbstractThe Petit Larousse illustré is a French dictionary first published in 1905. Its division in two main parts on language and onhistory and geography corresponds to a major milestone in French lexicography as well as a repository of general knowledgefrom this period. Although the value of many entries from 1905 remains intact, some descriptions now have a dimension thatis more historical than contemporary. They are nonetheless significant to analyze and understand cultural representations fromthis time. A comparison with more recent information or a verification of these entries would require a tedious manual work.In this paper, we describe a new lexical resource, where we connected all the dictionary entries of the history and geographypart to current data sources. For this, we linked each of these entries to a wikidata identifier. Using the wikidata links, we canautomate more easily the identification, comparison, and verification of historically-situated representations. We give a fewexamples on how to process wikidata identifiers and we carried out a small analysis of the entities described in the dictionaryto outline possible applications. The resource, i.e. the annotation of 20,245 dictionary entries with wikidata links, is availablefrom GitHub (https://github.com/pnugues/petit_larousse_1905/).Keywords: entity annotation, entity linking, digital humanities1. IntroductionThe Petit Larousse illustré is a one-volume dictionaryof French, first published in 1905. Its ambition was asmuch cultural as linguistic with numerous illustrationsand encyclopedic developments accompanying the def-initions of words, things, and people. It was also a trulypopular dictionary with, for instance, a sense order thatfollowed the sense frequency and not the word history.As a consequence, this dictionary was a true commer-cial success and became ubiquitous in French homes.From its first edition, this dictionary has been annuallyupdated until today and has kept a far-reaching culturalinfluence.1.1. The Petit Larousse IllustréThe Petit Larousse illustré stems from two main pre-vious encyclopedias: A large one, the Grand diction-naire universel du XIXe siècle in 15 volumes and twosupplements, published between 1866 an 1876 and ashortened and illustrated version of it: The NouveauLarousse illustré in seven volumes and a supplementfrom 1897 to 1904.While an offspring of them, the Petit Larousse illustréhas a completely new structure and format: It consistsof one compact and portable volume, divided into threeparts. These parts are of unequal sizes:1. The first one, langue française, is a dictionary ofFrench with some encyclopedic content. It is re-stricted to the common nouns, verbs, adjectives,adverbs, and grammatical words (1066 pages);2. The second part, locutions, contains quotes,mostly from classical Latin authors. It is muchsmaller than the two other parts (32 pages); and3. The third part, histoire et géographie, containsshort encyclopedic descriptions of people, coun-tries, locations, intellectual or art works (660pages).The third part, often called the “proper nouns,” has",
        "arxiv": "2206.11022",
        "doi": null,
        "file_name": "2206.11022.pdf",
        "file_path": "./sources/2206.11022/2206.11022.pdf",
        "title": "Connecting a French Dictionary from the Beginning of the 20th Century to Wikidata",
        "urls": {
            "git": [
                {
                    "#_appearances": 4,
                    "url": "https://github.com/pnugues/petit_larousse_1905"
                }
            ]
        }
    },
    "2206.11038": {
        "abstract": "AbstractOver the last years, supervised learning (SL) has established itself as the state-of-the-art for data-driven turbulencemodeling. In the SL paradigm, models are trained based on a dataset, which is typically computed a priori from a high-fidelity solution by applying the respective filter function, which separates the resolved and the unresolved flow scales.For implicitly filtered large eddy simulation (LES), this approach is infeasible, since here, the employed discretizationitself acts as an implicit filter function. As a consequence, the exact filter form is generally not known and thus, thecorresponding closure terms cannot be computed even if the full solution is available. The reinforcement learning (RL)paradigm can be used to avoid this inconsistency by training not on a previously obtained training dataset, but insteadby interacting directly with the dynamical LES environment itself. This allows to incorporate the potentially compleximplicit LES filter into the training process by design. In this work, we apply a reinforcement learning framework tofind an optimal eddy-viscosity for implicitly filtered large eddy simulations of forced homogeneous isotropic turbulence.For this, we formulate the task of turbulence modeling as an RL task with a policy network based on convolutionalneural networks that adapts the eddy-viscosity in LES dynamically in space and time based on the local flow state only.We demonstrate that the trained models can provide long-term stable simulations and that they outperform establishedanalytical models in terms of accuracy. In addition, the models generalize well to other resolutions and discretizations.We thus demonstrate that RL can provide a framework for consistent, accurate and stable turbulence modeling especiallyfor implicitly filtered LES.Keywords: Turbulence Modeling, Deep Reinforcement Learning, Large Eddy Simulation1. IntroductionMost flows in nature and in engineering are turbu-lent. Such turbulent flows are characterized by a widerange of active flow scales oftentimes spanning orders ofmagnitude. Even though the governing equations of fluidmotion, the Navier-Stokes equations, are known, the di-rect numerical simulation (DNS) of this broad range offlow scales is usually intractable. Instead, reduced modelequations are solved, which consider only the most impor-tant scales of the flow. The most popular approaches arethe Reynolds-averaged Navier-Stokes (RANS) equations,which compute the time averaged flow field, and the largeeddy simulation (LES), which resolves only the most en-ergetic flow scales in space and time. Both approachesintroduce additional terms into the governing equationson the coarse level, which embody the footprint of thenon-resolved fine scales onto the resolved flow field. Sincethese terms are a function of the unknown full solution, theequations remain unclosed. Turbulence models are typi-cally employed to approximate the unknown closure terms∗Corresponding authorEmail addresses: marius.kurz@iag.uni-stuttgart.de (MariusKurz), philipp.offenhaeuser@hpe.com (Philipp Offenhäuser),beck@iag.uni-stuttgart.de (Andrea Beck)based on the available coarse scale data in order to closethe equations. Despite decades of research, no overall bestmodel has emerged yet. Moreover, most models employempirical model parameters that have to be tuned to thespecific flow and discretization at hand. To this end, re-cent advances in turbulence modeling increasingly striveto complement the established mathematical and physicalmodeling strategies by the emerging data-driven paradigm",
        "arxiv": "2206.11038",
        "doi": null,
        "file_name": "2206.11038.pdf",
        "file_path": "./sources/2206.11038/2206.11038.pdf",
        "title": "Deep Reinforcement Learning for Turbulence Modeling in Large Eddy Simulations",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/tensorflow/agents"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/flexi-framework/relexi"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/flexi-framework/DRL_LES"
                }
            ]
        }
    },
    "2206.14504": {
        "abstract": "Abstract. We present a statistical model for German medical naturallanguage processing trained for named entity recognition (NER) as anopen, publicly available model. The work serves as a refined successorto our first GERNERMED model which is substantially outperformedby our work. We demonstrate the effectiveness of combining multipletechniques in order to achieve strong results in entity recognition per-formance by the means of transfer-learning on pretrained deep languagemodels (LM), word-alignment and neural machine translation. Due tothe sparse situation on open, public medical entity recognition modelsfor German texts, this work offers benefits to the German research com-munity on medical NLP as a baseline model. Since our model is based onpublic English data, its weights are provided without legal restrictionson usage and distribution. The sample code and the statistical model isavailable at:https://github.com/frankkramer-lab/GERNERMED-ppKeywords: natural language processing, medical NLP, medical namedentity recognition, transfer learning, German NLP, artificial intelligence1 IntroductionExtraction and processing of key information from medical notes and doctors’letters poses a common challenge in advanced digitization of health care sys-tems. In particular, research-oriented data mining of non-research-centric datasources (often referred to as second use) often requires expensive data harmo-nization processes in order to transform unstructured or semi-structured datainto strictly structured, uniform data representations such as HL7 or FHIR.While manually solving these processes can be carried out for document analy-sis on certain studies, it is rendered impractical for large scale text analysis onlegacy data or processing day-to-day clinical data.http://arxiv.org/abs/2206.14504v2https://github.com/frankkramer-lab/GERNERMED-pp2 Frei et al.Handling heterogeneous data from text-based documents is a central subjectof natural language processing. In recent years deep learning-inspired approacheshave been applied successfully to tackle various NLP tasks effectively. However,training deep language models requires proper datasets in regards to aspects likecorpus size, annotation work, data diversity and overall dataset quality, in orderto retrieve well-performing models. In medical NLP, obtaining such annotateddatasets remains rather difficult because the use and publication of medical datais highly restricted for the reasons of privacy and country-dependent data protec-tion legislation. Even though medical datasets have been published in English,such datasets for German texts in contrast are still frequently unavailable toexternal researchers.In this paper, we propose an approach of combining multiple ideas to obtaina German medical NLP model:– Translation: The state of German medical corpora is limited and the useof internal datasets for training and publication of models is legally disputedin regards to the fact that the possibility of privacy-concerned training dataextraction cannot be excluded. In contrast, medical datasets in English havealready been published and therefore, neural machine translation (NMT)can be applied to obtain German data from English datasets.– Annotation Projection: Annotation of large corpora is crucial for su-",
        "arxiv": "2206.14504",
        "doi": null,
        "file_name": "2206.14504.pdf",
        "file_path": "./sources/2206.14504/2206.14504.pdf",
        "title": "GERNERMED++: Transfer Learning in German Medical NLP",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/frankkramer-lab/GERNERMED-pp"
                }
            ]
        }
    },
    "2207.00921": {
        "abstract": "Abstract. We give a process for verifying numerical programs againsttheir functional specifications. Our implementation is capable of auto-matically verifying programs against tight error bounds featuring com-mon elementary functions. We demonstrate and evaluate our implemen-tation on several examples, yielding the first fully verified SPARK im-plementations of the sine and square root functions.The process integrates existing tools using a series of transformationsand derivations, building on the proving process in SPARK where Why3produces Verification Conditions (VCs) and tools such as SMT solversattempt to verify them. We add steps aimed specifically at VCs thatcontain inequalities with both floating-point operations and exact realfunctions. PropaFP is our open-source implementation of these steps.The steps include symbolic simplifications, deriving bounds via inter-val arithmetic, and safely replacing floating-point operations with ex-act operations, utilizing tools such as FPTaylor or Gappa to bound thecompound rounding errors of expressions. Finally, the VCs are passed toprovers such as dReal, MetiTarski or LPPaver which attempt to completethe proof or suggest possible counter-examples.Keywords: Floating-Point Computation · Software Verification · Au-tomated Proving · Interval Methods · Software Assurance.1 IntroductionContext. Safety-critical software often includes numerical calculations. Sincemost processors now contain a floating-point (FP) unit, these calculations of-ten use FP arithmetic to utilise the speed and precision of FP units.Those developing safety-critical programs need to provide guarantees that theprogram behaves in a precisely specified way. This can be achieved via formalverification, i.e., proving that the program adheres to some specification.For example, consider the Ada function in Listing 1.1 that computes a Taylorapproximation of the sine function. We specify that this function gives a resultvery close to the exact sine function under some conditions:X ∈ [−0.5, 0.5] =⇒ |Taylor_Sin’Result− sin(X)| ≤ 0.001 (1)? This project has received funding from AdaCore Ltd and from FFFFFFFFF F FF the Euro-pean Union’s Horizon 2020 research and innovation programme under the MarieSkłodowska-Curie grant agreement No 731143.arXiv:2207.00921v",
        "arxiv": "2207.00921",
        "doi": null,
        "file_name": "2207.00921.pdf",
        "file_path": "./sources/2207.00921/2207.00921.pdf",
        "title": "Auto-active Verification of Floating-point Programs via Nonlinear Real Provers",
        "urls": {
            "git": [
                {
                    "#_appearances": 5,
                    "url": "https://github.com/rasheedja/PropaFP"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/rasheedja/LPPaver"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/michalkonecny/aern2"
                }
            ]
        }
    },
    "2207.02242": {
        "abstract": "Abstract—We consider resource management problems inmulti-user wireless networks, which can be cast as optimizinga network-wide utility function, subject to constraints on thelong-term average performance of users across the network.We propose a state-augmented algorithm for solving the afore-mentioned radio resource management (RRM) problems, where,alongside the instantaneous network state, the RRM policy takesas input the set of dual variables corresponding to the constraints,which evolve depending on how much the constraints are violatedduring execution. We theoretically show that the proposed state-augmented algorithm leads to feasible and near-optimal RRMdecisions. Moreover, focusing on the problem of wireless powercontrol using graph neural network (GNN) parameterizations,we demonstrate the superiority of the proposed RRM algorithmover baseline methods across a suite of numerical experiments.Index Terms—Radio resource management, wireless networks,graph neural networks, Lagrangian duality, state augmentation,wireless power control.I. INTRODUCTIONWith the proliferation of 5G network implementationsacross the globe and research already underway for future6G wireless networks, novel wireless services and capabili-ties are expected to emerge that require carefully-optimizedmanagement of wireless resources. Aside from traditionalapproaches for addressing such radio resource management(RRM) problems [2]–[6], learning-based methods have re-cently gained significant traction and demonstrated superiorperformance over prior approaches [7]–[12]. Such methods areenvisioned to play a key role in current and future wirelessnetworks with the ubiquitous availability of computationalresources both at the end-user devices and within the networkinfrastructure [13]–[16].As a general formulation of the RRM problem, similarly toprior work [7], [10], [17], [18], we consider a network utilitymaximization problem, subject to multiple constraints, whereboth the utility and the constraints are defined based on thelong-term average performance of users across the network.A common method for solving such problems is to move tothe Lagrangian dual domain, where a single objective, i.e., theN. NaderiAlizadeh and A. Ribeiro are with the Department of Elec-trical and Systems Engineering, University of Pennsylvania, Philadelphia,PA 19104, USA (e-mails: {nnaderi, aribeiro}@seas.upenn.edu). M. Eisenis with Intel Labs, Intel Corporation, Hillsboro, OR 97124, USA (e-mail:mark.eisen@intel.com).This work was supported in part by ARL DCIST CRA under GrantW911NF-17-2-0181, the AI Institute for Learning-Enabled Optimization atScale (TILOS) (NSF CCF-2112665), and the NSF-Simons Research Collab-oration on the Mathematical and Scientific Foundations of Deep Learning(MoDL) (NSF DMS-2031985).This paper has been presented in part at the 2022 Asilomar Conference on",
        "arxiv": "2207.02242",
        "doi": "10.1109/tsp.2022.3229948",
        "file_name": "2207.02242.pdf",
        "file_path": "./sources/2207.02242/2207.02242.pdf",
        "title": "State-Augmented Learnable Algorithms for Resource Management in Wireless Networks",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/navid-naderi/StateAugmented_RRM_GNN"
                }
            ]
        }
    },
    "2207.02464": {
        "abstract": "ABSTRACTRecent years have seen the emergence of non-cooperative objects in space, like failed satellites andspace junk. These objects are usually operated or collected by free-float dual-arm space manipulators.Thanks to eliminating the difficulties of modeling and manual parameter-tuning, reinforcementlearning (RL) methods have shown a more promising sign in the trajectory planning of spacemanipulators. Although previous studies demonstrate their effectiveness, they cannot be applied intracking dynamic targets with unknown rotation (non-cooperative objects). In this paper, we proposeda learning system for motion planning of free-float dual-arm space manipulator (FFDASM) towardsnon-cooperative objects. Specifically, our method consists of two modules. Module I realizes themulti-target trajectory planning for two end-effectors within a large target space. Next, Module II takesas input the point clouds of the non-cooperative object to estimate the motional property, and thencan predict the position of target points on an non-cooperative object. We leveraged the combinationof Module I and Module II to track target points on a spinning object with unknown regularitysuccessfully. Furthermore, the experiments also demonstrate the scalability and generalization of ourlearning system.Keywords Space robotics ·Motion planning · Reinforcement learning1 IntroductionWith the continuous increase of non-cooperative space objects like faulty satellites and space junk, space manipulatorsbecome indispensable in many space maintenance tasks. Free-float dual-arm space manipulator (FFDASM) is acommon configure for space manipulators, of which the free-float advantage helps to save fuel consumed in adjustingthe pose and the dual-arm structure enlarges the workspace efficiently [1]. However, the characteristic of FFDASM hassome negative impacts on model-based algorithms in trajectory planning tasks. Due to the coupling effects between thebase and two manipulators, there are many time-varying and dynamic parameters in the Jacobian matrix, which arehard to be identified [2, 3]. Furthermore, trajectory planning aiming for a single target should take the joint trajectories∗Corresponding author: Tao Zhang(taozhang@tsinghua.edu.cn)arXiv:2207.02464v1  [cs.RO]  6 Jul 2022PRIME AI paperExternal perceptionPoint cloudsInternal perception",
        "arxiv": "2207.02464",
        "doi": null,
        "file_name": "2207.02464.pdf",
        "file_path": "./sources/2207.02464/2207.02464.pdf",
        "title": "A Learning System for Motion Planning of Free-Float Dual-Arm Space Manipulator towards Non-Cooperative Object",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/openai/baselines"
                }
            ]
        }
    },
    "2207.03414": {
        "abstract": "AbstractDose volume histogram (DVH) metrics are widely accepted evaluation criteria in the clinic. How-ever, incorporating these metrics into deep learning dose prediction models is challenging due totheir non-convexity and non-differentiability. We propose a novel moment-based loss function forpredicting 3D dose distribution for the challenging conventional lung intensity modulated radiationtherapy (IMRT) plans. The moment-based loss function is convex and differentiable and can easilyincorporate DVH metrics in any deep learning framework without computational overhead. Themoments can also be customized to reflect the clinical priorities in 3D dose prediction. For instance,using high-order moments allows better prediction in high-dose areas for serial structures. We useda large dataset of 360 (240 for training, 50 for validation and 70 for testing) conventional lung pa-tients with 2Gy × 30 fractions to train the deep learning (DL) model using clinically treated plansat our institution. We trained a UNet like CNN architecture using computed tomography (CT),planning target volume (PTV) and organ-at-risk contours (OAR) as input to infer correspondingvoxel-wise 3D dose distribution. We evaluated three different loss functions: (1) The popular MeanAbsolute Error (MAE) Loss, (2) the recently developed MAE + DVH Loss, and (3) the proposedMAE + Moments Loss. The quality of the predictions was compared using different DVH metricsas well as dose-score and DVH-score, recently introduced by the AAPM knowledge-based planninggrand challenge. Model with (MAE + Moment) loss function outperformed the model with MAE1arXiv:2207.03414v2  [cs.CV]  5 Sep 2022Moments-Based Loss Dose Prediction G. Jhanwar, et al.loss by significantly improving the DVH-score (11%, p<0.01) while having similar computationalcost. It also outperformed the model trained with (MAE+DVH) by significantly improving thecomputational cost (48%) and the DVH-score (8%, p<0.01). The code, pretrained models, dockercontainer, and Google Colab project along with a sample dataset are available on our DoseRTXGitHub (https://github.com/nadeemlab/DoseRTX).Keywords: Deep learning dose prediction, automated radiotherapy treatment planning.1 IntroductionDespite recent advances in optimization and treatment planning, intensity modulated radiation therapy(IMRT) treatment planning remains a time-consuming and resource-demanding task with the plan qual-",
        "arxiv": "2207.03414",
        "doi": null,
        "file_name": "2207.03414.pdf",
        "file_path": "./sources/2207.03414/2207.03414.pdf",
        "title": "Domain Knowledge Driven 3D Dose Prediction Using Moment-Based Loss Function",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/nadeemlab/DoseRTX"
                }
            ]
        }
    },
    "2207.12086": {
        "abstract": "Abstract. Data augmentation is one of the most successful techniquesto improve the classification accuracy of machine learning models in com-puter vision. However, applying data augmentation to tabular data is achallenging problem since it is hard to generate synthetic samples withlabels. In this paper, we propose an efficient classifier with a novel dataaugmentation technique for tabular data. Our method called CCRALcombines causal reasoning to learn counterfactual samples for the orig-inal training samples and active learning to select useful counterfactualsamples based on a region of uncertainty. By doing this, our method canmaximize our model’s generalization on the unseen testing data. We val-idate our method analytically, and compare with the standard baselines.Our experimental results highlight that CCRAL achieves significantlybetter performance than those of the baselines across several real-worldtabular datasets in terms of accuracy and AUC. Data and source codeare available at: https://github.com/nphdang/CCRAL.Keywords: Data Augmentation · Classification · Counterfactual reasoning ·Active learning · Tabular data1 IntroductionRecently, machine learning has become one of the most successful tools for sup-porting decisions, and it has been applied widely to many real-world applicationsincluding face recognition [36], security systems [3], disease detection [22], or rec-ommended systems [33]. Two core components of a machine learning tool are thealgorithm and the data. The algorithm can be classified into two mainstreams,namely classification and clustering while the data can be in different formats,e.g. tabular or image.When dealing with images in computer vision applications, machine learningmodels (or classifiers) often leverage data augmentation techniques to improvethe classification accuracy [14]. The main idea is that given an image of ‘dog’, ifwe rotate or flip the image, then we still recognize the object in the image as a‘dog’. By doing this geometric transformation, the label of an image is unchangedbut we can obtain different variants of the image, helping the machine learningclassifier to observe more data and improve its generalization. In addition toarXiv:2207.12086v1  [cs.LG]  25 Ju",
        "arxiv": "2207.12086",
        "doi": null,
        "file_name": "2207.12086.pdf",
        "file_path": "./sources/2207.12086/2207.12086.pdf",
        "title": "Efficient Classification with Counterfactual Reasoning and Active Learning",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/nphdang/CCRAL"
                }
            ]
        }
    },
    "2207.12783": {
        "abstract": "ABSTRACTVideo Question Answering (VideoQA) is the task of answering thenatural language questions about a video. Producing an answer re-quires understanding the interplay across visual scenes in video andlinguistic semantics in question. However, most leading VideoQAmodels work as black boxes, which make the visual-linguistic align-ment behind the answering process obscure. Such black-box naturecalls for visual explainability that reveals “What part of the videoshould the model look at to answer the question?”. Only a fewworks present the visual explanations in a post-hoc fashion, whichemulates the target model’s answering process via an additionalmethod. Nonetheless, the emulation struggles to faithfully exhibitthe visual-linguistic alignment during answering.Instead of post-hoc explainability, we focus on intrinsic inter-pretability to make the answering process transparent. At its core isgrounding the question-critical cues as the causal scene to yield an-swers, while rolling out the question-irrelevant information as theenvironment scene. Taking a causal look at VideoQA, we devise aself-interpretable framework, Equivariant and Invariant Groundingfor Interpretable VideoQA (EIGV). Specifically, the equivariantgrounding encourages the answering to be sensitive to the semanticchanges in the causal scene and question; in contrast, the invariantgrounding enforces the answering to be insensitive to the changesin the environment scene. By imposing them on the answeringprocess, EIGV is able to distinguish the causal scene from the envi-ronment information, and explicitly present the visual-linguisticalignment. Extensive experiments on three benchmark datasetsjustify the superiority of EIGV in terms of accuracy and visual in-terpretability over the leading baselines. Our code is available athttps://github.com/yl3800/EIGV.CCS CONCEPTS• Information systems → Question answering; Multimediaand multimodal retrieval.KEYWORDSVideo Question Answering, Invariant Learning, Equivariant Learn-ing, Interpretability∗ Corresponding author. This research is supported by the Sea-NExT Joint Lab, andthe CCCD Key Lab of Ministry of Culture and Tourism, USTC..This work is licensed under a Creative Commons AttributionInternational 4.0 License.MM ’22, October 10–14, 2022, Lisboa, Portugal© 2022 Copyright held by the owner/author(s).ACM ISBN 978-1-4503-9203-7/22/10.https://doi.org/10.1145/3503161.3548035ACM Reference Format:Yicong Li, Xiang Wang, Junbin Xiao, and Tat-Seng Chua. 2022. Equivariantand Invariant Grounding for Video Question Answering. In Proceedingsof the 30th ACM International Conference on Multimedia (MM ’22), Oct.10–14, 2022, Lisboa, Portugal. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3503161.3548035",
        "arxiv": "2207.12783",
        "doi": "10.1145/3503161.3548035",
        "file_name": "2207.12783.pdf",
        "file_path": "./sources/2207.12783/2207.12783.pdf",
        "title": "Equivariant and Invariant Grounding for Video Question Answering",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/yl3800/EIGV"
                }
            ]
        }
    },
    "2208.03118": {
        "abstract": "Abstract—This paper conceives a novel sparse code multipleaccess (SCMA) codebook design which is motivated by thestrong need for providing ultra-low decoding complexity andgood error performance in downlink Internet-of-things (IoT)networks, in which a massive number of low-end and low-costIoT communication devices are served. By focusing on the typicalRician fading channels, we analyze the pair-wise probability ofsuperimposed SCMA codewords and then deduce the designmetrics for multi-dimensional constellation construction andsparse codebook optimization. For significant reduction of thedecoding complexity, we advocate the key idea of projectingthe multi-dimensional constellation elements to a few overlappedcomplex numbers in each dimension, called low projection (LP).An emerging modulation scheme, called golden angle modulation(GAM), is considered for multi-stage LP optimization, wherethe resultant multi-dimensional constellation is called LP-GAM.Our analysis and simulation results show the superiority of theproposed LP codebooks (LPCBs) including one-shot decodingconvergence and excellent error rate performance. In particular,the proposed LPCBs lead to decoding complexity reduction by atleast 97% compared to that of the conventional codebooks, whilstowning large minimum Euclidean distance. Some examples ofthe proposed LPCBs are available at https://github.com/ethanlq/SCMA-codebook.Index Terms—Sparse code multiple access (SCMA), goldenangle modulation (GAM), codebook design, Internet-of-things(IoT), low complexity detection, Rician channels.I. INTRODUCTIONTHE Internet-of-things (IoT) represents a revolutionaryparadigm shift from the legacy human-centric networks(e.g., in 3G and 4G) to massive machine-type communica-tions, where the latter is a major use case in the 5G-and-beyond mobile networks [1]. Under this big picture, however,it is challenging to support the concurrent communicationsof massive IoT devices. Due to the limited time-frequencyresources, traditional orthogonal multiple access (OMA) maybe infeasible. A disruptive technique for addressing such achallenge is called non-orthogonal multiple access (NOMA)which permits several times of IoT devices larger than that inOMA systems communicating simultaneously [2].Qu Luo, Gaojie Chen, Pei Xiao and Yi Ma are with 5G & 6G InnovationCentre, Institute for Communication Systems (ICS), University of Surrey, UK,email:{q.u.luo, gaojie.chen, p.xiao, m.yi}@surrey.ac.uk.Zilong Liu is with the School of Computer Science and Electronic Engi-neering, University of Essex, UK, email: zilong.liu@essex.ac.uk.Amine Maaref is with the Canada Research Center, Huawei TechnologiesCompany Ltd., Ottawa, Canada, email: amine.maaref@huawei.com.Existing NOMA techniques can be mainly categorizedinto two classes: power-domain NOMA [3] and code-domainNOMA (CD-NOMA) [4]. The former works by superimposingmultiple users with distinctive power levels over the identical",
        "arxiv": "2208.03118",
        "doi": "10.1109/twc.2023.3244868",
        "file_name": "2208.03118.pdf",
        "file_path": "./sources/2208.03118/2208.03118.pdf",
        "title": "A Design of Low-Projection SCMA Codebooks for Ultra-Low Decoding Complexity in Downlink IoT Networks",
        "urls": {
            "git": [
                {
                    "#_appearances": 4,
                    "url": "https://github.com/ethanlq/SCMA-codebook"
                }
            ]
        }
    },
    "2208.08888": {
        "abstract": "Abstract—A novel clustering technique based on the projectiononto convex set (POCS) method, called POCS-based clusteringalgorithm, is proposed in this paper. The proposed POCS-basedclustering algorithm exploits a parallel projection method ofPOCS to find appropriate cluster prototypes in the featurespace. The algorithm considers each data point as a convex setand projects the cluster prototypes parallelly to the memberdata points. The projections are convexly combined to mini-mize the objective function for data clustering purpose. Theperformance of the proposed POCS-based clustering algorithmis verified through experiments on various synthetic datasets.The experimental results show that the proposed POCS-basedclustering algorithm is competitive and efficient in terms ofclustering error and execution speed when compared withother conventional clustering methods including Fuzzy C-Means(FCM) and K-Means clustering algorithms. Code is available at:https://github.com/tranleanh/pocs-based-clusteringIndex Terms—POCS, clustering, unsupervised learning, ma-chine learning, K-MeansI. INTRODUCTIONProjection onto convex set (POCS) is a powerful tool forsignal synthesis and image restoration which was originallyintroduced by Bregman in the mid-1960s [1]. The POCSmethod has been widely used to find a common point ofconvex sets in several signal processing problems. The maintarget of the POCS approach is to find a vector that residesin the intersection of convex sets. Bregman has shown thatsuccessive projections between two or more convex sets withnon-empty intersection converge to a point that exists in theintersection of the convex sets. In the case of disjoint closedconvex sets, the sequential projection does not converge to asingle point, instead it converges to greedy limit cycles whichare dependent on the order of the projections [1]. This propertyof POCS, however, can be applied to clustering problems.Clustering is an unsupervised data analysis technique thatcategories similar data points while separating them fromthe different ones [2]. Most clustering algorithms try to find*Corresponding Authorhomogeneous subgroups that have similar characteristics bythe type of metric employed. The K-Means clustering algo-rithm, which has been one of the most popular methods forgeneral clustering purposes [9], uses the Euclidean distance tomeasure the similarity [2]. The K-Means clustering algorithmalternates between assigning cluster membership for each datapoint to the nearest cluster center and computing the centerof each cluster as the prototype of its member data points.The objective of the K-Means clustering algorithm is to finda set of prototypes that minimize the cost function. The K-Means clustering algorithm terminates its training procedurewhen there is no further change in the assignment of instances",
        "arxiv": "2208.08888",
        "doi": "10.1109/iwis56333.2022.9920762",
        "file_name": "2208.08888.pdf",
        "file_path": "./sources/2208.08888/2208.08888.pdf",
        "title": "POCS-based Clustering Algorithm",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/tranleanh/pocs-based-clustering"
                }
            ]
        }
    },
    "2208.13094": {
        "abstract": "Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requiresprior specific permission and/or a fee. Request permissions from permissions@acm.org.CSCW ’22, November 03–05, 2022, Woodstock, NY© 2022 Association for Computing Machinery.ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00https://doi.org/xx.xxxx/xxxxxxx.xxxxxxx1arXiv:2208.13094v1  [cs.HC]  27 Aug 2022https://doi.org/xx.xxxx/xxxxxxx.xxxxxxxhttps://doi.org/xx.xxxx/xxxxxxx.xxxxxxxCSCW ’22, November 03–05, 2022, Woodstock, NY Joon Sung Park et al.for community self-moderation [19, 71]. Researchers have likewise sought to help reduce theprevalence of these behaviors, for example by creating tools for governance [34, 97] and automaticdetection of anti-social behaviors [14, 93].Given these harms, and the effort to combat them, it is critical to understand how much ofthe content in online communities remains anti-social. A large empirical foundation in socialpsychology demonstrates that if these behaviors are visible and widespread, it will encourageothers to engage in anti-social behaviors as well [25, 26]. Benchmarking progress, or regression,requires an honest accounting of the situation. Platforms themselves have begun measuring theprevalence of specific behaviors such as hate speech and harassment [29, 79, 84].Despite this need, empirically measuring the prevalence of anti-social behavior remains difficult.AI tools remain too error-prone to be fully relied upon [8, 44], and manual labeling via randomsampling is labor-intensive to perform at scale. In addition, defining which behaviors cross theline remains contentious, with different online communities applying different definitions [52] andplatforms each establishing different standards for content and enforcement [49, 52]. Therefore, itis no surprise that empirical studies that measure the prevalence of anti-social behavior are fewand far between, with platform-published performance metrics often tucked under the platforms’“transparency” or compliance reports using vaguely defined categories [29].In this paper, we develop a method to measure the proportion of these behaviors in online",
        "arxiv": "2208.13094",
        "doi": "10.1145/3555552",
        "file_name": "2208.13094.pdf",
        "file_path": "./sources/2208.13094/2208.13094.pdf",
        "title": "Measuring the Prevalence of Anti-Social Behavior in Online Communities",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/StanfordHCI/ContentModAudit_CodeRelease"
                }
            ]
        }
    },
    "2209.10831": {
        "abstract": "ABSTRACTSome boosting algorithms, such as LPBoost, ERLPBoost, and C-ERLPBoost, aim to solve the softmargin optimization problem with the `1-norm regularization. LPBoost rapidly converges to an�-approximate solution in practice, but it is known to take Ω(m) iterations in the worst case, where mis the sample size. On the other hand, ERLPBoost and C-ERLPBoost are guaranteed to converge toan �-approximate solution in O( 1�2ln mν) iterations, where ν ∈ [1,m] is a hyperparameter. However,the overall computations are very high compared to LPBoost.To address this issue, we propose a generic boosting scheme that combines the Frank-Wolfe algorithmand any secondary algorithm and switches one to the other iteratively. We show that the schemeretains the same convergence guarantee as ERLPBoost and C-ERLPBoost. One can incorporate anysecondary algorithm to improve in practice. This scheme comes from a unified view of boostingalgorithms for soft margin optimization. More specifically, we show that LPBoost, ERLPBoost, andC-ERLPBoost are instances of the Frank-Wolfe algorithm. In experiments on real datasets, one of theinstances of our scheme exploits the better updates of the second algorithm and performs comparablywith LPBoost.Keywords Boosting · Frank-Wolfe · Soft margin optimization1 IntroductionTheory and algorithms for large-margin classifiers have been studied extensively since those classifiers guarantee lowgeneralization errors when they have large margins over training examples (e.g., Schapire et al. [1998], Mohri et al.[2018]). In particular, the `1-norm regularized soft margin optimization problem, defined later, is a formulation offinding sparse large-margin classifiers based on the linear program (LP). This problem aims to optimize the `1-margin bycombining multiple hypotheses from some hypothesis classH. The resulting classifier tends to be sparse, so `1-marginoptimization is helpful for feature selection tasks. Off-the-shelf LP solvers can solve the problem, but they are still notefficient enough for a huge classH.Boosting is a framework for solving the `1-norm regularized margin optimization even though H is infinitely large.Various boosting algorithms have been invented. LPBoost [Demiriz et al., 2002] is a practical algorithm that often workseffectively. Although LPBoost terminates rapidly, It is shown that it takes Ω(m) iterations in the worst case, where m isthe number of training examples [Warmuth et al., 2007]. Shalev-Shwartz and Singer [2010] invented an algorithm calledCorrective ERLPBoost (we call this algorithm C-ERLPBoost for shorthand) in the paper on ERLPBoost [Warmuth et al.,2008]. C-ERLPBooost and ERLPBoost find �-approximate solutions in O(ln(m/ν)/�2) iterations, where ν ∈ [1,m] isthe soft margin parameter. The difference is the time complexity per iteration; ERLPBoost solves a convex program(CP) for each iteration, while C-ERLPBooost solves a sorting-like problem. Although ERLPBoost takes much time perarXiv:2209.10831v2  [cs.LG",
        "arxiv": "2209.10831",
        "doi": null,
        "file_name": "2209.10831.pdf",
        "file_path": "./sources/2209.10831/2209.10831.pdf",
        "title": "Boosting as Frank-Wolfe",
        "urls": {}
    },
    "2210.08826": {
        "abstract": "AbstractMany state-of-the-art noisy-label learning methods relyon learning mechanisms that estimate the samples’ cleanlabels during training and discard their original noisy la-bels. However, this approach prevents the learning of therelationship between images, noisy labels and clean la-bels, which has been shown to be useful when dealingwith instance-dependent label noise problems. Further-more, methods that do aim to learn this relationship re-quire cleanly annotated subsets of data, as well as dis-tillation or multi-faceted models for training. In this pa-per, we propose a new training algorithm that relies on asimple model to learn the relationship between clean andnoisy labels without the need for a cleanly labelled subset ofdata. Our algorithm follows a 3-stage process, namely: 1)self-supervised pre-training followed by an early-stoppingtraining of the classifier to confidently predict clean la-bels for a subset of the training set; 2) use the clean setfrom stage (1) to bootstrap the relationship between images,noisy labels and clean labels, which we exploit for effec-tive relabelling of the remaining training set using semi-supervised learning; and 3) supervised training of the clas-sifier with all relabelled samples from stage (2). By learningthis relationship, we achieve state-of-the-art performancein asymmetric and instance-dependent label noise prob-lems1. Code is available at https://github.com/btsmart/bootstrapping-label-noise.1. IntroductionSupervised deep learning has had great success gen-erating effective classification models from sets of la-belled training data [24, 26]. Modern deep learning mod-els require large-scale datasets to achieve state-of-the-art (SOTA) results [38, 39]. However, real-world large-1Supported by Australian Research Council through grantsDP180103232 and FT190100525.scale datasets, such as those collected from search en-gines or available from hospitals and clinics, tend to havea non-negligible amount of instance-dependent label noise(IDN) [32, 53]. Existing methods often attempt to addressinstance-independent label noise (IIN), such as symmetricor asymmetric noise [15, 58, 65]. Handling the IDN presentin large-scale real-world datasets has become one of themain research problems in the field.When naively trained with noisy-labelled data, deeplearning models generalise poorly because they can easilyoverfit the incorrectly labelled samples [62]. Many meth-ods have been developed for handling label noise, withSOTA approaches relying on sample relabelling mecha-nisms. These strategies are based on techniques to estimatethe relationship between images and clean labels, and after",
        "arxiv": "2210.08826",
        "doi": "10.1109/wacv56688.2023.00531",
        "file_name": "2210.08826.pdf",
        "file_path": "./sources/2210.08826/2210.08826.pdf",
        "title": "Bootstrapping the Relationship Between Images and Their Clean and Noisy Labels",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/btsmart/bootstrapping-label-noise"
                }
            ]
        }
    },
    "2210.15274": {
        "abstract": "AbstractIn knowledge distillation, previous feature distillation methods mainly focus onthe design of loss functions and the selection of the distilled layers, while theeffect of the feature projector between the student and the teacher remains under-explored. In this paper, we first discuss a plausible mechanism of the projectorwith empirical evidence and then propose a new feature distillation method basedon a projector ensemble for further performance improvement. We observe thatthe student network benefits from a projector even if the feature dimensions ofthe student and the teacher are the same. Training a student backbone without aprojector can be considered as a multi-task learning process, namely achievingdiscriminative feature extraction for classification and feature matching betweenthe student and the teacher for distillation at the same time. We hypothesize andempirically verify that without a projector, the student network tends to overfit theteacher’s feature distributions despite having different architecture and weightsinitialization. This leads to degradation on the quality of the student’s deep featuresthat are eventually used in classification. Adding a projector, on the other hand,disentangles the two learning tasks and helps the student network to focus better onthe main feature extraction task while still being able to utilize teacher features asa guidance through the projector. Motivated by the positive effect of the projectorin feature distillation, we propose an ensemble of projectors to further improve thequality of student features. Experimental results on different datasets with a seriesof teacher-student pairs illustrate the effectiveness of the proposed method. Codeis available at https://github.com/chenyd7/PEFD.1 IntroductionThe last decade has witnessed the rapid development of Convolutional Neural Networks (CNNs)[21, 31, 11, 22, 4]. The resulting increases in performance however, have come with substantialincreases in network size and this largely limits the applications of CNNs on edge devices [15].To alleviate this problem, knowledge distillation has been proposed for network compression. Thekey idea of distillation is to use the knowledge obtained by the large network (teacher) to guide theoptimization of the lightweight network (student) [14, 26, 33].Existing methods can be roughly categorized into logit-based, feature-based and similarity-baseddistillation [9]. Recent research shows that feature-based methods generally distill a better studentnetwork compared to the other two groups [32, 6]. We conjecture that the process of mimicking theteacher’s features provides a clearer optimization direction for the training of the student network.Despite the promising performance of feature distillation, it is still challenging to narrow the gapbetween the student and teacher’s feature spaces. To improve the feature learning ability of the∗Corresponding Author36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2210.15274v1  [cs",
        "arxiv": "2210.15274",
        "doi": null,
        "file_name": "2210.15274.pdf",
        "file_path": "./sources/2210.15274/2210.15274.pdf",
        "title": "Improved Feature Distillation via Projector Ensemble",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/chenyd7/PEFD"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/HobbitLong/RepDistiller"
                }
            ]
        }
    },
    "2211.08278": {
        "abstract": "Abstract—In perception tasks of automated vehicles (AVs)data-driven have often outperformed conventional approaches.This motivated us to develop a data-driven methodology tocompute occupancy grid maps (OGMs) from lidar measurements.Our approach extends previous work such that the estimatedenvironment representation now contains an additional layer forcells occupied by dynamic objects. Earlier solutions could onlydistinguish between free and occupied cells. The informationwhether an obstacle could move plays an important role forplanning the behavior of an AV. We present two approaches togenerating training data. One approach extends our previouswork on using synthetic training data so that OGMs withthe three aforementioned cell states are generated. The otherapproach uses manual annotations from the nuScenes [1] datasetto create training data. We compare the performance of bothmodels in a quantitative analysis on unseen data from the real-world dataset. Next, we analyze the ability of both approachesto cope with a domain shift, i.e. when presented with lidarmeasurements from a different sensor on a different vehicle.We propose using information gained from evaluation on real-world data to further close the reality gap and create bettersynthetic data that can be used to train occupancy grid mappingmodels for arbitrary sensor configurations. Code is available athttps://github.com/ika-rwth-aachen/DEviLOG.Index Terms—AD, perception, simulation, deep learningI. INTRODUCTIONAn automated vehicle can determine the drivable space inthe static environment by localizing itself on a high-definitionmap. These HD maps describe the exact road geometry andtraffic rules applying to each lane.However, these maps can be outdated so that it becomesnecessary to substitute them with measurement-based repre-*This research is accomplished within the project ”UNICARagil” (FKZ16EMO0284K). We acknowledge the financial support for the project by theFederal Ministry of Education and Research of Germany (BMBF).(a) Trained on Synthetic Data (b) Trained on nuScenesFig. 1: The left OGM was predicted by a model trainedon synthetic data and the right OGM was predicted by amodel trained on labels generated from annotations of thenuScenes [1] dataset. Green indicates belief mass for free,red for statically occupied and blue for dynamically occupiedcells. The lidar point cloud as input data is superimposedand colored by the class annotations, i.e. vehicles are orange,drivable surface is yellow and occupied space is grey.sentations to determine the actual drivable space. Grid-basedenvironment representations are particularly suitable for thisas they discretize a defined area around the vehicle into cellsthat can be aligned with the HD map. Distances measurede.g. by a lidar sensor can be used to assign an occupancystate to each cell in an OGM. Such OGMs are also used in",
        "arxiv": "2211.08278",
        "doi": "10.1109/iceccme55909.2022.9988605",
        "file_name": "2211.08278.pdf",
        "file_path": "./sources/2211.08278/2211.08278.pdf",
        "title": "Data-Driven Occupancy Grid Mapping using Synthetic and Real-World Data",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/ika-rwth-aachen/DEviLOG"
                }
            ]
        }
    },
    "2211.08333": {
        "abstract": "AbstractWe study applications of 3D printing to the broad goal of under-standing how mathematical objects vary continuously in families. Todo so, we model the varying parameter as the vertical axis of a 3Dprint, introducing the notion of a static animation: a 3D printed ob-ject each of whose layers is a member of the continuously deformingfamily. We survey examples and draw connections to algebraic geom-etry, complex dynamics, chaos theory, and more. We also include adetailed tutorial (with accompanying code and files) so that the readercan create static animations of their own.1 IntroductionAcross mathematical disciplines there is considerable interest in understand-ing how a mathematical object can deform according to a shift in a singleparameter, producing a varying family of related mathematical objects. Ifthese objects have small enough dimension, one can vary the parameter overtime and display how the object changes as an animation on a screen. Witha 3D printer we can replace the time dimension with the vertical z-axis, andstack each successively deformed layer on top of the last. This turns theentire parametrized family of objects into a single solid which we can hold inour hands, giving an entire new physical dimension to the deformation.The study of continuously varying families is ubiquitous in mathematics.To name a few examples: In algebraic geometry, one studies degenerations ofcurves, surfaces, and higher dimensional algebraic varieties, putting them incontinuously varying flat families which interpolate between one space and1arXiv:2211.08333v1  [math.HO]  15 Nov 2022the next [13, Chapter III.9]. In complex dynamics, one asks about limiting",
        "arxiv": "2211.08333",
        "doi": null,
        "file_name": "2211.08333.pdf",
        "file_path": "./sources/2211.08333/2211.08333.pdf",
        "title": "Deformation Spaces and Static Animations",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/gdorfsmanhopkins/staticAnimations"
                }
            ]
        }
    },
    "2211.12494": {
        "abstract": "AbstractGeneralized Zero-Shot Learning (GZSL) aims to traina classifier that can generalize to unseen classes, using aset of attributes as auxiliary information, and the visualfeatures extracted from a pre-trained convolutional neuralnetwork. While recent GZSL methods have explored var-ious techniques to leverage the capacity of these features,there has been an extensive growth of representation learn-ing techniques that remain under-explored. In this work, weinvestigate the utility of different GZSL methods when usingdifferent feature extractors, and examine how these models’pre-training objectives, datasets, and architecture design af-fect their feature representation ability. Our results indicatethat 1) methods using generative components for GZSL pro-vide more advantages when using recent feature extractors;2) feature extractors pre-trained using self-supervised learn-ing objectives combined with cross-entropy and knowledgedistillation provide better feature representations, increasingup to 15% performance when used with recent GZSL tech-niques; 3) specific feature extractors pre-trained with largerdatasets do not necessarily boost the performance of GZSLmethods. In addition, we investigate how GZSL methods fareagainst CLIP, a more recent multi-modal pre-trained modelwith strong zero-shot performance. We found that GZSLtasks still benefit from generative-based GZSL methods alongwith CLIP’s internet-scale pre-training to achieve state-of-the-art performance in fine-grained datasets. We release amodular framework for analyzing representation learningissues in GZSL here: https://github.com/uvavision/TV-GZSL.1. IntroductionDeep learning models have achieved remarkable accuracyin many computer vision classification tasks when labeleddata is available, and the data distribution is consistent dur-ing training and test time [19, 35, 43]. It is now possible totrain image classifiers that can distinguish with high accu-racy thousands of image categories [37]. However, in orderto enable a model to recognize novel categories, it is still nec-essary to collect a dataset with representative human-labeledArchitectureDataset Trained On Training StrategyImageNet 21k400M <I,T> pairsImageNet 1k SupervisedSelf-SupervisedContrastiveCNNViTMLPImage Feature Extractors GZSL StrategyEmbedding BasedGenerative Based",
        "arxiv": "2211.12494",
        "doi": null,
        "file_name": "2211.12494.pdf",
        "file_path": "./sources/2211.12494/2211.12494.pdf",
        "title": "On the Transferability of Visual Features in Generalized Zero-Shot Learning",
        "urls": {
            "git": [
                {
                    "#_appearances": 5,
                    "url": "https://github.com/openai/CLIP"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/uvavision/TV-GZSL"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/rwightman/pytorch-image-models"
                }
            ]
        }
    },
    "2212.09125": {
        "abstract": "AbstractUltra-fine entity typing (UFET) predicts ex-tremely free-formed types (e.g., president,politician) of a given entity mention (e.g., JoeBiden) in context. State-of-the-art (SOTA)methods use the cross-encoder (CE) based ar-chitecture. CE concatenates the mention (andits context) with each type and feeds the pairsinto a pretrained language model (PLM) toscore their relevance. It brings deeper interac-tion between mention and types to reach betterperformance but has to perform N (type setsize) forward passes to infer types of a singlemention. CE is therefore very slow in infer-ence when the type set is large (e.g., N = 10kfor UFET). To this end, we propose to performentity typing in a recall-expand-filter manner.The recall and expand stages prune the largetype set and generate K (K is typically lessthan 256) most relevant type candidates foreach mention. At the filter stage, we use anovel model called MCCE to concurrently en-code and score these K candidates in only oneforward pass to obtain the final type prediction.We investigate different variants of MCCEand extensive experiments show that MCCEunder our paradigm reaches SOTA perfor-mance on ultra-fine entity typing and is thou-sands of times faster than the cross-encoder.We also found MCCE is very effective infine-grained (130 types) and coarse-grained(9 types) entity typing. Our code is avail-able at http://github.com/modelscope/AdaSeq/tree/master/examples/MCCE.1 IntroductionUltra-fine entity typing (UFET) (Choi et al., 2018)aims to predict extremely fine-grained types (e.g.,president, politician) of a given entity mentionwithin its context. It provides detailed semanticunderstandings of entity mention and is a funda-∗ Kewei Tu is the corresponding author.3 Equal Contribution.PLMType k [CLS] Mention & Context  [SEP]hcls 0/1  Entailment for PLM [CLS] Mention & Context  [SEP]hcls Scores of    Type 1 … NType k……",
        "arxiv": "2212.09125",
        "doi": null,
        "file_name": "2212.09125.pdf",
        "file_path": "./sources/2212.09125/2212.09125.pdf",
        "title": "Recall, Expand and Multi-Candidate Cross-Encode: Fast and Accurate Ultra-Fine Entity Typing",
        "urls": {
            "git": [
                {
                    "#_appearances": 4,
                    "url": "http://github.com/modelscope/AdaSeq"
                }
            ]
        }
    },
    "2212.12678": {
        "abstract": "ABSTRACTBlind watermarking provides powerful evidence for copyright pro-tection, image authentication, and tampering identification. How-ever, it remains a challenge to design a watermarking model withhigh imperceptibility and robustness against strong noise attacks.To resolve this issue, we present a framework Combining theInvertible and Non-invertible (CIN) mechanisms. The CIN is com-posed of the invertible part to achieve high imperceptibility andthe non-invertible part to strengthen the robustness against strongnoise attacks. For the invertible part, we develop a diffusion andextraction module (DEM) and a fusion and split module (FSM)to embed and extract watermarks symmetrically in an invertibleway. For the non-invertible part, we introduce a non-invertibleattention-based module (NIAM) and the noise-specific selectionmodule (NSM) to solve the asymmetric extraction under a strongnoise attack. Extensive experiments demonstrate that our frame-work outperforms the current state-of-the-art methods of imper-ceptibility and robustness significantly. Our framework can achievean average of 99.99% accuracy and 67.66 𝑑𝐵 𝑃𝑆𝑁𝑅 under noise-freeconditions, while 96.64% and 39.28 𝑑𝐵 combined strong noise at-tacks. The code will be available in https://github.com/rmpku/CIN.CCS CONCEPTS• Security and privacy→ Digital rights management.KEYWORDSRobust blind watermarking; Invertible network∗Corresponding authors.CINCIN*Without NoiseJPEG Noise99.93 %48.31 dBMBRS99.99 %67.66 dB95.52 %33.50 dB77.49 %36.22 dB97.40 %",
        "arxiv": "2212.12678",
        "doi": "10.1145/3503161.3547950",
        "file_name": "2212.12678.pdf",
        "file_path": "./sources/2212.12678/2212.12678.pdf",
        "title": "Towards Blind Watermarking: Combining Invertible and Non-invertible Mechanisms",
        "urls": {
            "git": [
                {
                    "#_appearances": 1,
                    "url": "https://github.com/rmpku/CIN"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/RM1110/CIN"
                }
            ]
        }
    },
    "2212.13788": {
        "abstract": "Abstract—Deep learning (DL) analysis of Chest X-ray (CXR) and Computed tomography (CT) images has garnered a lot of attentionin recent times due to the COVID-19 pandemic. Convolutional Neural Networks (CNNs) are well suited for the image analysis taskswhen trained on humongous amounts of data. Applications developed for medical image analysis require high sensitivity and precisioncompared to any other fields. Most of the tools proposed for detection of COVID-19 claims to have high sensitivity and recalls but havefailed to generalize and perform when tested on unseen datasets. This encouraged us to develop a CNN model, analyze andunderstand the performance of it by visualizing the predictions of the model using class activation maps generated using(Gradient-weighted Class Activation Mapping) Grad-CAM technique. This study provides a detailed discussion of the success andfailure of the proposed model at an image level. Performance of the model is compared with state-of-the-art DL models and shown tobe comparable. The data and code used are available at https://github.com/aleesuss/c19.Index Terms—Convolutional Neural Network, COVID-19, explainable, radiology.F1 INTRODUCTIONEARLY diagnosis of coronavirus and effective preventionof transmission by isolating the infected individualshave been the core tasks in managing the COVID-19 pan-demic. Shortage of RT-PCR kits, longer times required forthe results, and high false negative rates during the earlyphases of infection have been a bottleneck faced worldwidefor timely action. Chest radiography imaging (chest X-rays(CXRs) and computed tomography (CT) scans) were usedin hospitals worldwide for faster triaging of patients andseveral studies have reported the advantages of using chestradiographs along with RT-PCR to confirm the diagnosisduring the early phase of pandemic [1], [2]. Earlier workon other strains of the coronavirus family, the Middle Eastrespiratory syndrome (MERS) and severe acute respiratorysyndrome (SARS) outbreaks also confirm the usefulnessof chest radiographs in the diagnosis of pulmonary dis-eases [3]. It has been observed that COVID patients exhibitchanges in their chest radiographs even before the onset ofother symptoms [4] making them a useful diagnostic toolfor early diagnosis. Since machine learning and artificialintelligence are well established methods in image analysis,there has been much attention in developing deep learning(DL) models for CXR and CT image analysis for the detec-tion of COVID-19. Some characteristic features associatedwith COVID-19 include ground-glass opacification (GGO),consolidation, and crazy lines [5], [6]. Of these, the mostcommon feature, GGOs, are not easily perceivable posingchallenges. Further, large inflow of patients during variouswaves of pandemic worldwide has caused heavy burden onthe hospital staff. Thus, automated and accurate detection• Suba S and N. Parekh are with CCNSB, International Institute ofInformation Technology, Prof. C R Rao Road, Hyderabad, India 500032.E-mail: suba.s@research.iiit.ac.in, nita@iiit.ac.in.of the disease is desirable during such situations. ArtificialIntelligence (AI) assisted diagnosis would prove efficientas machine learning models are good at capturing subtlepatterns in data and are quick in arriving at results withexpert level performances. But, despite the excellent per-",
        "arxiv": "2212.13788",
        "doi": null,
        "file_name": "2212.13788.pdf",
        "file_path": "./sources/2212.13788/2212.13788.pdf",
        "title": "Explainable and Lightweight Model for COVID-19 Detection Using Chest Radiology Images",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/aleesuss/c19"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/ieee8023/covid-chestX-ray-dataset"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/agchung/Figure1-"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/agchung/Actualmed-"
                }
            ]
        }
    },
    "2302.05564": {
        "abstract": "Abstract—DevOps is a set of practices that deals with coor-dination between development and operation teams and ensuresrapid and reliable new software releases that are essential inindustry. DevOps education assumes the vital task of preparingnew professionals in these practices using appropriate teachingmethods. However, there are insufficient studies investigatingteaching methods in DevOps. We performed an analysis basedon interviews to identify teaching methods and their relationshipwith DevOps educational challenges. Our findings show thatproject-based learning and collaborative learning are emergingas the most relevant teaching methods.Index Terms—DevOps, teaching methods, challenges, mixedmethodsI. INTRODUCTIONDevOps (Development and Operations) appears as a naturalevolution of the Agile movement [1], [2]. It aims to mitigateconflicts, approximate responsibilities, and improve the com-munication of Development and Operation teams [3]–[6]. ThisAgile perspective was essential for organizations to remaincompetitive in the technological business, meeting the needto build fast, resilient, and secure deliverables that involvedistributed systems at scale. Recent studies show that theevolution of DevOps practices in industry organizations alignswith business success [7]. Since 2009, there have been variousexamples of large organizations adopting DevOps, includingFacebook, Yahoo, Flickr, Netflix, and Etsy [8]–[10]. UsingDevOps practices, companies such as Amazon and Netflixdeploy thousands of times a day [11].In many IT sectors, mastering DevOps practices has becomea necessary skill [12], [13]. There is a high demand forDevOps practitioners, with many related job postings [14].However, there is a shortage of qualified professionals tomeet demand [15], [16]. Because of that, the industry hasa growing desire to have graduates educated in the use ofDevOps approaches and tools, better preparing students toapply DevOps practices in real projects [17], [18].DevOps teaching has its challenges [19]. From an edu-cational point of view, it is difficult to set up the DevOpsapproach from scratch [15], [18]. The challenge of creating aneffective approach intensifies when considering students whodo not have the necessary background to continuously evolvein learning. For example, students who have not worked inindustry or on projects of substantial size and complexity havedifficulty understanding DevOps [20], [21]. More, DevOpsresearch with an education focus is required to investigatethese aspects.Investing in DevOps teaching is a meaningful way tomitigate industry demand. From an educational perspective,the role of the educator is to guide students throughout thelearning process by applying the proper teaching methods and",
        "arxiv": "2302.05564",
        "doi": null,
        "file_name": "2302.05564.pdf",
        "file_path": "./sources/2302.05564/2302.05564.pdf",
        "title": "Overcoming Challenges in DevOps Education through Teaching Methods",
        "urls": {
            "zenodo": [
                {
                    "#_appearances": 2,
                    "url": "https://doi.org/10.5281/zenodo.7135611"
                }
            ]
        }
    },
    "2302.08664": {
        "abstract": "ABSTRACTOnline social networks have become an integral aspect of our dailylives and play a crucial role in shaping our relationships with others.However, bugs and glitches, even minor ones, can cause anythingfrom frustrating problems to serious data leaks that can have far-reaching impacts on millions of users.To mitigate these risks, fuzz testing, a method of testing withrandomised inputs, can provide increased confidence in the correctfunctioning of a social network. However, implementing traditionalfuzz testing methods can be prohibitively difficult or impracticalfor programmers outside of the network’s development team.To tackle this challenge, we present Socialz, a novel approachto social fuzz testing that (1) characterises real users of a socialnetwork, (2) diversifies their interaction using evolutionary com-putation across multiple, non-trivial features, and (3) collects per-formance data as these interactions are executed. With Socialz, weaim to provide anyone with the capability to perform comprehen-sive social testing, thereby improving the reliability and security ofonline social networks used around the world.KEYWORDSFuzz testing; graph social network; diversity optimisation.ACM Reference Format:Francisco Zanartu, Christoph Treude, and Markus Wagner. 2023. Socialz:Multi-Feature Social Fuzz Testing. In GECCO ’23: ACM Genetic and Evolu-tionary Computation Conference, July 15-19, 2023, Lisbon, Portugal. ACM,New York, NY, USA, 9 pages. https://doi.org/XXXXXXX.XXXXXXX1 INTRODUCTIONOnline social networks (OSNs) are an integral part of modern so-ciety, influencing a wide range of aspects of our daily lives. Thevast quantity of personal information shared on these platformsmakes them a treasure trove for companies seeking to reach outto potential customers and for individuals looking to grow theirsocial circle or entertain themselves. However, like any software,OSNs are prone to bugs and technical issues; consequences rangefrom poor user experience [6, 10] to massive data breaches affectingbillions of individuals [26].The recent rise of social bugs in software systems has promptedthe need for social testing [1]. However, for the research community,Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from permissions@acm.org.GECCO’23, July 15-19, 2023, Lisbon, Portugal© 2023 Association for Computing Machinery.ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00https://doi.org/XXXXXXX.XXXXXXXsocial testing poses several challenges. First and foremost, obtaining",
        "arxiv": "2302.08664",
        "doi": null,
        "file_name": "2302.08664.pdf",
        "file_path": "./sources/2302.08664/2302.08664.pdf",
        "title": "Socialz: Multi-Feature Social Fuzz Testing",
        "urls": {
            "git": [
                {
                    "#_appearances": 4,
                    "url": "https://github.com/fzanart/Socialz"
                },
                {
                    "#_appearances": 3,
                    "url": "https://gitlab.com/gitlab-org/gitlab"
                }
            ]
        }
    },
    "2303.10131": {
        "abstract": "Abstract—Implicit gender bias in software development is awell-documented issue, such as the association of technical roleswith men. To address this bias, it is important to understand it inmore detail. This study uses data mining techniques to investigatethe extent to which 56 tasks related to software development,such as assigning GitHub issues and testing, are affected byimplicit gender bias embedded in large language models. Wesystematically translated each task from English into a genderlesslanguage and back, and investigated the pronouns associatedwith each task. Based on translating each task 100 times indifferent permutations, we identify a significant disparity in thegendered pronoun associations with different tasks. Specifically,requirements elicitation was associated with the pronoun “he”in only 6% of cases, while testing was associated with “he” in100% of cases. Additionally, tasks related to helping others hada 91% association with “he” while the same association for tasksrelated to asking coworkers was only 52%. These findings reveala clear pattern of gender bias related to software developmenttasks and have important implications for addressing this issueboth in the training of large language models and in broadersociety.I. INTRODUCTIONImplicit gender bias is prevalent among professional soft-ware developers. For instance, a study of 142 professional soft-ware engineers undertaking an Implicit Association Test [1]found significant associations between men and technicalleadership positions, general technical positions, and careeradvancement. Garcia et al. [2] discovered that pink tasks,which are tasks that require high standards and timeliness butoffer little substantive development or visibility [3], were oftenassigned to female students in team projects. Terrell et al. [4]found that while pull requests submitted by women tend tobe accepted more often than those submitted by men, this isonly the case when the women’s identities are not immediatelyapparent.Gender bias can lead to a lack of representation and op-portunities for underrepresented groups, which can negativelyimpact innovation and productivity. For example, Vasilescuet al. [5] found gender diversity to be a significant positivepredictor of productivity in GitHub teams. Gender bias canalso perpetuate discrimination and create a hostile work en-vironment, leading to high turnover rates with the associatedknowledge loss [6] and a lack of diversity in the workforce.To effectively address these biases, it is crucial to understandthem in more detail. By identifying specific tasks and activitiesthat are affected by gender bias, we can target our efforts toeliminate bias more effectively. Understanding the nuances andcomplexities of bias, such as how it can manifest differentlydepending on the context, is crucial to creating an inclusiveand equitable software development community.",
        "arxiv": "2303.10131",
        "doi": null,
        "file_name": "2303.10131.pdf",
        "file_path": "./sources/2303.10131/2303.10131.pdf",
        "title": "She Elicits Requirements and He Tests: Software Engineering Gender Bias in Large Language Models",
        "urls": {
            "zenodo": [
                {
                    "#_appearances": 2,
                    "url": "https://doi.org/10.5281/zenodo.7745436"
                }
            ]
        }
    },
    "2303.10439": {
        "abstract": "Abstract—Stop words, which are considered non-predictive,are often eliminated in natural language processing tasks. How-ever, the definition of uninformative vocabulary is vague, so mostalgorithms use general knowledge-based stop lists to removestop words. There is an ongoing debate among academics aboutthe usefulness of stop word elimination, especially in domain-specific settings. In this work, we investigate the usefulness ofstop word removal in a software engineering context. To dothis, we replicate and experiment with three software engineeringresearch tools from related work. Additionally, we construct acorpus of software engineering domain-related text from 10,000Stack Overflow questions and identify 200 domain-specific stopwords using traditional information-theoretic methods. Our re-sults show that the use of domain-specific stop words significantlyimproved the performance of research tools compared to the useof a general stop list and that 17 out of 19 evaluation measuresshowed better performance.Online appendix: https://zenodo.org/record/7865748Index Terms—Software Engineering Documents, Natural Lan-guage Processing (NLP), Stop Words.I. INTRODUCTIONWith the development of natural language processing tech-nology, the amount of text data has exploded in various fields.This phenomenon is also occurring in the field of softwareengineering, and text analysis using natural language process-ing (NLP) techniques has become increasingly prevalent inthe field of software engineering [1], [2]. However, withouteffective pre-processing methods, raw text corpora often posecomputational and analytical barriers to NLP tasks such asindexing, text classification and information retrieval [3]. Be-cause of the large number of uninformative words presentin the raw text, these words are defined as ‘stop words’.They appear frequently in text mining tasks and carry littleinformation, simply connecting words in a sentence, suchas the words ‘the’, ‘and’, ‘to’, and so on. Theoretically, theremoval of stop words can improve the statistical significanceof important terms [4], [5]. Therefore, stop word removal isan important step in text pre-processing.Using a standard list of stop words to remove these high-frequency and uninformative words has become the standardin research and industry. Researchers often use off-the-shelfgeneric stop word lists, such as those provided by the NaturalLanguage Processing Toolkit (NLTK), to remove noise [6].The use of stop word lists is controversial because it isimpossible to have a uniform list of stop words and thesemantic importance of each word depends on the task andusage. Blind use of stop word lists may result in the lossof important information, which may adversely affect theaccuracy of text mining algorithms [7], [8]. However, stopwords are difficult to define rigorously, so there are few",
        "arxiv": "2303.10439",
        "doi": null,
        "file_name": "2303.10439.pdf",
        "file_path": "./sources/2303.10439/2303.10439.pdf",
        "title": "Stop Words for Processing Software Engineering Documents: Do they Matter?",
        "urls": {
            "git": [
                {
                    "#_appearances": 1,
                    "url": "https://github.com/mongodb/mongo"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/Alir3z4/stopwords"
                }
            ]
        }
    },
    "2303.13729": {
        "abstract": "Abstract—Although information theory has found success indisciplines, the literature on its applications to software evolutionis limit. We are still missing artifacts that leverage the data andtooling available to measure how the information content of aproject can be a proxy for its complexity. In this work, we exploretwo definitions of entropy, one structural and one textual, andapply it to the historical progression of the commit history of 25open source projects. We produce evidence that they generallyare highly correlated. We also observed that they display weakand unstable correlations with other complexity metrics. Ourpreliminary investigation of outliers shows an unexpected highfrequency of events where there is considerable change in theinformation content of the project, suggesting that such outliersmay inform a definition of surprisal.Index Terms—Information theory, entropy, software engineer-ingI. INTRODUCTIONIn a software engineering context, it makes intuitive sense towant to monitor the evolution of entropy, given that managingcomplexity is a key concept to the construction of softwaresystems [1]. Such a necessity gives rise to several strategiesthat have become foundational to programming, such as mod-ularity, clean interfaces and design patterns [2].Software applications that generate value do so by automat-ing and abstracting away complex processes. As softwareevolves, it is expected for its behaviour to be repaired andaugmented, which means the information content of the projectinvariably changes as the project is maintained. Many projectsstart by preforming relatively specific tasks, and eventuallygrow to offer larger sets of functionality in its domain.Consider, for the sake of illustration, that we want to writesoftware that implements the basic operations on real numbers.Upon defining the set of supported operations, one could pro-ceed by incrementally developing the calculator one operationat a time. From an information theoretic perspective, suchevolution means the communication channel represented byits source code transmits an increasing amount of informationover time.It common for unmanaged complexity to grow in projects,whether by schedule pressures or by poor design decisions [3].In our calculator, this could occur in different ways; the projectcould have a single file performing all supported operationsin the same file, in which case there would be too muchinformation for a reader to digest at the same time. Givenhumans’ limited working memory capacity, this can potentiallylead developers to experience cognitive load [4]. Anotherregular case would be when the code does not leverage naturallanguage properly to convey its meaning, like when one writescode using identifiers that obscure the purpose of the code.If we use the identifier ‘s’ everywhere we want to refer to",
        "arxiv": "2303.13729",
        "doi": null,
        "file_name": "2303.13729.pdf",
        "file_path": "./sources/2303.13729/2303.13729.pdf",
        "title": "Applying Information Theory to Software Evolution",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/javaparser/javaparser"
                }
            ]
        }
    },
    "2304.14628": {
        "abstract": "ABSTRACTOpen source software (OSS) development offers a unique opportu-nity for students in Software Engineering to experience and partic-ipate in large-scale software development, however, the impact ofsuch courses on students’ self-efficacy and the challenges faced bystudents are not well understood. This paper aims to address thisgap by analyzing data from multiple instances of OSS developmentcourses at universities in different countries and reporting on howstudents’ self-efficacy changed as a result of taking the course, aswell as the barriers and challenges faced by students.CCS CONCEPTS• Applied computing → Collaborative learning; • Softwareand its engineering → Open source model; • Informationsystems → Open source software.KEYWORDSopen source software, barriers, self-efficacy, educationACM Reference Format:Larissa Salerno, Simone de França Tonhão, Igor Steinmacher, and ChristophTreude. 2023. Barriers and Self-Efficacy: A Large-Scale Study on the Impactof OSS Courses on Student Perceptions. In Proceedings of the 2023 Conferenceon Innovation and Technology in Computer Science Education V. 1 (ITiCSE2023), July 8–12, 2023, Turku, Finland. ACM, New York, NY, USA, 7 pages.https://doi.org/10.1145/3587102.35887891 INTRODUCTION AND MOTIVATIONAs part of their coursework, students in Software Engineering oftendo not get the opportunity to participate in large-scale softwareprojects with hundreds of developers, thousands of files, and longproject history. Yet, many of the challenges inherent in softwaredevelopment only become apparent when software development isconducted at such a large scale.While it is often unrealistic to embedPermission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from permissions@acm.org.ITiCSE 2023, July 8–12, 2023, Turku, Finland© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0138-2/23/07. . . $15.00https://doi.org/10.1145/3587102.3588789students in an industry project for a semester, open source software(OSS) development offers a unique opportunity for students toexperience and participate in large-scale software development.Recognizing this opportunity, several universities are now offer-ing dedicated courses that introduce students to OSS developmentand guide them in making their first contribution to an open sourceproject. But what is the impact of such courses?Using data from four instances of three courses at three univer-sities in three countries and a total of 359 students, we report on",
        "arxiv": "2304.14628",
        "doi": "10.1145/3587102.3588789",
        "file_name": "2304.14628.pdf",
        "file_path": "./sources/2304.14628/2304.14628.pdf",
        "title": "Barriers and Self-Efficacy: A Large-Scale Study on the Impact of OSS Courses on Student Perceptions",
        "urls": {}
    },
    "2305.16365": {
        "abstract": "Abstract Continuous Integration (CI) is a software development practice that buildsand tests software frequently (e.g., at every push). One main motivator to adopt CI is thepotential to deliver software functionalities more quickly than not using CI. However,there is little empirical evidence to support that CI helps projects deliver softwarefunctionalities more quickly. Through the analysis of 162,653 pull requests (PRs) of87 GitHub projects, we empirically study whether adopting a CI service (TRAVISCI)can quicken the time to deliver merged PRs. We complement our quantitative studyby analyzing 450 survey responses from participants of 73 software projects. Ourresults reveal that adopting a CI service may not necessarily quicken the deliveryof merge PRs. Instead, the pivotal benefit of a CI service is to improve the decisionmaking on PR submissions, without compromising the quality or overloading theproject’s reviewers and maintainers. The automation provided by CI and the boostin developers’ confidence are key advantages of adopting a CI service. Furthermore,open-source projects planning to attract and retain developers should consider the useJoão Helis BernardoFederal Institute of Rio Grande do Norte (IFRN)Federal University of Rio Grande do Norte (UFRN)Natal, BrazilE-mail: joao.helis@ifrn.edu.brDaniel Alencar da CostaUniversity of OtagoDunedin, New ZealandE-mail: danielcalencar@otago.ac.nzUirá KuleszaFederal University of Rio Grande do Norte (UFRN)Natal, BrazilE-mail: uira@dimap.ufrn.brChristoph TreudeUniversity of MelbourneMelbourne, AustraliaE-mail: christoph.treude@unimelb.edu.auarXiv:2305.16365v1  [cs.SE]  25 May",
        "arxiv": "2305.16365",
        "doi": null,
        "file_name": "2305.16365.pdf",
        "file_path": "./sources/2305.16365/2305.16365.pdf",
        "title": "The Impact of a Continuous Integration Service on the Delivery Time of Merged Pull Requests",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/haraka/haraka"
                }
            ]
        }
    },
    "2305.16591": {
        "abstract": "Abstract Commit messages contain diverse and valuable types of knowledgein all aspects of software maintenance and evolution. Links are an example ofsuch knowledge. Previous work on “9.6 million links in source code comments”showed that links are prone to decay, become outdated, and lack bidirectionaltraceability. We conducted a large-scale study of 18,201,165 links from commitsin 23,110 GitHub repositories to investigate whether they suffer the same fate.Results show that referencing external resources is prevalent and that the mostfrequent domains other than github.com are the external domains of StackOverflow and Google Code. Similarly, links serve as source code context tocommit messages, with inaccessible links being frequent. Although repeatedlyreferencing links is rare (4%), 14% of links that are prone to evolve becomeunavailable over time; e.g., tutorials or articles and software homepages becomeunavailable over time. Furthermore, we find that 70% of the distinct linkssuffer from decay; the domains that occur the most frequently are related to� Corresponding author - Tao XiaoNara Institute of Science and Technology, JapanE-mail: tao.xiao.ts2@is.naist.jpSebastian BaltesUniversity of Adelaide, AustraliaE-mail: sebastian.baltes@adelaide.edu.auHideaki HataShinshu University, JapanE-mail: hata@shinshu-u.ac.jpChristoph TreudeUniversity of Melbourne, AustraliaE-mail: christoph.treude@unimelb.edu.auRaula Gaikovina Kula, Takashi Ishio, Kenichi MatsumotoNara Institute of Science and Technology, JapanE-mail: {raula-k,ishio,matumoto}@is.naist.jparXiv:2305.16591v1  [cs.SE]  26 May 202",
        "arxiv": "2305.16591",
        "doi": "10.1007/s10664-023-10325-8",
        "file_name": "2305.16591.pdf",
        "file_path": "./sources/2305.16591/2305.16591.pdf",
        "title": "18 Million Links in Commit Messages: Purpose, Evolution, and Decay",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/sbaltes/wayback-machine-retriever"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/sbaltes/git-log-extractor"
                }
            ],
            "zenodo": [
                {
                    "#_appearances": 4,
                    "url": "https://doi.org/10.5281/zenodo.7536500"
                }
            ]
        }
    },
    "2306.10019": {
        "abstract": "Abstract—A key drawback to using a Open Source third-partylibrary is the risk of introducing malicious attacks. In recentlytimes, these threats have taken a new form, when maintainersturn their Open Source libraries into protestware. This is definedas software containing political messages delivered through theselibraries, which can either be malicious or benign. Since develop-ers are willing to freely open-up their software to these libraries,much trust and responsibility are placed on the maintainers toensure that the library does what it promises to do. This papertakes a look into the possible scenarios where developers mightconsider turning their Open Source Software into protestware,using an ethico-philosophical lens. Using different frameworkscommonly used in AI ethics, we explore the different dilemmasthat may result in protestware. Additionally, we illustrate howan open-source maintainer’s decision to protest is influencedby different stakeholders (viz., their membership in the OSScommunity, their personal views, financial motivations, socialstatus, and moral viewpoints), making protestware a multifacetedand intricate matter.“When people feel they are not being heard, they mayresort to different measures to get their message across.In the case of programmers, they have the uniqueability to protest through their code.”Kula & Treude (2022) [1]“Consequently, he found himself confronted by twovery different modes of action; the one concrete,immediate, but directed towards only one individual;and the other an action addressed to an end infinitelygreater... but for that very reason ambiguous... He hadto choose between those two. What could help him tochoose? ”Sartre (1946)Existentialism Is a Humanism (trans.: Mairet) [2]I. INTRODUCTIONIn this article, we articulate the motivations behind main-tainers who turn their Open Source Software (OSS) intoprotestware. Although ethics in computing is not new, thephenomenon of Protestware is unique, in that the power ofresponsibility is placed on individuals (i.e., sometimes a singlelibrary maintainer), as opposed to the often-diffused responsi-bilities behind the deployment of AI and other technologies,due to the participation of more than one person. We thenexplore the dilemmas that a library maintainer may face. Wealso discuss potential guidelines and larger ethical implicationsfor the open source, industry, research, and education sectors.II. BACKGROUNDA. ContextIn March 2022, the maintainer of node-ipc [3], a widelyused software library, intentionally introduced a vulnerabilityinto their code. If the code was run within Russia or Belarus,",
        "arxiv": "2306.10019",
        "doi": null,
        "file_name": "2306.10019.pdf",
        "file_path": "./sources/2306.10019/2306.10019.pdf",
        "title": "Ethical Considerations Towards Protestware",
        "urls": {
            "git": [
                {
                    "#_appearances": 4,
                    "url": "https://github.com/RIAEvangelist/peacenotwar"
                },
                {
                    "#_appearances": 4,
                    "url": "https://github.com/RIAEvangelist/node-ipc"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/vshymanskyy/StandWithUkraine"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/terraform-aws-modules/terraform-aws-ec2-instance"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/medikoo/es5-ext"
                }
            ]
        }
    },
    "2306.10021": {
        "abstract": "Abstract The use of third-party packages is becoming increasingly popular andhas led to the emergence of large software package ecosystems with a maze ofinter-dependencies. Since the reliance on these ecosystems enables developers toreduce development effort and increase productivity, it has attracted the interest ofresearchers: understanding the infrastructure and dynamics of package ecosystemshas given rise to approaches for better code reuse, automated updates, and the avoid-ance of vulnerabilities, to name a few examples. But the reality of these ecosystemsalso poses challenges to software engineering researchers, such as: How do we ob-tain the complete network of dependencies along with the corresponding versioninginformation? What are the boundaries of these package ecosystems? How do we con-sistently detect dependencies that are declared but not used? How do we consistentlyidentify developers within a package ecosystem? How much of the ecosystem dowe need to understand to analyse a single component? How well do our approachesgeneralise across different programming languages and package ecosystems? In thischapter, we review promises and perils of mining the rich data related to softwarepackage ecosystems available to software engineering researchers.1.1 IntroductionThird-party libraries are a great way for developers to incorporate code withouthaving to write their own for every functionality required. By using these libraries,developers can save time and energy while still getting the functions they need. UsingRaula Gaikovina KulaNara Institute of Science and Technology, Japan, e-mail: raula-k@naist.jpKatsuro InoueNanzan University, Japan e-mail: inoue599@nanzan-u.ac.jpChristoph TreudeThe University of Melbourne, Australia e-mail: christoph.treude@unimelb.edu.au1arXiv:2306.10021v1  [cs.SE]  29 May 20232 Raula Gaikovina Kula, Katsuro Inoue, and Christoph Treude",
        "arxiv": "2306.10021",
        "doi": null,
        "file_name": "2306.10021.pdf",
        "file_path": "./sources/2306.10021/2306.10021.pdf",
        "title": "Promises and Perils of Mining Software Package Ecosystem Data",
        "urls": {
            "git": [
                {
                    "#_appearances": 1,
                    "url": "https://github.com/googleapis/repo-automation-bots"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/en/pull-requests"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/en/issues"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/en/graphql"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/en/discussions"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/en/code-security"
                },
                {
                    "#_appearances": 1,
                    "url": "https://github.com/en/authentication"
                }
            ]
        }
    },
    "2307.04291": {
        "abstract": "Abstract—Encountering outdated documentation is not a rareoccurrence for developers and users in the software engineeringcommunity. To ensure that software documentation is up-to-date,developers often have to manually check whether the documen-tation needs to be updated whenever changes are made to thesource code. In our previous work, we proposed an approach toautomatically detect outdated code element references in softwarerepositories and found that more than a quarter of the 1000most popular projects on GitHub contained at least one outdatedreference. In this paper, we present a GitHub Actions tool thatbuilds on our previous work’s approach that GitHub developerscan configure to automatically scan for outdated code elementreferences in their GitHub project’s documentation whenever apull request is submitted.Video—https://www.youtube.com/watch?v=4cA10vdlmnsIndex Terms—software repositories, outdated documentation,outdated references, code elements, workflow automationI. INTRODUCTIONNot only developers but also users often find encounteringoutdated software documentation a frustrating experience. Inour previous work [1], we found that 28.9% of the top 1000most popular projects1 on GitHub contain at least one outdatedreference to source code in their documentation. In the samepaper, we proposed an approach named DOCER (DetectingOutdated Code Element References) to automatically detectoutdated code element references in software repository doc-umentation. The approach works by extracting code elementreferences from documentation (README and wiki pages)using a list of regular expressions. These extracted referencesinclude variables, functions and class names found in the doc-umentation such as HttpClient, Promise.reject(err) andArrayList<String>. To determine if a reference is outdated,we match the reference to two revisions of the source code: therepository snapshot when the documentation was last updatedand the current revision. We compare the number of instancesfound in the two versions and flag the reference as outdated ifit existed in the snapshot but is no longer found in the currentrevision. Figure 1 shows an overview of the DOCER approach.In our previous paper, we provided an implementationthat developers can use to scan for outdated code elementreferences. However, running the script whenever new changesare proposed may be mundane and repetitive. To simplify this1Top 1000 projects ranked by the number of starsFig. 1. Overview of the DOCER approach introduced in our previous paperprocess, we created a tool based on GitHub Actions workflowthat is automatically triggered whenever a pull request issubmitted to the repository. This workflow automates all thesteps mentioned above and reports outdated references bycommenting on the pull request.In the following sections of this paper, we provide an in-",
        "arxiv": "2307.04291",
        "doi": null,
        "file_name": "2307.04291.pdf",
        "file_path": "./sources/2307.04291/2307.04291.pdf",
        "title": "Wait, wasn’t that code here before? Detecting Outdated Software Documentation",
        "urls": {
            "git": [
                {
                    "#_appearances": 3,
                    "url": "https://github.com/en/actions"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/wesleytanws/DOCER_tool"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/wesleytanws/DOCER"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/google/hs-portray"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/google/gnostic"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/google/clif"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/google/cctz"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/features/actions"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/actions/upload-artifact"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/actions/github-script"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/actions/checkout"
                }
            ]
        }
    },
    "2307.10793": {
        "abstract": "AbstractCompiler error messages serve as an initial resource for programmers dealingwith compilation errors. However, previous studies indicate that they oftenlack sufficient targeted information to resolve code issues. Consequently, pro-grammers typically rely on their own research to fix errors. Historically, StackOverflow has been the primary resource for such information, but recent ad-vances in large language models offer alternatives. This study systematicallyexamines 100 compiler error messages from three sources to determine themost effective approach for programmers encountering compiler errors. Fac-tors considered include Stack Overflow search methods and the impact ofmodel version and prompt phrasing when using large language models. Theresults reveal that GPT-4 outperforms Stack Overflow in explaining compilererror messages, the effectiveness of adding code snippets to Stack Overflowsearches depends on the search method, and results for Stack Overflow differsignificantly between Google and StackExchange API searches. Furthermore,GPT-4 surpasses GPT-3.5, with “How to fix” prompts yielding superior out-comes to “What does this error mean” prompts. These results offer valuableguidance for programmers seeking assistance with compiler error messages,underscoring the transformative potential of advanced large language mod-els like GPT-4 in debugging and opening new avenues of exploration forresearchers in AI-assisted programming.Keywords: Compiler errors, Stack Overflow, large language modelsarXiv:2307.10793v1  [cs.SE]  20 Jul 20231. IntroductionCompiler error messages, designed to guide error resolution, have previ-ously been described as “difficult to resolve” [51], “not very helpful” [52], andeven “useless” [55]. In fact, a previous study conducted on software engi-neering students using eye tracking revealed that participants spent between13 and 25% of their total task time reading error messages [6], which maysuggest inadequacy [38] of standard compiler error messages. As a result,",
        "arxiv": "2307.10793",
        "doi": null,
        "file_name": "2307.10793.pdf",
        "file_path": "./sources/2307.10793/2307.10793.pdf",
        "title": "Addressing Compiler Errors: Stack Overflow or Large Language Models?",
        "urls": {
            "git": [
                {
                    "#_appearances": 1,
                    "url": "https://github.com/patwdj/java-compiler-error-help"
                }
            ]
        }
    },
    "2308.09637": {
        "abstract": "Abstract—Managing dependencies between software servicesis a crucial task for any company operating cloud applications.Visualizations can help to understand and maintain these com-plex dependencies. In this paper, we present a force-directedservice dependency visualization and filtering tool that has beendeveloped and used within SAP. The tool’s use cases includeguiding service retirement as well as understanding servicedeployment landscapes and their relationship to the company’sorganizational structure. We report how we built and adapted thetool under strict time constraints to address the requirements ofour users. We further share insights on how we enabled internaladoption. For us, starting with a minimal viable visualizationand then quickly responding to user feedback was essential forconvincing users of the tool’s value. The final version of thetool enabled users to visually understand company-wide serviceconsumption, supporting data-driven decision making.I. INTRODUCTIONServices enable access to capabilities through clearly de-fined interfaces [1]. The notion of software applications asservices gained popularity in the late 1990s with Amazon’sservice-based model [2] and later as part of the service-oriented architecture paradigm [1]. Since then, different tech-niques for defining interfaces and consuming services haveemerged (e.g., SOAP or REST). Moreover, the appropriatesize of services, ranging from small microservices to largemonoliths [3], has been controversially discussed.In modern cloud environments, there is often a multitude ofservices that interact via interfaces, forming complex softwaresystems offered to customers as Software-as-a-Service (SaaS)solutions. Especially in the context of business software,such solutions are business-critical for customers, requiringa high degree of stability and reliability. Therefore, managingdependencies between hundreds of services constituting thebackbone of SaaS solutions becomes business-critical, too. Forinstance, before retiring a previously deprecated service, onemust ensure that all dependencies on it have been migrated.Reliable service metadata is imperative for handling suchdeprecation scenarios. A visualization of service dependenciesand related metadata can support monitoring a company’sservice landscape, enabling informed decisions on requiredmaintenance activities.In this paper, we present insights from an industrial casestudy at SAP. We developed a tailored node link visualizationthat enables the exploration of dependencies and metadataof SAP-managed services across different scenarios. Our per-spective is that of a team working on strategic projects, i.e.,Fig. 1. SAP-managed service dependencies of a large organizational unit.Node color encodes native cloud environment of a service. Two large clusterscorrespond to the older (yellow) and newer (green, blue) environments.executive projects with high priority. The first three authors",
        "arxiv": "2308.09637",
        "doi": null,
        "file_name": "2308.09637.pdf",
        "file_path": "./sources/2308.09637/2308.09637.pdf",
        "title": "Visually Analyzing Company-wide Software Service Dependencies: An Industrial Case Study",
        "urls": {}
    },
    "2308.0994": {
        "abstract": "ABSTRACTSoftware documentation captures detailed knowledge about a soft-ware product, e.g., code, technologies, and design. It plays an impor-tant role in the coordination of development teams and in conveyingideas to various stakeholders. However, software documentationcan be hard to comprehend if it is written with jargon and compli-cated sentence structure. In this study, we explored the potential oftext simplification techniques in the domain of software engineer-ing to automatically simplify GitHub README files. We collectedsoftware-related pairs of GitHub README files consisting of 14,588entries, aligned difficult sentences with their simplified counter-parts, and trained a Transformer-based model to automaticallysimplify difficult versions. To mitigate the sparse and noisy natureof the software-related simplification dataset, we applied generaltext simplification knowledge to this field. Since many general-domain difficult-to-simple Wikipedia document pairs are alreadypublicly available, we explored the potential of transfer learning byfirst training the model on the Wikipedia data and then fine-tuningit on the README data. Using automated BLEU scores and hu-man evaluation, we compared the performance of different transferlearning schemes and the baseline models without transfer learning.The transfer learning model using the best checkpoint trained on ageneral topic corpus achieved the best performance of 34.68 BLEUscore and statistically significantly higher human annotation scorescompared to the rest of the schemes and baselines. We conclude thatusing transfer learning is a promising direction to circumvent thelack of data and drift style problem in software README files sim-plification and achieved a better trade-off between simplificationand preservation of meaning.CCS CONCEPTS• Software and its engineering → Documentation; • Comput-ing methodologies→ Neural networks; • Applied computing→ Text editing.KEYWORDSSoftware Documentation, GitHub, Text Simplification, TransferLearningPermission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from permissions@acm.org.ESEC/FSE 2023, 11 - 17 November, 2023, San Francisco, USA© 2023 Association for Computing Machinery.ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00https://doi.org/XXXXXXX.XXXXXXXACM Reference Format:Haoyu Gao, Christoph Treude, and Mansooreh Zahedi. 2023. EvaluatingTransfer Learning for Simplifying GitHub READMEs. In Proceedings of The",
        "arxiv": "2308.0994",
        "doi": "10.1145/3611643.3616291",
        "file_name": "2308.0994.pdf",
        "file_path": "./sources/2308.0994/2308.0994.pdf",
        "title": "Evaluating Transfer Learning for Simplifying GitHub READMEs",
        "urls": {
            "git": [
                {
                    "#_appearances": 3,
                    "url": "https://github.com/en/get-started"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/trentm/python-markdown2"
                }
            ]
        }
    },
    "2308.12079": {
        "abstract": "Abstract—Most online code snippets do not run. This meansthat developers looking to reuse code from online sources mustmanually find and fix errors. We present an approach forautomatically evaluating and correcting errors in Node.js codesnippets: Node Code Correction (NCC). NCC leverages theability of the TypeScript compiler to generate errors and informcode corrections through the combination of TypeScript’s built-in codefixes, our own targeted fixes, and deletion of erroneouslines. Compared to existing approaches using linters, our findingssuggest that NCC is capable of detecting a larger number oferrors per snippet and more error types, and it is more efficientat fixing snippets. We find that 73.7% of the code snippets in NPMdocumentation have errors; with the use of NCC’s corrections,this number was reduced to 25.1%. Our evaluation confirms thatthe use of the TypeScript compiler to inform code corrections isa promising strategy to aid in the reuse of code snippets fromonline sources.I. INTRODUCTIONMost code snippets online do not run; existing work hasshown that only 15.2% of Node.js snippets in NPM packagedocumentation are runnable [1]. Because software developersfrequently reuse code from online sources [2], they often needto dedicate time to fixing errors. This introduces challengeswhen using third-party libraries: examples in documentationare intended to demonstrate usage and non-working snippetscan present a barrier to getting started.Because code snippets are not full, runnable programs withtest cases, existing work in automating the detection andfixing of errors has primarily focused on static analysis [3]–[8]. Code reuse tools such as NLP2TestableCode [3] andNCQ [5] combine error detection with heuristic fixes and linedeletion to aid developers in reusing snippets. This use of linedeletion aims to reduce snippets to an optimal form througha simple deletion operation, looking at errors to determine ifthe change should be ‘accepted’. Static analysis is also usefulfor measuring the quality of code; existing code reuse toolshave made use of parsers, linters and compilers to report errorsand find the ‘best’ snippet for a given search query [3], [5].Additionally, such tools can provide insights on the quality ofonline code in general: for example, Yang et al. [4] looked atthe usability of Stack Overflow snippets via static analysis.Research in Java leverages the compiler for error detectionand correction [3], [9], but JavaScript(and thus the Node.jsruntime environment), is an interpreted language that lackssuch a compiler. Similar work has instead relied on parsers andlinters [4]–[6]. For example, NCQ [5], a command-line REPL(Read-Eval-Print-Loop) programming environment, which au-tomates the process of reusing code snippets from NPMpackage documentation, uses ESLint [10] to report errors, andincreases the number of snippets without errors from 54.8% to",
        "arxiv": "2308.12079",
        "doi": null,
        "file_name": "2308.12079.pdf",
        "file_path": "./sources/2308.12079/2308.12079.pdf",
        "title": "Using the TypeScript compiler to fix erroneous Node.js snippets",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/microsoft/ts-fix"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/features/copilot"
                }
            ],
            "zenodo": [
                {
                    "#_appearances": 2,
                    "url": "https://doi.org/10.5281/zenodo.8272874"
                }
            ]
        }
    },
    "2309.03914": {
        "abstract": null,
        "arxiv": "2309.03914",
        "doi": null,
        "file_name": "2309.03914.pdf",
        "file_path": "./sources/2309.03914/2309.03914.pdf",
        "title": "DevGPT: Studying Developer-ChatGPT Conversations",
        "urls": {
            "git": [
                {
                    "#_appearances": 4,
                    "url": "https://github.com/NAIST-SE/DevGPT"
                }
            ],
            "zenodo": [
                {
                    "#_appearances": 2,
                    "url": "https://doi.org/10.5281/zenodo.8304091"
                }
            ]
        }
    },
    "2309.04197": {
        "abstract": "ABSTRACTA risk in adopting third-party dependencies into an applicationis their potential to serve as a doorway for malicious code to beinjected (most often unknowingly). While many initiatives fromboth industry and research communities focus on the most criticaldependencies (i.e., those most depended upon within the ecosys-tem), little is known about whether the rest of the ecosystem suffersthe same fate. Our vision is to promote and establish safer practisesthroughout the ecosystem. To motivate our vision, in this paper,we present preliminary data based on three representative samplesfrom a population of 88,416 pull requests (PRs) and identify unsafedependency updates (i.e., any pull request that risks being unsafeduring runtime), which clearly shows that unsafe dependency up-dates are not limited to highly impactful libraries. To draw attentionto the long tail, we propose a research agenda comprising six keyresearch questions that further explore how to safeguard againstthese unsafe activities. This includes developing best practises toaddress unsafe dependency updates not only in top-tier librariesbut throughout the entire ecosystem.CCS CONCEPTS• Software and its engineering;KEYWORDSSupply Chain, Libraries, Software EcosystemsACM Reference Format:Supatsara Wattanakriengkrai, Raula Gaikovina Kula, Christoph Treude,and Kenichi Matsumoto. 2023. Lessons from the Long Tail: Analysing UnsafeDependency Updates across Software Ecosystems. In Proceedings of the31st ACM Joint European Software Engineering Conference and Symposiumon the Foundations of Software Engineering (ESEC/FSE ’23), December 3–9,2023, San Francisco, CA, USA. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3611643.3613086Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from permissions@acm.org.ESEC/FSE ’23, December 3–9, 2023, San Francisco, CA, USA© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0327-0/23/12.https://doi.org/10.1145/3611643.36130861 INTRODUCTIONWidespread adoption and use of third-party library dependenciesin applications is evident by the massive scale of software ecosys-tems (e.g., NPM for JavaScript, Maven for Java, PyPI for Python).For example, the NPM ecosystem supports more than 2.42 millionJavaScript libraries1 as of 2022 [8]. Libraries in these ecosystemsform complex dependency relationships in which they depend oneach other for functionality. A side effect of this heavy reliance",
        "arxiv": "2309.04197",
        "doi": null,
        "file_name": "2309.04197.pdf",
        "file_path": "./sources/2309.04197/2309.04197.pdf",
        "title": "Lessons from the Long Tail: Analysing Unsafe Dependency Updates across Software Ecosystems",
        "urls": {
            "git": [
                {
                    "#_appearances": 2,
                    "url": "https://github.com/veged/coa"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/ossf/wg-best-practices-os-developers"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/ossf/scorecard"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/ossf/criticality_score"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/faisalman/ua-parser-js"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/dominictarr/rc"
                },
                {
                    "#_appearances": 2,
                    "url": "https://github.com/YfryTchsGD/Log4jAttackSurface"
                }
            ],
            "zenodo": [
                {
                    "#_appearances": 4,
                    "url": "https://doi.org/10.5281/zenodo.6719258"
                }
            ]
        }
    }
}