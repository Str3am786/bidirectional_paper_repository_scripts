\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{wacv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.


\usepackage{tablefootnote}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%% IMPORTANT - These next three lines are crucial.
%               (1) PLEASE enter your paper ID (given by CMT) replacing the
%                   '****' right below here with the ID from CMT.
%               (2) Leave the \wacvfinacopy commented out for the submission
%                   version, but UNCOMMENT it for your CAMERA-READY upload.
%               (3) For the camera-ready version, you may be asked to set a
%                   starting page number.  If so, replace the '9876' below with
%                   the starting page number assigned by the publication chair.
 
%(1)
\def\wacvPaperID{1206} % Enter the WACV Paper ID here

%(2)
\wacvalgorithmstrack   % Uncomment this line if you are submitting to the Algorithms Track.
%\wacvapplicationstrack % Uncomment this line if you are submitting to the Applications Track.

%(3)
%\wacvfinalcopy % *** Uncomment this line for the final submission

%(3)
\ifwacvfinal
% \def\assignedStartPage{9876} % *** Enter the assigned starting page number (instead of 9876)
\def\assignedStartPage{1}
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\ifwacvfinal
\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\else
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
\fi

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifwacvfinal
\setcounter{page}{\assignedStartPage}
\else
\pagestyle{empty}
\fi

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}




% BS: Our added packages/commands
\usepackage{blindtext}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{stfloats}
\usepackage{xcolor}
\usepackage{subfig}
\usepackage[sort&compress,square,comma,numbers]{natbib}
\setlength{\intextsep}{15pt}
\newcommand{\specialcell}[2][c]{\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}
\newcommand\cincludegraphics[2][]{\raisebox{-0.43\height}{\includegraphics[#1]{#2}}}
\newcommand{\gustavo}[1]{{\color{blue}GC: #1}}
\newcommand{\brandon}[1]{{\color{orange}BS: #1}}
\usepackage{color, colortbl}
\definecolor{Gray}{gray}{0.95}
\definecolor{LightGrey}{rgb}{0.88,0.88,0.88}
\definecolor{LightGreen}{rgb}{0.7,1,0.7}
\definecolor{LightYellow}{rgb}{1,1,0.4}
\definecolor{LightOrange}{rgb}{1,0.925,0.6}





\begin{document}

%%%%%%%%% TITLE
\title{Bootstrapping the Relationship Between Images and Their Clean and Noisy Labels}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\brandon{I've been dumping all the old graphs and things I don't need anymore in this document, but I will get rid of almost all of this for the actual supplementary material submission}

The results for 70\% PMD noise on CIFAR10 are not meaningful because in this setting, samples from several CIFAR10 classes are more likely to be annotated with the wrong than the correct label, so we do not show those results \brandon{Do we want to just include this sentence in the supplementary material?} \gustavo{It's better if we move it there.}. 

\begin{table}[!ht]
    \caption{\brandon{Old results, need redoing}. Accuracy of the predicted clean sets for 40\% Asymmetric noise on CIFAR10, showing the number of mislabellings for different balancing strategies and selection sizes}
    \vspace{0.2cm}
    \begin{minipage}{.44\columnwidth}
        \centering
        \scalebox{0.45}{
            \begin{tabular}{c|cc|cc|cc}
                \toprule
                Total & \multicolumn{2}{c}{Unbalanced} & \multicolumn{2}{c}{Class Balanced} & \multicolumn{2}{c}{Noise Balanced} \\
                \midrule
                {\hskip 0.4cm} - {\hskip 0.4cm} & {\hskip 0.1cm} No. {\hskip 0.1cm} & {\hskip 0.3cm} \% {\hskip 0.5cm} & {\hskip 0.1cm} No. {\hskip 0.1cm} & {\hskip 0.3cm} \% {\hskip 0.5cm} & {\hskip 0.1cm} No. {\hskip 0.1cm} & {\hskip 0.3cm} \% {\hskip 0.5cm} \\
                \midrule
                100 & 0 & (0.0\%) & 0 & (0.0\%) & 0 & (0.0\%) \\
                500 & 0 & (0.0\%) & 0 & (0.0\%) & 0 & (0.0\%) \\
                1000 & 0 & (0.0\%) & 1 & (0.10\%) & 0 & (0.0\%) \\
                2500 & 0 & (0.0\%) & 2 & (0.08\%) & 6 & (0.24\%) \\
                4000 & 0 & (0.0\%) & 4 & (0.10\%) & 13 & (0.33\%) \\
                5000 & 0 & (0.0\%) & 4 & (0.08\%) & 20 & (0.40\%) \\
                10000 & 0 & (0.0\%) & 17 & (0.17\%) & 56 & (0.56\%) \\
                20000 & 8 & (0.04\%) & 106 & (0.53\%) & 201 & (1.00\%) \\
                25000 & 195 & (0.78\%) & 214 & (0.86\%) & 344 & (1.38\%) \\
                40000 & 1100 & (2.75\%) & 1089 & (2.72\%) & 1259 & (3.15\%) \\
                50000 & 4290 & (8.58\%) & 4290 & (8.58\%) & 4290 & (8.58\%) \\
                \bottomrule
            \end{tabular}
        }
    \end{minipage}
    \hspace{0.1in}
    \begin{minipage}{.5\columnwidth}
        \centering
        \includegraphics[width=4.0cm]{Figures/selected_samples_resize.png}
    \end{minipage}
    \label{fig:selectedsamplesablation}
\end{table}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=.315\textwidth]{Figures/confidences_resize.png}\hfill
    \includegraphics[width=.315\textwidth]{Figures/confidences_clean_noisy_resize.png}\hfill
    \includegraphics[width=.315\textwidth]{Figures/clean_frac_against_num_selected_confidence_resize.png}
    \caption{Histograms showing the distribution of confidences for correct/incorrect classifications (left), for clean/noisy samples (middle), and how the highest confidence samples are disproportionally clean (right).}
    \label{fig:cleanandcorrectbias}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=.3\textwidth]{Figures/epochs_resize.png} {\hskip 0.3cm}
    \includegraphics[width=.3\textwidth]{Figures/selected_sample_accuracy_resize.png}
    \caption{\small Ablations showing how the number of training epochs (left) affect clean sets chosen with class-based balancing, and how the final model performance is affected by selection size (right) for 40\% Asym. noise on CIFAR10.}
    \label{fig:epochsandaugmentations}
\end{figure*}


\begin{table*}[!b]
    \begin{minipage}{.36\textwidth}
        \centering
        \caption{Test accuracy (\%) for Red Mini-ImageNet~\citep{jiang2020beyond}. Results from baseline methods are as presented in \citep{FaMUS}. Top methods ($\pm1\%$) are in \textbf{bold}. Our best method for each task is \underline{underlined}.}
        \vspace{0.20cm}
        \scalebox{0.72}{
            \begin{tabular}{lcccc}
                \toprule
                Method/ noise ratio & 20\% & 40\% & 60\%& 80\% \\
                \midrule
                Cross-entropy~\citep{FaMUS}    & 47.36 & 42.70 & 37.30 & 29.76 \\
                MixUp~\citep{zhang2018mixup}          & 49.10 & 46.40 & 40.58 & 33.58 \\
                DivideMix~\citep{li2020dividemix}     & 50.96 & 46.72 & 43.14 & 34.50 \\
                MentorMix~\citep{jiang2020beyond}  & 51.02 & 47.14 & 43.80 & 33.46 \\
                FaMUS~\citep{FaMUS}  & 51.42 & 48.06 & 45.10 & 35.50 \\
                PropMix~\citep{cordeiro2021propmix}& \textbf{61.24} & \textbf{56.22} & \textbf{52.84} & \textbf{43.42} \\
                \midrule
                Ours (Regular Model) & 59.68 & 50.92 & 46.06 & - \\
                {\hskip 0.5cm} + \textit{Test-Time Aug.} & \textit{60.66} & \textit{53.12} & \textit{47.48} & - \\
                Ours (Modified Model) & 60.06 & - & 47.44 & 37.44 \\
                {\hskip 0.5cm} + \textit{Test-Time Aug.} & \textit{61.62} & - & \textit{50.12} & \textit{39.74} \\
                \bottomrule
            \end{tabular}
        }
        \label{tab:red-noise_3}
    \end{minipage}
\end{table*}

\begin{table*}[t!]
    \begin{minipage}{0.25\textwidth}
        \centering
        \caption{\small Ablation showing how the number of number of test-time augmentations affect the clean sets chosen with class-based balancing.}
        \includegraphics[width=4.2cm]{Figures/augmentations_resize.png}
        \label{fig:epochsandaugmentations_2}
    \end{minipage}
    \hspace{0.1in}
    \begin{minipage}{.28\textwidth}
        \centering
        \caption{\small Accuracy using different null label methods for Asym. 40\% noise on CIFAR10} 
        \scalebox{0.8}{
            \begin{tabular}{lc|c}
                \toprule
                Training Strategy & {\hskip 0.1cm} & {\hskip 0.1cm} No. of Errors \\
                \midrule
                Zero Vectors & {\hskip 0.1cm} & 95.85 \\
                One Vectors & {\hskip 0.1cm} & xx.xx \\
                0.1 Vectors & {\hskip 0.1cm} & xx.xx \\
                \bottomrule
            \end{tabular}
        }
        \label{tab:null_label_ablation}
    \end{minipage}
    \hspace{0.1in}
    \begin{minipage}{.28\textwidth}
        \centering
        \caption{\small Accuracy using different model constructions for Asym. 40\% noise on CIFAR10} 
        \scalebox{0.8}{
            \begin{tabular}{lc|c}
                \toprule
                Training Strategy & {\hskip 0.1cm} & {\hskip 0.1cm} No. of Errors \\
                \midrule
                Concatenation & {\hskip 0.1cm} & 95.85 \\
                Mixture of Experts & {\hskip 0.1cm} & xx.xx \\
                Self-Attention & {\hskip 0.1cm} & xx.xx \\
                \bottomrule
            \end{tabular}
        }
        \label{tab:model_ablation}
    \end{minipage}
\end{table*}

% \begin{table*}[ht!]
%     \centering
%     \caption{Test accuracy (\%) for Polynomial Margin Diminishing Noise \cite{zhang2020learning}. Top methods are in \textbf{bold}.} 
%     \scalebox{0.75}{
%         \begin{tabular}{ccc|cc|cc||cc|cc|cc}
%             \toprule
%             \multicolumn{1}{c}{Dataset} & \multicolumn{6}{c}{CIFAR-10} & \multicolumn{6}{c}{CIFAR-100}\\    
%             \midrule
%             \multicolumn{1}{c}{Noise Type} & \multicolumn{2}{c}{Type-I} & \multicolumn{2}{c}{Type-II} & \multicolumn{2}{c}{Type-III} & \multicolumn{2}{c}{Type-I} & \multicolumn{2}{c}{Type-II} & \multicolumn{2}{c}{Type-III} \\
%             \midrule
%             \multicolumn{1}{c}{Noise Ratio} & {\hskip 0.2cm} 35\% {\hskip 0.1cm} & {\hskip 0.1cm} 70\% {\hskip 0.2cm} & {\hskip 0.2cm} 35\% {\hskip 0.1cm} & {\hskip 0.1cm} 70\% {\hskip 0.2cm} &{\hskip 0.2cm} 35\% {\hskip 0.1cm} & {\hskip 0.1cm} 70\% {\hskip 0.2cm} &{\hskip 0.2cm} 35\% {\hskip 0.1cm} & {\hskip 0.1cm} 70\% {\hskip 0.2cm} &{\hskip 0.2cm} 35\% {\hskip 0.1cm} & {\hskip 0.1cm} 70\% {\hskip 0.2cm} &{\hskip 0.2cm} 35\% {\hskip 0.1cm} & {\hskip 0.1cm} 70\% {\hskip 0.2cm} \\
%             \midrule
%             Cross-Entropy & 78.11 & 41.98 & 76.65 & 45.57 & 76.89 & 43.42 & 57.68 & 39.32 & 57.83 & 39.30 & 56.07 & 40.01 \\
%             % DivideMix \cite{li2020dividemix} [FILIPE] & 79.97 & 34.56 & 78.59 & 36.36 & 79.07 & 37.99 & \textbf{66.85} & 43.74 & \textbf{67.73} & 44.67 & \textbf{66.98} & 44.43 \\
%             PLC \cite{zhang2021learning} & 82.80 & \textbf{42.74} & 81.54 & \textbf{46.04} & 81.50 & \textbf{45.05} & 60.01 & 45.92 & 63.68 & 45.03 & 63.68 & 44.45 \\
%             \midrule
%             \textbf{Ours (Regular Model)} & \textbf{94.06} & 21.02 & \textbf{93.25} & 27.55 & \textbf{93.35} & 21.27 & 65.87 & \textbf{58.15} & 65.80 & - & 66.36 & - \\
%             {\hskip 0.5cm} + \textit{Test-Time Aug.} & \textit{94.72} & \textit{21.17} & \textit{93.79} & \textit{27.38} & \textit{93.97} & \textit{20.99} & \textit{66.83} & \textit{59.27} & \textit{66.48} & - & \textit{67.42} & - \\
%             \textbf{Ours (Modified Model)} & \textbf{94.00} & 19.18 & \textbf{93.76} & 27.28 & \textbf{94.23} & 19.94 & \textbf{68.25} & \textbf{58.69} & \textbf{68.14} & - & \textbf{68.22} & - \\
%             {\hskip 0.5cm} + \textit{Test-Time Aug.} & \textit{94.39} & \textit{19.71} & \textit{94.19} & \textit{27.17} & \textit{94.23} & \textit{20.21} & \textit{70.13} & \textit{59.39} & \textit{69.35} & - & \textit{70.13} & - \\
%             \bottomrule
%         \end{tabular}
%     }
%     \label{tab:results_pmd_2}
% \end{table*}

\clearpage

\brandon{TODO: Show an example of the model making different predictions when fed the same image with different noisy labels, and use this to demonstrate learned asymmetric or instance-dependant noise (eg. showing how changing the noisy label for two images in the same class effects them differently). We would include the prediction without a noisy label in this experiment.}

\brandon{TODO: Demonstrate that our final model can make more accurate predictions when noisy labels are present. For example, maybe we can train Animal10N with only 90\% of the data, and show how the accuracy of the model on the remaining 10\% of training data is much higher when noisy labels are present}.

% \brandon{TODO: Maybe a piecewise ablation showing how each component contributes to the final accuracy?}

\brandon{TODO: Emphasise how we don't use iterative relabelling/pseudo-labelling, and instead we follow SOTA SSL practices in just using consistency regularization?}

\brandon{TODO: If there is room we can have ablations showing that the choice of null labels (eg. zero vector, one vector, vector of 0.1s, etc.) does not greatly effect results, and we can also show that using a Mixture of Experts or self-attention model does not greatly effect results}.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
