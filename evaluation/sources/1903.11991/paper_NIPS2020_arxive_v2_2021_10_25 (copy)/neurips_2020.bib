@COMMENT LINE SEARCHES:

@article{line_analysis,
	author    = {Maximus Mutschler and
	Andreas Zell},
	title     = {Empirically explaining SGD from a line search perspective},
	year      = {2021},
	journal   = {ICANN}	
}

@article{labpal,
  author    = {Maximus Mutschler and
               Andreas Zell},
  title     = {Using a one dimensional parabolic model of the full-batch loss to
               estimate learning rates during training},
  journal   = {arxiv},
  year      = {2021},
}



@article{probabilisticLineSearch,
    title={Probabilistic Line Searches for Stochastic Optimization},
    author={Maren Mahsereci and Philipp Hennig},
    year={2017},
    journal={Journal of Machine Learning Research},
    volume = 18
}

@online{probabilisticLineSearchImpl,
  author = {Lukas Balles},
  title = {Probabilistic Line Search Tensorflow Implementation},
  year = 2017,
  url = {https://github.com/ProbabilisticNumerics/probabilistic_line_search/commit/a83dfb0},
  urldate = {2020-09-24}
}

@article{BigbatchLineSearch,
  title={Big batch SGD: Automated inference using adaptive batch sizes},
  author={De, Soham and Yadav, Abhay and Jacobs, David and Goldstein, Tom},
  journal={arXiv preprint arXiv:1610.05792},
  year={2016}
}

@article{hypergradientdescent,
  title={Online learning rate adaptation with hypergradient descent},
  author={Baydin, Atilim Gunes and Cornish, Robert and Rubio, David Martinez and Schmidt, Mark and Wood, Frank},
  journal={arXiv preprint arXiv:1703.04782},
  year={2017}
}

@article{more1994line,
  title={Line search algorithms with guaranteed sufficient decrease},
  author={Mor{\'e}, Jorge J and Thuente, David J},
  journal={ACM Transactions on Mathematical Software (TOMS)},
  volume={20},
  number={3},
  pages={286--307},
  year={1994},
  publisher={ACM New York, NY, USA}
}


@article{gradientOnlyLineSearch,
  title={Gradient-only line searches: An Alternative to Probabilistic Line Searches},
  author={Kafka, Dominic and Wilke, Daniel},
  journal={arXiv preprint arXiv:1903.09383},
  year={2019}
}

@article{empericalLineSearchApproximations,
  title={Empirical study towards understanding line search approximations for training neural networks},
  author={Chae, Younghwan and Wilke, Daniel N},
  journal={arXiv preprint arXiv:1909.06893},
  year={2019}
}

@article{backtracking_line_search,
  title={A stochastic line search method with convergence rate analysis},
  author={Paquette, Courtney and Scheinberg, Katya},
  journal={arXiv preprint arXiv:1807.07994},
  year={2018}
}


@article{backtracking_line_search_NIPS,
  title={Painless stochastic gradient: Interpolation, line-search, and convergence rates},
  author={Vaswani, Sharan and Mishkin, Aaron and Laradji, Issam and Schmidt, Mark and Gidel, Gauthier and Lacoste-Julien, Simon},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3727--3740},
  year={2019}
}


@COMMENT Optimizers:
@inproceedings{gauss_newton,
  title={Practical gauss-newton optimisation for deep learning},
  author={Botev, Aleksandar and Ritter, Hippolyt and Barber, David},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={557--565},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{cocob,
  title={Training deep networks without learning rates through coin betting},
  author={Orabona, Francesco and Tommasi, Tatiana},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2160--2170},
  year={2017}
}

@article{L4_alternative,
  title={Training neural networks for and by interpolation},
  author={Berrada, Leonard and Zisserman, Andrew and Kumar, M Pawan},
  journal={arXiv preprint arXiv:1906.05661},
  year={2019}
}

@article{averaging_weights,
  title={Averaging weights leads to wider optima and better generalization},
  author={Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:1803.05407},
  year={2018}
}

 
@article{backpropagation1,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={nature},
  volume={323},
  number={6088},
  pages={533},
  year={1986},
  publisher={Nature Publishing Group}
}

@article{grad_descent,
	added-at = {2008-10-07T16:03:39.000+0200},
	author = {Robbins, H. and Monro, S.},
	biburl = {https://www.bibsonomy.org/bibtex/2cc1b9aa8927ac4952e93f34094a3eaaf/brefeld},
	interhash = {93d54534a08c30eda9e34d1def03ffa3},
	intrahash = {cc1b9aa8927ac4952e93f34094a3eaaf},
	journal = {Annals of Mathematical Statistics},
	keywords = {imported},
	pages = {400-407},
	timestamp = {2008-10-07T16:03:40.000+0200},
	title = {A stochastic approximation method},
	volume = 22,
	year = 1951
}


@article{adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  journal   = {CoRR},
  volume    = {abs/1412.6980},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.6980},
  archivePrefix = {arXiv},
  eprint    = {1412.6980},
  timestamp = {Mon, 13 Aug 2018 16:47:35 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/KingmaB14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{radam,
	title={On the Variance of the Adaptive Learning Rate and Beyond},
	author={Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},
	year={2019},
	eprint={1908.03265},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}


@article{quickprop,
  title={An empirical study of learning speed in back-propagation networks},
  author={Fahlman, Scott E and others},
  year={1988},
  publisher={Carnegie Mellon University, Computer Science Department}
}

@article{quickpropnotgood,
	title={Neither Quick Nor Proper - Evaluation of QuickProp for Learning Deep Neural Networks},
	author={Clemens-Alexander Brust and Sven Sickert and Marcel Simon and Erik Rodner and Joachim Denzler},
	journal={CoRR},
	year={2016},
	volume={abs/1606.04333}
}

@inproceedings{L4,
title={L4: Practical loss-based stepsize adaptation for deep learning},
author={Rolinek, Michal and Martius, Georg},
booktitle={Advances in Neural Information Processing Systems},
pages={6434--6444},
year={2018}
}

@article{ratesautomatic,
  title={Automatic and Simultaneous Adjustment of Learning Rate and Momentum for Stochastic Gradient Descent},
  author={Rates, No More Pesky Learning}
}

@article{ratesautomatic,
  author    = {Tomer Lancewicki and
               Sel{\c{c}}uk K{\"{o}}pr{\"{u}}},
  title     = {Automatic and Simultaneous Adjustment of Learning Rate and Momentum
               for Stochastic Gradient Descent},
  journal   = {arxiv},
  year      = {2019},
}

@inproceedings{
	AdaBound,
	title={Adaptive Gradient Methods with Dynamic Bound of Learning Rate},
	author={Liangchen Luo and Yuanhao Xiong and Yan Liu},
	booktitle={International Conference on Learning Representations},
	year={2019},
	url={https://openreview.net/forum?id=Bkg3g2R9FX},
}

@inproceedings{
	AmsGrad,
	title={On the Convergence of Adam and Beyond},
	author={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},
	booktitle={International Conference on Learning Representations},
	year={2018},
	url={https://openreview.net/forum?id=ryQu7f-RZ},
}

@article{adagrad,
  title={Adaptive subgradient methods for on learning and stochastic optimization},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={Jul},
  pages={2121--2159},
  year={2011}
}

@article{adadelta,
  title={ADADELTA: An Adaptive Learning Rate Method},
  author={Matthew D. Zeiler},
  journal={CoRR},
  year={2012},
  volume={abs/1212.5701}
}

@article{rmsProp,
  title={Lecture 6.5-RMSProp, COURSERA: Neural networks for machine learning},
  author={Tieleman, Tijmen and Hinton, Geoffrey},
  journal={University of Toronto, Technical Report},
  year={2012}
}

@article{swa,
  title={Averaging weights leads to wider optima and better generalization},
  author={Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:1803.05407},
  year={2018}
}
@inproceedings{assymetric_valley,
  title={Asymmetric Valleys: Beyond Sharp and Flat Local Minima},
  author={He, Haowei and Huang, Gao and Yuan, Yang},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2549--2560},
  year={2019}
}

@inproceedings{largesclelandscape,
  title={Large scale structure of neural network loss landscapes},
  author={Fort, Stanislav and Jastrzebski, Stanislaw},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6706--6714},
  year={2019}
}

@book{nonlinear_programming,
  title={Linear and nonlinear programming},
  author={Luenberger, David G and Ye, Yinyu and others},
  volume={2},
  year={1984},
  publisher={Springer}
}


@book{numerical_optimization,
	title =     {Numerical Optimization},
	author =    {Jorge Nocedal, Stephen Wright},
	publisher = {Springer},
	isbn =      {9780387303031,0387303030},
	year =      {2006},
	series =    {Springer series in operations research},
	edition =   {2nd ed},
	volume =    {},
}

@COMMENT Scond Order Optimizers:




@InProceedings{oLBFGS,
	title = 	 {A Stochastic Quasi-Newton Method for Online Convex Optimization},
	author = 	 {Nicol N. Schraudolph and Jin Yu and Simon GÃ¼nter},
	booktitle = 	 {Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics},
	pages = 	 {436--443},
	year = 	 {2007},
	editor = 	 {Marina Meila and Xiaotong Shen},
	volume = 	 {2},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {San Juan, Puerto Rico},
	month = 	 {21--24 Mar},
	publisher = 	 {PMLR},
}

@article{S-LBFGS,
	title={Quasi-Newton Methods for Deep Learning: Forget the Past, Just Sample},
	author={Albert S. Berahas and Majid Jahani and Martin Tak{\'a}c},
	journal={CoRR},
	year={2019},
	volume={abs/1901.09997}
}


@inproceedings{KFAC,
  title={Optimizing neural networks with kronecker-factored approximate curvature},
  author={Martens, James and Grosse, Roger},
  booktitle={International conference on machine learning},
  pages={2408--2417},
  year={2015}
}


@inproceedings{L-sr1,
	title={L-sr1: a Second Order Optimization Method},
	author={Vivek Ramamurthy and Nigel Duffy},
	year={2017}
}

@article{gausnewton,
	title={Practical gauss-newton optimisation for deep learning},
	author={Botev, Aleksandar and Ritter, Hippolyt and Barber, David},
	journal={arXiv preprint arXiv:1706.03662},
	year={2017}
}


@COMMENT TODO Optimizer comparison paper:

@COMMENT Networks:

@inproceedings{efficientnet,
	title={EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
	author={Mingxing Tan and Quoc V. Le},
	booktitle={ICML},
	year={2019}
}

@article{mobilenet,
	title={MobileNetV2: Inverted Residuals and Linear Bottlenecks},
	author={Mark Sandler and Andrew G. Howard and Menglong Zhu and Andrey Zhmoginov and Liang-Chieh Chen},
	journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	year={2018},
	pages={4510-4520}
}

@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{alexnet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}


@article{vggnet,
  title={Very Deep Convolutional Networks for Large-Scale Image Recognition},
  author={Karen Simonyan and Andrew Zisserman},
  journal={CoRR},
  year={2014},
  volume={abs/1409.1556}
}

@inproceedings{denseNet,
  title={Densely connected convolutional networks.},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={CVPR},
  volume={1},
  pages={3},
  year={2017}
}


@COMMENT Visualisation:

@article{walkwithsgd,
  title={A walk with sgd},
  author={Xing, Chen and Arpit, Devansh and Tsirigotis, Christos and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1802.08770},
  year={2018}
}


@article{visualisationLossLandscape,
  title={Visualizing the Loss Landscape of Neural Nets},
  author={Hao Li and Zheng Xu and Gavin Taylor and Tom Goldstein},
  journal={CoRR},
  year={2017},
  volume={abs/1712.09913}
}

@article{LinePlots,
	title={Qualitatively characterizing neural network optimization problems},
	author={Goodfellow, Ian J and Vinyals, Oriol and Saxe, Andrew M},
	journal={arXiv preprint arXiv:1412.6544},
	year={2014}
}

@COMMENT Datasets

@COMMENT and CIFAR-100
@techreport{CIFAR-10,
	title={Learning multiple layers of features from tiny images},
	author={Krizhevsky, Alex and Hinton, Geoffrey},
	year={2009},
	institution={Citeseer}
}

@inproceedings{IMAGENET,
	AUTHOR = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
	TITLE = {{ImageNet: A Large-Scale Hierarchical Image Database}},
	BOOKTITLE = {CVPR09},
	YEAR = {2009}
	}
	
@COMMENT Frameworks
@misc{Tensorflow,
	title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
	url={https://www.tensorflow.org/},
	note={Software available from tensorflow.org},
	author={
	Mart\'{\i}n~Abadi and
	Ashish~Agarwal and
	Paul~Barham and
	Eugene~Brevdo and
	Zhifeng~Chen and
	Craig~Citro and
	Greg~S.~Corrado and
	Andy~Davis and
	Jeffrey~Dean and
	Matthieu~Devin and
	Sanjay~Ghemawat and
	Ian~Goodfellow and
	Andrew~Harp and
	Geoffrey~Irving and
	Michael~Isard and
	Yangqing Jia and
	Rafal~Jozefowicz and
	Lukasz~Kaiser and
	Manjunath~Kudlur and
	Josh~Levenberg and
	Dandelion~Man\'{e} and
	Rajat~Monga and
	Sherry~Moore and
	Derek~Murray and
	Chris~Olah and
	Mike~Schuster and
	Jonathon~Shlens and
	Benoit~Steiner and
	Ilya~Sutskever and
	Kunal~Talwar and
	Paul~Tucker and
	Vincent~Vanhoucke and
	Vijay~Vasudevan and
	Fernanda~Vi\'{e}gas and
	Oriol~Vinyals and
	Pete~Warden and
	Martin~Wattenberg and
	Martin~Wicke and
	Yuan~Yu and
	Xiaoqiang~Zheng},
	year={2015},
}

@article{PyTorch,
	title={Automatic differentiation in PyTorch},
	author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	year={2017}
}

@COMMENT Websites:


@misc{TFADAM,
	author = "{Google}",
	title = {TensorFlow adam optimizer documentation},
	howpublished = {\url{https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer}},
	note = {Accessed: 2019-11-12}
}


@COMMENT Stochastic components

@article{Dropout,
	author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
	title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
	journal = {Journal of Machine Learning Research},
	year    = {2014},
	volume  = {15},
	pages   = {1929-1958},
	url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}
@article{ShakeShake,
	title={Shake-Shake regularization},
	author={Xavier Gastaldi},
	journal={CoRR},
	year={2017},
	volume={abs/1705.07485}
}

@inproceedings{ShakeDrop,
	title={ShakeDrop Regularization for Deep Residual Learning.},
	author={Yoshihiro Yamada and Masakazu Iwamura and Takuya Akiba and Koichi Kise},
	year={2018}
}

@COMMENT Conjugate Gradien methods

@article{CGFletcher,
	title={Function minimization by conjugate gradients},
	author={Fletcher, Reeves and Reeves, Colin M},
	journal={The computer journal},
	volume={7},
	number={2},
	pages={149--154},
	year={1964},
	publisher={Oxford University Press}
}

@article{CGRibiere,
	title={Note sur la convergence de directions conjug{\'e}es},
	author={Ribi{\`e}re, G and Polak, E},
	journal={Rev. Francaise Informat Recherche Opertionelle},
	volume={16},
	pages={35--43},
	year={1969}
}
@article{CGDai,
	title={A nonlinear conjugate gradient method with a strong global convergence property},
	author={Dai, Yu-Hong and Yuan, Yaxiang},
	journal={SIAM Journal on optimization},
	volume={10},
	number={1},
	pages={177--182},
	year={1999},
	publisher={SIAM}
}
@book{CGHestenes,
	title={Methods of conjugate gradients for solving linear systems},
	author={Hestenes, Magnus Rudolph and Stiefel, Eduard},
	volume={49},
	number={1},
	year={1952},
	publisher={NBS Washington, DC}
}

@COMMENT SLS comparison

@inproceedings{xavier_weight_intializaiton,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010}
}

@incollection{lecun_weight_intializaiton,
  title={Efficient backprop},
  author={LeCun, Yann A and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
  booktitle={Neural networks: Tricks of the trade},
  pages={9--48},
  year={2012},
  publisher={Springer}
}

@COMMENT ethical and societal influences:

@article{yudkowsky2008artificial,
  title={Artificial intelligence as a positive and negative factor in global risk},
  author={Yudkowsky, Eliezer and others},
  journal={Global catastrophic risks},
  volume={1},
  number={303},
  pages={184},
  year={2008},
  publisher={Oxford University Press New York}
}
@incollection{muehlhauser2012singularity,
  title={The singularity and machine ethics},
  author={Muehlhauser, Luke and Helm, Louie},
  booktitle={Singularity Hypotheses},
  pages={101--126},
  year={2012},
  publisher={Springer}
}
@article{bostrom2014ethics,
  title={The ethics of artificial intelligence},
  author={Bostrom, Nick and Yudkowsky, Eliezer},
  journal={The Cambridge handbook of artificial intelligence},
  volume={1},
  pages={316--334},
  year={2014},
  publisher={Cambridge University Press Cambridge}
}

