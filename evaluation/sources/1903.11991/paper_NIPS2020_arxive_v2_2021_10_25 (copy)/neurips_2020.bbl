\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{Tensorflow}
M.~Abadi, A.~Agarwal, P.~Barham, E.~Brevdo, Z.~Chen, C.~Citro, G.~S. Corrado,
  A.~Davis, J.~Dean, M.~Devin, S.~Ghemawat, I.~Goodfellow, A.~Harp, G.~Irving,
  M.~Isard, Y.~Jia, R.~Jozefowicz, L.~Kaiser, M.~Kudlur, J.~Levenberg,
  D.~Man\'{e}, R.~Monga, S.~Moore, D.~Murray, C.~Olah, M.~Schuster, J.~Shlens,
  B.~Steiner, I.~Sutskever, K.~Talwar, P.~Tucker, V.~Vanhoucke, V.~Vasudevan,
  F.~Vi\'{e}gas, O.~Vinyals, P.~Warden, M.~Wattenberg, M.~Wicke, Y.~Yu, and
  X.~Zheng.
\newblock {TensorFlow}: Large-scale machine learning on heterogeneous systems,
  2015.
\newblock Software available from tensorflow.org.

\bibitem{probabilisticLineSearchImpl}
L.~Balles.
\newblock Probabilistic line search tensorflow implementation, 2017.

\bibitem{hypergradientdescent}
A.~G. Baydin, R.~Cornish, D.~M. Rubio, M.~Schmidt, and F.~Wood.
\newblock Online learning rate adaptation with hypergradient descent.
\newblock {\em arXiv preprint arXiv:1703.04782}, 2017.

\bibitem{S-LBFGS}
A.~S. Berahas, M.~Jahani, and M.~Tak{\'a}c.
\newblock Quasi-newton methods for deep learning: Forget the past, just sample.
\newblock {\em CoRR}, abs/1901.09997, 2019.

\bibitem{L4_alternative}
L.~Berrada, A.~Zisserman, and M.~P. Kumar.
\newblock Training neural networks for and by interpolation.
\newblock {\em arXiv preprint arXiv:1906.05661}, 2019.

\bibitem{bostrom2014ethics}
N.~Bostrom and E.~Yudkowsky.
\newblock The ethics of artificial intelligence.
\newblock {\em The Cambridge handbook of artificial intelligence}, 1:316--334,
  2014.

\bibitem{gausnewton}
A.~Botev, H.~Ritter, and D.~Barber.
\newblock Practical gauss-newton optimisation for deep learning.
\newblock {\em arXiv preprint arXiv:1706.03662}, 2017.

\bibitem{gauss_newton}
A.~Botev, H.~Ritter, and D.~Barber.
\newblock Practical gauss-newton optimisation for deep learning.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 557--565. JMLR. org, 2017.

\bibitem{quickpropnotgood}
C.-A. Brust, S.~Sickert, M.~Simon, E.~Rodner, and J.~Denzler.
\newblock Neither quick nor proper - evaluation of quickprop for learning deep
  neural networks.
\newblock {\em CoRR}, abs/1606.04333, 2016.

\bibitem{empericalLineSearchApproximations}
Y.~Chae and D.~N. Wilke.
\newblock Empirical study towards understanding line search approximations for
  training neural networks.
\newblock {\em arXiv preprint arXiv:1909.06893}, 2019.

\bibitem{CGDai}
Y.-H. Dai and Y.~Yuan.
\newblock A nonlinear conjugate gradient method with a strong global
  convergence property.
\newblock {\em SIAM Journal on optimization}, 10(1):177--182, 1999.

\bibitem{BigbatchLineSearch}
S.~De, A.~Yadav, D.~Jacobs, and T.~Goldstein.
\newblock Big batch sgd: Automated inference using adaptive batch sizes.
\newblock {\em arXiv preprint arXiv:1610.05792}, 2016.

\bibitem{IMAGENET}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock {ImageNet: A Large-Scale Hierarchical Image Database}.
\newblock In {\em CVPR09}, 2009.

\bibitem{adagrad}
J.~Duchi, E.~Hazan, and Y.~Singer.
\newblock Adaptive subgradient methods for on learning and stochastic
  optimization.
\newblock {\em Journal of Machine Learning Research}, 12(Jul):2121--2159, 2011.

\bibitem{quickprop}
S.~E. Fahlman et~al.
\newblock An empirical study of learning speed in back-propagation networks.
\newblock 1988.

\bibitem{CGFletcher}
R.~Fletcher and C.~M. Reeves.
\newblock Function minimization by conjugate gradients.
\newblock {\em The computer journal}, 7(2):149--154, 1964.

\bibitem{largesclelandscape}
S.~Fort and S.~Jastrzebski.
\newblock Large scale structure of neural network loss landscapes.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6706--6714, 2019.

\bibitem{ShakeShake}
X.~Gastaldi.
\newblock Shake-shake regularization.
\newblock {\em CoRR}, abs/1705.07485, 2017.

\bibitem{xavier_weight_intializaiton}
X.~Glorot and Y.~Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In {\em Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 249--256, 2010.

\bibitem{LinePlots}
I.~J. Goodfellow, O.~Vinyals, and A.~M. Saxe.
\newblock Qualitatively characterizing neural network optimization problems.
\newblock {\em arXiv preprint arXiv:1412.6544}, 2014.

\bibitem{TFADAM}
{Google}.
\newblock Tensorflow adam optimizer documentation.
\newblock
  \url{https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer}.
\newblock Accessed: 2019-11-12.

\bibitem{assymetric_valley}
H.~He, G.~Huang, and Y.~Yuan.
\newblock Asymmetric valleys: Beyond sharp and flat local minima.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2549--2560, 2019.

\bibitem{resnet}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{CGHestenes}
M.~R. Hestenes and E.~Stiefel.
\newblock {\em Methods of conjugate gradients for solving linear systems},
  volume~49.
\newblock NBS Washington, DC, 1952.

\bibitem{denseNet}
G.~Huang, Z.~Liu, L.~Van Der~Maaten, and K.~Q. Weinberger.
\newblock Densely connected convolutional networks.
\newblock In {\em CVPR}, volume~1, page~3, 2017.

\bibitem{averaging_weights}
P.~Izmailov, D.~Podoprikhin, T.~Garipov, D.~Vetrov, and A.~G. Wilson.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock {\em arXiv preprint arXiv:1803.05407}, 2018.

\bibitem{swa}
P.~Izmailov, D.~Podoprikhin, T.~Garipov, D.~Vetrov, and A.~G. Wilson.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock {\em arXiv preprint arXiv:1803.05407}, 2018.

\bibitem{numerical_optimization}
S.~W. Jorge~Nocedal.
\newblock {\em Numerical Optimization}.
\newblock Springer series in operations research. Springer, 2nd ed edition,
  2006.

\bibitem{gradientOnlyLineSearch}
D.~Kafka and D.~Wilke.
\newblock Gradient-only line searches: An alternative to probabilistic line
  searches.
\newblock {\em arXiv preprint arXiv:1903.09383}, 2019.

\bibitem{adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock {\em CoRR}, abs/1412.6980, 2014.

\bibitem{CIFAR-10}
A.~Krizhevsky and G.~Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem{alexnet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  1097--1105, 2012.

\bibitem{lecun_weight_intializaiton}
Y.~A. LeCun, L.~Bottou, G.~B. Orr, and K.-R. M{\"u}ller.
\newblock Efficient backprop.
\newblock In {\em Neural networks: Tricks of the trade}, pages 9--48. Springer,
  2012.

\bibitem{visualisationLossLandscape}
H.~Li, Z.~Xu, G.~Taylor, and T.~Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock {\em CoRR}, abs/1712.09913, 2017.

\bibitem{radam}
L.~Liu, H.~Jiang, P.~He, W.~Chen, X.~Liu, J.~Gao, and J.~Han.
\newblock On the variance of the adaptive learning rate and beyond, 2019.

\bibitem{nonlinear_programming}
D.~G. Luenberger, Y.~Ye, et~al.
\newblock {\em Linear and nonlinear programming}, volume~2.
\newblock Springer, 1984.

\bibitem{AdaBound}
L.~Luo, Y.~Xiong, and Y.~Liu.
\newblock Adaptive gradient methods with dynamic bound of learning rate.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{probabilisticLineSearch}
M.~Mahsereci and P.~Hennig.
\newblock Probabilistic line searches for stochastic optimization.
\newblock {\em Journal of Machine Learning Research}, 18, 2017.

\bibitem{KFAC}
J.~Martens and R.~Grosse.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature.
\newblock In {\em International conference on machine learning}, pages
  2408--2417, 2015.

\bibitem{more1994line}
J.~J. Mor{\'e} and D.~J. Thuente.
\newblock Line search algorithms with guaranteed sufficient decrease.
\newblock {\em ACM Transactions on Mathematical Software (TOMS)},
  20(3):286--307, 1994.

\bibitem{muehlhauser2012singularity}
L.~Muehlhauser and L.~Helm.
\newblock The singularity and machine ethics.
\newblock In {\em Singularity Hypotheses}, pages 101--126. Springer, 2012.

\bibitem{line_analysis}
M.~Mutschler and A.~Zell.
\newblock Empirically explaining sgd from a line search perspective.
\newblock {\em ICANN}, 2021.

\bibitem{labpal}
M.~Mutschler and A.~Zell.
\newblock Using a one dimensional parabolic model of the full-batch loss to
  estimate learning rates during training.
\newblock {\em arxiv}, 2021.

\bibitem{cocob}
F.~Orabona and T.~Tommasi.
\newblock Training deep networks without learning rates through coin betting.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2160--2170, 2017.

\bibitem{backtracking_line_search}
C.~Paquette and K.~Scheinberg.
\newblock A stochastic line search method with convergence rate analysis.
\newblock {\em arXiv preprint arXiv:1807.07994}, 2018.

\bibitem{PyTorch}
A.~Paszke, S.~Gross, S.~Chintala, G.~Chanan, E.~Yang, Z.~DeVito, Z.~Lin,
  A.~Desmaison, L.~Antiga, and A.~Lerer.
\newblock Automatic differentiation in pytorch.
\newblock 2017.

\bibitem{L-sr1}
V.~Ramamurthy and N.~Duffy.
\newblock L-sr1: a second order optimization method.
\newblock 2017.

\bibitem{ratesautomatic}
N.~M. P.~L. Rates.
\newblock Automatic and simultaneous adjustment of learning rate and momentum
  for stochastic gradient descent.

\bibitem{AmsGrad}
S.~J. Reddi, S.~Kale, and S.~Kumar.
\newblock On the convergence of adam and beyond.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{CGRibiere}
G.~Ribi{\`e}re and E.~Polak.
\newblock Note sur la convergence de directions conjug{\'e}es.
\newblock {\em Rev. Francaise Informat Recherche Opertionelle}, 16:35--43,
  1969.

\bibitem{grad_descent}
H.~Robbins and S.~Monro.
\newblock A stochastic approximation method.
\newblock {\em Annals of Mathematical Statistics}, 22:400--407, 1951.

\bibitem{L4}
M.~Rolinek and G.~Martius.
\newblock L4: Practical loss-based stepsize adaptation for deep learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6434--6444, 2018.

\bibitem{backpropagation1}
D.~E. Rumelhart, G.~E. Hinton, and R.~J. Williams.
\newblock Learning representations by back-propagating errors.
\newblock {\em nature}, 323(6088):533, 1986.

\bibitem{mobilenet}
M.~Sandler, A.~G. Howard, M.~Zhu, A.~Zhmoginov, and L.-C. Chen.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock {\em 2018 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, pages 4510--4520, 2018.

\bibitem{oLBFGS}
N.~N. Schraudolph, J.~Yu, and S.~GÃ¼nter.
\newblock A stochastic quasi-newton method for online convex optimization.
\newblock In M.~Meila and X.~Shen, editors, {\em Proceedings of the Eleventh
  International Conference on Artificial Intelligence and Statistics}, volume~2
  of {\em Proceedings of Machine Learning Research}, pages 436--443, San Juan,
  Puerto Rico, 21--24 Mar 2007. PMLR.

\bibitem{vggnet}
K.~Simonyan and A.~Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em CoRR}, abs/1409.1556, 2014.

\bibitem{Dropout}
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock {\em Journal of Machine Learning Research}, 15:1929--1958, 2014.

\bibitem{efficientnet}
M.~Tan and Q.~V. Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In {\em ICML}, 2019.

\bibitem{rmsProp}
T.~Tieleman and G.~Hinton.
\newblock Lecture 6.5-rmsprop, coursera: Neural networks for machine learning.
\newblock {\em University of Toronto, Technical Report}, 2012.

\bibitem{backtracking_line_search_NIPS}
S.~Vaswani, A.~Mishkin, I.~Laradji, M.~Schmidt, G.~Gidel, and
  S.~Lacoste-Julien.
\newblock Painless stochastic gradient: Interpolation, line-search, and
  convergence rates.
\newblock pages 3727--3740, 2019.

\bibitem{walkwithsgd}
C.~Xing, D.~Arpit, C.~Tsirigotis, and Y.~Bengio.
\newblock A walk with sgd.
\newblock {\em arXiv preprint arXiv:1802.08770}, 2018.

\bibitem{ShakeDrop}
Y.~Yamada, M.~Iwamura, T.~Akiba, and K.~Kise.
\newblock Shakedrop regularization for deep residual learning.
\newblock 2018.

\bibitem{yudkowsky2008artificial}
E.~Yudkowsky et~al.
\newblock Artificial intelligence as a positive and negative factor in global
  risk.
\newblock {\em Global catastrophic risks}, 1(303):184, 2008.

\bibitem{adadelta}
M.~D. Zeiler.
\newblock Adadelta: An adaptive learning rate method.
\newblock {\em CoRR}, abs/1212.5701, 2012.

\end{thebibliography}
