
\begin{proposition_ap}
	PAL converges on  $f : \mathbb{R}^n \rightarrow \mathbb{R}, \mathbf{x} \mapsto c+\mathbf{r}^T\mathbf{x}+\mathbf{x}^T\mathbf{Q}\mathbf{x}$ with $\mathbf{Q}\in \mathbb{R}^{n\times n}$ hermitian and positive definite.
\end{proposition_ap}
%\vspace{-0.5cm}
%\small
%\doublespacing
\begin{proof}
	\quad
\\For this prove we consider a basic \pal\ without the features introduced in Section \ref{subsec:features}.
Note, that during the proof we will see, that $a>0$ and $b<0$. Thus, only the update step for this case has to be considered (see Section \ref{subsec:casediscrimination}.
	
 \quad \\ 
 $f(\mathbf{x})$ is convex since $\mathbf{Q}$ is positive definite. Thus it has one minimum.\\
 Without loss of generality we set $c=0,\mathbf{r}=\mathbf{0},\mathbf{x}_n \neq 0$ %since the optimization problem does not change when the minimum location is moved to the origin and its value is set to 0. \\We get:
 \begin{equation}
 f(\mathbf{x})=\mathbf{x}^T\mathbf{Q}\mathbf{x} \text{ and }\nabla_{\mathbf{x}} f(\mathbf{x})= f'(\mathbf{x})=2\mathbf{Q}\mathbf{x}
   \end{equation}
The values of $f(x)$ along a line through $\mathbf{x}$ in the direction of $-f'(\mathbf{x})$ are given by:
 \begin{equation}
 f(-f'(\mathbf{x})\hat{s}+\mathbf{x})
   \end{equation}   
%The update step  $\hat{s}_{min}$ to the minimum of the line is:
%\begin{equation}
%\hat{s}_{min}= \argmin{\hat{s}} f(-f'(\mathbf{x}_n)\hat{s}+\mathbf{x}_n)
%\end{equation}
 %We set $s_{upd}=s_{min}||f'(x_n)||$ to simplify the further steps. $||f'(x_n)|| > 0$ since otherwise we would already be in the extremum. Note that $f(\mathbf{x})$ is convex.\\
 %Note that $f(-f'(\mathbf{x}_n)\hat{s}+\mathbf{x}_n)= a\hat{s}^2+b\hat{s}+c$ since $f(\mathbf{x})$ is an $n$ dimensional parabolic function and the values along each line of $f(\mathbf{x})$ are a parabolic function again. $a > 0$ since $\mathbf{Q}$ is positive definite.\\
Now we expand the line function:
\begin{equation}
\begin{aligned}
f(-f'(\mathbf{x})\hat{s}+\mathbf{x})&=
f(-2\mathbf{Q}\mathbf{x}\hat{s}+\mathbf{x})\\&=(-2\mathbf{Q}\mathbf{x}\hat{s}+\mathbf{x})^T\mathbf{Q}(-2\mathbf{Q}\mathbf{x}\hat{s}+\mathbf{x})
\\&=\underbrace{4\mathbf{x}^T\mathbf{Q}^3\mathbf{x}}_{=:a}\hat{s}^2+\underbrace{-4\mathbf{x}^T\mathbf{Q}^2\mathbf{x}}_{=:b}\hat{s}+\underbrace{\mathbf{x}^T\mathbf{Q}\mathbf{x}}_{=:c}
\end{aligned}
\end{equation}
 \\ Here we see that $f(\hat{s})$ is indeed a parabolic function with $a>0$, $b<0$ and $c>0$ since $\mathbf{Q}^3$, $\mathbf{Q}^2$ and $\mathbf{Q}$ are positive definite.\\
The location of the minimum $s_{min}$ of $f(\hat{s})$ is given by:
\begin{equation}
	 \hat{s}_{min}=\argmin{\hat{s}} f(-f'(\mathbf{x})\hat{s}+\mathbf{x})=-\frac{b}{2a}
\end{equation}
 \pal\ determines $\hat{s}_{min}$ exactly with $\hat{s}_{min}=\frac{s_{upd}}{||f'(x)||}$ (see equation 1 and 2). $||f'(x)||>0$  since otherwise we are already in the minimum.\\

\noindent The value at the minimum is given by: 
\begin{equation}
f(\hat{s}_{min})\\= a(\frac{-b}{2a})^2+b(\frac{-b}{2a})+c\\=-\frac{b^2}{4a}+c\\
%=-\frac{(4\mathbf{x}^T\mathbf{Q}^2\mathbf{x})^2}{16\mathbf{x}^T\mathbf{Q}^3\mathbf{x}}+\mathbf{x}^T\mathbf{Q}\mathbf{x}
=-\underbrace{\frac{(-\mathbf{x}^T\mathbf{Q}^2\mathbf{x})^2}{\mathbf{x}^T\mathbf{Q}^3\mathbf{x}}}_{=:g(\mathbf{x})}+\mathbf{x}^T\mathbf{Q}\mathbf{x}
\\=-g(\mathbf{x})+f(\mathbf{x}) \\
\end{equation}
Since $\mathbf{Q}^2$ and $\mathbf{Q}^3$ are positive definite and $\mathbf{x}\neq 0$: \begin{equation}g(\mathbf{x})>0\end{equation}
% Since $f(\mathbf{x})$ is positive we have:\\
% \begin{equation} 0<f(\mathbf{x}_{n+1})<f(\mathbf{x}_{n})<\ldots<f(\mathbf{x}_{0})
% \end{equation}
Now we consider the sequence $f(\mathbf{x}_n)$, with $\mathbf{x}_n$ defined by \textit{PAL} (see Equation 1): \\
\begin{equation}
\mathbf{x}_{n+1}=-\frac{f'(\mathbf{x}_n)}{||f'(x_n)||}\hat{s}_{upd}+\mathbf{x}_n= -f'(\mathbf{x}_n)\hat{s}_{min}+\mathbf{x}_n
\end{equation}
 It is easily seen by induction that:
 \begin{equation}
  0<f(\mathbf{x}_{n+1})<f(\mathbf{x}_{n})=\sum\limits_{i=0}^{n-1} -g(\mathbf{x}_i) +f(\mathbf{x}_0)< f(\mathbf{x}_0). 
\end{equation}
  $g(\mathbf{x}_{n})$ converges to 0. Since $\forall n:g(\mathbf{x_n})>0$ and $\sum\limits_{i=0}^{n-1} -g(\mathbf{x}_i)$ is bounded. \\
  \\ Now we have to show that $\mathbf{x}_n$ converges to 0.
 \\ We have:
  \begin{equation}
  g(\mathbf{x}_n)=\frac{(\mathbf{x}_n^T\mathbf{Q}^2\mathbf{x}_n)^2}{\mathbf{x}_n^T\mathbf{Q}^3\mathbf{x}_n}=\frac{\langle\mathbf{x}_n,\mathbf{Q}^2\mathbf{x}_n\rangle^2}{\langle\mathbf{x}_n,\mathbf{Q}^3\mathbf{x}_n\rangle}
  \end{equation}
 Now we use the theorem of Courant-Fischer:
   \begin{equation}
   \begin{aligned}
   \langle x,x\rangle\min\{\lambda_1,\dots,\lambda_n\}\leq \langle x,Ax \rangle\leq \langle x,x \rangle \max\{\lambda_1,\dots,\lambda_n\}\\ \text{ for any symmetric }  A \in \mathbb{R}^{n\times n} \text{ with } \lambda_1,\dots,\lambda_n
      \end{aligned}
   \end{equation}
    And get: 
\begin{equation}
g(\mathbf{x}_n) \geq \frac{\lambda^2_{\mathbf{Q}^2\min} \langle \mathbf{x}_n,\mathbf{x}_n \rangle^2}{\lambda_{\mathbf{Q}^3\max} \langle \mathbf{x}_n,\mathbf{x}_n \rangle}=C \frac{||\mathbf{x}_n||^4}{||\mathbf{x}_n||^2}=C ||\mathbf{x}_n||^2
\end{equation} 
with \begin{equation}C=\frac{\lambda^2_{\mathbf{Q}^2\min}}{\lambda_{\mathbf{Q}^3\max}}>0 \text{ since all } \lambda \text{ of the positive definite } \mathbf{Q} \text{ are positive} \end{equation}
 Thus, we have:
\begin{equation}
 g(\mathbf{x}_n) \geq C ||\mathbf{x}_n||^2 \geq 0 \end{equation} 
 Since $g(\mathbf{x}_n)$ converges to 0,  $C ||\mathbf{x}_n||^2 $ converges to 0. \\This means, $\mathbf{x}_n$ converges to $\mathbf{0} $, which is the location of the minimum. 
\end{proof}
\singlespacing
\normalsize

 
%\end{document}
