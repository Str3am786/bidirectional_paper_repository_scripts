\documentclass[10pt, conference, twocolumn, compsocconf]{IEEEtran}

%\pagestyle{plain}

% from IEEE template
\usepackage{cite}
%\renewcommand\citepunct{, }
\usepackage{amsmath,amssymb,amsfonts}
%\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
%%%%%%

\usepackage[dvipsnames]{xcolor}

\usepackage{amsthm}
\usepackage[ruled]{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{alltt}
\usepackage{caption}
\usepackage{paralist}
\usepackage{nicefrac}
\usepackage{booktabs}
\usepackage{xspace}
\usepackage{mathtools}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{verbatim}
\usepackage{subfigure}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage[capitalize]{cleveref}
\usepackage[per-mode=symbol,detect-all]{siunitx}
\usepackage{graphics}
\usepackage{wrapfig}
\usepackage{enumitem}
\usepackage{float}
\usepackage{courier}
\usepackage{bm}
\usepackage{xfp}
\usepackage{makecell}

%
% as a last resort to fit in the pages limit, uncomment the use of times 
%\usepackage{times}
%
\usepackage{stackengine}
\usepackage[normalem]{ulem}
\usepackage{mathpartir}

\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\usepackage{caption}
\captionsetup[figure]{font=small}

% https://coolors.co/e4572e-17bebb-ffc914-2e282a-76b041
\definecolor{color0}{RGB}{228,87,46}
\definecolor{color1}{RGB}{23,190,187}
\definecolor{color2}{RGB}{255,201,20}
\definecolor{color3}{RGB}{46,40,42}
\definecolor{color4}{RGB}{118,176,65}

% https://coolors.co/fa7921-fe9920-b9a44c-566e3d-0c4767
\definecolor{color0}{RGB}{250,121,33}
\definecolor{color1}{RGB}{254,153,32}
\definecolor{color2}{RGB}{185,164,76}
\definecolor{color3}{RGB}{86,110,61}
\definecolor{color4}{RGB}{12,71,103}

% https://coolors.co/e4fde1-8acb88-648381-575761-ffbf46
\definecolor{color0}{RGB}{228,253,225}
\definecolor{color1}{RGB}{138,203,136}
\definecolor{color2}{RGB}{100,131,129}
\definecolor{color3}{RGB}{87,87,97}
\definecolor{color4}{RGB}{255,191,70}

% https://coolors.co/e23b3e-f3722c-f8961e-f9c74f-7eb356-43aa8b-277da1-16323c
\definecolor{color0}{RGB}{226,59,62}
\definecolor{color1}{RGB}{243,114,44}
\definecolor{color2}{RGB}{248,150,30}
\definecolor{color3}{RGB}{249,199,79}
\definecolor{color4}{RGB}{126,179,86}
\definecolor{color5}{RGB}{67,170,139}
\definecolor{color6}{RGB}{39,125,161}
\definecolor{color7}{RGB}{21,49,60}
\definecolor{color8}{RGB}{180,215,228}

\definecolor{nnedgecolor}{RGB}{90,90,90}

\usepackage{tikz,ifthen,pgfplots}
\usetikzlibrary{arrows,trees,backgrounds,automata,shapes,decorations,plotmarks,fit,calc,positioning,shadows,chains}
\usetikzlibrary{arrows.meta,bending}
\tikzstyle{every pin edge}=[<-,shorten <=1pt]
\tikzstyle{every path}=[draw=color7!50]
\tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]
\tikzstyle{input neuron}=[neuron, fill=color4]
\tikzstyle{output neuron}=[neuron, fill=color0]
\tikzstyle{hidden neuron}=[neuron, fill=color6!80]
\tikzstyle{annot} = [text width=4em, text centered]
\tikzstyle{nnedge} = [-{stealth},shorten >=0.1cm, shorten <=0.05cm,line width=0.8pt,nnedgecolor]
\usetikzlibrary{calc}

\newcommand{\relu}{\text{ReLU}\xspace}
\newcommand{\argmax}{\text{argmax}\xspace}
\newcommand{\Mo}{\mathbf{I}}

\newcommand{\kfr}{$k$-forward-redundant}
\newcommand{\kfracy}{$k$-forward-redundancy}

\newcommand{\errorMinimizerFunction}{l_m}

\newcommand{\basic}{\mathcal{B}}
\newcommand{\nonbasic}{\mathcal{NB}}
\newcommand{\allvars}{\mathcal{X}}
\newcommand{\ub}{u}
\newcommand{\lb}{l}
\newcommand{\reluSet}{R}
\newcommand{\assignment}{\alpha{}}

\newcommand{\qflra}{\texttt{QF\_LRA}}
\newcommand{\tr}{\mathcal{T}_{\mathbb{R}}}
\newcommand{\trr}{\mathcal{T}_{\mathbb{R}R}}

\newcommand{\textbftt}[1]{\textbf{\texttt{#1}}}

\newcommand{\vect}[1]{\langle#1\rangle}

\newcommand{\sat}{\texttt{SAT}}
\newcommand{\unsat}{\texttt{UNSAT}}
\newcommand{\timeout}{\texttt{TIMEOUT}}
\newcommand{\memout}{\texttt{MEMOUT}}
\newcommand{\error}{\texttt{ERROR}}
\newcommand{\unknown}{\texttt{UNKNOWN}}

\newcommand{\act}{\texttt{active}}
\newcommand{\inact}{\texttt{inactive}}

\newcommand{\Sc}{\mathsf{S}}
\newcommand{\Ec}{\mathsf{E}}
\newcommand{\Rc}{\mathsf{R}}
\newcommand{\lc}{\mathsf{L}}
\newcommand{\uc}{\mathsf{U}}
\newcommand{\ac}{\mathsf{A}}
\newcommand{\reals}{\mathbb{R}}

\newcommand{\rn}[1]{\textsf{#1}}
\newcommand{\rns}[1]{\textsf{\small #1}}

\newcommand{\SAT}{\textbf{\texttt{SAT}}}
\newcommand{\UNSAT}{\textbf{\texttt{UNSAT}}}

\DeclareMathOperator{\polarityconstraint}{\mathbf{choosePhase}}
\DeclareMathOperator{\flipPhase}{\mathbf{flipPhase}}
\DeclareMathOperator{\getUnfixedRelus}{\mathbf{getUnfixedReLUs}}



\DeclareMathOperator{\Solve}{\mathbf{solve}}
\DeclareMathOperator{\dequeue}{dequeue}
\DeclareMathOperator{\enqueue}{enqueue}
\DeclareMathOperator{\allUnfixedRelus}{allUnfixedRelus}
\DeclareMathOperator{\notempty}{notEmpty}
\DeclareMathOperator{\Partition}{\mathbf{partition}}
\DeclareMathOperator{\Simplex}{simplex}
\DeclareMathOperator{\Empty}{empty}
\DeclareMathOperator{\Repair}{repair}
\DeclareMathOperator{\Branch}{selectBranchingReLU}
\DeclareMathOperator{\Reluplex}{reluplex}

\renewcommand{\gg}{\texttt{gg}\xspace}

\newcommand{\dnc}{S\textup{\&}C\xspace}
\newcommand{\ggMarabou}{\textrm{gg-Marabou}\xspace}
\newcommand{\DnCMarabou}{\dnc-Marabou\xspace} %TODO change this name.
\newcommand{\infraThread}{\texttt{thread}\xspace}
\newcommand{\infraLocal}{\texttt{gg-local}\xspace}
\newcommand{\infraLambda}{\texttt{gg-lambda}\xspace}

\usepackage{bussproofs}

\newcommand{\xhdr}[1]{{\noindent\bfseries #1}.}


\newcommand{\drule}[2]{
\renewcommand{\arraystretch}{1.2}
\(\begin{array}{c}
#1 \\
\hline 
#2
\end{array}\)
}
\newif\ifcomments
\commentstrue

\newif\ifoutline
\outlinefalse

\newif\iflong
\longtrue


%Consider using this Comment macro for individual comments. One can remove
%comments by passing arguments to the latex compiler, or setting commentsfalse
\newcommand{\Comment}[2]{{\color{#1}{$\curlyvee$}\ifcomments\marginpar{\small\raggedright\color{#1} #2}}\fi}
\newcommand{\CommentLine}[2]{{\color{#1}{\vrule\vrule}\ifcomments\marginpar{\small\raggedright\color{#1} #2}}\fi}
%\newcommand{\Comment}[1]{\color{orange}\curlyvee\ifcomments\marginpar{\small\color{orange} #1}\fi}

\newcommand{\todo}[1]{{\color{red}{\textbf{[TODO]} #1}}}

\newcommand{\changebars}[2]{[{\color{magenta}{#1}}][{\color{magenta}\sout{#2}}]}

\newcommand{\rulename}[1]{\ensuremath{\mathsf{#1}}\xspace}
\newcommand{\irulename}[2]{\ensuremath{\mathsf{#1}_{#2}}\xspace}

\renewcommand{\paragraph}[1]{\vspace{1mm}\noindent{\bf #1}\ }

\newcommand{\guy}[1]{\marginpar{\textcolor{orange}{Guy: #1}}}
\newcommand{\ori}[1]{\marginpar{\textcolor{blue}{Ori: #1}}}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

%%%
%%% Forbidden magic, to be removed for final version
%%%
%\addtolength{\textheight}{1cm}
%\setlength{\floatsep}{0.2cm}
%\setlength{\dblfloatsep}{0.2cm}
%\setlength{\textfloatsep}{0.2cm}
%\setlength{\dbltextfloatsep}{0.2cm}
%\setlength{\abovedisplayskip}{0.1cm}
%\setlength{\belowdisplayskip}{\abovedisplayskip}
%\setlength{\intextsep}{0.2cm}
%%%%

\begin{document}

\title{Pruning and Slicing Neural Networks\\
using Formal Verification}

\author{
\IEEEauthorblockN{Ori Lahav and Guy Katz}
\IEEEauthorblockA{The Hebrew University of Jerusalem, Jerusalem, Israel}
\{ori.lahav, guykatz\}@cs.huji.ac.il
}


\maketitle

\begin{abstract}
\blfootnote{[*] This is the extended version
of a paper with the same title that is about to appear in FMCAD 2021.
See \url{https://fmcad.org/}}
  Deep neural networks (DNNs) play an increasingly important role in
  various computer systems. In order to create these networks,
  engineers typically specify a desired topology, and then use an
  automated training algorithm to select the network's weights. While
  training algorithms have been studied extensively and are well
  understood, the selection of topology remains a form of art, and can
  often result in networks that are unnecessarily large --- and
  consequently are incompatible with end devices that have limited
  memory, battery or computational power. Here, we propose to address
  this challenge by harnessing recent advances in DNN verification.
  We present a framework and a methodology for discovering
  redundancies in DNNs --- i.e., for finding neurons that are not
  needed, and can be removed in order to reduce the size of the DNN.
  By using sound verification techniques, we can formally guarantee
  that our simplified network is equivalent to the original, either
  completely, or up to a prescribed tolerance. Further, we show how to
  combine our technique with \emph{slicing}, which results in a
  \emph{family} of very small DNNs, which are together equivalent to
  the original. Our approach can produce DNNs that are significantly
  smaller than the original, rendering them suitable for deployment on
  additional kinds of systems, and even more amenable to subsequent
  formal verification.  We provide a proof-of-concept implementation
  of our approach, and use it to evaluate our techniques on several
  real-world DNNs.

\end{abstract}

\section{Introduction}

The wide-spread adoption of \emph{deep learning}~\cite{GoBeCo16} has
caused a significant leap forward in many domains within computer
science.  \emph{Deep neural networks} (\emph{DNNs}) have now become
the state of the art solution for a myriad of real-world problems,
such as game playing~\cite{SiHuMaGuSiVaScAnPaLaDi16}, image
recognition~\cite{SiZi14}, and autonomous
vehicles~\cite{BoDeDwFiFlGoJaMoMuZhZhZhZi16,JuLoBrOwKo16}. This trend
is likely to continue and intensify, thus creating an urgent need for
tools and techniques to analyze and manipulate DNNs.

A part of the appeal of DNNs is that they are produced in a mostly
automated way.  In order to create a DNN for a particular task at
hand, engineers first specify the network architecture ---
specifically, the number of layers in the network, the size and type
of each layer, and the inter-layer connections. Then, they invoke an
automated training algorithm for assigning weights to the network's
edges\cite{GoBeCo16}. While the automated training process has been
extensively studied and is generally well understood~\cite{GoBeCo16},
the choice of network architecture is still performed according to
various rules of thumb, and is considered a form of art. This can
often lead to a choice of architecture that is wasteful --- i.e.,
which results in a large DNN, whereas a smaller DNN could have
achieved similar
accuracy~\cite{GoFeMaBaKa20,HaMaDa15,IaHaMoAsDaKe16}. For DNNs
intended to run on devices with limited resources (e.g., mobile
phones, or embedded circuits), excessive DNN size can be a limiting
factor~\cite{JuLoBrOwKo16}.

One successful approach for mitigating this difficulty is to first
train a large network, and then shrink it by removing \emph{redundant
  neurons}.  Informally, we say that a neuron is redundant if removing
it does not change the DNN's output; and thus, removing
it from a network $N$ results in a smaller network, $N'$, that is
\emph{equivalent} to $N$.  In order to identify redundant neurons within a
DNN, prior work has focused primarily on \emph{heuristic pruning}:
heuristically identifying neurons and edges that contribute little to
the network's output, removing these neurons, and then performing
additional training of the
network~\cite{HaMaDa15,IaHaMoAsDaKe16}. These methods have been highly
successful in reducing DNN sizes, but they provide no formal
guarantees; i.e., the removed neurons are not guaranteed to have been
redundant, and the simplified network can thus be dramatically
different from the original, producing different results for various
inputs~\cite{liebenwein2021lost}.

Recently, there has been a surge of interest in the formal
verification of neural networks
(e.g.,~\cite{KaBaDiJuKo17,KaBaDiJuKo21,KuKaGoJuBaKo18,HuKwWaWu17,GeMiDrTsChVe18,WaPeWhYaJa18,AmWuBaKa21}, and
many others). These new capabilities have made it possible to identify
and remove redundancies in a network, in a way that \emph{guarantees}
that the smaller network is completely equivalent to the
original~\cite{GoFeMaBaKa20}. Specifically, Gokulanathan et al.~showed
how verification could be used to identify and remove ``dead''
neurons, i.e. neurons whose output is $0$ regardless of the network's
inputs. This approach was shown to reduce network sizes by up to
$10\%$, which is quite significant, while preserving complete
equivalence to the original network.

% Targeting this problem, a work by \todo{sumathi etc} shown a method
% for finding redundancy by finding neurons which their output is 0 for
% all inputs, by using formal verification tools. These methods find
% redundancies, but up to a limited amount.  Additionally, these works
% finds neurons which can be removed individually - but doesn't take
% into account the error introduced when removing the neurons
% simultanously. \todo{really they doesn't take this into account?}

Here, we propose a new technique, which also attempts to apply formal
verification in order to remove neurons from a DNN, but which is
significantly stronger. Specifically, our technique:
\begin{inparaenum}[(i)]
\item can identify additional kinds of redundant neurons (beyond
  ``dead'' neurons), whose removal does not affect the network's
  outputs at all; and
\item can identify additional redundant neurons, whose removal
  \emph{does} affect the network's outputs, but only up to a small,
  provable bound.
\end{inparaenum}

% In this paper we improve both on amount of redundancy found, and on the analysis of error introduced by the redundancy removal process. We focus on removing a subset of a ReLU neural network's neurons (background on DNNs and ReLU is in the following section) which give us a local minima of the network size. Further research has to be done in order to acheive a global minimum of network size.

% Our suggested method involves formally proving which neurons are independetly redundant, e.g. can be removed without affecting the network output. Then we show in which cases independently redundant neurons can be removed simultanously.

% In addition, we dramatically increase the amount of redundant neurons found by using three techniques:

% \begin{enumerate}
% 	\item Broader definiton of redundant neuron ({\it Active-Redundancy}, {\it Forward-Redundancy} and {\it Relaxed-Redundancy}
% 	\item Allowance of $\epsilon$ error
% 	\item Input Slicing
% \end{enumerate}

Finally, we propose a method that takes our approach to the extreme,
by integrating it with \emph{network slicing}. This method, in which a
network is simplified into a family of much smaller sub-networks, is
appropriate for cases where fast inference is crucial: an input is
checked to identify the appropriate sub-network for handling it, and
then only that network needs to be evaluated for that specific input.
Slicing is achieved by partitioning the DNN's input
domain into small sub-domains, maintaining a separate DNN for each
input sub-domain, and then applying the aforementioned simplification
techniques on each of these DNNs. We demonstrate that the use of 
small input sub-domains causes many neurons to become redundant, and
consequently removable.


For evaluation purposes, we implemented our approach in an
open-source, publicly available tool~\cite{ourCode}. As a
backend, our tool uses the Marabou DNN verification
tool~\cite{Marabou2019}. We note, however, that our approach is
agnostic of the underlying verification engine --- indeed, it could be
integrated with any other tool, and will consequently benefit from any
development in DNN verification technology.  We evaluated our approach
on a set of airborne collision avoidance networks~\cite{JuLoBrOwKo16},
obtaining highly favorable results. Specifically, we were able to
achieve a reduction of up to $71$\% in overall network sizes, while
keeping the outputs identical (up to a prescribed tolerance) to those
produced by the original DNN.  This reduction in network sizes is a
significant improvement over the previous state of the
art~\cite{GoFeMaBaKa20}. Further, while prior techniques were
specifically tailored to networks with only a specific activation
function (i.e., rectified linear units~\cite{GoFeMaBaKa20}), our
technique is applicable to multiple kinds of DNNs.


The rest of this paper is organized as follows. In
Section~\ref{sec:background}, we provide the necessary background on
DNNs and their verification. Next, in Section~\ref{sec:formalization}
we present the basic building block of our approach, namely the
removal of a single neuron. We then specify multiple kinds of neurons
that can be removed in Section~\ref{sec:linear_funcs}, and discuss the
simultaneous removal of neurons in
Section~\ref{sec:simultaneousRemoval}.  Subsequently, in
Section~\ref{sec:input-slicing} we present how \emph{input slicing}
and simplification can be used to improve network evaluation time.  An
evaluation appears in Section~\ref{sec:evaluation}, followed by a
discussion of related work in Section~\ref{sec:relatedWork}. We then
conclude in Section~\ref{sec:conclusion}.


\section{Background: DNNs and their Verification}
\label{sec:background}

A deep neural network~\cite{GoBeCo16} is a directed, acyclic graph,
whose nodes (also referred to as \emph{neurons}) are grouped into
layers. The first layer is the \emph{input layer}; the final layer is
the \emph{output layer}; and the intermediate layers are the
\emph{hidden layers}. When the network is evaluated, the input neurons
are assigned some values (e.g., sensor readings), and these values are
then propagated through the network, layer by layer, until the output
values are computed. In \emph{regression} networks, the numeric value
of the output is of interest, while in the case of
\emph{classification} networks, the output
neurons correspond to possible \emph{labels} that the network can
classify the input into; and the label whose neuron obtained the
highest score is the one returned by the network.

Each layer in the DNN has a type, which determines how its neuron
values are computed. Here, we will focus on two types: \emph{weighted
  sum} layers, and \emph{piecewise-linear activation} layers.  In a
weighted-sum layer, the value of a neuron $y$ is computed as
$y=b + \sum c_iv_i$ for neurons $v_i$ from preceding layers, where the
\emph{weights} $c_i$ are determined when the network is first
trained. In a piecewise-linear activation layer, the value of neuron
$y$ is computed as
\begin{equation*}
y=
\begin{cases}
  a_1x+b_1 & \text{if } s_1\leq x < s_2,\\
  a_2x+b_2 & \text{if } s_2\leq x < s_3,\\
  \ldots \\
  a_kx+b_k & \text{if } s_k\leq x \leq s_{k+1}
\end{cases}
\end{equation*}
where $x$ is a neuron from some preceding layer, and the $a_i$, $b_i$ and
$s_i$ parameters determine the piecewise linear function being computed. A
common example of a piecewise-linear activation function is the
\relu{} function, given by
\begin{wrapfigure}[9]{r}{3.8cm}
  \vspace{-0.5cm}
  \begin{center} 
\includegraphics[scale=0.25]{figures/relu}
    \captionsetup{size=small}
    \captionof{figure}{The \relu{} function.}
    \label{fig:relu}
  \end{center}
\end{wrapfigure}
\vspace{-0.3cm}
\begin{equation*}
y=\max(x,0)=
\begin{cases}
  0 & \text{if } x < 0 \\
  x & \text{if } x \geq 0 
\end{cases}
\end{equation*}
(see Fig.~\ref{fig:relu}). Together, weighted-sum layers and
piecewise-linear activation functions make up many common DNN
architectures~\cite{GoBeCo16}. Typically, they are used in alternation
(see Fig.~\ref{fig:dnn}). Extending our approach to activation
functions that are not piecewise-linear remains a work in progress.

% ReLU DNN is a function with $n$ inputs and $m$ outputs. This
% function is built with layers - the input being layer $0$, then
% subsequent hidden layers and an output layer. Each hidden layer is
% comprised of $t$ neurons. Each neuron apply the ReLU function over a
% weighted-sum of the previous layer's neurons's outputs. The ReLU
% function is defined as $ReLU(x) = max(0, x)$ (see Figures
% \ref{fig:dnn} and \ref{fig:relu}).


\begin{figure}[htp]
\centering %
\scalebox{0.65}{%
\noindent\begin{tikzpicture}
\def\layersepedges{2cm}
\def\layersep{4cm}
\def\forwardsep{1.8cm}
\def\vertSepFactoryI{0.7}
\def\vertSepFactory{2}
\def\shiftFactory{1.9cm}
    \foreach \name / \y in {1,...,5}
        \node[input neuron, label={[label distance=4mm]90:\ifthenelse{\y=1}{Inputs}{}}] (I-\name) at (0,-\vertSepFactoryI * \y) {};

    \foreach \name / \y in {1,...,3}
        \path[yshift=\shiftFactory]
            node[hidden neuron, label={[label distance=2mm]\ifthenelse{\y=1}{\bf{WS}}{}}] (B1-\name) at (\layersepedges + 0*\layersep,-\vertSepFactory * \y cm) {};
            
    \foreach \name / \y in {1,...,3}
        \path[yshift=\shiftFactory]
            node[hidden neuron, label={[label distance=2mm]\ifthenelse{\y=1}{\bf{ReLU}}{}}] (F1-\name) at (\layersepedges + 0*\layersep + \forwardsep,-\vertSepFactory * \y cm) {};

    \foreach \name / \y in {1,...,3}
        \path[yshift=\shiftFactory]
            node[hidden neuron, label={[label distance=2mm]\ifthenelse{\y=1}{\bf{WS}}{}}] (B2-\name) at (\layersepedges + 1*\layersep,-\vertSepFactory * \y cm) {};

    \foreach \name / \y in {1,...,3}
        \path[yshift=\shiftFactory]
            node[hidden neuron, label={[label distance=2mm]\ifthenelse{\y=1}{\bf{ReLU}}{}}] (F2-\name) at (\layersepedges + 1*\layersep + \forwardsep,-\vertSepFactory * \y cm) {};

    \foreach \name / \y in {1,...,3}
        \path[yshift=\shiftFactory]
            node[hidden neuron, label={[label distance=2mm]\ifthenelse{\y=1}{\bf{ReLU}}{}}] (F3-\name) at (\layersepedges + 1*\layersep + 2cm + \forwardsep,-\vertSepFactory * \y cm) {};

    \foreach \name / \y in {1,...,5}
    % Draw the output layer node
        \node[output neuron, label={[label distance=4mm]90:\ifthenelse{\y=1}{Outputs}{}}]
        (O-\name) at (\layersepedges*2 + 1*\layersep + 2cm + \forwardsep,-\vertSepFactoryI * \y cm) {};

    \foreach \source in {1,...,5}
        \foreach \dest in {1,...,3}
%            \path (I-\source.east) edge  (B1-\dest.west);
            \draw[nnedge] (I-\source.east) -- (B1-\dest.west);
            
    \foreach \source in {1,...,3}
        \foreach \dest in {1,...,5}
%            \path (F3-\source.east) edge (O-\dest.west);
            \draw[nnedge] (F3-\source.east) -- (O-\dest.west);

    \foreach \source in {1,...,3}
        \foreach \dest in {1,...,3}
%            \path (F1-\source) edge (B2-\dest.west);
            \draw[nnedge] (F1-\source) -- (B2-\dest.west);

    \foreach \source in {1,...,3}
%    	\path (B1-\source) edge node[above]{ReLU} (F1-\source);
        \draw[nnedge] (B1-\source) -- node[above]{ReLU} (F1-\source);
    	
    \foreach \source in {1,...,3}
%    	\path (B2-\source) edge node[above]{ReLU} (F2-\source);
        \draw[nnedge] (B2-\source) -- node[above]{ReLU} (F2-\source);
        
	\node [xshift=\forwardsep/2,yshift=0.25cm,rotate=90] at ($(B1-1)!.5!(B1-2)$) {\textbf{\ldots}};
	\node [xshift=\forwardsep/2,yshift=0.25cm,rotate=90] at ($(B1-2)!.5!(B1-3)$) {\textbf{\ldots}};
	
	\node [xshift=\forwardsep/2,yshift=0.25cm,rotate=90] at ($(B2-1)!.5!(B2-2)$) {\textbf{\ldots}};
	\node [xshift=\forwardsep/2,yshift=0.25cm,rotate=90] at ($(B2-2)!.5!(B2-3)$) {\textbf{\ldots}};
	
	\node [rotate=90] at ($(F3-1)!.5!(F3-2)$) {\textbf{\ldots}};
	\node [rotate=90] at ($(F3-2)!.5!(F3-3)$) {\textbf{\ldots}};
	\node at ($(F2-2)!.5!(F3-2)$) {\textbf{\ldots}};

\end{tikzpicture}
}
\caption {An illustration of a DNN with alternating weighted-sum (WS) and ReLU layers.}
\label{fig:dnn}
\end{figure}

More formally, we regard a DNN $N$ with $k$ inputs and $m$ outputs as
a mapping $\mathbb{R}^k\rightarrow\mathbb{R}^m$. The DNN is given as a
sequence of layers $L_1,\ldots, L_n$, where $L_1$ is the input layer
and $L_n$ is the output layer. We use $s_i$ to denote the size of
layer $L_i$, and use $v_i^1,\ldots,v_i^{s_i}$ to refer to the
individual neurons of $L_i$. We use $V_i$ to refer to the column
vector $[v_i^1,\ldots,v_i^{s_i}]^T$.  When the network is being
evaluated, we assume that the input values $V_1$ are given, and that
$V_2,\ldots,V_n$ are computed iteratively. The type of each hidden
layer is given via the mapping
$T_N:\mathbb{N}\rightarrow\mathcal{T}$. For simplicity we set
$\mathcal{T}=\{\text{weighted-sum}, \relu{}\}$, although our technique
applies to all types of piecewise-linear activation functions.


In a weighted-sum layer $L_i$, each neuron $v_i^j$ is associated with
a linear function $v_i^j=b_i^j + \sum c_{l,t}\cdot v_l^t$; i.e.,
$v_i^j$ is computed as a weighted-sum of neurons $v_l^t$ from
preceding layers $l<i$, plus a bias value $b_i^j$. In a ReLU layer
$L_i$, each neuron $v_i^j$ is associated with a specific neuron
$v_l^t$ from a preceding layer $l<i$, and its value is given by
$v_i^j=\relu{}(v_l^t)=\max(v_l^t, 0)$. Note that each neuron's value
depends only on neurons from preceding layers.

In recent years, various security and safety issues have been
discovered in DNNs~\cite{SzZaSuBrErGoFe13,KaBaDiJuKo17}. This has led
the verification community to study the \emph{DNN verification
  problem}~\cite{LiArLaBaKo19}. Generally, this problem is defined by
a set of constraints $P$ on the DNN's inputs, and a set of constraints
$Q$ on the DNN's outputs; and solving it entails finding (or proving
the non-existence of) an input $x$ such that $P(x)\wedge Q(N(x))$;
i.e., an input $x$ that satisfies the input condition, and is mapped
by the DNN to a point that satisfies the output condition. When $P$
and $Q$ characterize an unsafe behavior of the DNN, an $\unsat{}$
answer to the aforementioned query indicates that the DNN is safe;
whereas a $\sat{}$ answer, accompanied by a satisfying assignment,
demonstrates an unsafe behavior. This formalization is sufficiently
expressive for capturing many properties of
interest~\cite{KaBaDiJuKo17}.  Many approaches for solving the DNN
verification problem have been proposed recently
(e.g.,~\cite{KaBaDiJuKo17,HuKwWaWu17,GeMiDrTsChVe18,WaPeWhYaJa18}, and
many others). The techniques we discuss in this work use a DNN
verification engine as a backend, and do not depend on the precise
method used --- and so we do not elaborate on this topic. We refer the
interested reader to~\cite{LiArLaBaKo19} for a survey.

\section{Removing a Single Neuron}
\label{sec:formalization}

The core of our DNN simplification approach is the identification, and
then the removal, of \emph{redundant neurons}. Given a DNN $N$, we seek to
identify a redundant neuron $v_i^j$, and then produce another network,
$N'$, which is identical to $N$ except for the redundant neuron that
has been removed. Ideally, we would like to ensure that $N$ and $N'$ are
equivalent; i.e., that $\forall x. N(x)=N'(x)$. Because $N'$ is
obtained from $N$ by removing a neuron, it is smaller; and this
process can be repeated iteratively, to eventually obtain a
significantly smaller network that is equivalent to $N$.
Of course, the key points that need addressing are:
\begin{inparaenum}[(i)]
\item how to technically remove a redundant neuron from the network; and
\item how to identify redundant neurons.
\end{inparaenum}
In this section we focus on the first challenge, and describe the
mechanics of removing a neuron.

In order to maintain compatibility with the original network, we will
refrain from removing neurons from the network's input or output
layers; all other neurons are considered candidates for removal. We
distinguish between neurons in weighted-sum layers, and neurons in
activation function layers. In fact, our proposed approach only
supports the removal of weighted-sum neurons that feed only into other
weighted-sum neurons; and the removal of activation function neurons
will be performed by first transforming them into weighted-sum
neurons, as described in later sections.

Consider a neuron $v$ computed as a weighted-sum
\[
  v= b_v + \sum c_i \cdot x_i,
\]
where $x_i$ are neurons from preceding layers.
 Suppose that $v$ only feeds into other weighted-sum neurons, and let
 $u$ be such a neuron:
\[
  u = b_u  + c\cdot v +  \sum d_i \cdot y_i,
\]
where $y_i$ are again neurons from preceding layers. 
In this case, 
$u$'s equation can be updated into:
\[
  u = (b_u + c\cdot b_v) + \sum c\cdot c_i\cdot x_i + \sum d_i\cdot y_i.
\]
If this process is repeated for every (weighted-sum) neuron that $v$
feeds into, then afterwards $v$ will have no outgoing
edges. Consequently, $v$ could then  be eliminated from the network altogether.
It is straightforward to show that such an operation will never affect
the value of $u$, and that the modified network will thus be
completely equivalent to the original. Also, identifying neurons that
can be eliminated is simple, and amounts to searching for weighted-sum
neurons that are only connected to other weighted-sum neurons.

In practice, DNN topology usually alternates between weighted-sum and
activation function layers, and so consecutive weighted-sum neurons
are likely to be scarce. Our strategy will thus be to replace
activation function neurons with weighted-sum neurons, in a way that
will enable neuron removal while preserving network accuracy.  As an
example, let us consider a ReLU neuron, $y=\relu{}(x)$.
Because of layer-type alternation, it is reasonable to assume that $x$
is a weighted-sum neuron. In this case, if we can express $y$ as a linear
function of $x$, i.e. $y=ax+b$ for some $a$ and $b$, then the previous
case of two consecutive weighted-sum neurons applies: we can remove
$x$ entirely, change $y$'s type to weighted-sum, and connect $y$ to
$x$'s inputs. Further, if $y$ also feeds into
weighted-sum neurons, then we can apply simplification once again, and
remove $y$ as well. An illustration appears in Fig.~\ref{fig:removing}.

\begin{figure}[htp]
\centering%
\scalebox{0.6}{%
\begin{tikzpicture}
\def\layersepedges{2cm}
\def\layersep{4cm}
\def\forwardsep{1.8cm}
\def\vertSepFactoryI{0.7}
\def\vertSepFactory{2}
\def\shiftFactory{1.9cm}
\def\opac{0.2}
\def\shift{8cm}

    \foreach \name / \y in {1,...,2}
        \path[opacity=\opac, yshift=\shiftFactory]
            node[hidden neuron] (F0-\name) at (0,-\vertSepFactory * \y cm) {};
            
    \path[opacity=\opac, yshift=\shiftFactory]
        node[hidden neuron] (B1-1) at (\forwardsep,-\vertSepFactory * 1 cm) {};
    \path[yshift=\shiftFactory]
        node[hidden neuron, label={[label distance=1mm]90:$x$}] (B1-2) at (\forwardsep,-\vertSepFactory * 2 cm) {};

    \path[opacity=\opac, yshift=\shiftFactory]
        node[hidden neuron] (F1-1) at (\forwardsep*2,-\vertSepFactory * 1 cm) {};
    \path[yshift=\shiftFactory]
        node[hidden neuron, label={[label distance=1mm]90:$y$}] (F1-2) at (\forwardsep*2,-\vertSepFactory * 2 cm) {};

    \foreach \name / \y in {1,...,2}
		\path [opacity=\opac, yshift=\shiftFactory]
            node[hidden neuron] (B2-\name) at (\forwardsep*3,-\vertSepFactory * \y cm) {};

    \foreach \source in {1,...,2} {
        \draw [opacity=\opac] (F0-\source.east) edge [nnedge] (B1-1.west);
        \draw (F0-\source.east) edge [nnedge] (B1-2.west);
    }
    
	%[opacity=0.2]
    \path[opacity=\opac] (B1-1.east) edge[nnedge] node[above]{ReLU} (F1-1);
    \path (B1-2.east) edge[nnedge] node[above]{ReLU} (F1-2);

    \foreach \dest in {1,...,2} {
        \path[opacity=\opac] (F1-1.east) edge[nnedge] (B2-\dest.west);
        \path (F1-2.east) edge[nnedge] (B2-\dest.west);
    }

	\draw[-{Triangle[width=18pt,length=8pt]}, line width=10pt, color=color3](6.2,-1.1) -- (7.2, -1.1);

    \foreach \name / \y in {1,...,2}
        \path[opacity=\opac, yshift=\shiftFactory]
            node[hidden neuron] (Ft0-\name) at (\shift,-\vertSepFactory * \y cm) {};
            
    \path[opacity=\opac, yshift=\shiftFactory]
        node[hidden neuron] (Bt1-1) at (\shift+\forwardsep,-\vertSepFactory * 1 cm) {};

    \path[opacity=\opac, yshift=\shiftFactory]
        node[hidden neuron] (Ft1-1) at (\shift+\forwardsep*2,-\vertSepFactory * 1 cm) {};

    \foreach \name / \y in {1,...,2}
        \path[opacity=\opac, yshift=\shiftFactory]
            node[hidden neuron] (Bt2-\name) at (\shift+\forwardsep*3,-\vertSepFactory * \y cm) {};

    \foreach \source in {1,...,2} {
        \path[opacity=\opac] (Ft0-\source.east) edge[nnedge] (Bt1-1.west);
    }
    
    \path[opacity=\opac] (Bt1-1.east) edge[nnedge] node[above]{ReLU} (Ft1-1);

    \foreach \dest in {1,...,2} {
        \path[opacity=\opac] (Ft1-1.east) edge[nnedge] (Bt2-\dest.west);
        %\path (Ft0-1) edge (Bt2-\dest.west);
        %\path (Ft0-2) edge (Bt2-\dest.west);
    }
    
    \draw [-{stealth},line width=0.8pt,nnedgecolor] [thick,draw=color7!50] plot [smooth,tension=1] coordinates {(Ft0-2.east)  ([shift={(-2.5, -1.5)}]Bt2-1.west) (Bt2-1.west)};
    \draw[-{stealth},line width=0.8pt,nnedgecolor] [thick,draw=color7!50][thick,draw=color7!50] plot [smooth,tension=0.6] coordinates {(Ft0-2.east)  (Bt2-2.west)};
    
    \draw[-{stealth},line width=0.8pt,nnedgecolor] [thick,draw=color7!50][thick,draw=color7!50] plot [smooth,tension=1] coordinates {(Ft0-1.east)  ([shift={(-2.3, -0.8)}]Bt2-1.west) (Bt2-1.west)};
    \draw[-{stealth},line width=0.8pt,nnedgecolor] [thick,draw=color7!50][thick,draw=color7!50] plot [smooth,tension=0.8] coordinates {(Ft0-1.east)  ([shift={(-1.8, 0.3)}]Bt2-2.west) (Bt2-2.west)};
\end{tikzpicture}
}
\caption {Illustration: removing a neuron. $x$ is a weighted-sum neuron which feeds into $y$,
a \relu{} neuron. After converting $y$ into a weighted-sum neuron, both $x$ and $y$ can be
removed.\label{fig:removing}}
\end{figure}

%We'll now present a categorization and formalization of redundant
%neurons.

The aforementioned steps constitute the framework of our approach --- to
repeat, until saturation, the two steps:
\begin{inparaenum}[(i)]
\item identify any weighted-sum neurons that only feed into weighted
  sum neurons, and remove them; and
\item identify any activation function neurons that can be changed
  into weighted-sum neurons, without harming the network's accuracy.
  % identify, using heuristics and simulations, neurons that we
  %   suspect are redundant; and\ori{"heuristics" suggests we don't find all neurons which are redundant alone, which isn't true - as we have almost no false-negatives}
  % \item apply formal verification to confirm that they are indeed
  %   redundant, and if so, remove them from the network.
\end{inparaenum}
The key remaining issue is how to identify those neurons to which step
2 can be applied. We elaborate on this issue in the following sections.

% A piecewise-linear 
% activation function $y=f(x)$ can be expressed as:
% \begin{equation*}
% y=
% \begin{cases}
%   a_1x+b_1 & \text{if } s_1\leq x < s_2,\\
%   a_2x+b_2 & \text{if } s_2\leq x < s_3,\\
%   \ldots \\
%   a_kx+b_k & \text{if } s_k\leq x < s_{k+1}
% \end{cases}
% \end{equation*}
% For example, in the $y=\relu{}(x)$ case, we can indeed write that
% $y=0$ if $-\infty<x<0$, and $y=x$ if $0\leq x<\infty$. The same
% applies to a variety of other functions, such as leaky ReLUs, max
% pool, absolute values, and others.


\section{Linearizing Activation Functions}
\label{sec:linear_funcs}
We next propose various criteria for determining which activation
function neuron can be changed into weighted-sum neurons. Applying
these criteria in practice is discussed later, in
Section~\ref{sec:simultaneousRemoval}.

\medskip
\noindent
\textbf{Phase Redundancy.}  In order to transform an activation
function neuron into a weighted-sum neuron
without changing the network's outputs, we leverage the properties of
piecewise-linear functions. Let $x$ be a weighted-sum neuron
and let $y=f(x)$ be an activation function neuron; then, by
definition, the value range of $x$ is divided into segments
$[s_1,s_2], [s_2,s_3], \ldots [s_k,s_{k+1}]$, and in each segment $y$
is a linear function (a weighted-sum) of $x$.  If we are able to
discover that $x$ is in fact restricted to one of these segments, i.e.
$s_i\leq x< s_{i+1}$ for some $i$, then we can safely discard the
constraint $y=f(x)$ and replace it with a linear constraint
$y=a_ix+b_i$, thus changing $y$ to be a weighted-sum neuron. We stress
that this change does not alter the value of $y$, and consequently
does not alter the network's outputs. When this phenomenon occurs, we
say that $y$ is \emph{phase-redundant}.  For the ReLU function, this
happens if we discover that $x< 0$ ($y$ is
\emph{inactive-redundant}), or $x\geq 0$ ($y$ is
\emph{active-redundant}). As previously stated, transforming the
piecewise-linear constraint into a linear one will often allow us to
eliminate two neurons from the network, without changing its outputs.

% In order to identify phase-redundant neurons, we propose two
% approaches. The first is to apply \emph{abstract interpretation}
% techniques (e.g.,~\cite{WaPeWhYaJa18,GeMiDrTsChVe18}), capable of
% deducing lower and upper bounds on the values hidden neurons in the
% network can take. These approaches are lightweight, but due to their
% over-approximating nature may fail to discover some phase-redundant
% neurons. Another, more computationally expensive approach is to apply
% DNN verification: for example, for $y=\relu{}(x)$, we can use a DNN
% verifier to determine whether it is possible, for any input, that
% $x\geq 0$. If the verifier returns \unsat{}, we know that always
% $x<0$, and that $y$ is inactive-redundant. Similar queries can be used
% to discover the phase-redundancy of any piecewise-linear activation
% function neuron.


% %% Guy: the two paragraphs below will likely be removed, and merged
% %% with relaxed-redundancy
% Replacing phase-redundant neurons with linear constraints has the
% desirable property that it does not affect the network's output at
% all. However, by relaxing our requirements and allowing some small
% inaccuracies to be introduced, we can remove additional
% neurons. Specifically, we present the notion of an
% $\epsilon$-phase-redundant neuron: a neuron that is \emph{almost}
% fixed to one of its linear phases, i.e.
% $s_i-\epsilon \leq x < s_{i+1}+\epsilon$, for some small
% $\epsilon>0$. We propose to treat an $\epsilon$-phase-redundant neuron
% as fixed, and replace it with the appropriate linear
% consternation. For example, in the $y=\relu{}(x)$ case, if we discover
% that $x>-0.001$ we could treat the \relu{} as strictly active, and
% replace it with the linear constraint $y=x$.  As it turns out, the
% error introduced by such an operation is small, depends directly on
% $\epsilon$, and can be bounded.

% A slightly more relaxed version of this approach is to replace a
% ``nearly-fixed'' activation function with a linear constraint; e.g.,
% if $x\geq -\epsilon$, to treat the ReLU constraint $y=\relu{}(x)$ as
% if it was active-redundant, and replace it with a linear
% constraint. In this case we say that the neuron is
% $\epsilon$-\emph{phase-redundant}; in
% Section~\ref{sec:simultaneousRemoval} we discuss, and bound, the
% precision error introduced by replacing such neurons with linear
% constraints.
 

\medskip
\noindent
\textbf{Forward Redundancy.}  Phase-redundancy captures the case where
an activation function neuron is fixed to a single linear phase, for
all possible inputs. However, there actually exist \emph{unstable}
activation-function neurons, i.e. neurons not fixed to a particular
linear phase, which can still be soundly transformed into weighted-sum
neurons computing one of these linear phases. Intuitively, this
happens when neuron $y$'s assignment affects its $k$ succeeding
layers, for some $k>0$, but gets ``canceled out'' in layer $k+1$. A
small, illustrative example appears in
Fig.~\ref{fig:forward-red-example}.  When replacing $y$ with a
weighted-sum neuron only affects neurons that are at most $k$ layers
away from $y$, we say that $y$ is \emph{\kfr{}}. Much like
phase-redundant neurons, \kfr{} neurons can be removed from the
network without harming its accuracy.

\begin{figure}[htp]
\centering %
\scalebox{0.65}{%
\noindent\begin{tikzpicture}
\def\layersepedges{2cm}
\def\layersep{3.3cm}
\def\forwardsep{1cm}
\def\vertSepFactoryI{0.3}
\def\vertSepFactory{1.5}
\def\shiftFactory{1.9cm}

	% Input
    \node[input neuron, label={[label distance=2mm]90:Input}, label={[label distance=1.5mm]270:$[-1,1]$}] (I-1) at (0,-\vertSepFactoryI * 1) {};

	\path[yshift=\shiftFactory]
            node[hidden neuron, text=white, label={[label distance=2mm]\bf{WS}}] (B1-1) at (\layersepedges + 0*\layersep,-\vertSepFactory * 1 cm) {$+0$};
	\path[yshift=\shiftFactory]
            node[hidden neuron, text=white] (B1-2) at (\layersepedges + 0*\layersep,-\vertSepFactory * 2 cm) {$+1$};

    \path[yshift=\shiftFactory]
            node[hidden neuron, color=color2, text=white, label={[label distance=2mm]\bf{ReLU}}] (F1-1) at (\layersepedges + 0*\layersep + \forwardsep,-\vertSepFactory * 1 cm) {\large $y$};
\path[yshift=\shiftFactory]
            node[hidden neuron] (F1-2) at (\layersepedges + 0*\layersep + \forwardsep,-\vertSepFactory * 2 cm) {};

    \foreach \name / \y in {1,...,2}
        \path[yshift=\shiftFactory]
            node[hidden neuron, text=white, label={[label distance=2mm]\ifthenelse{\y=1}{\bf{WS}}{}}] (B2-\name) at (\layersepedges + 1*\layersep,-\vertSepFactory * \y cm) {$+1$};

    \foreach \name / \y in {1,...,2}
        \path[yshift=\shiftFactory]
            node[hidden neuron, label={[label distance=2mm]\ifthenelse{\y=1}{\bf{ReLU}}{}}] (F2-\name) at (\layersepedges + 1*\layersep + \forwardsep,-\vertSepFactory * \y cm) {};
            
    \foreach \name / \y in {1,...,2}
        \path[yshift=\shiftFactory]
            node[hidden neuron, text=white, label={[label distance=2mm]\ifthenelse{\y=1}{\bf{WS}}{}}] (B3-\name) at (\layersepedges + 2*\layersep,-\vertSepFactory * \y cm) {$+0$};

    \foreach \name / \y in {1,...,2}
        \path[yshift=\shiftFactory]
            node[hidden neuron, label={[label distance=2mm]\ifthenelse{\y=1}{\bf{ReLU}}{}}] (F3-\name) at (\layersepedges + 2*\layersep + \forwardsep,-\vertSepFactory * \y cm) {};

    \node[output neuron, label={[label distance=2mm]90:Output}]
        (O-1) at (\layersepedges*1 + 2*\layersep + 2cm + \forwardsep,-\vertSepFactoryI * 1 cm) {};

	% Input -> first layer
    \foreach \source in {1}
        \foreach \dest in {1,...,2}
            \draw[nnedge] (I-\source.east) -- node[above]{$1$}  (B1-\dest.west);
            
    \foreach \source in {1,...,2}
        \draw[nnedge] (B1-\source) -- (F1-\source);
        
    \draw[nnedge] (F1-1) -- node[above]{$1$} (B2-1.west);
    \draw[nnedge] (F1-1) -- node[above, near end]{$-1$} (B2-2.west);
    \draw[nnedge] (F1-2) -- node[above]{$1$} (B2-1.west);
    \draw[nnedge] (F1-2) -- node[below]{$1$} (B2-2.west);

    	
    \foreach \source in {1,...,2}
        \draw[nnedge] (B2-\source) -- (F2-\source);
      
    \draw[nnedge] (F2-1) -- node[above]{$1$} (B3-1.west);
    \draw[nnedge] (F2-1) -- node[above, near end]{$1$} (B3-2.west);
    \draw[nnedge] (F2-2) -- node[above]{$1$} (B3-1.west);
    \draw[nnedge] (F2-2) -- node[below]{$1$} (B3-2.west);

    \foreach \source in {1,...,2}
        \draw[nnedge] (B3-\source) -- (F3-\source);
    
    % Last layer -> output
    \foreach \source in {1,...,2}
        \foreach \dest in {1}
            \draw[nnedge] (F3-\source.east) -- node[above]{$1$} (O-\dest.west);
        

\end{tikzpicture}
}

\caption {The \textcolor{color1}{\textbf{orange}} \relu{} neuron, marked $y$,
is \emph{2-forward-redundant}.
Replacing $y$ with a constant zero affects the following WS and ReLU layers,
but it does not affect the last WS layer (and thus the network output).
For example, observe that if we input $1$ into the network, $y$
evaluates to $1$, and the network's output evaluates to $12$. This
output value is unchanged even 
if we replace $y$'s value with $0$. A careful examination of the
network reveals that this will always be the case, regardless of the
network's input value.}
\label{fig:forward-red-example}
\end{figure}

More formally, let $v_i^j$ be an activation function neuron, and let
$N'$ be a network obtained from $N$ replacing $v$ with a weighted-sum neuron
$v_i^j = b_i^j + \sum c_kx_k$. Let $V_1$ denote an input vector, on which
both $N$ and $N'$ are evaluated; and let $V_2,\ldots, V_n$ and
$V_2',\ldots, V_n'$ denote the layer evaluations of $N$ and $N'$
(respectively) on
 $V_1$. If, for every $V_1$, it holds that
$V_{i+k}=V'_{i+k}$, then we say that neuron $v_i^j$ is \kfr{}
(note that this implies $V_{i+k'}=V'_{i+k'}$ for every $k'>k$).  We
note that a neuron that is phase-redundant is also \kfr{}, for any
$k\geq 0$.

%See Fig.~\ref{fig:3-forward-red} for an illustration.

% \ori{should we remove this figure?}
% \begin{figure}[htp]
% \centering%
% \scalebox{0.65}{%
% \begin{tikzpicture}
% \def\layersepedges{2cm}
% \def\layersep{4cm}
% \def\forwardsep{2cm}
% \def\vertSepFactoryI{0.7}
% \def\vertSepFactory{0.7}
% \def\shiftFactory{1.1cm}
% \def\pathopacity{0.4}

%     \foreach \name / \y in {1,...,5}
%         \node[input neuron, label={[label distance=4mm]90:\ifthenelse{\y=1}{Inputs}{}}] (I-\name) at (0,-\vertSepFactoryI * \y) {};

%     \foreach \name / \y in {1,...,8}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron, label={[label distance=0.6cm, color=LimeGreen, xshift=-0.25cm, text depth=-1ex, rotate=90]right:\ifthenelse{\y=1}{\huge \bf $=$}{}}] (B1-\name) at (\layersepedges + 0*\layersep,-\vertSepFactory * \y cm) {};

%     \foreach \name / \y in {1,2,3,5,6,7,8}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron, label={[label distance=0.6cm, color=OrangeRed, xshift=-0.35cm, text depth=-1ex, rotate=90]right:\ifthenelse{\y=1}{\huge \bf $\neq$}{}}] (F1-\name) at (\layersepedges + 0*\layersep + \forwardsep,-\vertSepFactory * \y cm) {};
%     \foreach \name / \y in {4}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron, color=color2, label={[label distance=0.6cm, color=OrangeRed, xshift=-0.35cm, text depth=-1ex, rotate=90]right:\ifthenelse{\y=1}{\huge \bf $\neq$}{}}] (F1-\name) at (\layersepedges + 0*\layersep + \forwardsep,-\vertSepFactory * \y cm) {};

%     \foreach \name / \y in {1,...,8}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron, label={[label distance=0.6cm, color=OrangeRed, xshift=-0.35cm, text depth=-1ex, rotate=90]right:\ifthenelse{\y=1}{\huge \bf $\neq$}{}}] (B2-\name) at (\layersepedges + 1*\layersep,-\vertSepFactory * \y cm) {};

%     \foreach \name / \y in {1,...,8}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron, label={[label distance=0.6cm, color=OrangeRed, xshift=-0.35cm, text depth=-1ex, rotate=90]right:\ifthenelse{\y=1}{\huge \bf $\neq$}{}}] (F2-\name) at (\layersepedges + 1*\layersep + \forwardsep,-\vertSepFactory * \y cm) {};

%     \foreach \name / \y in {1,...,8}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron, label={[label distance=0.6cm, color=LimeGreen, xshift=-0.25cm, text depth=-1ex, rotate=90]right:\ifthenelse{\y=1}{\huge \bf $=$}{}}] (F3-\name) at (\layersepedges + 1*\layersep + 2cm + \forwardsep,-\vertSepFactory * \y cm) {};

%     \foreach \name / \y in {1,...,5}
%     % Draw the output layer node
%         \node[output neuron, label={[label distance=4mm]90:\ifthenelse{\y=1}{Outputs}{}}]
%         (O-\name) at (\layersepedges*2 + 1*\layersep + 2cm + \forwardsep,-\vertSepFactoryI * \y cm) {};

%     \foreach \source in {1,...,5}
%         \foreach \dest in {1,...,8}
%             \path[opacity=\pathopacity] (I-\source.east) edge[nnedge]  (B1-\dest.west);
            
%     \foreach \source in {1,...,8}
%         \foreach \dest in {1,...,5}
%             \path[opacity=\pathopacity] (F3-\source.east) edge[nnedge] (O-\dest.west);

%     \foreach \source in {1,2,3,5,6,7,8}
%         \foreach \dest in {1,...,8}
%             \path[opacity=\pathopacity] (F1-\source.east) edge[nnedge] (B2-\dest.west);
%     \foreach \source in {4}
%         \foreach \dest in {1,...,8}
%             \path (F1-\source.east) edge[nnedge] (B2-\dest.west);

%     \foreach \source in {1,...,8}
%         \foreach \dest in {1,...,8}
%             \path[opacity=\pathopacity] (B1-\source.east) edge[nnedge] (F1-\dest.west);
            
%     \foreach \source in {1,...,8}
%         \foreach \dest in {1,...,8}
%             \path (B2-\source.east) edge[nnedge] (F2-\dest.west);
            
%     \foreach \source in {1,...,8}
%         \foreach \dest in {1,...,8}
%             \path (F2-\source.east) edge[nnedge] (F3-\dest.west);




% 	\begin{scope}[yshift={7cm}]

%     \foreach \name / \y in {1,...,5}
%         \node[input neuron, label={[label distance=4mm]90:\ifthenelse{\y=1}{Inputs}{}}] (I-\name) at (0,-\vertSepFactoryI * \y) {};

%     \foreach \name / \y in {1,...,8}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron] (SB1-\name) at (\layersepedges + 0*\layersep,-\vertSepFactory * \y cm) {};

%     \foreach \name / \y in {1,...,8}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron] (SF1-\name) at (\layersepedges + 0*\layersep + \forwardsep,-\vertSepFactory * \y cm) {};

%     \foreach \name / \y in {1,...,8}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron] (SB2-\name) at (\layersepedges + 1*\layersep,-\vertSepFactory * \y cm) {};

%     \foreach \name / \y in {1,...,8}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron] (SF2-\name) at (\layersepedges + 1*\layersep + \forwardsep,-\vertSepFactory * \y cm) {};

%     \foreach \name / \y in {1,...,8}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron] (SF3-\name) at (\layersepedges + 1*\layersep + 2cm + \forwardsep,-\vertSepFactory * \y cm) {};

%     \foreach \name / \y in {1,...,5}
%     % Draw the output layer node
%         \node[output neuron, label={[label distance=4mm]90:\ifthenelse{\y=1}{Outputs}{}}]
%         (O-\name) at (\layersepedges*2 + 1*\layersep + 2cm + \forwardsep,-\vertSepFactoryI * \y cm) {};
%     \foreach \source in {1,...,5}
%         \foreach \dest in {1,...,8}
%             \path[opacity=\pathopacity] (I-\source.east) edge[nnedge]  (SB1-\dest.west);
            
%     \foreach \source in {1,...,8}
%         \foreach \dest in {1,...,5}
%             \path[opacity=\pathopacity] (SF3-\source.east) edge[nnedge] (O-\dest.west);

%     \foreach \source in {1,...,8}
%         \foreach \dest in {1,...,8}
%             \path[opacity=\pathopacity] (SF1-\source.east) edge[nnedge] (SB2-\dest.west);

%     \foreach \source in {1,...,8}
%         \foreach \dest in {1,...,8}
%             \path[opacity=\pathopacity] (SB1-\source.east) edge[nnedge] (SF1-\dest.west);
            
%     \foreach \source in {1,...,8}
%         \foreach \dest in {1,...,8}
%             \path[opacity=\pathopacity] (SB2-\source.east) edge[nnedge] (SF2-\dest.west);
            
%     \foreach \source in {1,...,8}
%         \foreach \dest in {1,...,8}
%             \path[opacity=\pathopacity] (SF2-\source.east) edge[nnedge] (SF3-\dest.west);


% \end{scope}


% \end{tikzpicture}
% }
% \caption {{\bf 3-Forward-Redundancy} example. The \textcolor{color1}{\textbf{orange}} neuron had it's ReLU function replaced, which affected the subsequent layers except for the last hidden layer, and so the output doesn't change.\label{fig:3-forward-red}}
% \end{figure}

% In order to identify forward-redundant neurons, we again propose to apply
% DNN verification. We elaborate on the exact formulation of the needed
% query later on.


\medskip
\noindent
\textbf{Relaxed Redundancy.}  So far, we discussed replacing a
piecewise-linear activation neuron with a weighted-sum neuron that
corresponds to one of the activation function's linear segments; e.g.,
in the case of $y=\relu{}(x)$, neuron $y$ would be changed into a
weighted-sum neuron computing either $y=0$ or $y=x$.  We observe that,
although these linear functions are natural candidates for replacing
the original constraint, in fact any linear function $y=\ell(x)$ could
be used. Specifically, given an activation function $y=f(x)$ and some
known lower and upper bounds $lb$ and $ub$ for $x$ (computed, e.g.,
using interval arithmetic~\cite{KaBaDiJuKo17} or abstract
interpretation~\cite{WaPeWhYaJa18,GeMiDrTsChVe18}), we propose to find
a linear function $\ell(x)$ that has \emph{minimal error} compared to
$f(x)$.  We define this error to be
\[
  \max_{lb\leq x\leq ub}|f(x)-\ell(x)|
  \]
See Fig.~\ref{fig:relaxed} for an
illustration of replacing a \relu{} constraint, whose phase is not fixed,
with three linear constraints. In each illustration, the blue line is
the \relu{}, the dashed line is the linear replacement, and the red
area is the introduced error. In case (c), the maximal introduced error (the
height of the red region) is the smallest among the three options.

\begin{figure}[htp]

\centering     %%% not \center
\subfigure[Replacing a \relu{} with the zero function]{\includegraphics[scale=0.25]{figures/relaxed_zero}}
\subfigure[Replacing a \relu{} with identity function]{\includegraphics[scale=0.25]{figures/relaxed_ident}}
\subfigure[Replacing a \relu{} with an arbitrary linear function]{\includegraphics[scale=0.25]{figures/relaxed_relax}}

\caption {Replacing a \relu{} with linear functions.}
\label{fig:relaxed}
\end{figure}

Unlike in the phase-redundancy and \kfracy{} cases, setting
$y=\ell(x)$ will introduce some imprecision to the network's output.
The motivation is that by replacing $y=f(x)$ with $y=\ell(x)$ that has
minimal error, we would be introducing only a small imprecision, while enabling
the removal of $y$. Let $e_t$ be some user-defined error threshold;
when replacing $y=f(x)$ with $\ell(x)$ introduces an error $e$ such
that $e\leq e_t$, we say that neuron $y$ is \emph{relaxed-redundant}.


Let us focus on the $y=\relu{}(x)$ function as an example, and suppose
we know that $x\in [lb,ub]$. If $lb<0$ and $ub>0$, the neuron is not phase-redundant.
In this case, a linear function $y=\errorMinimizerFunction{}(x)$ with
minimal error can be easily computed, and is given by:
\[
  \errorMinimizerFunction{}(x) = \frac{ub}{ub - lb} \cdot x
  + \frac{-lb\cdot ub}{2(ub - lb)}.
\]
It is straightforward to check that the maximum error is obtained when
$x = 0$, and it is given by $\frac{-lb \cdot ub}{2(ub - lb)}$ (a proof
appears in Appendix~\ref{appendix:relax_analysis}).  Unsurprisingly,
when $lb$ or $ub$ are close to $0$, the error becomes very small ---
indicating that such \relu{}s, which are ``almost phase-redundant'',
could be removed at a small cost to precision.  It should be noted,
however, that minimizing the maximum error introduced by the removal
of a single neuron does not necessarily minimize the overall
imprecision introduced to the network's outputs.
%% Guy: we do discuss it later - the lemma that lets you compute the imprecision...

% Identifying relaxed-redundant neurons is a local operation, that does
% not require verification; for every neuron we can compute the maximum
% error introduced by replacing it with $\errorMinimizerFunction$, and
% see whether it exceeds the threshold. One issue is that removing
% multiple relaxed-redundant neurons could compound the introduced
% error; we elaborate on how to overcome this concern in
% Section~\ref{sec:simultaneousRemoval}.

% Guy: I think that the below text does not add much. Commenting it
% out for now.
% We note that, in the ReLU case, this method reduces the maximum error
% by a \textbf{factor of 2 at least} compared to replacing the neuron
% with a constant 0 or the identity function.  For example, suppose
% $\ell = -1$ and $u = 1$. Here, replacing the $ReLU$ with identity or
% constant 0 will lead to a maximum error of $1$, while replacing it
% with $\errorMinimizerFunction{}(x)$ leads to a maximum error of
% $0.25$, which is 4x improvement. Analysis and a formal proof of
% $\errorMinimizerFunction{}(x)$ for the ReLU case can be found in
% Appendix \ref{appendix:relax_analysis}. \todo{does not yet exists}

\medskip
\noindent
\textbf{Result-Preserving Redundancy.}  In classification networks, it
may be acceptable to give up some precision, as long as the output
label for each input is unchanged; i.e., if the original network
classified input $x$ as label $l$ with $80\%$ confidence, it may be
acceptable to remove neurons in a way that reduces this confidence to
$60\%$, as long as $x$ is still classified as $l$.

More formally, let $y=f(x)$ be an activation neuron in a network $N$,
and let $N'$ denote the same network with $y$ replaced by a weighted
sum neuron, $y=\ell(x)$. If, for every input vector $V_1$, it holds
that $\argmax(V_n)=\argmax(V'_n)$, i.e. if both networks classify each
input vector in the same way (regardless of the actual output neuron
values computed), then we say that neuron $y$ is
\emph{result-preserving redundant}. See Fig.~\ref{fig:respres-example}
for an example.

\begin{figure}[htp]
\centering %
\scalebox{0.65}{%
\noindent\begin{tikzpicture}
\def\layersepedges{2.4cm}
\def\layersep{3.9cm}
\def\forwardsep{1.5cm}
\def\vertSepFactoryI{0.}
\def\vertSepFactory{1.3}
\def\shiftFactory{1.9cm}

	% Input
    \node[input neuron, minimum size=0.9cm, label={[label distance=2mm]90:Input}, label={[label distance=1.5mm]270:$[-1,1]$}] (I-1) at (0,-\vertSepFactoryI * 1) {};

	\path[yshift=\shiftFactory]
            node[hidden neuron, minimum size=0.9cm, text=white, label={[label distance=2mm]\bf{WS}}] (B1-1) at (\layersepedges + 0*\layersep,-\vertSepFactory * 1 cm) {$+0$};
	\path[yshift=\shiftFactory]
            node[hidden neuron, minimum size=0.9cm, text=white] (B1-2) at (\layersepedges + 0*\layersep,-\vertSepFactory * 2 cm) {$-0.2$};

    \path[yshift=\shiftFactory]
            node[hidden neuron, minimum size=0.9cm, label={[label distance=2mm]\bf{ReLU}}] (F1-1) at (\layersepedges + 0*\layersep + \forwardsep,-\vertSepFactory * 1 cm) {};
\path[yshift=\shiftFactory]
            node[hidden neuron, minimum size=0.9cm, color=color2, text=white] (F1-2) at (\layersepedges + 0*\layersep + \forwardsep,-\vertSepFactory * 2 cm) {\large \bf $y$};
            
    \path[yshift=\shiftFactory]
            node[hidden neuron, minimum size=0.9cm, text=white, label={[label distance=2mm]{\bf{WS}}}] (B3-1) at (\layersepedges + 1*\layersep,-\vertSepFactory * 1 cm) {$+0$};
\path[yshift=\shiftFactory]
            node[hidden neuron, minimum size=0.9cm, text=white] (B3-2) at (\layersepedges + 1*\layersep,-\vertSepFactory * 2 cm) {$+0.1$};

    \foreach \name / \y in {1,...,2}
        \path[yshift=\shiftFactory]
            node[output neuron, minimum size=0.9cm, text=white, label={[label distance=1.5mm]\ifthenelse{\y=1}{Outputs}{}}] (F3-\name) at (\layersepedges + 7 + 1*\layersep + \forwardsep,-\vertSepFactory * \y cm) {$\#\y$};

	% Input -> first layer
    \foreach \source in {1}
        \foreach \dest in {1,...,2}
            \draw[nnedge] (I-\source.east) -- node[above]{$1$}  (B1-\dest.west);
            
    \foreach \source in {1,...,2}
        \draw[nnedge] (B1-\source) -- (F1-\source);
        
    \draw[nnedge] (F1-1) -- node[above]{$2$} (B3-1.west);
    \draw[nnedge] (F1-1) -- node[above, near end]{$1$} (B3-2.west);
    \draw[nnedge] (F1-2) -- node[above]{$1$} (B3-1.west);
    \draw[nnedge] (F1-2) -- node[below]{$-1$} (B3-2.west);

    \foreach \source in {1,...,2}
        \draw[nnedge] (B3-\source) -- (F3-\source);
        

\end{tikzpicture}
}
\caption {The \textcolor{color1}{\textbf{orange}} \relu{}, marked $y$, is
  result-preserving redundant and can be replaced with a constant zero.
  Observe that any input in range $(0.1, 1]$
  is classified as label $\#1$, while any input in range $[-1, 0.1)$ is
  classified as label $\#2$. The \relu{} in
  \textcolor{color1}{\textbf{orange}} is active only for inputs in
  $(0.2, 1]$, and it only increases the confidence in label $\#1$.
  For example, the network output for input $0.5$ is $[1.3, 0.3]^T$,
  and after replacing $y$ with $0$ the output becomes $[1.0, 0.6]^T$. Label $\#1$ still
  wins, but with a lower confidence.
  Thus, $y$ is result-preserving redundant --- replacing it with a
  constant zero does not change the winning class, for the entire
  input domain.}
\label{fig:respres-example}
\end{figure}

Note that result-preserving redundancy is, in a way, more permissive
than the previous categories: we do not directly try to bound the
imprecision introduced, but rather only try to maintain the same
output \emph{label} for every input.  Clearly, any neuron that is
phase-redundant or \kfr{} is also result-preserving; and it is
reasonable to assume that relaxed-redundant neurons with a small error
would also be result-preserving redundant. The motivation for
considering this kind of redundancy is that, due to its more
permissive nature, it can identify additional redundant neurons.

Our definition of result-preserving
redundancy can also be slightly relaxed, to exclude inputs whose
classification was \emph{borderline}; i.e., inputs whose
highest-scored label and the second-highest label received very
similar scores. Intuitively, with this alteration, a neuron is
considered result-preserving redundant if it does not change the
classification of any inputs which were previously classified with a
high degree of confidence, but may flip the classification of inputs
about which the DNN was not sure to begin with. The motivation for
this change is to allow the removal of additional neurons.

\section{Neuron Removal Strategies}
\label{sec:simultaneousRemoval}
In Section~\ref{sec:formalization} we laid the theoretical foundations
of our DNN simplification approach, by defining four kinds of
redundant neurons that could be removed to reduce network size. There
exist many strategies for applying these definitions in practice, in
order to reduce network sizes.  Intuitively, a good strategy is one
that identifies large sets of neurons that can be removed
simultaneously, in a way that is computationally efficient.  In this
Section, we propose one such strategy, which we have empirically
observed to perform well.

\medskip
\noindent
\textbf{Step 1: Bound Estimation using MILP.}
Let $v$ be an activation function neuron which we are considering for
removal. In this context, it is useful to deduce lower and upper
bounds for $v$ that are as tight as possible. Such bounds could lead,
for example, to the classification of $v$ as phase-redundant, or
enable us to compute $\errorMinimizerFunction(v)$ and declare $v$ to
be relaxed-redundant.

Mixed-Integer Linear Programming (MILP)~\cite{Dantzig1963} is a
well-studied method for solving a system of linear constraints with
real and integer variables. In the context of DNN verification, MILP
can be used to derive lower and upper bounds on the values that the
various neurons in the DNN can obtain~\cite{Ehlers2017,TjXiTe17}. This
is done by encoding a linear over-approximation of the neural network
into the MILP solver, and then using the solver's objective function
to maximize/minimize each of the individual neurons. For example,
after encoding a network $N$, we could set the solver's objective
function to $1\cdot v$, where $v$ is some neuron in $N$; and the
optimal solution discovered would then constitute $v$'s upper bound.

As a first step in the simplification process, we propose to run such
MILP queries for every neuron that is candidate for removal. The
number of resulting queries can be large --- two queries per neuron,
one for each bound --- but the gains are significant, as the
discovered bounds can often be quite tight~\cite{TjXiTe17}. At the end
of this step, we immediately remove all phase-redundant neurons.

In practice, it is useful to run the MILP solver with a short timeout
(e.g., 10 second)
for each neuron. In case a timeout occurs, modern
solvers are able to provide a sound approximation of the optimal
solution~\cite{Gurobi}.  In our experiments, we observed that this
initial step already detects a large number of phase-redundant
neurons.
%
% Guy: commenting out for shortening
%
% Further, the discovered bounds
% can also be used in choosing the optimal {\it relaxed-redundant}
% function, and in calculating a bound on this approximation's error
% (explained later).

\medskip
\noindent
\textbf{Step 2: Simulations.}  After the MILP phase is concluded, we
are left with multiple activation-function neurons whose phases are
not yet fixed. It is possible that some of these neurons are also
phase-redundant, but that the bounds discovered in the MILP pass were
too loose to indicate this. It is also possible that they are \kfr{}
or result-preserving redundant.  At this point we wish to quickly
\emph{rule out} as many of these candidates as possible, before
applying computationally expensive steps to dispatch the remaining
candidates.

To do this, we follow in the footsteps of Gokulanathan et
al.~\cite{GoFeMaBaKa20}, and apply \emph{simulations}; i.e., we
evaluate the network on a large number of random inputs, and for each
input record the values assigned to the network's neurons. Simulations
can easily show that a neuron is not phase-redundant, by demonstrating
two different inputs for which the neuron is in two different linear
phases. Similarly, they can show that a neuron is not \kfr{} or
result-preserving redundant.

\medskip
\noindent
\textbf{Step 3: Formal Verification.}  After the MILP and simulation
phases, we are left with activation-function neurons that are
candidates for removal, if we can prove them redundant. We now apply
formal verification to classify these remaining neurons. Specifically,
for each candidate neuron $v$, we:
\begin{inparaenum}[(i)]
\item apply verification to check whether $v$ is fixed to one if its
  linear phases, and is hence phase-redundant; and if not,
\item if $N$ is a classification network,
  apply verification to check whether $v$ is result-preserving
  redundant; else, if $N$ is a regression network, 
  apply verification to check whether $v$ is \kfr{}, for a value of $k$ that
  corresponds to the output layer.
\end{inparaenum}
Each of these conditions can be posed as a DNN verification query, as
described next. As soon as a neuron is marked redundant, it is
removed, and the process continues.

In order to determine whether $v=f(x)$ is phase-redundant, we must
check whether $x$ is restricted to a certain linear segment. Let
$[s_1,s_2], [s_2,s_3], \ldots [s_k,s_{k+1}]$ be the set of possible
segments. For each such segment $[s_i,s_{i+1}]$, we can encode the DNN into the
verifier, and pose the query: $\exists V_1. (x<s_i) \vee (x>s_{i+1})$. If
the answer is \unsat{}, we know that $x$ is indeed fixed into segment 
$[s_i,s_{i+1}]$. An illustration appears in Fig.~\ref{fig:inactive_query}.


\begin{figure}[htp]
\centering%
\scalebox{0.65}{%
\begin{tikzpicture}
\def\layersepedges{2cm}
\def\layersep{4cm}
\def\forwardsep{1.8cm}
\def\vertSepFactoryI{0.7}
\def\vertSepFactory{2}
\def\shiftFactory{1.9cm}

    \foreach \name / \y in {1,...,5}
        \node[input neuron, label={[label distance=4mm]90:\ifthenelse{\y=1}{Inputs}{}}] (I-\name) at (0,-\vertSepFactoryI * \y) {};

    \foreach \name / \y in {1,...,3}
        \path[yshift=\shiftFactory]
            node[hidden neuron] (B1-\name) at (\layersepedges + 0*\layersep,-\vertSepFactory * \y cm) {};

    \foreach \name / \y in {1,...,3}
        \path[yshift=\shiftFactory]
            node[hidden neuron] (F1-\name) at (\layersepedges + 0*\layersep + \forwardsep,-\vertSepFactory * \y cm) {};

    \path[yshift=\shiftFactory]
            node[hidden neuron, color=color2, text=black] (B2-2) at (\layersepedges + 1*\layersep,-\vertSepFactory * 2 cm) {$\text{ }x > 0\text{ }$};
            



    \foreach \name / \y in {1,3}
        \path[opacity=0.2, yshift=\shiftFactory]
            node[hidden neuron] (B2-\name) at (\layersepedges + 1*\layersep,-\vertSepFactory * \y cm) {};

\foreach \name / \y in {1,...,3}
        \path[opacity=0.2, yshift=\shiftFactory]
            node[hidden neuron] (F2-\name) at (\layersepedges + 1*\layersep + \forwardsep,-\vertSepFactory * \y cm) {};
            
    \path[opacity=0.5, yshift=\shiftFactory]
            node (F2-2-title) at (\layersepedges + 1*\layersep + \forwardsep,-\vertSepFactory * 2 cm) {$v$};

    \foreach \name / \y in {1,...,3}
        \path[opacity=0.2, yshift=\shiftFactory]
            node[hidden neuron] (F3-\name) at (\layersepedges + 1*\layersep + 2cm + \forwardsep,-\vertSepFactory * \y cm) {};

    \foreach \name / \y in {1,...,5}
    % Draw the output layer node
        \node[output neuron, opacity=0.2, label={[label distance=4mm, opacity=0.2]90:\ifthenelse{\y=1}{Outputs}{}}]
        (O-\name) at (\layersepedges*2 + 1*\layersep + 2cm + \forwardsep,-\vertSepFactoryI * \y cm) {};
        
        
        

    \foreach \source in {1,...,5}
        \foreach \dest in {1,...,3}
            \path (I-\source.east) edge[nnedge] (B1-\dest.west);
            
    \foreach \source in {1,...,3}
    	\path (B1-\source) edge[nnedge] node[above]{ReLU} (F1-\source);
    	
    \foreach \source in {1,...,3}
    	\path (F1-\source) edge[nnedge] (B2-2);
    	
    \path[opacity=0.4] (B2-2) edge[nnedge] node[above]{ReLU} (F2-2);
    
	

\end{tikzpicture}
}
\caption {A query for determining whether \relu{} node $v=\relu{}(x)$ is
  \emph{phase-redundant}: we check whether it is possible that
  $x>0$, and if not, we conclude that $v$ is inactive-redundant.
  To facilitate the verification process, the neurons in subsequent layers, as
  well as all other neurons in layer 2 (grayed out), are not encoded.}
\label{fig:inactive_query}
\end{figure}

Determining whether $v=f(x)$  is \kfr{} is done by
creating a query where the part of the network starting from the
neuron in question is duplicated. One copy of the network is the
unmodified one, and in the other copy $v=f(x)$ is replaced with a
weighted-sum neuron, $v'=\ell(x)$.   We query the verifier whether it
is possible that a neuron $k$ layers away from $v$ is assigned 
different values in the original and modified copies. If the answer is
\unsat{}, the neuron is \kfr{}. See Fig.~\ref{fig:forward-query}
for an illustration.


\begin{figure}[htp]
\centering%
\scalebox{0.65}{%
\begin{tikzpicture}
\def\layersepedges{2cm}
\def\layersep{4cm}
\def\forwardsep{2cm}
\def\vertSepFactoryI{0.7}
\def\vertSepFactory{0.7}
\def\shiftFactory{1.1cm}
\def\pathopacity{0.4}
    %\foreach \name / \y in {1,...,5}
    %    \node[input neuron, label={[label distance=4mm]90:\ifthenelse{\y=1}{Inputs}{}}] (I-\name) at (0,-\vertSepFactoryI * \y) {};

    %\foreach \name / \y in {1,...,8}
    %    \path[yshift=\shiftFactory]
    %        node[hidden neuron, label={[label distance=0.6cm, color=LimeGreen, xshift=-0.25cm, text depth=-1ex, rotate=90]right:\ifthenelse{\y=1}{}{}}] (B1-\name) at (\layersepedges + 0*\layersep,-\vertSepFactory * \y cm) {};

    %\foreach \name / \y in {1,2,3,5,6,7,8}
    %    \path[yshift=\shiftFactory]
    %        node[hidden neuron, label={[label distance=0.6cm, color=OrangeRed, xshift=-0.35cm, text depth=-1ex, rotate=90]right:\ifthenelse{\y=1}{}{}}] (F1-\name) at (\layersepedges + 0*\layersep + \forwardsep,-\vertSepFactory * \y cm) {};
    \foreach \name / \y in {4}
        \path[yshift=\shiftFactory]
            node[hidden neuron, color=color2, label={[label distance=0.6cm, color=OrangeRed, xshift=-0.35cm, text depth=-1ex, rotate=90]right:\ifthenelse{\y=1}{\huge \bf $\neq$}{}}] (F1-\name) at (\layersepedges + 0*\layersep + \forwardsep,-\vertSepFactory * \y cm) {};

    \foreach \name / \y in {1,...,8}
        \path[yshift=\shiftFactory]
            node[hidden neuron, label={[label distance=0.6cm, color=OrangeRed, xshift=-0.35cm, text depth=-1ex, rotate=90]right:\ifthenelse{\y=1}{}{}}] (B2-\name) at (\layersepedges + 1*\layersep,-\vertSepFactory * \y cm) {};

    \foreach \name / \y in {1,...,8}
        \path[yshift=\shiftFactory]
            node[hidden neuron, label={[label distance=0.6cm, color=OrangeRed, xshift=-0.35cm, text depth=-1ex, rotate=90]right:\ifthenelse{\y=1}{}{}}] (F2-\name) at (\layersepedges + 1*\layersep + \forwardsep,-\vertSepFactory * \y cm) {};

    \foreach \name / \y in {1,...,8}
        \path[yshift=\shiftFactory]
            node[hidden neuron, label={[label distance=0.6cm, color=LimeGreen, xshift=-0.25cm, text depth=-1ex, rotate=90]right:\ifthenelse{\y=1}{}{}}] (F3-\name) at (\layersepedges + 1*\layersep + 2cm + \forwardsep,-\vertSepFactory * \y cm) {};

%             node[hidden neuron, label={[label distance=0.6cm, color=OrangeRed, xshift=-0.35cm, text depth=-1ex, rotate=90]right:\ifthenelse{\y=1}{}{}}] (F1-\name) at (\layersepedges + 0*\layersep + \forwardsep,-\vertSepFactory * \y cm) {};

    \foreach \name / \y in {1,...,5}
    % Draw the output layer node
        \node[output neuron, label={[xshift=0.3 cm, yshift=1.1 cm, rotate=90]\ifthenelse{\y=1}{\huge \bf $\stackrel{?}{=}$}{}}]
        (O-\name) at (\layersepedges*2 + 1*\layersep + 2cm + \forwardsep,-\vertSepFactoryI * \y cm) {};

    %\foreach \source in {1,...,5}
    %    \foreach \dest in {1,...,8}
    %        \path[opacity=\pathopacity] (I-\source.east) edge[nnedge]  (B1-\dest.west);
            
    \foreach \source in {1,...,8}
        \foreach \dest in {1,...,5}
            \path[opacity=\pathopacity] (F3-\source.east) edge[nnedge] (O-\dest.west);

    \foreach \source in {4}
        \foreach \dest in {1,...,8}
            \path[opacity=\pathopacity] (F1-\source.east) edge[nnedge] (B2-\dest.west);

    \foreach \source in {1,...,8}
        \foreach \dest in {1,...,8}
            \path[opacity=\pathopacity] (B2-\source.east) edge[nnedge] (F2-\dest.west);
            
    \foreach \source in {1,...,8}
        \foreach \dest in {1,...,8}
            \path[opacity=\pathopacity] (F2-\source.east) edge[nnedge] (F3-\dest.west);




	\begin{scope}[yshift={5.7cm}]

    \foreach \name / \y in {1,...,5}
        \node[input neuron, label={[label distance=4mm]90:\ifthenelse{\y=1}{Inputs}{}}] (I-\name) at (0,-\vertSepFactoryI * \y) {};

    \foreach \name / \y in {1,...,8}
        \path[yshift=\shiftFactory]
            node[hidden neuron] (SB1-\name) at (\layersepedges + 0*\layersep,-\vertSepFactory * \y cm) {};

    \foreach \name / \y in {1,...,8}
        \path[yshift=\shiftFactory]
            node[hidden neuron] (SF1-\name) at (\layersepedges + 0*\layersep + \forwardsep,-\vertSepFactory * \y cm) {};

    \foreach \name / \y in {1,...,8}
        \path[yshift=\shiftFactory]
            node[hidden neuron] (SB2-\name) at (\layersepedges + 1*\layersep,-\vertSepFactory * \y cm) {};

    \foreach \name / \y in {1,...,8}
        \path[yshift=\shiftFactory]
            node[hidden neuron] (SF2-\name) at (\layersepedges + 1*\layersep + \forwardsep,-\vertSepFactory * \y cm) {};

    \foreach \name / \y in {1,...,8}
        \path[yshift=\shiftFactory]
            node[hidden neuron] (SF3-\name) at (\layersepedges + 1*\layersep + 2cm + \forwardsep,-\vertSepFactory * \y cm) {};

    \foreach \name / \y in {1,...,5}
    % Draw the output layer node
        \node[output neuron, label={[label distance=4mm]90:\ifthenelse{\y=1}{Outputs}{}}]
        (O-\name) at (\layersepedges*2 + 1*\layersep + 2cm + \forwardsep,-\vertSepFactoryI * \y cm) {};
    \foreach \source in {1,...,5}
        \foreach \dest in {1,...,8}
            \path[opacity=\pathopacity] (I-\source.east) edge[nnedge]  (SB1-\dest.west);
            
    \foreach \source in {1,...,8}
        \foreach \dest in {1,...,5}
            \path[opacity=\pathopacity] (SF3-\source.east) edge[nnedge] (O-\dest.west);

    \foreach \source in {1,...,8}
        \foreach \dest in {1,...,8}
            \path[opacity=\pathopacity] (SF1-\source.east) edge[nnedge] (SB2-\dest.west);

    \foreach \source in {1,...,8}
        \foreach \dest in {1,...,8}
            \path[opacity=\pathopacity] (SB1-\source.east) edge[nnedge] (SF1-\dest.west);
            
    \foreach \source in {1,...,8}
        \foreach \dest in {1,...,8}
            \path[opacity=\pathopacity] (SB2-\source.east) edge[nnedge] (SF2-\dest.west);
            
    \foreach \source in {1,...,8}
        \foreach \dest in {1,...,8}
            \path[opacity=\pathopacity] (SF2-\source.east) edge[nnedge] (SF3-\dest.west);


\end{scope}


    \foreach \source in {1,2,3,5,6,7,8}
        \foreach \dest in {1,...,8}
            \path[opacity=0.2] (SF1-\source.east) edge[nnedge] (B2-\dest.west);
            
    \foreach \source in {1,...,8}
        \foreach \dest in {4}
            \path[opacity=\pathopacity] (SB1-\source.east) edge[nnedge] (F1-\dest.west);
            


\end{tikzpicture}
}
\caption {{\bf 4-Forward-Redundancy} query illustration. The neuron in
\textcolor{color1}{\textbf{orange}} is the neuron being checked for forward-redundancy.
In this case the layer being checked is at distance 4, which happens to be the output
layer.
\label{fig:forward-query}}
\end{figure}

Determining whether $v=f(x)$ is result-preserving redundant
is done by creating a query similar to the \kfr{} case, only this time
we ask the verifier whether there exists
an input that the two networks classify differently. If the answer is
\unsat{}, we know that the neuron is indeed result-preserving
redundant.

\medskip
\noindent
\textbf{Step 4: Relaxed Redundancy and Accumulative Error.}  The
aforementioned steps were aimed at identifying and removing redundant
neurons, without introducing any imprecision into the simplified
network. Last but not least, we discuss the removal of
relaxed-redundant neurons. Recall that relaxed-redundant neurons are
determined by a user-specified error threshold $e_t$.  Identifying
these neurons is thus a local operation, that does not require
verification; for every neuron we can compute the maximum error
introduced by replacing it with $\errorMinimizerFunction$, and see
whether it exceeds the threshold.

%
% Guy: commenting out the iterative approach part, since we don't have
% any experimental results to back it up with
%
%
% With the other kinds of redundant neurons, things are not as
% straightforward. For example, it is possible that two neurons $v_1$ and
% $v_2$ are each forward-redundant, but that when one of them is removed
% the other ceases to be forward-redundant. Another possibility is that
% $v_1$ and $v_2$ are each relaxed-redundant, i.e. removing them
% introduces an acceptable error, but that removing both of them
% compounds the error and sends it above the acceptable threshold. 

% To remedy this issue, we propose two essentially different approaches.
% The first approach is \emph{iterative} --- repeatedly identify and
% remove redundant neurons, as specified previously. The second apporach
% involves calculating an upper bound on the error introduced by removing
% the individually redundant neurons simultaneously. This approach is faster,
% but not applicable to all types of redundancies.

% The \emph{iterative} approach can be divided into two approaches:
% \begin{inparaenum}[(i)]
%   \item the \emph{greedy approach}: starting with the original network
%     $N$, each time we remove a single redundant neuron to obtain a
%     smaller network $N'$, and then repeat the process for $N'$, until saturation. As is
%     often the case with greedy algorithms, this approach will remove a
%     set $\mathbb{R}$ of neurons that is maximal, but not necessary
%     maximum; and
%   \item the \emph{verification-based approach}: here, starting at
%     the original network $N$, we first find the set of all neurons
%     that can be (individually) removed. Then, we heuristically
%     construct sets of these neurons, remove them from the network,
%     and then use verification to ensure that the resulting network
%     $N'$ has precision that is sufficiently close to $N$. This
%     approach is more expensive --- there are many sets to try, and
%     verification is computationally costly --- but it can potentially
%     find larger sets of simultaneously removable neurons.
%   \end{inparaenum}
    

While each relaxed-redundant neuron can be identified locally,
removing multiple neurons simultaneously runs the risk of compounding
the overall error, beyond the permitted threshold.  To circumvent this
issue and allow the efficient removal of multiple relaxed-redundant
neurons, we introduce the following lemma:

\begin{lemma}
  Let $N$ be a neural network, and let $N'$ be a simplified  network,
  obtained from $N$ by removing relaxed-redundant neurons $u_1,\ldots,u_n$. Consider
  another neuron $v$ in $N'$ that is relaxed-redundant, and let $e_{in}$
  denote the error to $v$'s \emph{input}, previously introduced by the removal of
  $u_1,\ldots,u_n$. Let $e_v$ denote the error introduced by the
  removal of $v$. Then, if we remove $v$, the overall error introduced
  to its \emph{output} is upper bounded by:
\[
	e_{in} + e_v
\]
\end{lemma}

This lemma tells us that the iterative removal of relaxed-redundant
neurons does not compound the introduced error; instead, the error
introduced by the removal of each neuron is only added to the error
already introduced by the removal of other neurons.
% For example, in the case of active-redundancy the error is \emph{additive}
% (because $\ell = \text{Identity}$), while in the case of inactive-redundancy
% the error does not get compounded (because $\ell = \text{Zero}$).
This enables us, through a straightforward computation, to upper bound
the overall imprecision (on the output layer) that the removal of a
set of relaxed-redundant neurons might cause. Consequently, our
proposed strategy is to begin removing relaxed-redundant neurons with
small error rates, each time recomputing the overall network
inaccuracy, until hitting the prescribed overall error threshold.  A
full, formal description of these claims appears in
Appendix~\ref{appendix:sim_error_bounds}.

\section{Introducing Redundancies via Input Slicing}
\label{sec:input-slicing}
So far, our simplification efforts have hinged on the existence of
redundant neurons. Next, we introduce a technique that can cause
neurons to become redundant, even if they are initially not so.

The core idea is to:
\begin{inparaenum}[(i)]
  \item \emph{slice the input domain} $\mathcal{D}$ of the DNN $N$ into smaller
    sub-domains $\mathcal{D}_1,\ldots, \mathcal{D}_n$;
  \item duplicate the original network $n$ times, resulting in
    networks $N_1,\ldots,N_n$, such that network $N_i$ is associated
    with domain $\mathcal{D}_i$; and
  \item apply the simplification process
    described in
    Section~\ref{sec:simultaneousRemoval} for each $N_i$, separately. 
  \end{inparaenum}
  Intuitively, splitting the input domain into sub-domains can serve
  to separate ``simpler'' inputs regions, in which many neurons are
  phase-redundant, from more ``complex'' input domains where neurons
  fluctuate between phases. Various heuristics can be used for splitting
  the input domain, depending on the network in question. 
  A simple splitting method, which we used in our evaluation, is to
  split the range of each input coordinate into $n$ even sub-ranges.
%  while a more sophisticated one might involve convolutions.
  % \ori{i mean,
% basic input splitting approach probably won't work for image processing.
% networks, but one may  run convolution and then do 'input-splitting' on
% the result of the convolution. i think this is an interesting idea that
% may work. not sure if we should include it}
% \guy{we mention this later, i think, when we talk about slicing
% hidden layers}
  
  %% Guy: we didn't talk about ACAS Xu yet, so this belongs in the
  %% evaluation section.
  %
% For example, the ACAS Xu collision avoidance networks produce a nearly
% constant output for large portions of their input space. Thus, when we
% restrict our attention to a simple input region, our simplification
% technique has the potential for removing a great many neurons.\ori{'a great amount of'?}

  After the slicing and simplification is done, we are left with a
  family of DNNs $N_1,\ldots,N_n$, which are together equivalent to
  the original $N$.  Evaluation is then performed in two steps: given
  an input vector $V_1$, we first identify the domain $\mathcal{D}_i$
  to which $V_1$ belongs; and then compute $N_i(V_1)$ and return the
  result. As our evaluation shows, the resulting $N_i$ networks can be
  quite small, resulting in a significant improvement to the expected
  number of operations required for evaluating the network. This
  improvement might come at the expense of increased space
  requirements for storing the resulting family of networks, making
  this approach suitable for cases where space is abundant but fast
  inference is crucial.  We note that, as a side effect, the resulting
  networks may be easier to
  verify~\cite{WaPeWhYaJa18,WuOzZeIrJuGoFoKaPaBa20}.


%
% Guy: per our discussion, commenting out the "tree" section for now
%

% For each subspace we essentially produce a different specialized network. How do we represent the networks efficiently?


% \paragraph{Representation of Neurons and Subspaces.}
% \guy{Not sure I understand the motivation (see paragraph I added above,
%   ``Evaluating a network''. Can you elaborate?}
% For each neuron we'll want to create a representation of all subspaces which is redundant in. We create a tree in the following manner for each neuron:
% \begin{enumerate}
% 	\item The root node represents the whole input space
% 	\item Every other node is a subspace of the parent node, where all sibling nodes are distinct subspaces and together form the parent node subspace.
% 	\item In each node there is the redundancy state of the neuron (non-redundant or type of redundancy)
% 	\item If the neuron is redundant in a subspace, the corresponding node will not have children.
% \end{enumerate}
% See Figure \ref{fig:tree-rep} for an example, for one input and split of two every depth.

% \tikzset{
%   treenode/.style = {align=center, inner sep=0pt, text centered,
%     font=\sffamily},
%   arn_n/.style = {treenode, circle, white, scale=1.3, draw=black,
%     fill=black, text width=1cm},% arbre rouge noir, noeud noir
%   arn_r/.style = {treenode, circle, red, draw=red, 
%     text width=1.5em, very thick},% arbre rouge noir, noeud rouge
%   arn_x/.style = {treenode, rectangle, draw=black,
%     minimum width=0.5em, minimum height=0.5em}% arbre rouge noir, nil
% }

% \begin{figure}[htp]
% \centering%
% \scalebox{0.85}{%
% \tikzset{
%   treenode/.style = {shape=rectangle, rounded corners,
%                      draw, align=center,
%                      top color=black, bottom color=black},
%   root/.style     = {shape=rectangle, rounded corners,
%                      draw, align=center,font=\ttfamily\normalsize, color=black, fill=white},
%   env/.style      = {treenode, font=\ttfamily\normalsize, color=white},
%   dummy/.style    = {circle,draw,font=\ttfamily\normalsize}
% }
% \begin{tikzpicture}
%   [
%     grow                    = down,
%     sibling distance        = 6em,
%     level distance          = 6em,
%     edge from parent/.style = {draw, -latex},
%     every node/.style       = {font=\footnotesize},
%   ]
%   \node [root] {Layer 2, Neuron 34}
%     child {
%     	node [env] {inactive red.}
%     	edge from parent node [fill=white, above] {$\bm{[0,0.5]}$}
%    	}
%     child {
%     	node [dummy] {}
% 		child {
% 			node [dummy] {}
% 			child {
% 				node [dummy] {$\bm{\ldots}$}
% 				edge from parent node [fill=white, above] {$\bm{[0.5, 0.625]}$}
% 			}
% 			child {
% 				node [dummy] {$\bm{\ldots}$}
% 				edge from parent node [fill=white, below] {$\bm{[0.625,0.75]}$}
%            }
%            edge from parent node [fill=white, below] {$\bm{[0.5, 0.75]}$}
%         }
%         child {
%         	node [env] {active red.}
%             edge from parent node [fill=white, above] {$\bm{[0.75,1]}$}
%         }
%         edge from parent node [fill=white, below] {$\bm{[0.5,1]}$}
%    };
% \end{tikzpicture}
% }
% \caption{An example of a input slicing redundancy tree of a neuron.\label{fig:tree-rep}}
% \end{figure}

% We can construct this tree for any depth which is practical. Given this representation, evaluation is as follows: find a subspace the input is in, which is small enough so that we arrive at a leaf node in all trees. Then read use the activation state written in the node, or ReLU if the neuron is non-redundant.

% By exploiting the fact we can verify neurons individually and then combine them together, we enabled this relatively efficient representation of all the subspaces networks.

\medskip
\noindent
\textbf{Discussion: Dependency on Input Dimensions.}
Our proposed slicing method relies on splitting the input domain, by
restricting input neurons to various values. This approach works quite
well on DNNs with relatively few input neurons (e.g., the ACAS Xu
family of networks~\cite{JuLoBrOwKo16}; see
Section~\ref{sec:evaluation} for details). For networks with a
larger number of input neurons (e.g., image recognition networks),
the number of input sub-domains might be prohibitively large. Indeed, a
similar phenomenon has been observed for verification techniques that
rely on input slicing~\cite{WaPeWhYaJa18,WuOzZeIrJuGoFoKaPaBa20}.

One approach for mitigating this difficulty is through performing
slicing not on the input layer, but on some smaller intermediate layer $L_k$
in the network. Then, the network would be evaluated by evaluating the
original network's layers $L_1\ldots L_{k-1}$, and then using the
values computed for layer $L_k$ in choosing from a set of networks for
continuing the evaluation. We speculate that for an intermediate layer
of a moderate size, this approach could lead to improved performance
over input slicing. We leave this for future work.

% A possible fix for this issue which we didn't investigate enough is instead of splitting on the input layer, split on an hidden layer. In other words, run the network as usual up to layer N, and then use the 'input'-splitting method on layer N+1 values. This way we believe each neuron will have more significant individual meaning. Another approach is to split on a function of the input (such as convolution) instead of the input itself.

% In addition to the challenge mentioned above, networks with large amount of inputs require smarter subspace splitting. In ACAS we used a simple "split every coordinate by two" rule. This rule would make the tree explode in high-input network, and so a smarter rule should be divised for it. For example, for the Pensieve network we chose to split only on a subset of inputs.

\medskip
\noindent
\textbf{Extreme Slicing: Complete Linearization.}  We observe that
input slicing can be used to completely linearize every sub-domain of
the input space; that is, if the resulting sub-domains are
sufficiently small, then in each network $N_i$ all activation
functions will become phase-redundant, effectively collapsing the DNN
into a linear transformation.  Additionally, even if the slicing does
not fix the phase of all activation function neurons, extreme slicing
tends to decrease the error introduced by removing relaxed-redundant
neurons; and thus, complete linearization could be achieved by
removing these neurons, even if they have not become phase-redundant.
This linearization approach can thus be regarded as providing us with
a simple, piecewise-linear approximation of the network as a whole ---
with an upper bound on the error in each sub-domain.  Our experimental
results in Section~\ref{sec:evaluation} demonstrate very low error
rates on most sub-domains.

Complete linearization incorporates a trade-off: in order to obtain
very small, nearly-linear networks, the input domain would have to be
sliced many times. Users can fine-tune the number of slices used, and
consequently the sizes of the resulting DNNs, to their specific needs.


%. For 60\% of the subspaces the error was zero (see figure
%\todo{ref}).
% We speculate that this is a result of the special nature of the ACAS
% Xu system, which produces a nearly constant advisory in 
% large areas of its input space.


% The more a network is 'close' to being piecewise-linear, the better this method will work.
% \todo{graph for complete lin}

% \guy{Todo: lets move this graph to the evalaution section}

% \begin{figure}[ht]
% \begin{center}
% \includegraphics[scale=0.2]{figures/complete_linear.png}
% \end{center}
% %\vspace{-0.5cm}
% \caption {Error for Complete Linearization \label{fig:complete_linear}}
% \end{figure}



\section{Evaluation}
\label{sec:evaluation}
We created a proof-of-concept implementation of our approach as a
Python framework, available online~\cite{ourCode} (together with all
benchmarks reported in this section). The framework provides all the
functionality discussed so far: after importing a network, it can run
MILP queries to compute neuron bounds; perform simulations; and
identify phase-redundant, \kfr{} and result-preserving redundant
neurons, by running verification queries. The framework uses
the Gurobi~\cite{Gurobi} MILP solver and the Marabou~\cite{Marabou2019} DNN
verification engine as backends, although other backends could also be
used.

For evaluation purposes, we conducted extensive experiments on the
ACAS Xu system: an airborne collision avoidance system, implemented as
a family of 45 neural networks~\cite{JuLoBrOwKo16}. Each of these
neural networks has 5 input neurons, 5 output neurons, and 6 hidden
layers with 50 neurons each and ReLU activation functions (310 neurons
in total). Keeping the network sizes small was a key consideration in
developing the ACAS Xu system~\cite{JuLoBrOwKo16}, making it a prime
candidate on which to apply simplification techniques.

We began by comparing our approach to that of
Gokulanathan et al.~\cite{GoFeMaBaKa20}, which is the current
state-of-the-art in verification-based simplification of DNNs. Their
technique can be regarded as a private-case of ours, in which only
specific phase-redundant neurons (specifically, inactive-redundant
ReLUs) are removed. We compared that approach to our framework,
configured to identify and remove both active-redundant and
inactive-redundant ReLUs, and also to remove relaxed-redundant neurons.
We ran both tools on all 45 ACAS Xu networks; the
results appear in Table~\ref{table:sumathi}.


\newcommand{\redbymilp}{10.09}
\newcommand{\redbyformal}{2.42}
\newcommand{\inactivered}{12.09}
\newcommand{\activered}{0.42}
\newcommand{\relaxedfour}{0.22}
\newcommand{\relaxedthree}{1.22}
\newcommand{\relaxedtwo}{2.18}
\newcommand{\totalred}{\inactivered+\activered+\relaxedtwo}

\begin{table}[htp]
  \centering
  \caption{Phase-Redundancy and Relaxed-Redundancy on ACAS Xu networks.}
  \scalebox{0.88}{
    \begin{tabular}{ |c||c|c|c|c|c| } 
 \hline
      & Inactive & Active & \multicolumn{3}{c|}{Relaxed-Redundant} \\
      \cline{4-6}
      & Redundant & Redundant & $\epsilon=10^{-4}$ & $\epsilon=10^{-3}$ & $\epsilon=10^{-2}$ \\
 % \thead{Relaxed\\ ($\epsilon=1e-4$)} &
 % \thead{Relaxed\\ ($\epsilon=1e-3$)} &
 % \thead{Relaxed\\ ($\epsilon=1e-2$)}\\
 \hline
 
 %\thead{\% of Red.\\ Neurons} &
 %$\fpeval{round(\inactivered / (\totalred) * 100, 1)}\%$ &
 %$\fpeval{round((\inactivered+\activered) / (\totalred) * 100, 1)}\%$ &
 %$\fpeval{round((\inactivered+\activered+\relaxedfour) / (\totalred) * 100, 1)}\%$ &
 %$\fpeval{round((\inactivered+\activered+\relaxedthree) / (\totalred) * 100, 1)}\%$ &
 %$\fpeval{round((\inactivered+\activered+\relaxedtwo) / (\totalred) * 100, 1)}\%$
 %\\ 
 %\hline
 
 \thead{\% of all\\ neurons} &
 $\fpeval{round(\inactivered / 300. * 100, 1)}\%$ &
 $\fpeval{round((\inactivered+\activered) / 300. * 100, 1)}\%$ &
 $\fpeval{round((\inactivered+\activered+\relaxedfour) / 300. * 100, 1)}\%$ &
 $\fpeval{round((\inactivered+\activered+\relaxedthree) / 300. * 100, 1)}\%$ &
 $\fpeval{round((\inactivered+\activered+\relaxedtwo) / 300. * 100, 1)}\%$
 \\ 

 \hline

 \thead{\% of \\redundant\\ neurons} &
 baseline &
 $\fpeval{round((\activered) / (\inactivered) * 100, 1)}\%$ &
 $\fpeval{round((\activered+\relaxedfour) / (\inactivered) * 100, 1)}\%$ &
 $\fpeval{round((\activered+\relaxedthree) / (\inactivered) * 100, 1)}\%$ &
 $\fpeval{round((\activered+\relaxedtwo) / (\inactivered) * 100, 1)}\%$
 \\ 
 
 \hline
 	\thead{output error\\ bound} &
 	0 & 0 & 0.02 & 2.64 & 525.1
  \\
  \hline
 	
    \end{tabular}
  }
  \label{table:sumathi}
\end{table}

The table depicts the accumulated numbers of redundant neurons, when
read from left to right (which is the order in which the techniques
were applied). First, inactive-redundant neurons are removed (this is
the technique of~\cite{GoFeMaBaKa20}), accounting for $4$\% of all
neurons in the network.
Active-redundant neurons are next, removing another $0.2$\%
of all neurons, which is a $3.5$\% increase in the number of removed
neurons. Finally, relaxed-redundant neurons are removed, with three
possible alternative $\epsilon$ values. The most permissive one,
$\epsilon=10^{-2}$, leads to the removal of $4.9$\% of the neurons in
total, which is a $21.5$\% increase over the baseline --- but the
resulting network error bound in this case, $525.1$, is quite high.
$\epsilon=10^{-3}$ appears a better choice, with a total removal rate
of $4.6$\% and a significantly smaller error bound of $2.64$.  We note
that our evaluation indicates that the output error bounds currently
computed are far from tight; devising tighter bounding schemes is a
work in progress.

% First, we'll demonstrate \emph{relaxed-redundancy} on 10 ACAS networks
% with a comparison to the state-of-the-art method. Then, we will combine two other
% tools we demonstrated - \emph{result-preserving redundancy} and \emph{input-slicing}
% and show the maximum gains using our methods.

% \subsection{Evaluation 1: Relaxed-Redundancy}

% \noindent
% \textbf{Methodology.} To evaluate the significance of relaxed-redundancy,
% we chose an identical
% setting the methodology of Sumathi\todo{ref}. We took a random subset of 10
% ACAS networks and ran
% the steps required for relaxed-redundancy, namely:
% \begin{inparaenum}[(i)]
% \item bound estimation using MILP
% \item formal verification
% \end{inparaenum}
% and then we averaged the numbers. Phase-redundanct neurons are strictly redundant
% (e.g., not $epsilon$-redundant) while for relaxed-redundancy, we've tried a few
% different $e$ and computed the maximum error according to $\errorMinimizerFunction{}(x)$.

% \noindent
% \textbf{Results.} The results are shown in the following table:

In our second experiment, we evaluated our complete simplification
pipeline. First, we applied input-slicing, dividing the input domain
into 32,768 equal sub-domains (3 rounds of bisecting the range of each
of the $5$ input neurons in 2). Next, for each sub-domain we:
\begin{inparaenum}[(i)]
\item ran MILP and removed any discovered phase-redundant neurons;
\item ran simulations, and then formal verification to discover and
  remove any remaining phase-redundant neurons; and
\item identified all result-preserving neurons, and greedily attempted to
  simultaneously remove large sets thereof, using verification.
\end{inparaenum}
We note that identifying the largest set possible of result-preserving neurons that
can be removed simultaneously is a difficult problem, and our current
heuristic was a simple, greedy approach. Devising more sophisticated
heuristics is left for future work.


\newcommand{\statsMilp}{68.5}
\newcommand{\statsNonRedBySim}{26.4}
\newcommand{\statsRedByFormal}{1.7}
\newcommand{\statsResPres}{12.3}
\newcommand{\statsUnknowns}{7.2}
\newcommand{\statsTotalRed}{\fpeval{\statsMilp+\statsRedByFormal+\statsResPres}}

We ran the MILP step on all 32,768 sub-domains, which resulted in the
discovery of 67.3\%
phase-redundant neurons on average in each sub-domain.
We continued to run the pipeline on a
sample of 50 sub-domains selected at random.
Most notably, we observed an average removal of \emph{
  \statsTotalRed\% redundant neurons} (out of all neurons in the
network), with \statsUnknowns\% additional neurons still candidates
for removal, but for which the underlying verification engine
timed-out. Of the \statsTotalRed\% removed neurons, \fpeval{\statsMilp + \statsRedByFormal}\% were
phase-redundant, which is a very significant increase from the 4.2\% neurons
removed when the pipeline was run over the entire input domain. This
demonstrates the high effectiveness of input slicing. In addition,
about 21\% of phase-redundant neurons were active-redundant, which
signifies the importance of the generalization from ``dead neurons''~\cite{GoFeMaBaKa20} to
phase-redundancy. The remaining
\statsResPres\% neurons removed were result-preserving redundant.
Fig.~\ref{fig:evaluation_acas} shows the breakdown.

\begin{figure}[htp]
\begin{center}
\includegraphics[scale=0.5]{figures/acas_10_average}
\end{center}
\caption {Redundant neuron removal, averaged over 10 ACAS Xu input
  sub-domains.}
\label{fig:evaluation_acas}
\end{figure}

% \begin{table}[htp]
%     \centering
%   \caption{Input slicing and simplification results.}
%   \scalebox{1.0}{
% \begin{tabular}{ |c||c|c| } 
%  \hline
%  & Phase Red. & Result-Preserving Red.\\
%  \hline
 
%  \thead{Full Subspace} &
%  $\fpeval{round((\inactivered+\activered) / 300. * 100, 1)}\%$ &
%  \emph{not evaluated}
%  \\ 
%  \hline
 
%  \thead{Subspace Sliced\\ (depth 3)} &
%  $\fpeval{\statsMilp+\statsRedByFormal}\%$ &
%  \thead{$\statsResPres\%$\\
%  ($\fpeval{\statsMilp+\statsRedByFormal+\statsResPres}\%$ total)}
%  \\ 
 
%  \hline
%  & \multicolumn{2}{|c|}{\thead{(percentage of total neurons)}} \\
%  \hline
% \end{tabular}
%   }
%   \label{table:evaluation_acas}
% \end{table}


Slicing is highly beneficial for neuron removal, but results in a
large number of sub-domains that need to be checked. Within our
pipeline, verification steps are the most expensive, whereas MILP
queries and simulations are relatively cheap. We observe, however,
that MILP queries already account for most of the removed neurons.
Specifically, \statsMilp\% of all phase-redundant neurons removed were
discovered through MILP (about \fpeval{round((\statsMilp /
  \statsTotalRed)*100, 1)}\% of all redundant neurons), with a 10 second
timeout for each individual MILP query. 

The next step, namely simulations, is also computationally cheap and
highly effective.  For each sub-domain, we ran 100,000 simulations;
and out of the of \fpeval{100 - \statsMilp}\% neurons which were still
candidates for removal after the MILP phase, an average of
\statsNonRedBySim\% of the neurons were ruled not phase-redundant
through simulations. This left only a small number of candidates to be
dispatched through verification (\fpeval{100 - \statsMilp -
  \statsNonRedBySim}\% of the neurons), which in turn discovered the
remaining \statsRedByFormal\% redundant neurons, on average. In our
experiment, each Marabou verification query was run with a 4-hour timeout. 


As discussed above, we used a fairly na\"ive strategy for discovering
result-preserving redundant neurons. Specifically, we ran formal
verification on each candidate neuron to check whether it was 
individually result-preserving redundant; this resulted in a set of 
candidates for removal.  Then, we ran result-preserving simulations, iteratively
removing additional candidate neurons from the network, as
long as the simulations could
not find a counter-example to the redundancy of the currently removed
set.
%We applied this process
%until we had a few tens of candidate sets for each subset, and then
%ran a formal verification query on each set.
Finally, we ran a single verification query to verify that removing
our selected neurons was indeed a result-preserving operation.
On 75\% of the
sub-domains checked, this strategy worked. In sub-domains where we were
successful, we found an additional \fpeval{round(\statsResPres*2,
  1)}\% forward-redundant and result-preserving redundant neurons;
whereas in sub-domains where we were not successful, we had
a similar amount of candidates for removal on average.
%
% Guy: commenting out to shorten. I didn't feel like this figure
% contributed much this late in the paper.
%
% Fig.~\ref{fig:respres-on-simplified-network} illustrates a formal
% verification query used to determine the simultaneous
% result-preserving redundancy of a set of neurons.

% \begin{figure}[htp]
% \centering%
% \scalebox{0.65}{%
% \begin{tikzpicture}
% \def\layersepedges{2cm}
% \def\layersep{4cm}
% \def\forwardsep{2cm}
% \def\vertSepFactoryI{0.7}
% \def\vertSepFactory{0.7}
% \def\shiftFactory{1.1cm}
% \def\pathopacity{0.4}
%     %\foreach \name / \y in {1,...,5}
%     %    \node[input neuron, label={[label distance=4mm]90:\ifthenelse{\y=1}{Inputs}{}}] (I-\name) at (0,-\vertSepFactoryI * \y) {};

%     %\foreach \name / \y in {1,...,8}
%     %    \path[yshift=\shiftFactory]
%     %        node[hidden neuron, label={[label distance=0.6cm, color=LimeGreen, xshift=-0.25cm, text depth=-1ex, rotate=90]right:\ifthenelse{\y=1}{}{}}] (B1-\name) at (\layersepedges + 0*\layersep,-\vertSepFactory * \y cm) {};

%     %\foreach \name / \y in {1,2,3,5,6,7,8}
%     %    \path[yshift=\shiftFactory]
%     %        node[hidden neuron, label={[label distance=0.6cm, color=OrangeRed, xshift=-0.35cm, text depth=-1ex, rotate=90]right:\ifthenelse{\y=1}{}{}}] (F1-\name) at (\layersepedges + 0*\layersep + \forwardsep,-\vertSepFactory * \y cm) {};
%     \foreach \name / \y in {4}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron, color=color2, label={[label distance=0.6cm, color=OrangeRed, xshift=-0.35cm, text depth=-1ex, rotate=90]right:\ifthenelse{\y=1}{\huge \bf $\neq$}{}}] (F1-\name) at (\layersepedges + 0*\layersep + \forwardsep,-\vertSepFactory * \y cm) {};

%     \foreach \name / \y in {3,4,5,6,7,8}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron] (SF2-\name) at (\layersepedges + 1*\layersep + \forwardsep,-\vertSepFactory * \y cm) {};

%     \foreach \name / \y in {1,...,8}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron, label={[label distance=0.6cm, color=OrangeRed, xshift=-0.35cm, text depth=-1ex, rotate=90]right:\ifthenelse{\y=1}{}{}}] (B2-\name) at (\layersepedges + 1*\layersep,-\vertSepFactory * \y cm) {};

%     \foreach \name / \y in {3,4,5,6,7,8}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron, label={[label distance=0.6cm, color=OrangeRed, xshift=-0.35cm, text depth=-1ex, rotate=90]right:\ifthenelse{\y=1}{}{}}] (F2-\name) at (\layersepedges + 1*\layersep + \forwardsep,-\vertSepFactory * \y cm) {};
            
%     \foreach \name / \y in {1,2}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron, color=color8, label={[label distance=0.6cm, color=OrangeRed, xshift=-0.35cm, text depth=-1ex, rotate=90]right:\ifthenelse{\y=1}{}{}}] (F2-\name) at (\layersepedges + 1*\layersep + \forwardsep,-\vertSepFactory * \y cm) {};

%     \foreach \name / \y in {1,2,6,8}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron, label={[label distance=0.6cm, color=LimeGreen, xshift=-0.25cm, text depth=-1ex, rotate=90]right:\ifthenelse{\y=1}{}{}}] (F3-\name) at (\layersepedges + 1*\layersep + 2cm + \forwardsep,-\vertSepFactory * \y cm) {};
            
%     \foreach \name / \y in {3}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron, color=color2, label={[label distance=0.6cm, color=LimeGreen, xshift=-0.25cm, text depth=-1ex, rotate=90]right:\ifthenelse{\y=1}{}{}}] (F3-\name) at (\layersepedges + 1*\layersep + 2cm + \forwardsep,-\vertSepFactory * \y cm) {};
            
%     \foreach \name / \y in {4,5,7}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron, color=color8, label={[label distance=0.6cm, color=LimeGreen, xshift=-0.25cm, text depth=-1ex, rotate=90]right:\ifthenelse{\y=1}{}{}}] (F3-\name) at (\layersepedges + 1*\layersep + 2cm + \forwardsep,-\vertSepFactory * \y cm) {};

%     \foreach \name / \y in {1,...,5}
%     % Draw the output layer node
%         \node[output neuron, label={[label distance=4mm]90:\ifthenelse{\y=1}{Outputs}{}}]
%         (O-\name) at (\layersepedges*2 + 1*\layersep + 2cm + \forwardsep,-\vertSepFactoryI * \y cm) {};

%     %\foreach \source in {1,...,5}
%     %    \foreach \dest in {1,...,8}
%     %        \path[opacity=\pathopacity] (I-\source.east) edge[nnedge]  (B1-\dest.west);
            
%     \foreach \source in {1,...,8}
%         \foreach \dest in {1,...,5}
%             \path[opacity=\pathopacity] (F3-\source.east) edge[nnedge] (O-\dest.west);

%     \foreach \source in {1,2,3,5,6,7,8}
%         \foreach \dest in {1,...,8}
%             \path[opacity=0.2] ([yshift=0cm] SF1-\source.east) edge[nnedge] (B2-\dest.west);
%     \foreach \source in {4}
%         \foreach \dest in {1,...,8}
%             \path[opacity=\pathopacity] (F1-\source.east) edge[nnedge] (B2-\dest.west);

%     \foreach \source in {1,...,8}
%         \foreach \dest in {4}
%             \path[opacity=\pathopacity] ([yshift=0cm] SB1-\source.east) edge[nnedge] (F1-\dest.west);
            
%     \foreach \name / \y in {1,3,8}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron] (B2-\name) at (\layersepedges + 1*\layersep,-\vertSepFactory * \y cm) {};
            
%     \foreach \name / \y in {2}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron, color=color2] (B2-\name) at (\layersepedges + 1*\layersep,-\vertSepFactory * \y cm) {};
            
%     \foreach \name / \y in {4,5,6,7}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron, color=color8] (B2-\name) at (\layersepedges + 1*\layersep,-\vertSepFactory * \y cm) {};
            
%     \foreach \source in {1,...,8}
%         \foreach \dest in {1,...,8}
%             \path[opacity=\pathopacity] (B2-\source.east) edge[nnedge] (F2-\dest.west);
            
            
%     \foreach \source in {1,...,8}
%         \foreach \dest in {1,...,8}
%             \path[opacity=\pathopacity] (F2-\source.east) edge[nnedge] (F3-\dest.west);




% 	\begin{scope}[yshift={5.7cm}]

%     \foreach \name / \y in {1,...,5}
%         \node[input neuron, label={[label distance=4mm]90:\ifthenelse{\y=1}{Inputs}{}}] (I-\name) at (0,-\vertSepFactoryI * \y) {};

%     \foreach \name / \y in {2,3,4,7,8}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron] (SB1-\name) at (\layersepedges + 0*\layersep,-\vertSepFactory * \y cm) {};
            
%     \foreach \name / \y in {1,5,6}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron, color=color8] (SB1-\name) at (\layersepedges + 0*\layersep,-\vertSepFactory * \y cm) {};

%     \foreach \name / \y in {1,3,4,6}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron] (SF1-\name) at (\layersepedges + 0*\layersep + \forwardsep,-\vertSepFactory * \y cm) {};
            
%     \foreach \name / \y in {2,5,7,8}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron, color=color8] (SF1-\name) at (\layersepedges + 0*\layersep + \forwardsep,-\vertSepFactory * \y cm) {};

%     \foreach \name / \y in {1,2,3,8}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron] (SB2-\name) at (\layersepedges + 1*\layersep,-\vertSepFactory * \y cm) {};
            
%     \foreach \name / \y in {4,5,6,7}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron, color=color8] (SB2-\name) at (\layersepedges + 1*\layersep,-\vertSepFactory * \y cm) {};

%     \foreach \name / \y in {3,4,5,6,7,8}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron] (SF2-\name) at (\layersepedges + 1*\layersep + \forwardsep,-\vertSepFactory * \y cm) {};
            
%     \foreach \name / \y in {1,2}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron, color=color8] (SF2-\name) at (\layersepedges + 1*\layersep + \forwardsep,-\vertSepFactory * \y cm) {};

%     \foreach \name / \y in {1,2,3,6,8}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron] (SF3-\name) at (\layersepedges + 1*\layersep + 2cm + \forwardsep,-\vertSepFactory * \y cm) {};
            
%     \foreach \name / \y in {4,5,7}
%         \path[yshift=\shiftFactory]
%             node[hidden neuron, color=color8] (SF3-\name) at (\layersepedges + 1*\layersep + 2cm + \forwardsep,-\vertSepFactory * \y cm) {};

%     \foreach \name / \y in {1,...,5}
%     % Draw the output layer node
%         \node[output neuron, label={[label distance=4mm]90:\ifthenelse{\y=1}{Outputs}{}}]
%         (O-\name) at (\layersepedges*2 + 1*\layersep + 2cm + \forwardsep,-\vertSepFactoryI * \y cm) {};
%     \foreach \source in {1,...,5}
%         \foreach \dest in {1,...,8}
%             \path[opacity=\pathopacity] (I-\source.east) edge[nnedge]  (SB1-\dest.west);
            
%     \foreach \source in {1,...,8}
%         \foreach \dest in {1,...,5}
%             \path[opacity=\pathopacity] (SF3-\source.east) edge[nnedge] (O-\dest.west);

%     \foreach \source in {1,...,8}
%         \foreach \dest in {1,...,8}
%             \path[opacity=\pathopacity] (SF1-\source.east) edge[nnedge] (SB2-\dest.west);

%     \foreach \source in {1,...,8}
%         \foreach \dest in {1,...,8}
%             \path[opacity=\pathopacity] (SB1-\source.east) edge[nnedge] (SF1-\dest.west);
            
%     \foreach \source in {1,...,8}
%         \foreach \dest in {1,...,8}
%             \path[opacity=\pathopacity] (SB2-\source.east) edge[nnedge] (SF2-\dest.west);
            
%     \foreach \source in {1,...,8}
%         \foreach \dest in {1,...,8}
%             \path[opacity=\pathopacity] (SF2-\source.east) edge[nnedge] (SF3-\dest.west);


% \end{scope}
% \end{tikzpicture}
% }
% \caption {{\bf Result-Preserving} query illustration. The neurons in \textcolor{color1}{\textbf{orange}}
% are the neurons being checked for redundancy and the neurons in
% \textcolor{color8}{\textbf{light blue}} are
% the ones already removed. Note that the upper copy has the same removed neurons as the lower copy.\label{fig:respres-on-simplified-network}}
% \end{figure}


In the final step of our experiment, we tested our hypothesis that
slicing can lead to the complete linearization of some of the
sub-domains. Indeed, for some of the sub-domains explored, the
simplification pipeline was able to remove \emph{all} neurons,
resulting in a DNN that is effectively a linear transformation. We
noticed, however, a high variability --- for example, in another
sub-domain we were only able to remove 58\% of the neurons.  See
Fig.~\ref{fig:subspace_comp} for additional details. We conclude that
there is an inherent difference between the sub-domains: apparently,
some of them compute simpler transformations than others.

\begin{figure}[htp]
\begin{center}
\includegraphics[scale=0.5]{figures/space_linear}
\includegraphics[scale=0.5]{figures/space_complex}
\includegraphics[scale=0.6]{figures/legend}
\end{center}
\caption {An ``almost'' linear sub-domain (left) vs. a complex sub-domain (right).}
\label{fig:subspace_comp}
\end{figure}

\section{Related Work}
\label{sec:relatedWork}


The pruning of DNNs in order to reduce their sizes has received
significant attention from the machine learning community in recent
years. The most common approaches are based on heuristically
identifying neurons and edges that seem to contribute little to the
network's output, removing these neurons and edges, and performing
additional training of the
network~\cite{HaMaDa15,IaHaMoAsDaKe16}. Other approaches apply
quantization: by using fewer bits to store the network's weights or
activation functions, the DNN's footprint is
decreased~\cite{HuCoSoElBe17,RaOrReFa16,HuCoSoElBe16}. A common trait
of these approaches is that, while they achieve a significant reduction
in memory, they provide no guarantees about the resemblance of the
smaller network to the original.

The most closely related work to our own is that of Gokulanathan et
al.~\cite{GoFeMaBaKa20}. There, the authors use formal verification to
remove dead neurons from a network, ensuring that the resulting
network is equivalent to the original. Additionally, simulations are
used to reduce the number of verification queries that need to be
dispatched. Our work uses similar principles, but significantly
extends them: we consider additional kinds of redundancy
(phase-redundancy, \kfr{}, and result-preserving redundancy) that
produce equivalent networks, and also relaxed-redundancy which
removes additional neurons by introducing a bounded amount of
imprecision.

Our work uses the Marabou DNN verification engine as a
backend~\cite{Marabou2019,StWuZeJuKaBaKo20,KaBaDiJuKo17Fvav,CaKaBaDi17,
GoKaPaBa18,ElKaKaSc21,AmScKa21,KaBaKaSc19};
 but any of the many approaches and tools
that have been proposed in recent years could be used as well. These
approaches leverage SMT solvers (e.g.,~\cite{HuKwWaWu17}), based on LP
and MILP solvers (e.g.,~\cite{LoMa17,Eh17,TjXiTe17, BuTuToKoMu18}),
the propagation of symbolic intervals and abstract interpretation
(e.g.,~\cite{WaPeWhYaJa18, GeMiDrTsChVe18, WeZhChSoHsBoDhDa18,
  TrBkJo20}), abstraction-refinement techniques (e.g.,~\cite{ElGoKa20,
  AsHaKrMu20}), and many others. Recent work has extended beyond
answering yes/no questions about DNNs, targeting tasks such as
automated DNN repair~\cite{KoLoJaBl20,GoAdKeKa20} and quantitative
verification~\cite{BaShShiMeSa19}.  Verification approaches have also
been proposed for recurrent networks~\cite{ZhShGuGuLeNa20,JaBaKa20},
which could potentially also be simplified.  As DNN verification
technology improves, the scalability of our approach will also
increase.


\section{Conclusion and Future Work}
\label{sec:conclusion}
Neural networks often suffer from a high degree of redundancy, which
affects evaluation time, memory footprint and verification costs. In
this paper we presented a novel technique to identify and remove such
redundancy. Our framework is customizable, allowing users to safely
trade network precision for size reduction, while maintaining the
introduced imprecision within a prescribed bound.

In the future, we plan to extend our work along multiple
axes. Specifically, we plan to research more intelligent techniques
for input domain slicing than coordinate-splitting; and also
compositional techniques that would allow us to split the network into
several sub-networks, identify redundancies in each of them, and then
re-combine the pruned network into a single network that is smaller
than the original. In addition, we plan to explore ways of combining
our pruning techniques with techniques from the related field of 
Boolean circuit simplification~\cite{ChMaCh96}.

\medskip
\noindent
\textbf{Acknowledgements.} We thank Ittai Rubinstein and Haoze Wu for
their contributions to this project.  The project was partially
supported by the Israel Science Foundation (grant number 683/18) and
the Binational Science Foundation (grant number 2017662).

{
\bibliographystyle{abbrv}
\bibliography{redundancy}
}




\newpage
\onecolumn




\begin{appendices}

\section{$\errorMinimizerFunction{}(x)$ Function Formula Proof}
\label{appendix:relax_analysis}

\subsection{Introduction}

In Section \ref{sec:linear_funcs} we introduced $\errorMinimizerFunction{}(x)$:
\[
  \errorMinimizerFunction{}(x) = \frac{u}{u - l} \cdot x
  + \frac{-l\cdot u}{2(u - l)}.
\]

\subsection{Proof}

\begin{figure}[ht]
\begin{center}
\includegraphics[scale=0.7]{figures/relaxed_relax_appendix}
\end{center}
%\vspace{-0.5cm}
\caption {$\errorMinimizerFunction{}(x)$ function illustration with a few relevant points. \label{fig:relaxed_appendix}}
\end{figure}

We would like to find $f(x) = ax+b$ where the maximum error is minimized.
The first observation is that in order $f(x)$ to minimize the maximum error,
it must be below $(l, 0)$ and $(u, u)$, but above $(0, f(0))$ (otherwise,
the maximum error could be trivially improved). See Figure \ref{fig:relaxed_appendix}.

Follows from this observation, that our goal is to minimize the following term:
\[
	max(\{|0 - f(l)|, |0 - f(0)|, |u - f(u)|\})
\]

Re-writing the term with $f(x)$ definition we get:
\[
	max(\{-al-b, b, (1-a)u-b\})
\]

Observe that $a = \frac{u}{u - l}$, $b = \frac{-l\cdot u}{2(u - l)}$ is a minimum: any $\pm\epsilon$ change to $a$ or $b$ will result in the increasement of one of the three terms.

In addition, note that in this case the 3 terms inside the $max$ are equal and thus the maximum error is $b$.

\newpage
\onecolumn

\section{Simultanous Neuron Removal Error Bounds}
\label{appendix:sim_error_bounds}

\subsection{Introduction}

Terminology:

\[forward_i^{(0)} = \mbox{input number \#i}\]
\[backward_j^{(i)} = \mbox{hidden layer j neuron \#i backward value}\]
\[= bias_j^{(i)} + \sum_k w_{j,k}^{(i)} \cdot forward^{(i-1)}_{(k)}\]
\[forward_j^{(i)} = func_j^{(i)}(backward_j^{(j)} )\]
$\mbox{where} func_j^{(i)} = \mbox{ReLU or Identity or Zero}$. Last layer has func = Identity, and every neuron's ReLU can be potentially replaced with Identity or Zero in the amended network. If the variables have \(\overline{overline}\), it means they are variables of the amended network. Assuming a subset (or all) of the neurons are redundant in the input space, e.g., their $func_j^{(i)}$ may have been replaced with one of Identity, Zero or positively-sloped $f(x)$ and still not be far from the original output.

Specifically: If the neuron is to be replaced with Zero we require:
\[ backward_j^{(i)} \leq \epsilon_j^{(i)} \]
If is to be replaced with Identity we require:
\[ -\epsilon_j^{(i)} \leq backward_j^{(i)} \]
And if is to be replaced with any positively-sloped linear $f(x)$ (for example, $\errorMinimizerFunction{}(x)$) we require the maximum error to be $\epsilon_j^{(i)}$ at maximum (in the case of $\errorMinimizerFunction{}(x)$ we will have $\epsilon_j^{(i)} = \frac{-l\cdot u}{2(u - l)}$).
This leads to an error of no more than $\epsilon_j^{(i)}$ on the forward value of this neuron.

We would like to bound the error in the network when we replace all these neurons' $func$s simultaneously.

\subsection{Input layer has no errors}
\noindent
The input layer has no error, and so:
\[ forward_j^{(0)} - 0 \leq \overline{forward}_j^{(0)} \leq forward_j^{(0)} + 0 \]
So we set the lower and upper error bounds of the input layer to zero --- ${\bf err}_j^{(l,0)} = 0$ and ${\bf err}_j^{(u,0)} = 0$.

\subsection{Bounding hidden layer's backward value}
\noindent
Each neuron backward value holds:
\[ \overline{backward}_j^{(i)} = bias_j^{(i)} + \sum_k w_{j,k}^{(i)} \cdot \overline{forward}^{(i-1)}_{k} = \]
\[ bias_j^{(i)} + \sum_{k,w \geq 0} w_{j,k}^{(i)} \cdot \overline{forward}^{(i-1)}_{k} + \sum_{k,w \leq 0} w_{j,k}^{(i)} \cdot \overline{forward}^{(i-1)}_{k} = (*) \]

\noindent
Upper bound for $(*)$:
\[ (*) \leq bias_j^{(i)} + \sum_{k,w \geq 0} w_{j,k}^{(i)} \cdot (forward^{(i-1)}_{j} + {\bf err}_k^{(u,i-1)}) + \sum_{k,w \leq 0} w_{j,k}^{(i)} \cdot (forward^{(i-1)}_{j} - {\bf err}_k^{(l,i-1)}) \]
\[ = bias_j^{(i)} + \sum_k w_{j,k}^{(i)} \cdot forward^{(i-1)}_{j} + \sum_{k,w \geq 0} w_{j,k}^{(i)} \cdot {\bf err}_k^{(u,i-1)} - \sum_{k,w \leq 0} w_{j,k}^{(i)} \cdot {\bf err}_k^{(l,i-1)} \]
\[ = bias_j^{(i)} + \sum_k w_{j,k}^{(i)} \cdot forward^{(i-1)}_{j} + \sum_{k,w \geq 0} w_{j,k}^{(i)} \cdot {\bf err}_k^{(u,i-1)} - \sum_{k,w \leq 0} w_{j,k}^{(i)} \cdot {\bf err}_k^{(l,i-1)} \]
\[ = backward^{(i)}_{j} + \sum_{k,w \geq 0} w_{j,k}^{(i)} \cdot {\bf err}_k^{(u,i-1)} - \sum_{k,w \leq 0} w_{j,k}^{(i)} \cdot {\bf err}_k^{(l,i-1)} \]
\[ = backward^{(i)}_{j} + B^{(i)}_{j} \]

\noindent
Lower bound for $(*)$:
\[ (*) \geq backward^{(i)}_{j} - ( \sum_{k,w \geq 0} w_{j,k}^{(i)} \cdot {\bf err}_k^{(l,i-1)} - \sum_{k,w \leq 0} w_{j,k}^{(i)} \cdot {\bf err}_k^{(u,i-1)} ) = backward^{(i)}_{j} - A^{(i)}_{j} \]

\noindent
Overall we have:
\[ backward^{(i)}_{j} - A^{(i)}_{j} \leq \overline{backward}_j^{(i)} \leq backward^{(i)}_{j} + B^{(i)}_{j} \]

\subsection{Bounding hidden layer's forward value}
\noindent
Split into cases:

\noindent
{\bf 1. Neuron is left unchanged}, e.g. $\overline{func}_j^{(i)} = ReLU$
\[
    \overline{forward}_j^{(i)} = ReLU(\overline{backward}_j^{(j)}) \geq ReLU({backward}_j^{(i)} - A_j^{(i)}) \geq ReLU({backward}_j^{(i)}) - A_j^{(i)}
\]
\[
    = {forward}_j^{(i)} - A_j^{(i)}
\]
\noindent
The last inequality is true because $-A_j^{(i)} \leq 0$. Upper bound:
\[
    \overline{forward}_j^{(i)} = ReLU(\overline{backward}_j^{(j)}) \leq ReLU({backward}_j^{(i)} + B_j^{(i)}) \leq ReLU({backward}_j^{(i)}) + B_j^{(i)}
\]
\[
    = {forward}_j^{(i)} + B_j^{(i)}
\]
\noindent
The last inequality is true because $B_j^{(i)} \geq 0$.

\noindent
So we set:
\[{\bf err}_j^{(l,i)} = A_j^{(i)} \mbox{,\ \ } {\bf err}_j^{(u,i)} = B_j^{(i)} \]

\noindent
{\bf 2. Neuron is replaced with Identity}, e.g. $\overline{func}_j^{(i)} = Identity$

\noindent
We have (from the assumptions):
\[ -\epsilon_j^{(i)} \leq backward_j^{(i)} \]
And we also have the backward value bound:
\[ backward^{(i)}_{j} - A^{(i)}_{j} \leq \overline{backward}_j^{(i)} \leq backward^{(i)}_{j} + B^{(i)}_{j} \]
Combining these:
\[ -\epsilon_j^{(i)} - A^{(i)}_{j} \leq \overline{backward}_j^{(i)} = \overline{forward}_j^{(i)} \leq backward^{(i)}_{j} + B^{(i)}_{j} \]

\noindent
If $backward_j^{(i)} \leq 0$ then we have $forward_j^{(i)} = 0$ and so:
\[ forward_j^{(i)} - (A^{(i)}_{j} + \epsilon_j^{(i)}) \leq \overline{forward}_j^{(i)} \leq backward^{(i)}_{j} + B^{(i)}_{j} \leq forward_j^{(i)} + B^{(i)}_{j} \]
And if $backward_j^{(i)} \geq 0$ then we have $forward_j^{(i)} = backward_j^{(i)}$ and so:
\[ forward^{(i)}_{j} - A^{(i)}_{j} \leq \overline{forward}_j^{(i)} \leq forward^{(i)}_{j} + B^{(i)}_{j} \]
So overall we set:
\[{\bf err}_j^{(l,i)} = A^{(i)}_{j} + \epsilon_j^{(i)} \mbox{,\ \ } {\bf err}_j^{(u,i)} = B_j^{(i)} \]

\noindent
{\bf 3. Neuron is replaced with Zero}, e.g. $\overline{func}_j^{(i)} = Zero$

\noindent
We have (from the assumptions):
\[ backward_j^{(i)} \leq \epsilon_j^{(i)} \]
\[ \Downarrow \]
\[ forward_j^{(i)} \leq \epsilon_j^{(i)} \]
\[ \Downarrow \]
\[ forward_j^{(i)} - \epsilon_j^{(i)} \leq 0 = \overline{forward}_j^{(i)} \leq forward_j^{(i)} + 0 \]

\noindent
So overall we set:
\[{\bf err}_j^{(l,i)} = \epsilon_j^{(i)} \mbox{,\ \ } {\bf err}_j^{(u,i)} = 0 \]

\noindent
{\bf 4. Neuron is replaced with positively sloped $f(x)$}, e.g. $\overline{func}_j^{(i)} = f(x)$

\noindent
We have:
\[ \overline{forward}_j^{(i)} = f(\overline{backward}_j^{(i)}) \]
Combining this with the backward value bound we get (using the fact $f(x)$ is positively sloped):
\[ f(backward^{(i)}_{j} - A^{(i)}_{j}) \leq \overline{forward}_j^{(i)} \leq f(backward^{(i)}_{j} + B^{(i)}_{j}) \]
Using $f(x)$ linearity:
\[ -\epsilon_j^{(i)} - f(A^{(i)}_{j}) \leq f(backward^{(i)}_{j}) - f(A^{(i)}_{j}) \leq \overline{forward}_j^{(i)} \leq f(backward^{(i)}_{j}) + f(B^{(i)}_{j}) \leq \epsilon_j^{(i)} + f(B^{(i)}_{j}) \]
So we set:
\[{\bf err}_j^{(l,i)} = f(A^{(i)}_{j}) + \epsilon_j^{(i)} \mbox{,\ \ } {\bf err}_j^{(u,i)} = f(B_j^{(i)}) + \epsilon_j^{(i)} \]


\subsection{Output Layer}
\noindent
We have:
\[ backward^{(i)}_{j} - A^{(i)}_{j} \leq \overline{backward}_j^{(i)} \leq backward^{(i)}_{j} + B^{(i)}_{j} \]
And:
\[ forward^{(i)}_{j} = backward^{(i)}_{j} \mbox{ , } \overline{forward}^{(i)}_{j} = \overline{backward}^{(i)}_{j} \]
So we set:
\[{\bf err}_j^{(l,i)} = A_j^{(i)} \mbox{,\ \ } {\bf err}_j^{(u,i)} = B_j^{(i)} \]

\end{appendices}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
