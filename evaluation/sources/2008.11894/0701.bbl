\begin{thebibliography}{10}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }
\providecommand{\doi}[1]{https://doi.org/#1}

\bibitem{berthelot2019mixmatch}
Berthelot, D., Carlini, N., Goodfellow, I., Papernot, N., Oliver, A., Raffel,
  C.: Mixmatch: A holistic approach to semi-supervised learning. In: NeurIPS
  (2019)

\bibitem{bossard2014food}
Bossard, L., Guillaumin, M., Van~Gool, L.: Food-101--mining discriminative
  components with random forests. In: ECCV. pp. 446--461. Springer (2014)

\bibitem{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A
  large-scale hierarchical image database. In: CVPR. pp. 248--255 (2009)

\bibitem{dietterich2000ensemble}
Dietterich, T.G.: Ensemble methods in machine learning. In: International
  Workshop on Multiple Classifier Systems. pp. 1--15. Springer (2000)

\bibitem{gal2016uncertainty}
Gal, Y.: Uncertainty in deep learning. University of Cambridge  \textbf{1}, ~3
  (2016)

\bibitem{gal2016dropout}
Gal, Y., Ghahramani, Z.: Dropout as a bayesian approximation: Representing
  model uncertainty in deep learning. In: ICML. pp. 1050--1059 (2016)

\bibitem{guo2017calibration}
Guo, C., Pleiss, G., Sun, Y., Weinberger, K.Q.: On calibration of modern neural
  networks. In: ICML. pp. 1321--1330 (2017)

\bibitem{guo2018curriculumnet}
Guo, S., Huang, W., Zhang, H., Zhuang, C., Dong, D., Scott, M.R., Huang, D.:
  Curriculumnet: Weakly supervised learning from large-scale web images. In:
  ECCV. pp. 135--150. Springer (2018)

\bibitem{han2019deep}
Han, J., Luo, P., Wang, X.: Deep self-learning from noisy labels. In: ICCV. pp.
  5138--5147 (2019)

\bibitem{he2016deep}
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image
  recognition. In: CVPR. pp. 770--778 (2016)

\bibitem{he2019bag}
He, T., Zhang, Z., Zhang, H., Zhang, Z., Xie, J., Li, M.: Bag of tricks for
  image classification with convolutional neural networks. In: CVPR. pp.
  558--567 (2019)

\bibitem{hendrycks2020augmix}
Hendrycks, D., Mu, N., Cubuk, E.D., Zoph, B., Gilmer, J., Lakshminarayanan, B.:
  {AugMix}: A simple data processing method to improve robustness and
  uncertainty. ICLR  (2020)

\bibitem{hinton2015distilling}
Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural
  network. In: NIPS Deep Learning and Representation Learning Workshop (2015)

\bibitem{jiang2018mentornet}
Jiang, L., Zhou, Z., Leung, T., Li, L.J., Fei-Fei, L.: Mentornet: Learning
  data-driven curriculum for very deep neural networks on corrupted labels. In:
  ICML. pp. 2304--2313 (2018)

\bibitem{kipf2016gcn}
Kipf, T.N., Welling, M.: Semi-supervised classification with graph
  convolutional networks. In: ICLR (2017)

\bibitem{lakshminarayanan2017simple}
Lakshminarayanan, B., Pritzel, A., Blundell, C.: Simple and scalable predictive
  uncertainty estimation using deep ensembles. In: NIPS. pp. 6402--6413 (2017)

\bibitem{lee2018cleannet}
Lee, K.H., He, X., Zhang, L., Yang, L.: Cleannet: Transfer learning for
  scalable image classifier training with label noise. In: CVPR. pp. 5447--5456
  (2018)

\bibitem{li2019product}
Li, Q., Peng, X., Cao, L., Du, W., Xing, H., Qiao, Y.: Product image
  recognition with guidance learning and noisy supervision. arXiv preprint
  arXiv:1907.11384  (2019)

\bibitem{li2017WebVision}
Li, W., Wang, L., Li, W., Agustsson, E., Van~Gool, L.: Webvision database:
  Visual learning and understanding from web data. arXiv preprint
  arXiv:1708.02862  (2017)

\bibitem{mahajan2018exploring}
Mahajan, D., Girshick, R., Ramanathan, V., He, K., Paluri, M., Li, Y.,
  Bharambe, A., van~der Maaten, L.: Exploring the limits of weakly supervised
  pretraining. In: ECCV. pp. 181--196. Springer (2018)

\bibitem{muller2019does}
M{\"u}ller, R., Kornblith, S., Hinton, G.E.: When does label smoothing help?
  In: NeurIPS. pp. 4694--4703 (2019)

\bibitem{ovadia2019can}
Ovadia, Y., Fertig, E., Ren, J., Nado, Z., Sculley, D., Nowozin, S., Dillon,
  J., Lakshminarayanan, B., Snoek, J.: Can you trust your model's uncertainty?
  evaluating predictive uncertainty under dataset shift. In: NeurIPS. pp.
  13991--14002 (2019)

\bibitem{patrini2017making}
Patrini, G., Rozza, A., Krishna~Menon, A., Nock, R., Qu, L.: Making deep neural
  networks robust to label noise: A loss correction approach. In: CVPR. pp.
  1944--1952 (2017)

\bibitem{pereyra2017regularizing}
Pereyra, G., Tucker, G., Chorowski, J., Kaiser, {\L}., Hinton, G.: Regularizing
  neural networks by penalizing confident output distributions. arXiv preprint
  arXiv:1701.06548  (2017)

\bibitem{radosavovic2018data}
Radosavovic, I., Doll{\'a}r, P., Girshick, R., Gkioxari, G., He, K.: Data
  distillation: Towards omni-supervised learning. In: CVPR. pp. 4119--4128
  (2018)

\bibitem{reed2014bootstrap}
Reed, S., Lee, H., Anguelov, D., Szegedy, C., Erhan, D., Rabinovich, A.:
  Training deep neural networks on noisy labels with bootstrapping. In: ICLR
  (2015)

\bibitem{shah2019inferring}
Shah, M., Viswanathan, K., Lu, C.T., Fuxman, A., Li, Z., Timofeev, A., Jia, C.,
  Sun, C.: Inferring context from pixels for multimodal image classification.
  In: CIKM. pp. 189--198. ACM (2019)

\bibitem{snell2017prototypical}
Snell, J., Swersky, K., Zemel, R.: Prototypical networks for few-shot learning.
  In: NIPS. pp. 4077--4087 (2017)

\bibitem{sun2017revisiting}
Sun, C., Shrivastava, A., Singh, S., Gupta, A.: Revisiting unreasonable
  effectiveness of data in deep learning era. In: ICCV. pp. 843--852 (2017)

\bibitem{sun2014deep}
Sun, Y., Wang, X., Tang, X.: Deep learning face representation from predicting
  10,000 classes. In: CVPR. pp. 1891--1898 (2014)

\bibitem{szegedy2016rethinking}
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.: Rethinking the
  inception architecture for computer vision. In: CVPR. pp. 2818--2826 (2016)

\bibitem{tanaka2018joint}
Tanaka, D., Ikami, D., Yamasaki, T., Aizawa, K.: Joint optimization framework
  for learning with noisy labels. In: CVPR. pp. 5552--5560 (2018)

\bibitem{thulasidasan2019mixup}
Thulasidasan, S., Chennupati, G., Bilmes, J.A., Bhattacharya, T., Michalak, S.:
  On mixup training: Improved calibration and predictive uncertainty for deep
  neural networks. In: NeurIPS. pp. 13888--13899 (2019)

\bibitem{tu2020learning}
Tu, Y., Niu, L., Chen, J., Cheng, D., Zhang, L.: Learning from web data with
  self-organizing memory module. In: CVPR. pp. 12846--12855 (2020)

\bibitem{xia2019anchor}
Xia, X., Liu, T., Wang, N., Han, B., Gong, C., Niu, G., Sugiyama, M.: Are
  anchor points really indispensable in label-noise learning? In: NeurIPS. pp.
  6838--6849 (2019)

\bibitem{xie2019unsupervised}
Xie, Q., Dai, Z., Hovy, E., Luong, M.T., Le, Q.V.: Unsupervised data
  augmentation for consistency training. arXiv preprint arXiv:1904.12848
  (2019)

\bibitem{yalniz2019billion}
Yalniz, I.Z., J{\'e}gou, H., Chen, K., Paluri, M., Mahajan, D.: Billion-scale
  semi-supervised learning for image classification. arXiv preprint
  arXiv:1905.00546  (2019)

\bibitem{yu2018efficient}
Yu, X., Liu, T., Gong, M., Batmanghelich, K., Tao, D.: An efficient and
  provable approach for mixture proportion estimation using linear independence
  assumption. In: CVPR. pp. 4480--4489 (2018)

\bibitem{zhang2018mixup}
Zhang, H., Cisse, M., Dauphin, Y.N., Lopez-Paz, D.: mixup: Beyond empirical
  risk minimization. ICLR  (2018)

\bibitem{zhang2019metacleaner}
Zhang, W., Wang, Y., Qiao, Y.: Metacleaner: Learning to hallucinate clean
  representations for noisy-labeled visual recognition. In: CVPR. pp.
  7373--7382 (2019)

\bibitem{zhou2017places}
Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., Torralba, A.: Places: A 10
  million image database for scene recognition. TPAMI  (2017)

\bibitem{zhu2005semi}
Zhu, X.J.: Semi-supervised learning literature survey. Tech. rep., University
  of Wisconsin-Madison Department of Computer Sciences (2005)

\end{thebibliography}
