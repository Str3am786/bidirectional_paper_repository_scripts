\begin{thebibliography}{36}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,
  Ghemawat, Irving, Isard et~al.}]{abadi2016tensorflow}
Mart{\'\i}n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
  Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et~al.
  2016.
\newblock Tensorflow: A system for large-scale machine learning.
\newblock In \emph{12th USENIX symposium on operating systems design and
  implementation (OSDI 16)}, pages 265--283.

\bibitem[{Al-Rfou et~al.(2016)Al-Rfou, Pickett, Snaider, Sung, Strope, and
  Kurzweil}]{reddit}
Rami Al-Rfou, Marc Pickett, Javier Snaider, Yun-hsuan Sung, Brian Strope, and
  Ray Kurzweil. 2016.
\newblock Conversational contextual cues: The case of personalization and
  history for response ranking.
\newblock \emph{arXiv e-prints}, pages arXiv--1606.

\bibitem[{Bowman et~al.(2015)Bowman, Angeli, Potts, and Manning}]{snli}
Samuel Bowman, Gabor Angeli, Christopher Potts, and Christopher~D Manning.
  2015.
\newblock A large annotated corpus for learning natural language inference.
\newblock In \emph{Proceedings of the 2015 Conference on Empirical Methods in
  Natural Language Processing}, pages 632--642.

\bibitem[{Cer et~al.(2017)Cer, Diab, Agirre, Lopez-Gazpio, and Specia}]{sts_b}
Daniel Cer, Mona Diab, Eneko Agirre, I{\~n}igo Lopez-Gazpio, and Lucia Specia.
  2017.
\newblock Semeval-2017 task 1: Semantic textual similarity multilingual and
  crosslingual focused evaluation.
\newblock In \emph{Proceedings of the 11th International Workshop on Semantic
  Evaluation (SemEval-2017)}, pages 1--14.

\bibitem[{Cer et~al.(2018)Cer, Yang, Kong, Hua, Limtiaco, John, Constant,
  Guajardo-Cespedes, Yuan, Tar et~al.}]{use}
Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni~St
  John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, et~al.
  2018.
\newblock Universal sentence encoder for english.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pages 169--174.

\bibitem[{Chen et~al.(2020)Chen, Liu, Shen, Yuan, Zhou, Wu, He, and
  Zhou}]{jddc}
Meng Chen, Ruixue Liu, Lei Shen, Shaozu Yuan, Jingyan Zhou, Youzheng Wu,
  Xiaodong He, and Bowen Zhou. 2020.
\newblock The jddc corpus: A large-scale multi-turn chinese dialogue dataset
  for e-commerce customer service.
\newblock In \emph{Proceedings of The 12th Language Resources and Evaluation
  Conference}, pages 459--466.

\bibitem[{Conneau and Kiela(2018)}]{conneau2018senteval}
Alexis Conneau and Douwe Kiela. 2018.
\newblock Senteval: An evaluation toolkit for universal sentence
  representations.
\newblock \emph{arXiv preprint arXiv:1803.05449}.

\bibitem[{Conneau et~al.(2017)Conneau, Kiela, Schwenk, Barrault, and
  Bordes}]{infer_sent}
Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo{\"\i}c Barrault, and Antoine
  Bordes. 2017.
\newblock Supervised learning of universal sentence representations from
  natural language inference data.
\newblock In \emph{Proceedings of the 2017 Conference on Empirical Methods in
  Natural Language Processing}, pages 670--680.

\bibitem[{Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova}]{bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186.

\bibitem[{Gao et~al.(2021)Gao, Yao, and Chen}]{gao2021simcse}
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
\newblock Simcse: Simple contrastive learning of sentence embeddings.
\newblock \emph{arXiv preprint arXiv:2104.08821}.

\bibitem[{Giorgi et~al.(2020)Giorgi, Nitski, Bader, and
  Wang}]{giorgi2020declutr}
John~M Giorgi, Osvald Nitski, Gary~D Bader, and Bo~Wang. 2020.
\newblock Declutr: Deep contrastive learning for unsupervised textual
  representations.
\newblock \emph{arXiv preprint arXiv:2006.03659}.

\bibitem[{Gururangan et~al.(2020)Gururangan, Marasovi{\'c}, Swayamdipta, Lo,
  Beltagy, Downey, and Smith}]{bert_dapt}
Suchin Gururangan, Ana Marasovi{\'c}, Swabha Swayamdipta, Kyle Lo, Iz~Beltagy,
  Doug Downey, and Noah~A Smith. 2020.
\newblock Don’t stop pretraining: Adapt language models to domains and tasks.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 8342--8360.

\bibitem[{Henderson et~al.(2017)Henderson, Al-Rfou, Strope, Sung, Luk{\'a}cs,
  Guo, Kumar, Miklos, and Kurzweil}]{henderson2017efficient}
Matthew Henderson, Rami Al-Rfou, Brian Strope, Yun-Hsuan Sung, L{\'a}szl{\'o}
  Luk{\'a}cs, Ruiqi Guo, Sanjiv Kumar, Balint Miklos, and Ray Kurzweil. 2017.
\newblock Efficient natural language response suggestion for smart reply.
\newblock \emph{arXiv preprint arXiv:1705.00652}.

\bibitem[{Henderson et~al.(2020)Henderson, Casanueva, Mrk{\v{s}}i{\'c}, Su,
  Wen, and Vuli{\'c}}]{convert}
Matthew Henderson, I{\~n}igo Casanueva, Nikola Mrk{\v{s}}i{\'c}, Pei-Hao Su,
  Tsung-Hsien Wen, and Ivan Vuli{\'c}. 2020.
\newblock Convert: Efficient and accurate conversational representations from
  transformers.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: Findings}, pages 2161--2174.

\bibitem[{Hill et~al.(2016)Hill, Cho, and Korhonen}]{fast_sent}
Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.
\newblock Learning distributed representations of sentences from unlabelled
  data.
\newblock In \emph{Proceedings of the 2016 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 1367--1377.

\bibitem[{Kiros et~al.(2015)Kiros, Zhu, Salakhutdinov, Zemel, Urtasun,
  Torralba, and Fidler}]{skip_thought}
Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard~S Zemel, Raquel Urtasun,
  Antonio Torralba, and Sanja Fidler. 2015.
\newblock Skip-thought vectors.
\newblock In \emph{Advances in neural information processing systems.}, page
  3294––3302.

\bibitem[{Li et~al.(2020)Li, Zhou, He, Wang, Yang, and Li}]{bert_flow}
Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, and Lei Li. 2020.
\newblock On the sentence embeddings from pre-trained language models.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 9119--9130.

\bibitem[{Li et~al.(2018)Li, Wang, Sun, Panda, Liu, and Gao}]{mdc}
Xiujun Li, Yu~Wang, Siqi Sun, Sarah Panda, Jingjing Liu, and Jianfeng Gao.
  2018.
\newblock Microsoft dialogue challenge: Building end-to-end task-completion
  dialogue systems.
\newblock \emph{arXiv e-prints}, pages arXiv--1807.

\bibitem[{Logeswaran and Lee(2018)}]{quick_thought}
Lajanugen Logeswaran and Honglak Lee. 2018.
\newblock An efficient framework for learning sentence representations.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Marelli et~al.(2014)Marelli, Menini, Baroni, Bentivogli, Bernardi,
  and Zamparelli}]{sick_r}
Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella
  Bernardi, and Roberto Zamparelli. 2014.
\newblock A sick cure for the evaluation of compositional distributional
  semantic models.
\newblock In \emph{Proceedings of the Ninth International Conference on
  Language Resources and Evaluation (LREC'14)}, pages 216--223.

\bibitem[{Meng et~al.(2021)Meng, Xiong, Bajaj, Tiwary, Bennett, Han, and
  Song}]{meng2021coco}
Yu~Meng, Chenyan Xiong, Payal Bajaj, Saurabh Tiwary, Paul Bennett, Jiawei Han,
  and Xia Song. 2021.
\newblock Coco-lm: Correcting and contrasting text sequences for language model
  pretraining.
\newblock \emph{arXiv preprint arXiv:2102.08473}.

\bibitem[{Nair and Hinton(2010)}]{relu}
Vinod Nair and Geoffrey~E Hinton. 2010.
\newblock Rectified linear units improve restricted boltzmann machines.
\newblock In \emph{Proceedings of the 27th International Conference on
  International Conference on Machine Learning}, pages 807--814.

\bibitem[{Oord et~al.(2018)Oord, Li, and Vinyals}]{oord2018representation}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals. 2018.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{arXiv preprint arXiv:1807.03748}.

\bibitem[{Pennington et~al.(2014)Pennington, Socher, and
  Manning}]{pennington2014glove}
Jeffrey Pennington, Richard Socher, and Christopher~D Manning. 2014.
\newblock Glove: Global vectors for word representation.
\newblock In \emph{Proceedings of the 2014 conference on empirical methods in
  natural language processing (EMNLP)}, pages 1532--1543.

\bibitem[{Radford et~al.()Radford, Wu, Child, Luan, Amodei, and
  Sutskever}]{gpt_2}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.

\bibitem[{Reimers and Gurevych(2019)}]{sent_bert}
Nils Reimers and Iryna Gurevych. 2019.
\newblock Sentence-bert: Sentence embeddings using siamese bert-networks.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 3973--3983.

\bibitem[{Robertson and Zaragoza(2009)}]{bm25}
Stephen Robertson and Hugo Zaragoza. 2009.
\newblock The probabilistic relevance framework: Bm25 and beyond.
\newblock \emph{Information Retrieval}, 3(4):333--389.

\bibitem[{Su et~al.(2021)Su, Cao, Liu, and Ou}]{bert_whitening}
Jianlin Su, Jiarun Cao, Weijie Liu, and Yangyiwen Ou. 2021.
\newblock Whitening sentence representations for better semantics and faster
  retrieval.
\newblock \emph{arXiv preprint arXiv:2103.15316}.

\bibitem[{Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and
  Bowman}]{wang2018glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R
  Bowman. 2018.
\newblock Glue: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock \emph{arXiv preprint arXiv:1804.07461}.

\bibitem[{Williams et~al.(2018)Williams, Nangia, and Bowman}]{mnli}
Adina Williams, Nikita Nangia, and Samuel Bowman. 2018.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock In \emph{Proceedings of the 2018 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long Papers)}, pages 1112--1122.

\bibitem[{Wu et~al.(2020)Wu, Wang, Gu, Khabsa, Sun, and Ma}]{wu2020clear}
Zhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa, Fei Sun, and Hao Ma. 2020.
\newblock Clear: Contrastive learning for sentence representation.
\newblock \emph{arXiv preprint arXiv:2012.15466}.

\bibitem[{Yan et~al.(2021)Yan, Li, Wang, Zhang, Wu, and Xu}]{yan2021consert}
Yuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng Zhang, Wei Wu, and Weiran Xu. 2021.
\newblock Consert: A contrastive framework for self-supervised sentence
  representation transfer.
\newblock \emph{arXiv preprint arXiv:2105.11741}.

\bibitem[{Yang et~al.(2018)Yang, Yuan, Cer, Kong, Constant, Pilar, Ge, Sung,
  Strope, and Kurzweil}]{co_dan}
Yinfei Yang, Steve Yuan, Daniel Cer, Sheng-yi Kong, Noah Constant, Petr Pilar,
  Heming Ge, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. 2018.
\newblock Learning semantic textual similarity from conversations.
\newblock In \emph{Proceedings of The Third Workshop on Representation Learning
  for NLP}, pages 164--174.

\bibitem[{Zhang et~al.(2020)Zhang, He, Liu, Lim, and Bing}]{is_bert}
Yan Zhang, Ruidan He, Zuozhu Liu, Kwan~Hui Lim, and Lidong Bing. 2020.
\newblock An unsupervised sentence embedding method by mutual information
  maximization.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 1601--1610.

\bibitem[{Zhang et~al.(2018)Zhang, Li, Zhu, Zhao, and Liu}]{edc}
Zhuosheng Zhang, Jiangtong Li, Pengfei Zhu, Hai Zhao, and Gongshen Liu. 2018.
\newblock Modeling multi-turn conversation with deep utterance aggregation.
\newblock In \emph{Proceedings of the 27th International Conference on
  Computational Linguistics}, pages 3740--3752.

\bibitem[{Zhu et~al.(2015)Zhu, Kiros, Zemel, Salakhutdinov, Urtasun, Torralba,
  and Fidler}]{toronto}
Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun,
  Antonio Torralba, and Sanja Fidler. 2015.
\newblock Aligning books and movies: Towards story-like visual explanations by
  watching movies and reading books.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 19--27.

\end{thebibliography}
