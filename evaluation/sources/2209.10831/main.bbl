\begin{thebibliography}{14}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Schapire et~al.(1998)Schapire, Freund, Bartlett, and
  Lee]{schapire+:as98}
Robert~E. Schapire, Yoav Freund, Peter Bartlett, and Wen~Sun Lee.
\newblock {Boosting the margin: a new explanation for the effectiveness of
  voting methods}.
\newblock \emph{The Annals of Statistics}, 26\penalty0 (5):\penalty0
  1651--1686, 1998.

\bibitem[Mohri et~al.(2018)Mohri, Rostamizadeh, and
  Talwalker]{mohri+:mitpress18}
Mehryar Mohri, Afshin Rostamizadeh, and Amee Talwalker.
\newblock \emph{{Foundation of Machine Learning}}.
\newblock The MIT Press, second edition, 2018.

\bibitem[Demiriz et~al.(2002)Demiriz, Bennett, and Shawe-Taylor]{demiriz+:ml02}
A~Demiriz, K~P Bennett, and J~Shawe-Taylor.
\newblock {Linear Programming Boosting via Column Generation}.
\newblock \emph{Machine Learning}, 46\penalty0 (1-3):\penalty0 225--254, 2002.

\bibitem[Warmuth et~al.(2007)Warmuth, Glocer, and
  R{\"{a}}tsch]{warmuth+:nips07}
M~Warmuth, K~Glocer, and G~R{\"{a}}tsch.
\newblock {Boosting Algorithms for Maximizing the Soft Margin}.
\newblock In \emph{Advances in Neural Information Processing Systems 20 (NIPS
  2007)}, pages 1585--1592, 2007.

\bibitem[Shalev{-}Shwartz and Singer(2010)]{shalev-shwartz+:jml10}
Shai Shalev{-}Shwartz and Yoram Singer.
\newblock On the equivalence of weak learnability and linear separability: new
  relaxations and efficient boosting algorithms.
\newblock \emph{Mach. Learn.}, 80\penalty0 (2-3), 2010.

\bibitem[Warmuth et~al.(2008)Warmuth, Glocer, and Vishwanathan]{warmuth+:alt08}
Manfred~K. Warmuth, Karen~A. Glocer, and S.~V.~N. Vishwanathan.
\newblock Entropy regularized lpboost.
\newblock In \emph{Algorithmic Learning Theory, 19th International Conference,
  {ALT} 2008, Budapest, Hungary, October 13-16, 2008. Proceedings}, volume 5254
  of \emph{Lecture Notes in Computer Science}, pages 256--271. Springer, 2008.

\bibitem[Borwein and Lewis(2006)]{borwein+:springer06}
Jonathan~M. Borwein and Adrian~S. Lewis.
\newblock \emph{Convex Analysis}, pages 65--96.
\newblock Springer New York, 2006.

\bibitem[Frank and Wolfe(1956)]{marguerite+:nrl56}
Marguerite Frank and Philip Wolfe.
\newblock An algorithm for quadratic programming.
\newblock \emph{Naval Research Logistics Quarterly}, 3\penalty0
  (1‐2):\penalty0 95--110, 1956.

\bibitem[Jaggi(2013)]{jaggi:icml13}
Martin Jaggi.
\newblock Revisiting frank-wolfe: Projection-free sparse convex optimization.
\newblock In \emph{Proceedings of the 30th International Conference on Machine
  Learning, {ICML} 2013}, volume~28 of \emph{{JMLR} Workshop and Conference
  Proceedings}, pages 427--435. JMLR.org, 2013.

\bibitem[Wang et~al.(2015)Wang, Wang, Weinan, and Schapire]{wang+:arxiv15}
Chu Wang, Yingfei Wang, E~Weinan, and Robert~E. Schapire.
\newblock Functional frank-wolfe boosting for general loss functions.
\newblock \emph{ArXiv}, abs/1510.02558, 2015.

\bibitem[Herbster and Warmuth(2001)]{herbster+:jmlr01}
Mark Herbster and Manfred~K. Warmuth.
\newblock Tracking the best linear predictor.
\newblock \emph{J. Mach. Learn. Res.}, 1:\penalty0 281–309, sep 2001.

\bibitem[Lacoste{-}Julien and Jaggi(2015)]{lacoste-julien+:nips15}
Simon Lacoste{-}Julien and Martin Jaggi.
\newblock On the global linear convergence of frank-wolfe optimization
  variants.
\newblock In \emph{Advances in Neural Information Processing Systems 28: Annual
  Conference on Neural Information Processing Systems 2015, December 7-12,
  2015, Montreal, Quebec, Canada}, pages 496--504, 2015.

\bibitem[Chen and Guestrin(2016)]{tianqi+:kdd16}
Tianqi Chen and Carlos Guestrin.
\newblock Xgboost: A scalable tree boosting system.
\newblock In \emph{Proceedings of the 22nd ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, KDD '16, page 785–794. Association
  for Computing Machinery, 2016.

\bibitem[Ke et~al.(2017)Ke, Meng, Finley, Wang, Chen, Ma, Ye, and
  Liu]{ke+:nips17}
Guolin Ke, Qi~Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei
  Ye, and Tie{-}Yan Liu.
\newblock Lightgbm: {A} highly efficient gradient boosting decision tree.
\newblock In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna~M. Wallach,
  Rob Fergus, S.~V.~N. Vishwanathan, and Roman Garnett, editors, \emph{Advances
  in Neural Information Processing Systems 30: Annual Conference on Neural
  Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA,
  {USA}}, pages 3146--3154, 2017.

\end{thebibliography}
