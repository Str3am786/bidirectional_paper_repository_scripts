\section{Technical lemmas and proofs}
\label{appendix:technical_lem.}
The following lemma shows the maximum value of the relative entropy 
from the uniform distribution 
over the capped probability simplex $\psimplex{m}_{\nu}$.
\begin{lem.}
    \label{lem:relative_entropy_bound}
    $
        \max_{\bm{d} \in \psimplex{m}_{\nu}}
        \Delta(\bm{d}) \leq \ln \frac{m}{\nu}
    $.
\end{lem.}
\begin{proof}
    Since the relative entropy from the uniform distribution 
    achieves its maximal value 
    at the extreme points of $\psimplex{m}_{\nu}$, 
    a maximizer has the form 
    \begin{align}
        \nonumber
        \bm{d} = (
            \underbrace{1/\nu, 1/\nu, \dots, 1/\nu}_{k \text{ elements}},
            s,
            0, 0, \dots, 0
        ),
        \quad s = 1 - \frac{k}{\nu} \leq \frac 1 \nu
    \end{align}
    for some $k \in [m]$. 
    Plugging the maximizer into $\Delta(\bm{d})$, 
    we can write the objective function as 
    $\Delta(\bm{d}) = (k/\nu) \ln (m/\nu) + s \ln (sm)$. 
    If $s = 0$, $\Delta(\bm{d}) = \ln (m/\nu)$ holds since $k = \nu$. 
    If $s > 0$, $k / \nu < 1$ so that 
    \begin{align*}
        \Delta(\bm{d}) \leq \frac k \nu \ln \frac m \nu
            + \left(1 - \frac k \nu\right) \ln \frac m \nu
            = \ln \frac m \nu.
    \end{align*}
\end{proof}
Therefore, by setting $\eta = \frac{2}{\epsilon} \ln \frac{m}{\nu}$, 
the entropy term does not exceed $\frac{\epsilon}{2}$. 

The following lemma shows the dual problem of the edge minimization. 
\begin{lem.}
    \label{lem:dual_problem}
    Let $f : \mathbb R^m \to \{0, +\infty\}$, 
    $g : \mathbb R^n \to \mathbb R$ be 
    functions defined as 
    \begin{align}
        \nonumber
        f(\bm{d}) =
        \begin{cases}
                  0 & \bm{d} \in    \psimplex{m}_{\nu} \\
            +\infty & \bm{d} \notin \psimplex{m}_{\nu}
        \end{cases},
        \qquad \qquad \qquad
        g(\bm{\theta}) = \max_{j \in [n]} \theta_j.
    \end{align}
    Then, the dual problem of edge minimization 
    % $\min_{\bm{d}} f(\bm{d}) + g(A^\top \bm{d})$ is
    % $\max_{\bm{w} \in \Delta_n} - f^\star(A\bm{w})$.
    \begin{align}
        \label{eq:appendix_edge_minimization}
        \min_{\bm{d}} f(\bm{d}) + g(A^\top \bm{d})
    \end{align}
    is the soft margin maximization 
    \begin{align}
        \label{eq:appendix_smm}
        \max_{\bm{w} \in \psimplex{n}} - f^\star( -A \bm{w} ).
    \end{align}
    Further, the strong duality holds. 
\end{lem.}
\begin{proof}
    We can use Theorem~\ref{thm:strong_duality} to derive the dual problem. 
    Since 
    \begin{align*}
        \bm{g}^\star(\bm{w}) = 
        \begin{cases}
                  0 & \bm{w} \in    \psimplex{n} \\
            +\infty & \bm{w} \notin \psimplex{n}
        \end{cases},
    \end{align*}
    one can verify the dual form is given as~(\ref{eq:appendix_smm}). 
    To prove the strong duality, 
    it is enough to prove 
    $
        \bm{0} \in \interior\left(
            \domain g - A^\top \domain f
        \right)
    $. 
    By definition, $\domain g = \mathbb R^n$ and 
    $\domain f = \psimplex{m}_{\nu}$ and hence 
    \begin{align*}
        \domain g - A^\top \domain f
        = \left\{
            \bm{w} - A^\top \bm{d} \mid
            \bm{w} \in \mathbb{R}^n, \bm{d} \in \psimplex{m}_{\nu}
        \right\}.
    \end{align*}
    Obviously, $\bm{0} \in \interior (\domain g - A^\top \domain f)$ 
    and thus the strong duality holds. 
\end{proof}
Since 
\begin{align*}
    f^\star(-A\bm{w}) 
    = \sup_{\bm{d}} \left[ - \bm{d}^\top A \bm{w} - f(\bm{d}) \right]
    = \max_{\bm{d} \in \psimplex{m}_{\nu}} - \bm{d}^\top A \bm{w}
    = \min_{\bm{d} \in \psimplex{m}_{\nu}} \bm{d}^\top A \bm{w},
\end{align*}
we can write the dual problem~(\ref{eq:appendix_smm}) explicitly:
\begin{align*}
    \max_{\bm{w} \in \psimplex{n}} - f^\star ( -A \bm{w} )
        = - \min_{\bm{w} \in \psimplex{n}}
            \max_{\bm{d} \in \psimplex{m}_{\nu}}
        \bm{d}^\top A \bm{w}.
\end{align*}
% By the same derivation, we get the dual problem 
% for the regularized edge minimization problem. 
We get the dual problem for the regularized edge minimization problem 
by a similar derivation. 
\begin{cor.}
    \label{lem:smoothed_dual_problem}
    Let $f, g$ be the functions defined in Lemma~\ref{lem:dual_problem} and 
    let $\Delta(\bm{d}) = \sum_{i=1}^m d_i \ln d_i + \ln(m)$ be 
    the relative entropy function from the uniform distribution. 
    Define $\tilde{f} = f + (1/\eta) \Delta$ for some $\eta > 0$. 
    Then, the dual problem of 
    \begin{align}
        \nonumber
        \min_{\bm{d}} \tilde{f}(\bm{d}) + g(A^\top \bm{d})
        \qquad \text{ is } \qquad
        \max_{\bm{w} \in \psimplex{n}} - \tilde{f}^\star(-A \bm{w}).
    \end{align}
\end{cor.}

\subsection{Proof of Lemma~\ref{lem:fenchel_conjugate_functions}}
\begin{proof}
    By the definition of Fenchel conjugate, 
    \begin{align*}
        \tilde{f}^\star(\bm{\theta})
        & = \sup_{\bm{d}} \left\{
            \bm{d}^\top \bm{\theta} - \tilde{f}(\bm{d})
        \right\}
        \geq \sup_{\bm{d}} \left\{
            \bm{d}^\top \bm{\theta} - f(\bm{d}) - c
        \right\}
        = f^\star(\bm{\theta}) - c, \\
        \tilde{f}^\star(\bm{\theta})
        & = \sup_{\bm{d}} \left\{
            \bm{d}^\top \bm{\theta} - \tilde{f}(\bm{d})
        \right\}
        \leq \sup_{\bm{d}} \left\{
            \bm{d}^\top \bm{\theta} - f(\bm{d})
        \right\}
        = f^\star(\bm{\theta}).
    \end{align*}
\end{proof}
By Lemma~\ref{lem:relative_entropy_bound} 
and~\ref{lem:fenchel_conjugate_functions}, 
we get 
$
    \tilde{f}^\star (-A\bm{w}) - \frac{\epsilon}{2}
    \leq f(-A\bm{w}) \leq \tilde{f}^\star(-A\bm{w})
$ for all $-A \bm{w}$ 
if $\eta \geq \frac{2}{\epsilon} \ln \frac{m}{\nu}$. 


\subsection{
    Proof of Theorem~\ref{thm:mlpboost_convergence_guarantee}
    for the short-step FW rule~\ref{alg:ss_rule}
}
Recall that in the proof of Theorem~\ref{thm:mlpboost_convergence_guarantee}, 
we showed the inequality 
$\epsilon_t - \epsilon_{t+1} \geq \frac{1}{8 \eta} \epsilon_t^2$ 
for the short-step case. 
We prove $\epsilon_t \leq \frac{8\eta}{t + 2}$ by induction on $t$. 
For the base case, $t = 1$, by Lemma~\ref{lem:fenchel_conjugate_functions}, 
\begin{align*}
    \epsilon_1
    = \min_{\tau \in \{0, 1\}} (\bm{d}_\tau^\top A)_{j_{\tau + 1}}
        + \tilde{f}^\star (- A\bm{w}_1)
    \leq 1 + f^\star (- A\bm{w}_1)
    \leq 2
    \leq \frac{8\eta}{1 + 2}.
\end{align*}
For the inductive case, assume that $\epsilon_t \leq \frac{8\eta}{t+2}$ 
for $t \geq 1$. 
By the inequality
$\epsilon_t - \epsilon_{t+1} \geq \frac{1}{8 \eta} \epsilon_t^2$, 
we have
\begin{align}
    \label{eq:appendix_shortstep_01}
    \epsilon_{t+1} 
    \leq \left(1 - \frac{1}{8\eta} \epsilon_t\right) \epsilon_t.
\end{align}
By simple calculation, 
one can see that the maximizer $\epsilon$ of the RHS over $\mathbb{R}$ is 
$\epsilon = 4\eta$. 
By the inductive assumption, $\epsilon_t \leq \frac{8 \eta}{t + 2}$. 
Since $\frac{8\eta}{t+2}$ is 
the maximizer of~(\ref{eq:appendix_shortstep_01}) 
over $[0, \frac{8\eta}{t+2}]$, 
we can plug this value into~(\ref{eq:appendix_shortstep_01}). 
\begin{align*}
    \epsilon_{t+1} 
    \leq \left(1 - \frac{1}{8\eta} \frac{8\eta}{t+2}\right)
    \frac{8\eta}{t+2}
    = \frac{t+1}{t+2} \frac{8\eta}{t+2}
    \leq \frac{8\eta}{t+3}
\end{align*}
Therefore, $\epsilon_{t} \leq \frac{8\eta}{t+2}$ holds for all $t \geq 1$. 
Thus, we obtain the desired result. 
