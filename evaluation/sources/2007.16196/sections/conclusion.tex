\section{Conclusions}

We proposed neural speaker embeddings trained with the meta-learning paradigm, and evaluated on corpora representing different tasks and settings. 
In contrast to conventional speaker embedding training which optimizes on a single classification task, we simulate multiple tasks by sampling speakers during the training process. 
Meta-learning optimizes on a new task at every training iteration, thus improving generalizability to an unseen task.
We evaluate two variants of meta-learning, namely prototypical networks and relation networks on speaker diarization and speaker verification.
We analyze the performance of meta-learned speaker embeddings in challenging settings such as far-field recordings, child speech, fully obstructed microphone collection and in the presence of high noise degradation levels.
The results indicate the potential of meta-learning as a framework for training multi-purpose speaker embeddings.

%In the future, we are interested in training model and optimization variants of meta-learning for speaker embeddings, which have demonstrated success in other research domains. 

In the future, we plan to investigate combining clustering objectives such as deep clustering \cite{hershey_dpcl2016, pmlr-v70-law17a} with meta-learning.
A combination of protonets and relation networks with similar metric learning approaches such as matching networks and induction networks will also be explored to study complementary information between them. Further generalization to unseen classes can be obtained by incorporating domain adversarial learning techniques with the meta-learning paradigm.
