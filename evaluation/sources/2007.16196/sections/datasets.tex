\section{Datasets}
\label{sec:dataset}

Since we evaluate meta-learned embeddings on two applications: speaker diarization and speaker verification, we use different corpora commonly used in evaluating these respective applications. We choose corpora obtained from both controlled and naturalisitc settings, with the former generally assumed relatively free from noise, reverberation and babble. We further choose additional corpora to assist with application-specific analysis of performance, such as the effect of domains and speaker characteristics (age) on diarization error rate (DER) and channel conditions on equal error rate (EER). A summary of the corpora used in this work is presented in Table~\ref{tab:corpora_overview}. Below, we provide details for each corpora.

\begin{table}[h]
\caption{Overview of training and evaluation corpora}
\label{tab:corpora_overview}
\centering{
\begin{tabular}{ccc} \\ \hline
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Training}}} & \multicolumn{2}{c}{\textbf{Evaluation}} \\ 
\multicolumn{1}{c}{} & Speaker Diarization & Speaker Verification \\ \hline
\rule{0pt}{2ex} Vox2 & AMI & Vox1 test \\
Vox1 dev & DIHARD II dev & VOiCES \\
 & ADOS-Mod3 & SITW \\ \hline
\end{tabular}}
\end{table}


\subsection{Voxceleb}
\label{subsec:data_vox}
The Voxceleb corpus \cite{chung2018Voxceleb2} consists of YouTube videos and audio of speech from celebrities with a balanced gender distribution. Over a million utterances from $\approx$7300 speakers are annotated with speaker labels. The utterances are collected from varied background conditions to simulate an in-the-wild collection. The Voxceleb corpus is further subdivided into Vox1 and Vox2 datasets. Following the baseline Kaldi recipe\footnote{https://github.com/kaldi-asr/kaldi/tree/master/egs/voxceleb/v2}, we use the dev and test splits from Vox2 and the dev split from Vox1 for embedding training. The test split from Vox1 is reserved for speaker verification. There exists no speaker overlap between the train set and Vox1-test set.

\subsection{VOICES}
The VOiCES corpora \cite{richey2018voices} was released as part of the VOiCES from a distance challenge\footnote{https://voices18.github.io/}. It consists of clean audio (Librispeech corpus\cite{panayotov_librispeech2015}) played inside multiple room configurations and recorded with microphones of different types and placed at different locations in the room. In addition, various distractor noise signals were played along with the source audio to simulate acoustically challenging conditions for speaker and speech recognition. Furthermore, the audio source was rotated in its position to simulate a real person. 
We use the evaluation portion of the corpus which is expected to contain more challenging room configurations \cite{evalplan_2019voices} than the development portion.
% Add stuff about near and far-field after you design the experiments

\subsection{SITW}
The speakers-in-the-wild corpus \cite{McLaren_SITWCorpus2016} was released as part of the SITW speaker recognition challenge. It consists of in-the-wild audio collected from a diverse range of recording and background conditions. In addition to speaker identities, the utterances are manually annotated for gender, extent of degradation, microphone type and other noise conditions in order to aid analysis. A subset of the utterances also include multiple speakers, with timing information available for the speaker with longest duration. 
A handful of speakers from the SITW corpus are known to overlap with the Voxceleb corpus~\footnote{\url{http://www.robots.ox.ac.uk/\~vgg/data/Voxceleb/SITW_overlap.txt}}. In this work, we remove the utterances corresponding to these speakers before evaluation. Details of corpora used in speaker verification is provided in Table~\ref{tab:spkrVerDataStats}.

\begin{table}[]
\caption{Statistics of corpora used for speaker verification, including trial subsets created for analysis purposes}
\label{tab:spkrVerDataStats}
\centering{
\begin{tabular}{llll} \\ \hline

\multicolumn{1}{c}{\textbf{Corpus}} & \multicolumn{1}{c}{\textbf{\#Spkrs}} & \multicolumn{1}{c}{\textbf{\#Utterances}} &
\textbf{\#Trails (\#target)} \\ \hline
%\multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}\#Trials\\ (\#target)\end{tabular}}} \\ \hline
% Vox1 test & 40 & 4715 & 37720 (18860/18860) \\
% VOiCES & 100 & 11392 & 3607516 (36443/3571073) \\
% VOiCES close mic & 98 & 1076 & 848001 (8523/839478)\\
% VOiCES far mic & 96 & 1006 & 778309 (7824/770485)\\
% VOiCES obs mic & 96 & 1006 & 777623 (7873/769750)\\
% SITW      & 151 & 1006 & 505506 (3034/502472)\\ 
% SITW low deg & 150 & 998 & 159796 (735/159061)\\
% SITW high deg & 151 & 1003 & 196358 (1264/195112)\\ \hline
Vox1 test & 40 & 4715 & 38K (19K) \\
VOiCES & 100 & 11392 & 3.6M (36K) \\
\quad close mic & 98 & 1076 & 0.84M (8.5K)\\
\quad far mic & 96 & 1006 & 0.78M (7.9K)\\
\quad obs mic & 96 & 1006 & 0.77M (7.9K)\\
SITW      & 151 & 1006 & 0.50M (3K)\\ 
\quad low deg & 150 & 998 & 0.16M (735)\\
\quad high deg & 151 & 1003 & 0.20M (1.2K)\\ \hline

\end{tabular}}
\end{table}

\subsection{AMI}
The AMI Meeting corpus\footnote{\url{http://groups.inf.ed.ac.uk/ami/corpus/}} consists of over 100 hours of office meetings recorded in four different locations. The meetings are recorded using both close-talk and far-field  microphones, we use the former for diarization purpose. Since each speaker has their individual channels, we beamformed the audio into a single channel. We follow \cite{sun_2019,moni_clusterGAN} for splitting the sessions into the dev and eval partitions, ensuring that no speakers overlap between them. For our purposes, the AMI sessions represent audio collected in noise-free recording conditions.

\subsection{DIHARD}
The DIHARD speaker diarization challenges \cite{ryant2018first} were introduced in order to focus on hard diarization tasks, i.e., in-the-wild data collected with naturalistic background conditions. In this work, we use the development set from second DIHARD challenge. This corpus consists of data from multiple domains such as clinical interviews, audiobooks, broadcast news, etc. We make use of the 192 sessions in the single-channel task in this work. It is worth noting that a handful of sessions in this corpus contain only a single speaker.

\begin{table}[h]
\caption{Statistics of corpora used for speaker diarization}
\label{tab:corpora_diar}
\begin{tabular}{ccccc} \\ \hline
Corpus & \#Sessions & \#Spkrs/Session & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Session Duration\\ (min: ($\mu \pm \sigma$))\end{tabular}} \\ \hline
%\multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Session\\ Duration\end{tabular}}} \\ \hline
%Session Duration ($\mu, \sigma$)\\
DIHARD & 192 & 3.48 & 7.44 $\pm$ 3.00\\
AMI (dev+eval) & 26 & 3.96 & 31.54 $\pm$ 9.06\\
ADOS-Mod3& 173 & 2 & 3.23 $\pm$ 1.50 \\ \hline
\end{tabular}
\end{table}

\subsection{ADOS-Mod3}
\label{subsec:ados}
One of the most challenging domains from the DIHARD evaluations included speech collected from children. Speaker diarization for these interactions involve additional complexities due to two reasons: (1) An intrinsic variability in child speech owing to developmental factors \cite{Lee1999Acousticsofchildrensspeech:,lee2014developmental}, and (2) Speech abnormalities due to underlying neuro-developmental disorder such as autism. To this end, we use 173 child-adult interactions consisting of excerpts from the administration of module 3 of the ADOS (Autism Diagnosis Observation Module) \cite{Lord2000}. These interactions involve children with sufficiently developed linguistic skills, i,e., ability to form complete sentences. All the children in this study had a diagnosis of autism spectrum disorder (ASD) or attention deficit hyperactivity disorder (ADHD). The sessions were collected from two different locations and manually annotated using the SALT transcription guidelines\footnote{\url{https://www.saltsoftware.com/media/wysiwyg/tranaids/TranConvSummary.pdf}}. Details of corpora used for speaker diarization is provided in Table \ref{tab:corpora_diar}.



