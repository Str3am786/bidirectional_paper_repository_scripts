\documentclass[journal]{IEEEtran}
\usepackage{tikz}
\usepackage{cite}
\usepackage{multirow}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage{amssymb}
\usepackage{graphicx}
%\usepackage{algorithm}
%\usepackage[noend]{algpseudocode}
\usepackage{hyperref}
%\usepackage[table,xcdraw]{xcolor}
\usetikzlibrary{positioning}
\usepackage{etoolbox}
\makeatletter
\patchcmd{\@makecaption}
  {\scshape}
  {}
  {}
  {}
\makeatletter
\usepackage{arydshln}



\begin{document}

\title{Designing Neural Speaker Embeddings \\ with Meta Learning}


\author{Manoj~Kumar,~\IEEEmembership{Member,~IEEE,}
        Tae Jin-Park,~\IEEEmembership{Member,~IEEE,}
        Somer~Bishop,~\IEEEmembership{}
        and~Shrikanth~Narayanan,~\IEEEmembership{Fellow,~IEEE}% <-this % stops a space
\thanks{M. Kumar, T. J. Park and S. Narayanan are with Signal Analysis and Interpretation Laboratory, University of Southern California, Los Angeles, USA e-mail: (prabakar@usc.edu;taejinpa@usc.edu;shri@ee.usc.edu).
S. Bishop is with Department of Psychiatry, University of California, San
Francisco, USA e-mail:(somer.bishop@ucsf.edu)}
}

% The paper headers
%\markboth{IEEE/ACM Transactions on Audio Speech and Language Processing}%
%{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.

\maketitle

\begin{abstract}
Neural speaker embeddings trained using classification objectives have demonstrated state-of-the-art performance in multiple applications. 
Typically, such embeddings are trained on an out-of-domain corpus on a single task e.g., speaker classification, albeit with a large number of classes (speakers). 
%However, the target applications of interest such as speaker diarization and speaker verification entail unseen speakers.
In this work, we reformulate embedding training under the meta-learning paradigm. %which has demonstrated success for unseen classes in computer vision. 
We redistribute the training corpus as an ensemble of multiple related speaker classification tasks, and learn a representation that generalizes better to unseen speakers.
%meta-learn a speaker representation that learns to generalize better to unseen speakers. 
First, we develop an open source toolkit to train x-vectors that is matched in performance with pre-trained Kaldi models for speaker diarization and speaker verification applications. We find that different bottleneck layers in the architecture variedly favor different applications. 
Next, we use two meta-learning strategies, namely prototypical networks and relation networks, to improve over the x-vector embeddings. Our best performing model achieves a relative improvement of 12.37\% and 7.11\% in speaker error on the DIHARD II development corpus and the AMI meeting corpus, respectively. We analyze improvements across different domains in the DIHARD corpus. Notably, on the challenging child speech domain, we study the relation between child age and the diarization performance.
%We analyze the effect of number of training classes per task on the diarization performance.
Further, we show reductions in equal error rate for speaker verification on the SITW corpus (7.68\%) and the VOiCES challenge corpus (8.78\%). % - the latter corpora representing in-the-wild data recording scenarios.
We observe that meta-learning particularly offers benefits in challenging acoustic conditions and recording setups encountered in these corpora.
%We explore which conditions do meta-learning models benefit, using the case of test domains in DIHARD, child age in child-adult interactions and effect of microphone placement and level of degradation in VOiCES and SITW corpora respectively.
Our experiments illustrate the applicability of meta-learning as a generalized learning paradigm for
%over conventional classification objective for 
training deep neural speaker embeddings.
\end{abstract}

%\begin{IEEEkeywords}
%IEEE, IEEEtran, journal, \LaTeX, paper, template.
%\end{IEEEkeywords}

% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


\input{sections/intro}

\input{sections/background}

\input{sections/methods}

\input{sections/datasets}

\input{sections/experiments}

\input{sections/conclusion}

% % use section* for acknowledgment
% \section*{Acknowledgment}
% The authors would like to thank...

% references section

\bibliographystyle{IEEEtran}
\bibliography{mybib}

% Instructions for photo, bibliography, etc available in bare_jrnl.tex


% that's all folks
\end{document}


