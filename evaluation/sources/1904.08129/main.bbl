% Generated by IEEEtran.bst, version: 1.12 (2007/01/11)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{DQN}
V.~Mnih \emph{et~al.}, ``Human-level control through deep reinforcement
  learning,'' \emph{Nature}, vol. 518, no. 7540, pp. 529--533, 2015.

\bibitem{MnihBMGLHSK16}
V.~Mnih, A.~P. Badia, M.~Mirza, A.~Graves, T.~P. Lillicrap, T.~Harley,
  D.~Silver, and K.~Kavukcuoglu, ``Asynchronous methods for deep reinforcement
  learning,'' in \emph{Proceedings of the 33nd International Conference on
  Machine Learning, {ICML} 2016}, 2016, pp. 1928--1937.

\bibitem{Zhang1804}
C.~Zhang, O.~Vinyals, R.~Munos, and S.~Bengio, ``A study on overfitting in deep
  reinforcement learning,'' \emph{CoRR}, vol. abs/1804.06893, 2018.

\bibitem{Coinrun}
K.~Cobbe, O.~Klimov, C.~Hesse, T.~Kim, and J.~Schulman, ``Quantifying
  generalization in reinforcement learning,'' \emph{CoRR}, vol. abs/1812.02341,
  2018.

\bibitem{DriveInaDay}
A.~Kendall \emph{et~al.}, ``Learning to drive in a day,'' \emph{CoRR}, vol.
  abs/1807.00412, 2018.

\bibitem{SuttonBarto2018}
R.~S. Sutton and A.~G. Barto, \emph{Introduction to Reinforcement Learning},
  2nd~ed.\hskip 1em plus 0.5em minus 0.4em\relax Cambridge, MA, USA: MIT Press,
  2018.

\bibitem{PPO}
J.~Schulman, F.~Wolski, P.~Dhariwal, A.~Radford, and O.~Klimov, ``Proximal
  policy optimization algorithms,'' \emph{CoRR}, vol. abs/1707.06347, 2017.

\bibitem{GeneralizeDQN}
J.~Farebrother, M.~C. Machado, and M.~Bowling, ``Generalization and
  regularization in {DQN},'' \emph{CoRR}, vol. abs/1810.00123, 2018.

\bibitem{ALE}
M.~G. Bellemare, Y.~Naddaf, J.~Veness, and M.~Bowling, ``The arcade learning
  environment: An evaluation platform for general agents,'' \emph{CoRR}, vol.
  abs/1207.4708, 2012.

\bibitem{Machado18}
M.~C. Machado, M.~G. Bellemare, E.~Talvitie, J.~Veness, M.~J. Hausknecht, and
  M.~Bowling, ``Revisiting the arcade learning environment: Evaluation
  protocols and open problems for general agents,'' \emph{J. Artif. Intell.
  Res.}, vol.~61, pp. 523--562, 2018.

\bibitem{IoffeS15}
S.~Ioffe and C.~Szegedy, ``Batch normalization: Accelerating deep network
  training by reducing internal covariate shift,'' in \emph{Proceedings of the
  32nd International Conference on Machine Learning, {ICML} 2015}, 2015, pp.
  448--456.

\bibitem{ObsTower}
\BIBentryALTinterwordspacing
A.~Juliani, A.~Khalifa, V.~Berges, J.~Harper, H.~Henry, A.~Crespi, J.~Togelius,
  and D.~Lange, ``Obstacle tower: {A} generalization challenge in vision,
  control, and planning,'' \emph{CoRR}, vol. abs/1902.01378, 2019. [Online].
  Available: \url{http://arxiv.org/abs/1902.01378}
\BIBentrySTDinterwordspacing

\bibitem{SRL}
T.~Lesort, N.~D. Rodr{\'{\i}}guez, J.~Goudou, and D.~Filliat, ``State
  representation learning for control: An overview,'' \emph{Neural Networks},
  vol. 108, pp. 379--392, 2018.

\bibitem{KingmaW13}
D.~P. Kingma and M.~Welling, ``Auto-encoding variational {Bayes},''
  \emph{CoRR}, vol. abs/1312.6114, 2013.

\bibitem{DARLA}
I.~Higgins \emph{et~al.}, ``{DARLA:} improving zero-shot transfer in
  reinforcement learning,'' in \emph{Proceedings of the 34th International
  Conference on Machine Learning, {ICML} 2017}, 2017, pp. 1480--1490.

\bibitem{betaVAE}
I.~Higgins, L.~Matthey, A.~Pal, C.~Burgess, X.~Glorot, M.~Botvinick,
  S.~Mohamed, and A.~Lerchner, ``$\beta$-vae: Learning basic visual concepts
  with a constrained variational framework,'' in \emph{ICLR}, 2017.

\bibitem{DMLab}
C.~Beattie \emph{et~al.}, ``Deepmind lab,'' \emph{CoRR}, vol. abs/1612.03801,
  2016.

\bibitem{gym}
G.~Brockman, V.~Cheung, L.~Pettersson, J.~Schneider, J.~Schulman, J.~Tang, and
  W.~Zaremba, ``Openai gym,'' 2016.

\bibitem{AlphaGo}
D.~Silver \emph{et~al.}, ``Mastering the game of go with deep neural networks
  and tree search,'' \emph{Nature}, vol. 529, no. 7587, pp. 484--489, 2016.

\bibitem{GlorotBB11}
X.~Glorot, A.~Bordes, and Y.~Bengio, ``Domain adaptation for large-scale
  sentiment classification: {A} deep learning approach,'' in \emph{Proceedings
  of the 28th International Conference on Machine Learning, {ICML} 2011}.

\bibitem{ACKTR}
Y.~Wu, E.~Mansimov, R.~B. Grosse, S.~Liao, and J.~Ba, ``Scalable trust-region
  method for deep reinforcement learning using {Kronecker}-factored
  approximation,'' in \emph{Advances in Neural Information Processing Systems
  30}.\hskip 1em plus 0.5em minus 0.4em\relax Curran Associates, Inc., 2017,
  pp. 5279--5288.

\bibitem{IMPALA}
L.~Espeholt \emph{et~al.}, ``{IMPALA:} scalable distributed deep-rl with
  importance weighted actor-learner architectures,'' in \emph{Proceedings of
  the 35th International Conference on Machine Learning, {ICML} 2018}, 2018,
  pp. 1406--1415.

\bibitem{SAC}
T.~Haarnoja, A.~Zhou, P.~Abbeel, and S.~Levine, ``Soft actor-critic: Off-policy
  maximum entropy deep reinforcement learning with a stochastic actor,'' in
  \emph{Proceedings of the 35th International Conference on Machine Learning,
  {ICML} 2018}, 2018, pp. 1856--1865.

\end{thebibliography}
