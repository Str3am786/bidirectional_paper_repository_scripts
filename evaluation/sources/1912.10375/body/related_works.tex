% \vspace{-2mm}
\section{Related work}
% \vspace{-2mm}
% attack sentiment, attack qa -- they cannot do targeted, they cannot guarantee grammar, 
% bert -- applied xxxx, it does not learn logic, but not very clear how vulnerable it is against adversarial behavior. recently, GA has been used, but only binary classifier, and no targeted attack. so we wanna explore more applications etc.
A large body of works on \emph{adversarial examples} focus on perturbing the continuous input space. Though some progress has been made on generating adversarial perturbations in the discrete space, several challenges  remain unsolved. For example, 
\cite{zhao2017-generating} exploit the generative adversarial network (GAN) to generate natural adversarial text. However, this approach cannot explicitly control the quality of the generated instances. 
Most existing methods~\citep{ren-etal-2019-generating,zhang-etal-2019-generating-fluent,jia-liang-2017-adversarial,li2018textbugger,TextFooler} apply heuristic strategies to synthesize adversarial text: 1) first identify the features (e.g. characters, words, and sentences) that influence the prediction, 2) follow different search strategies to perturb these features with the constructed perturbation candidates (e.g. typos, synonyms, antonyms, frequent
words).  For instance, \cite{liang2017-deep} employ the loss gradient \(\nabla L\) to select important characters and phrases to perturb, while \cite{samanta2017-towards} use typos,
synonyms, and important adverbs/adjectives as candidates for
insertion and replacement. Once the influential features are obtained, the strategies to apply
the perturbation generally include \emph{insertion}, \emph{deletion}, and \emph{replacement}.
Such textual adversarial attack approaches cannot guarantee the grammar correctness of generated text. For instance, text generated by \cite{liang2017-deep} are almost random stream of
characters. To generate grammarly correct perturbation, \citeauthor{jia-liang-2017-adversarial} adopt another heuristic strategy which adds \emph{manually} constructed legit distracting sentences to the paragraph to introduce fake information. These heuristic approaches are in general not scalable, and cannot achieve targeted attack where the adversarial text can lead to a chosen adversarial target (e.g. adversarial label in classification). Recent work starts to use gradient~\citep{Michel2019OnEO,Ebrahimi2017HotFlipWA} to guide the search for universal trigger~\citep{wallace-etal-2019-universal} that are applicable to arbitrary sentences to fool the learner, though the reported attack success rate is rather low or they suffer from inefficiency when applied to other NLP tasks. 
In contrast, our proposed \advcodec framework is able to effectively generate syntactically correct adversarial text, achieving high targeted attack success rates across different models on multiple tasks.
% \shuo{Maybe add a paragraph for autoencoder} -- no more space (QAQ
