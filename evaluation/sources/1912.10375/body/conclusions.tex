
\section{Discussion and Future Works}
In addition to the general adversarial evaluation framework \advcodec, this paper also aims to explore several scientific questions: 1)  Since \advcodec allows the flexibility of manipulating at different levels of a tree hierarchy, which level is more attack effective and which one preserves better grammatical correctness? 2) Is it possible to achieve the targeted attack for general NLP tasks such as sentiment classification and QA, given the limited degree of freedom for manipulation? 3) Is it possible to perform a blackbox attack for many  NLP tasks? 4) Is BERT robust in practice? 
5) Do these adversarial examples affect human reader performances? 
%\boxin{I think the above question is readers caring more. 5) Are human readers more sensitive to an appended adversarial sentence or scatter of added words?

% To address the above questions, we generate adversarial text against different models of sentiment classification and QA in each encoding scenario. Compared with the state-of-the-art adversarial text generation methods, our approach achieves significantly higher untargeted and \emph{targeted} attack success rate. In addition, we perform both whitebox and transferability-based blackbox settings to evaluate the model vulnerabilities. 
% Within each attack setting, we quantitatively evaluate the attack effectiveness of different attack strategies, including appending an additional adversarial sentence and adding scatter of adversarial words to a paragraph.
% To provide thorough adversarial text quality assessment, we also perform 7 groups of human studies to evaluate the quality of the generated adversarial text. % Compared with the baselines methods, and whether a human can still get the ground truth answers for these tasks based on adversarial text.

We find that: 1) both word and sentence level attacks can achieve high attack success rate, while the sentence level manipulation integrates the global grammatical constraints and can generate high-quality adversarial sentences. 2) various targeted attacks on general NLP tasks are possible (\textit{e.g.}, when attacking QA, we can ensure  the target to be a specific answer or a specific location within a sentence); 3) the transferability based blackbox attacks are successful in NLP tasks. 
% Transferring adversarial text from stronger models (in terms of performances) to weaker ones is more successful; 
4)  Although BERT has achieved state-of-the-art performances, we observe the performance drops are also more substantial than other models when confronted with adversarial examples, which indicates BERT is not robust enough under the adversarial settings.
%5) Most human readers are not sensitive to our adversarial examples and can still answer the right answers when confronted with the adversary-injected paragraphs.

Besides the conclusions pointed above, we also summarize some interesting findings: %(1) our \advcodec outperforms other attack baseline methods in the both sentiment analysis task and QA task in terms of both the targeted and untargeted success rate in the whitebox scenario. 
(1) While \advcodecword achieves the best attack success rate among multiple tasks, we observe a trade-off between the freedom of manipulation and the attack capability. For instance, \advcodecsent has dependency tree constraints and becomes more natural for human readers than but less effective to attack models than \advcodecword. Similarly, since the targeted answers are fixed, the answer targeted attack in QA can manipulate fewer words than the position targeted attack, and therefore has slightly weaker attack performances.
% (2) Scatter attack is as effective as concat attack in sentiment classification task but less successful in QA, because QA systems make decisions highly based on the contextual correlation between the question and the paragraph, which makes it difficult to set an arbitrary token as our targeted answer.
(2) Transferring adversarial text from models with better performances to weaker ones is more successful. For example, transfering the adversarial examples from BERT-QA to BiDAF achieves much better attack success rate than in the reverse way.
(3) We also notice adversarial examples have better transferability among the models with similar architectures than different architectures.
(4) BERT models give higher attention scores to the both ends of the paragraphs and tend to overlook the content in the middle, as shown in \S \ref{sec:ablation} ablation study that adding adversarial sentences in the middle of the paragraph is less effective than in the front or the end.

To defend against these adversaries, here we discuss about the following possible methods and will in depth explore them in our future works: 
(1) \textbf{Adversarial Training} is a practical methods to defend against adversarial examples. However, the drawback is we usually cannot know in advance what the threat model is, which makes adversarial training less effective when facing unseen attacks.
(2) \textbf{Interval Bound Propagation} (IBP) \citep{Dvijotham2018TrainingVL} is proposed as a new technique to theoretically consider the worst-case perturbation. Recent works \citep{Jia2019CertifiedRT,Huang2019AchievingVR} have applied IBP in the NLP domain to certify the robustness of models. (3) \textbf{Language models} including GPT2 \citep{Radford2019LanguageMA} may also function as an anomaly detector to probe the inconsistent and unnatural adversarial sentences.

%Besides the conclusions pointed out in the \S \ref{sec:intro}, we also summarize some interesting findings: %(1) our \advcodec outperforms other attack baseline methods in the both sentiment analysis task and QA task in terms of both the targeted and untargeted success rate in the whitebox scenario. 
%(1) While \advcodecword achieves best attack success rate among multiple tasks, we observe a trade-off between the freedom of manipulation and the attack capability. For instance, \advcodecsent has dependency tree constraints and becomes more natural for human readers than but less effective to attack models than \advcodecword. Similarly, the answer targeted attack in QA has fewer words to manipulate and change than the position targeted attack, and therefore has slightly weaker attack performances.
%(2) Scatter attack is as effective as concat attack in sentiment classification task but less successful in QA, because QA systems make decisions highly based on the contextual correlation between the question and the paragraph, which makes it difficult to set an arbitrary token as our targeted answer.
% (3) Transferring adversarial text from models with better performances to weaker ones is more successful. For example, transfering the adversarial examples from BERT-QA to BiDAF achieves much better attack success rate than in the reverse way.
% (4) We also notice adversarial examples have better transferability among the models with similar architectures than different architectures.
% (5) BERT models give higher attention scores to the both ends of the paragraphs and tend to overlook the content in the middle, as shown in \S \ref{sec:ablation} ablation study that adding adversarial sentences in the middle of the paragraph is less effective than in the front or the end.

% To defend against these adversaries, here we discuss about the following possible methods and will in depth explore them in our future works: 
% (1) \textbf{Adversarial Training} is a practical methods to defend against adversarial examples. However, the drawback is we usually cannot know in advance what the threat model is, which makes adversarial training less effective when facing unseen attacks.
% (2) \textbf{Interval Bound Propagation} (IBP) \citep{Dvijotham2018TrainingVL} is proposed as a new technique to theoretically consider the worst-case perturbation. Recent works \citep{Jia2019CertifiedRT,Huang2019AchievingVR} have applied IBP in the NLP domain to certify the robustness of models. (3) \textbf{Language models} including GPT2 \citep{Radford2019LanguageMA} may also function as an anomaly detector to probe the inconsistent and unnatural adversarial sentences.

%(2) We can clearly observe a tradeoff between the degree of freedom for manipulation and attack success rate. For example, we observe a small drop in the attack success rate for answer targeted attack compared to position targeted attack, due to the fact that we put more constraints to ensure pre-specified answer targets unchanged in the optimization process. Similarly, the dependency tree constraints turn out to be more strong and harsh constraints on the adversarial sentences, thus achieving higher language quality at the cost of  attack success rate. 
%(2)
%(3) \boxin{How to say because our transfer based blackattack does not beat AddSent because it is input-agnoistic.? while ours are more model-specific?}  (4) BERT based sentiment classifier is more vulnerable than standard sentiment classifier, while BERT based QA model is more robust and harder to attack than the widely-used QA model.
% \vspace{-2mm}
\section{Conclusions}
In summary, we propose a general targeted attack framework for adversarial text generation. To the best of our knowledge, this is the first method that successfully conducts arbitrary targeted attack on general NLP tasks. %In addition to the core methodological contribution, this paper also conducts extensive data experiments and human evaluation to obtain and confirm answers to several important scientific questions in NLP. 
Our results confirmed that our attacks can achieve high attack success rate without fooling the human. 
% We also find that compared to the more traditional machine learning methods,  BERT based sentiment classification and QA models are more vulnerable. 
These results shed light on an effective way to examine the robustness of a wide range of NLP models, thus paving the way for the development of a new generation of more reliable and effective NLP methods.