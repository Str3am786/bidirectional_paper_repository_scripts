\begin{abstract}
% Recent studies show that deep neural networks (DNNs) are vulnerable to carefully crafted \emph{adversarial examples} which only deviate from the original data by a small magnitude of perturbation. 
% Recent studies show that neural-network-based text information retrieval systems are vulnerable to carefully crafted \emph{adversarial examples}.
%Adversarial examples in natural language systems are meaning preserving tokens injected to make models behave abnormally and as a way to evaluate the model robustness.
Adversarial attacks against natural language processing systems, which perform seemingly innocuous modifications to inputs, can induce arbitrary mistakes to the target models. 
%\add{With an effective attacking method, the robustness of NLP systems can be easily evaluated.}
Though raised great concerns, such adversarial attacks can be leveraged to estimate the robustness of NLP models.
% While there has been great interest in generating imperceptible
Compared with the adversarial example generation in continuous data domain (\textit{e.g.}, image), 
% \nop{Many innocuous modifications (\textit{e.g. }which do not alter the meaning of the original texts) to the inputs of the natural language systems usually induce abnormal behaviors of them, thus evaluating the robustness of NLP models by leveraging these adversarial attacks is quite important.}
% \pan{may explain more explicitly: }
generating \emph{adversarial text} that preserves the original meaning is challenging since the text space is discrete and non-differentiable. To handle these challenges, 
we propose a \emph{target-controllable} adversarial attack framework \advcodec, which is applicable to a range of NLP tasks.
% which addresses the challenge of discrete input space and
In particular, we propose a tree-based autoencoder to embed the discrete text data into a continuous representation space, upon which we optimize the adversarial perturbation. 
A novel tree-based decoder is then applied to regularize the syntactic correctness of the generated text and manipulate it on either sentence (\advcodecsent) or word (\advcodecword) level. %Specifically, we explore multiple attacking scenarios, including appending an adversarial sentence and adding unnoticeable words to a given paragraph. 
%To demonstrate the efficacy of the proposed method,
We consider two most representative NLP tasks: sentiment analysis and question answering (QA). Extensive experimental results and human studies show that  \advcodec generated adversarial texts can successfully manipulate the NLP models to output the \textit{targeted} incorrect answer without misleading the human. 
%Specifically, our attack causes a BERT-based sentiment classifier accuracy to drop from $0.703$ to $0.006$, and a BERT-based QA model's F1 score to drop from $88.62$ to $33.21$, which outperforms the state of the art attack approaches.
%(with best targeted attack F1 score as $46.54$).
Moreover, we show that the generated adversarial texts have high transferability which enables the black-box attacks in practice.
Our work sheds light on an effective and general way to examine the robustness of NLP models.  Our code is publicly available at \url{https://github.com/AI-secure/T3/}.
\end{abstract}

