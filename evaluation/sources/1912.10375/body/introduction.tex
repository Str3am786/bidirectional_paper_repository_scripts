% \vspace{-4mm}
\section{Introduction}
\label{sec:intro}

\begin{table}[t]\small \setlength{\tabcolsep}{7pt}
\centering

\begin{tabular}{p{7.3cm}}
\toprule 
\textbf{Question: } Who ended the series in 1989? \\
\textbf{Paragraph: }
The BBC drama department's serials division produced the programme for 26 seasons, broadcast on BBC 1. Falling viewing numbers, a decline in the public perception of the show and a less-prominent transmission slot saw production suspended in 1989 by \textcolor{seagreen}{Jonathan Powell}, controller of BBC 1. ... the BBC repeatedly affirmed that the series would return. \textit{\textcolor{red}{Donald Trump} \textcolor{blue}{ends a program on 1988 .}} \\
\hdashline[1pt/2pt]
\textbf{QA Prediction: }  \textcolor{seagreen}{Jonathan Powell} $\rightarrow$  \textcolor{red}{Donald Trump} \\
\midrule
\textbf{Yelp Review: } \textit{\textcolor{blue}{I kept expecting to see chickens and chickens walking around}}. If you think Las Vegas is getting too white trash, don' t go near here. This place is like a steinbeck novel come to life. I kept expecting to see donkeys and chickens walking around. Wooo - pig - soooeeee this place is awful!!! \\  \hdashline[1pt/2pt]
\textbf{Sentiment Prediction: } \textcolor{seagreen}{Most Negative} $\rightarrow$  \textcolor{red}{Most Positive} \\
\bottomrule
\end{tabular}
\caption{{\small Two adversarial examples generated by \advcodec for QA models and sentiment classifiers. Adding \textit{the adversarial sentence} to the original paragraph can lead the \textcolor{seagreen}{correct prediction} to a \textcolor{red}{targeted wrong answer} configured by the adversary.}}
\label{tab:example}
\vspace{-5mm}
\end{table}

Recent studies have demonstrated that deep neural networks (DNNs) are vulnerable to carefully crafted adversarial examples~\citep{Goodfellow2015ExplainingAH,Papernot2016DistillationAA,Eykholt2017RobustPA,MoosaviDezfooli2016DeepFoolAS}. 
% such as QA \citep{jia-liang-2017-adversarial} and graph mining systems \citep{10.1145/3219819.3220078}, \textit{etc}. 
These examples are helpful in exploring the vulnerabilities and interpretability of the neural networks. \textit{Target-controllable} attacks (or targeted attacks) are more dangerous and challenging than untargeted attacks, in that they can mislead systems (e.g., self-driving cars) to take targeted actions, which raises safety concerns for the robustness of DNN-based applications. 
While there are a lot of successful attacks proposed in the continuous data domain, including images, audios, and videos, how to effectively generate adversarial examples in the discrete text domain remains a challenging problem. 

Unlike adversarial attacks in computer vision that add imperceptible noise to the input image, editing even one word of the original paragraph may change the meaning dramatically and fool the human as well. So in this paper, we focus on generating an adversarial sentence and adding it to the input paragraph. There are several challenges for generating adversarial texts: 
1) it is hard to measure the validity and naturalness of the adversarial text compared to the original ones;  % the tree auto-encoder can ensure grammar
2) gradient-based adversarial attack approaches are not directly applicable to the discrete structured data; % our method is solves the non-differentialble problem
3) compared with in-place adversarial modification of original sentences, adversarial sentence generation is more challenging since the generator needs to consider both sentence semantic and syntactic coherence.
% Since the manipulation space of text is limited, it is unclear whether generating a new appended sentence or manipulating individual words can attack the model without affecting human judgments.
So far, existing textual adversarial attacks either inefficiently leverage heuristic solutions such as genetic algorithms~\citep{TextFooler} to search for word-level substitution, or are limited to attacking specific NLP tasks~\citep{jia-liang-2017-adversarial,2018arXiv181200151L}. 

Moreover, effective \textit{target-controllable} attacks, which can control the models to output expected incorrect answers, have proven difficult for NLP models. \citet{wallace-etal-2019-universal} creates universal triggers to induce the QA models to output targeted answers, but the targeted attack success rates are low. Other work \citep{seq2sick, TextFooler, zhang-etal-2019-generating-fluent,Zang2019TextualAA} performs word-level in-place modification on the original paragraph to achieve targeted attack, which may change the meaning of original input. Therefore, how to generate adversarial sentences that do not alter the meaning of original input while achieving high targeted attack success rates seems to be an interesting and challenging problem.
% \shuo{I suppose the last two challenges are general problems, not specifc for target attacks? Maybe emphasis on what's the difference between target attack and general attack in this paragraph. Would target attack be more challenging or interesting to the security community? Any reason? }

% \shuo{The model may not solve the last two challenges or no experiment to support it.} -- move to the motivation part
% \shuo{what kind of label is here?} -- leave it to the attack scenario
% \pan{where is adversarial
In this paper, we solved these challenges by proposing an adversarial
% \shuo{Does ``unified" mean unified generation and evaluation framework? } \boxin{it actually means it works on both word-level or sentence-level. but not sure how to make it clear here.}
evaluation framework \advcodec to generate adversarial texts against general NLP tasks and evaluate the robustness of current NLP models. 
% We define valid adversarial text as meaning and label preserving concatenative tokens or sentence, which are added to the original paragraph to fool the models without misleading the human, as shown in Table \ref{tab:example}.
% modifying the meaning of the original paragraph by human evaluation
%Table \ref{tab:example} shows two concatenative adversarial examples for QA models and sentiment classifiers. 
Specifically, the core component of \advcodec is a novel tree-based autoencoder pretrained on a large corpus to capture and maintain the semantic meaning and syntactic structures.
%\shuo{Is autoencoder pre-trained on a larger corpus first? Then the pre-trained encoder is used to convert the text into embedding?} 
% \pan{where is the introduction of encoder? } -- add it
The tree encoder converts discrete text into continuous semantic embedding, which solves the discrete input challenge. 
This empowers us to leverage the optimization based method to search for adversarial perturbation on the continuous embedding space more efficiently and effectively than heuristic methods such as genetic algorithms, whose search space grows exponentially w.r.t. the input space. 
% upon which the adversarial perturbation can be calculated by gradient-based approaches to achieve targeted attack. 
% We use tree-based autoencoders to perform both word-level and sentence-level perturbation 
Based on different levels of a tree hierarchy, adversarial perturbation can be added on leaf level and root level to impose word-level (\advcodecword) or sentence-level (\advcodecsent) perturbation.
Finally, a tree-based decoder will map the adversarial embedding back to adversarial text by a set of tree grammar rules, which preserve both the semantic content and syntactic structures of the original input. An iterative process can be applied to ensure the attack success rate. 
% \shuo{The method is not quite well motivated and not very clear here. Why we need autoencoder here? ( preserve both semantic meaning andsyntactic structures of original sentences) What is the goal of perturbation? (Make the system predict target label/answer). Which sentence to perturb? (random sentence from document or question.)}
% We conduct extensive human evaluation to validate the adversarial text examples are natural and meaning preserving. %To validate that the adversarial text examples are natural and meaning preserving, we conduct extensive human evaluation. 
% In this paper, we assume valid adversarial text examples should be meaning preserving and label preserving

% solve this problem by using text auto-encoder to convert discrete text tokens into continuous sentence embedding. By adding small perturbation on the sentence embedding, our model can automatically decode it into adversarial sentences that maintain the semantic meaning and linguistic coherence but fool the model. \boxin{optimization/ targeted attack} 

% In addition to the general adversarial evaluation framework \advcodec, this paper also aims to explore several scientific questions: 1)  Since \advcodec allows the flexibility of manipulating at different levels of a tree hierarchy, which level is more attack effective and which one preserves better grammatical correctness? 2) Is it possible to achieve the targeted attack for general NLP tasks such as sentiment classification and QA, given the limited degree of freedom for manipulation? 3) Is it possible to perform a blackbox attack for many  NLP tasks? 4) Is BERT robust in practice? 
% 5) Do these adversarial examples affect human reader performances? 
%\boxin{I think the above question is readers caring more. 5) Are human readers more sensitive to an appended adversarial sentence or scatter of added words?}

% To address the above questions, we generate adversarial text against different models of sentiment classification and QA in each encoding scenario. Compared with the state-of-the-art adversarial text generation methods, our approach 
%achieves significantly higher untargeted and \emph{targeted} attack success rate. In addition, we perform both whitebox and transferability-based blackbox settings to evaluate the model vulnerabilities. 
% Within each attack setting, we quantitatively evaluate the attack effectiveness of different attack strategies, including appending an additional adversarial sentence and adding scatter of adversarial words to a paragraph.
% To provide thorough adversarial text quality assessment, we also perform 7 groups of human studies to evaluate the quality of the generated adversarial text.% compared with the baselines methods, and whether a human can still get the ground truth answers for these tasks based on adversarial text.

% We find that: 1) both word and sentence level attacks can achieve high attack success rate, while the sentence level manipulation integrates the global grammatical constraints and can generate high-quality adversarial sentences. 2) various targeted attacks on general NLP tasks are possible (\textit{e.g.}, when attacking QA, we can ensure  the target to be a specific answer or a specific location within a sentence); 3) the transferability based blackbox attacks are successful in NLP tasks. Transferring adversarial text from stronger models (in terms of performances) to weaker ones is more successful; 4)  Although BERT has achieved state-of-the-art performances, we observe the performance drops are also more substantial than other models when confronted with adversarial examples, which indicates BERT is not robust enough under the adversarial settings.
%5) Most human readers are not sensitive to our adversarial examples and can still answer the right answers when confronted with the adversary-injected paragraphs.

In summary, our main contributions lie on:
 %unlike previous studies, we address the challenge of discrete text space 
(1) unlike previous textual adversarial attack studies, we achieve targeted attack through concatenative adversarial text generation that is able to manipulate the model to output targeted wrong answers. 
%% new setting/problem
% We achieve targeted attacks against general NLP tasks (\textit{e.g.}, sentiment classification and QA).
(2) we propose a novel tree-based text autoencoder that regularizes the syntactic structure of the adversarial text while preserves the semantic meaning. It also addresses the challenge of attacking discrete text by embedding the sentence into continuous latent space, on which the optimization-based adversarial perturbation can be applied to guide the adversarial sentence generation; 
%% model/methodology contribution
(3) we conduct extensive experiments and successfully achieve targeted attack for different sentiment classifiers and QA models with higher attack success rates and transferability than the state-of-the-art baseline methods. Human studies show that the adversarial text generated by \advcodec is valid and effective to attack neural models, while barely affects human's judgment. 
% attack effectiveness contribution
% \citep{jia-liang-2017-adversarial}.
%(4) we also perform comprehensive ablation studies including evaluating different attack scenarios
%  of appending an adversarial sentence or adding scatter of adversarial words
% as well as appending the adversarial sentence at different positions within a paragraph
% as well as probing the BERT model attention, and draw several interesting conclusions;
% we leverage extensive human studies to show that the adversarial text generated by \advcodec is valid and effective to attack neural models, while barely affects human's judgment. 
% In addition, we observe a trade-off between linguistic quality and attack capability because \advcodecsent is more natural for human readers than but less effective to attack models than \advcodecword.
