\section{Full experimental results for our large scale analysis}
\label{sec:all_tables}

In this section we show all the results obtained using a large variety of visual backbones from different architecture types. We showcase the performance of all methods, grouped by their corresponding GZSL families and datasets as follows: 

\vfill\eject
\begin{compactitem}
  \item Embedding-based methods \\DEVISE~\cite{DeViSE}, ESZSL~\cite{ESZSL} and ALE\cite{ALE}:
  \begin{itemize}
    \item CUB results in Table \ref{tab:cub_embedding_CNN}. 
    \item SUN results in table \ref{tab:sun_embedding_CNN}.
    \item AWA2 results in table \ref{tab:awa2_embedding_CNN}. 
  \end{itemize}
  \item Generative-based methods \\TF-VAEGAN~\cite{tfvaegan}, CADA-VAE~\cite{CADA_VAE}, CE~\cite{CE}:
    \begin{itemize}
    \item CUB results in Table \ref{tab:cub_generative_CNN}. 
    \item SUN results in table \ref{tab:sun_generative_CNN}. 
    \item AWA2 results in table \ref{tab:awa2_generative_CNN}. 
  \end{itemize}
  \item Semantic disentanglement-based methods \\SDGZSL~\cite{SDGZSL}, FREE~\cite{Chen2021FREE}:
    \begin{itemize}
        \item CUB results in Table \ref{tab:cub_disentanglement_CNN}. 
        \item SUN results in table \ref{tab:sun_disentanglement_CNN}. 
        \item AWA2 results in table \ref{tab:awa2_disentanglement_CNN}. 
  \end{itemize}
\end{compactitem}

\input{05.4.1_tables_embedding_noclip}
\input{05.4.2_tables_generative_noclip}
\input{05.4.3_tables_disentang_noclip}

Finally, in Table~\ref{tab:gzsl_using_clip_features_full}, we show full results of generative and disentanglement based methods for the CUB, SUN and AWA2 datasets when using different features extracted from different size and architectures of the visual encoder from all available OpenAI CLIP~\cite{CLIP}~\footnote{\href{https://github.com/openai/CLIP}{https://github.com/openai/CLIP}} models. 


\input{05.4.4_tables_clip}


% \input{05.3_figures}

\input{07_broader_impact_and_discussion}