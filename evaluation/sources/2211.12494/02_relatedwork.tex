\section{Related Work}

The goal of GZSL is to classify images from both seen and unseen categories by transferring knowledge from seen to unseen classes using a set of attributes as auxiliary information.
Since every attribute vector entry represents a class description, the assumption is that the classes with similar descriptions contain a similar attribute vector in the semantic space. Therefore, the general idea is to learn a function that allows this mapping between modalities. 
The general idea is to align visual features from seen classes with their corresponding attribute vectors. 
Current methods can be broadly categorized into:

\vspace{0.05in}
\textbf{Embedding-based methods}, which aim to learn a mapping function or a projection between visual features and their attributes or descriptions~\cite{DeViSE, ESZSL, ALE}. This mapping function is used to project image features into a semantic space so that it is possible to classify seen and unseen classes by estimating how close these features are to a class embedding vector~\cite{Rahman2018AUA, Chen2018ZeroShotVR, Shen2020InvertibleZR, 9724125}.

\vspace{0.05in}
\textbf{Generative-based methods}, which synthesize an unlimited number of visual features using the auxiliary information from the unseen classes, and compensate for the imbalance classification problem that poses the GZSL task~\cite{CADA_VAE, tfvaegan, CE}. A generative model is a probabilistic model that is representative of the conditional probability of the observable input $X$, given a target $y$~\cite{VAEs, GANs}. Recent advances in generative modeling have gained a significant amount of attention. In the GZSL setting, generative models are leveraged to learn to generate visual features or images for the unseen classes\cite{Su_2022_CVPR, Kong_2022_CVPR}. This is achieved using samples from the seen classes and semantic representations of both seen and unseen classes. Generative-based methods convert the GZSL problem into a supervised learning problem by generating samples for both seen and unseen classes.

\vspace{0.05in}
\textbf{Semantic disentanglement-based methods}, which aim to factorize the useful dimensions of a given visual feature to learn the attribute-visual alignment. The assumption is that the image features extracted from a ResNet101 that was pre-trained on Imagenet, broadly used in all traditional GSZL datasets~\cite{CUB, SUN, AWA2}, are not ideal for the zero-shot learning task~\cite{Tong2019HierarchicalDO, Chen2021FREE, SDGZSL}. Since these features are not representative with respect to the specific attributes that describe the image parts/composition, not all the dimensions of these features are semantically related to the given attributes; thus, it is necessary to factorize or disentangle the useful dimensions to avoid bias when trying to learn the attribute-visual alignment.
In this context, entanglement refers to the property of not having independence among attributes of one representation. In an entangled representation, all the factors of variation are mixed, and there is no explicit separation that represents the important characteristics in the images~\cite{Bengio2013RepresentationLA}. On the other hand, given an image dataset of birds such as the CUB dataset~\cite{CUB}, a disentangled representation may consist of separate dimensions for wing color, breast color, bill shape, tail pattern, crown color, wing pattern, etc~\cite{Eastwood2018AFF}.

