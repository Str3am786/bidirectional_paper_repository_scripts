\input{05.4_tables_cliponly}

\input{05.4.4.1_subset_methods_plus_clip}

\section{Results using Unimodal Feature Extractors}
\label{sec:results_unimodal}

We evaluate all GZSL methods trained with feature extractors pre-trained using ImageNet-1k. In Figure~\ref{fig:model_backbone_size_vs_method}, we show the Harmonic Mean performance of different methods when using a specific model to extract the features of image samples. Surprisingly, CADA-VAE~\cite{CADA_VAE} and tfVAEGAN~\cite{tfvaegan} consistently outperform all methods. While current disentanglement methods show significant improvements when using features from transformer-based architectures, they are outperformed by the generative-based methods.
% From Figure~\ref{fig:model_backbone_size_vs_method}, 
We can also observe that DINO~\cite{DINO} provides better
feature representations for all methods across all datasets.


We also want to evaluate the impact of using a feature extractor with the \textit{same architecture type but trained with different learning objectives}. In Figure~\ref{fig:model_backbone_diff_objectives}, we show the Harmonic Mean performance of
different GZSL methods when using a Resnet model architecture pre-trained on Imagenet-1k as the image feature extractor. Surprisingly, the features extracted from DINO~\cite{DINO} increase the Harmonic Mean performance up to 15\% in both fine-grained datasets (i.e., CUB and SUN datasets). More surprisingly, the feature vectors extracted from MOCO perform worse than traditional supervised learning models trained with a cross-entropy objective function. MOCO's training objective is formulated by the InfoNCE loss, which encourages the model to maximize the Mutual Information (MI) between $N$ random samples containing one positive sample, and minimize the Mutual information between the anchor sample and $N-1$ negative samples. 
On the other hand, in DINO, a teacher and a student model are trained by feeding two different random transformations of an input image to each network; the objective is to maximize the similarity between both outputs, which is encouraged and measured with a cross-entropy loss. 
Thus, the generalization capabilities shown with DINO support prior observations when using its image features in classification tasks with respect to other self-supervised techniques~\cite{DINO}.


We then evaluate the impact of using feature extractors that were pre-trained with more data variety and size and summarize our findings in Figure~\ref{fig:model_backbone_data_size}. Surprisingly, features extracted from backbones trained with more data (Imagenet-21k) do not always perform better, but features extracted from bigger architectures seem consistently better. 


We show more detailed results on CUB, SUN and AWA2 using GZSL methods grouped in their corresponding families in the following subsections.
Please refer to the Appendix to check the full list of numerical results for all uni-modal backbones and GZSL methods. 

 % \vspace{0.02in}
\subsection{Results of \textbf{Embedding}-based Methods}

The ViT$_{\text{huge}}$ features pre-trained on ImageNet-21k seem to be the best for all the methods using ALE. 
However, for the AWA2 dataset, all methods perform better using the features extracted from a network pre-trained using ImageNet-1k.
For CUB and SUN datasets, the performance gap against the features extracted from a network trained using ImageNet-1k and ImageNet-21k is not significant for all methods.
More detailed results are available in Tables~\ref{tab:cub_embedding_CNN}, \ref{tab:sun_embedding_CNN} and \ref{tab:awa2_embedding_CNN} from Section~\ref{sec:all_tables} in the Appendix. 

 
% \vspace{0.02in}
\subsection{Results of \textbf{Generative}-based Methods} 

The most performant visual features are extracted from a ViT$_{\text{huge}}$ pretrained on ImageNet-21k and fine-tuned with the seen classes, using the CADA-VAE method. More interestingly, the features from a ViT$_{\text{large}}$ pretrained on ImageNet-1k seem competitive with the features from a ViT$_{\text{large}}$ pretrained on ImageNet-21k for the CE and tfVAEGAN methods respectively.
% We show results in Table~\ref{tab:subset_generative_based} when using embedding based GZSL methods. 
More detailed results are available in Tables~\ref{tab:cub_generative_CNN}, \ref{tab:sun_generative_CNN} and \ref{tab:awa2_generative_CNN} from Section~\ref{sec:all_tables} in the Appendix. 

% \input{04.4_subset_tables_generative_noclip}
 % \vspace{0.02in}
\subsection{Results of \textbf{Disentanglement}-based Methods} 

% We show results in Table~\ref{tab:subset_disentanglement_based} when using disentanglement based GZSL methods. 
The most performant visual features are extracted from a ViT$_{\text{large}}$ pre-trained on ImageNet-1k using the SDGZSL method. 
Here, the features extracted from architectures pre-trained using ImageNet-21k perform worse than the ones pre-trained using ImageNet-1k, except for the CUB dataset.
More detailed results are available in Tables~\ref{tab:cub_disentanglement_CNN}, \ref{tab:sun_disentanglement_CNN} and \ref{tab:awa2_disentanglement_CNN} from Section~\ref{sec:all_tables} in the Appendix. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[hbt!]
\centering
\scalebox{0.90}{
\includegraphics[width=.3\textwidth]{Images/cub_results_on_finetuned_vitl14.pdf}\hfill
\hspace{1cm}
\includegraphics[width=.3\textwidth]{Images/sun_results_on_finetuned_vitl14.pdf}\hfill
\hspace{1cm}
\includegraphics[width=.3\textwidth]{Images/awa2_results_on_finetuned_vitl14.pdf}
}
% \vspace{-0.2in}
\caption{\textbf{Fine-tuning CLIP with seen samples}. Results of fine-tuning a CLIP model using the images and different text descriptions, including the class name, the attribute text, and combining both text captions. We show accuracy variations after 110k training iterations for CUB\cite{CUB}, SUN\cite{SUN} and AWA2\cite{AWA2}, where B$_S$ and B$_S$ indicate the base performance of seen and unseen classes without fine-tuning. C$_S$ refers to the seen classes, and C$_U$ refers to the unseen (novel) classes after fine-tuning.} 
\label{fig:finetuned_clip_accs}
\vspace{-0.1in}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Results using CLIP as a Multimodal Feature Extractor} 
\label{sec:results_multimodal}
% \vspace{-0.2in}
\textbf{Direct evaluation of CLIP} using the images and class names without any further pre-training or post-processing: we use different template captions to generate the textual descriptions (e.g., \textit{"An image of a [class name]"}) and chose to evaluate the best-performing model with the template captions proposed by OpenAI to test on Imagenet~\cite{openai_2022}.
% ~\footnote{\href{https://bit.ly/3mt7XhS}{https://bit.ly/3mt7XhS}}. 
We also evaluate a CLIP model trained with the publicly available LAION-400M dataset~\cite{LAION400M} in the publicly available visual transformer backbone (i.e.~ViTB32). We show results in Table~\ref{tab:all_datasets_clip_only}. Overall, we expected CLIP to perform well in all the selected datasets, even with CUB, whose class names are particularly specific; similar results on this dataset have been reported in concurrent work~\cite{Vogel2022VLTabooAA}. Moreover, our results show that generative-based models work on par and even outperform CLIP when using the Resnet101 fine-tuned features, indicating that there might be room for improvement. 


\textbf{Evaluation of CLIP performance after fine-tuning} using the class
names, the attribute values and a combination of class names
$\&$ attribute values: Figure~\ref{fig:finetuned_clip_accs} shows the accuracy
variations after fine-tuning CLIP for 110k iterations using different types of text prompts, using only the seen training set for each dataset. Interestingly, we observed that fine-tuning enhances both the seen and unseen accuracy.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[hbt!]
\centering
\scalebox{0.94}{
\includegraphics[width=.33\textwidth]{Images/cub_upperbound.pdf}\hfill
\hspace{0.5cm}
\includegraphics[width=.33\textwidth]{Images/sun_upperbound.pdf}\hfill
\hspace{0.5cm}
\includegraphics[width=.33\textwidth]{Images/awa2_upperbound.pdf}
}
\vspace{-0.25in}
\caption{\textbf{Fine-tuning diverse feature extractors with seen samples}. We show the difference between: a) the best-seen accuracy achieved among all GZSL methods, and b) the best-seen accuracy after training and evaluating the seen classes using a linear classifier probe. We use different visual features extracted from different backbones and training strategies. The classification upper-bound only increases significantly when fine-tuning the RN101\cite{RNs} features but remains the same with the ViT\cite{ViT} features regardless of the classification objective (i.e., cross-entropy vs. contrastive). }.
\label{fig:seen_acc_difference}
\vspace{-0.1in}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Evaluation of the GZSL methods using the visual features extracted from the visual encoder of CLIP}
% from (A) and (B), 
to train all GZSL methods: We show in Table~\ref{tab:gzsl_using_clip_features} the effect of using CLIP features and which fine-tuned model performs the best. Unsurprisingly, using both class name and class attributes outperforms other backbones. Similarly to uni-modal backbones, the generative-based models (e.g., CADA-VAE and tfVAEGAN) outperform other methods on all datasets. More surprisingly, both methods also outperform the CLIP results in CUB and SUN, but CLIP alone outperforms all methods in the AWA2 dataset. 
We run experiments using all eight visual encoders available from CLIP model, but show the most relevant results in Table~\ref{tab:gzsl_using_clip_features}. Please refer to the Appendix to check the full list of results.




\section{Fine-tuning Feature Extractors}


 
We observe that in the generative and disentanglement-based methods, there seems to be a trade-off in the multi-modal latent space, where some features from the seen set are distilled to the projected features of the unseen set. Typically, these methods augment the training set and convert the problem into a classification task, thus, the final seen accuracy is penalized by the classifier. 
For this reason, we also investigate how much these models penalize the final accuracy of the seen sets by measuring the difference of a classifier trained only using the seen set versus the best-seen accuracy achieved among all GZSL generative and disentanglement-based methods. 
The results are shown in Figure~\ref{fig:seen_acc_difference}. 
We observe that while fine-tuning a Resnet101 model increment the seen accuracy of both the classifier and the GZSL method, it does not increase the seen accuracy of recent models such as vision transformers. 
% Also, the improvement of the seen accuracy of the GZSL result does not improve substantially as it does with the Resnet101.
We also observe that the seen accuracy of the GZSL method does not improve substantially when using ViT features, 
% More surprisingly, 
and fine-tuning on AWA2 hurts the best seen accuracy on the GZSL result while not improving the classifier seen accuracy. 


