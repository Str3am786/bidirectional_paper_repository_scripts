\begin{table*}[!htbp]
\newcolumntype{Y}{>{\raggedright\arraybackslash}X}
\newcolumntype{Z}{>{\centering\arraybackslash}X}
\centering
\footnotesize
\setlength\tabcolsep{1pt}
\renewcommand{\arraystretch}{1.2}

\begin{tabularx}{\textwidth}{l c l c l c YYY c YYY c YYY}
\toprule

\multicolumn{17}{@{\hskip 0.11in}c}{\bf \shortstack{Generative Based GZSL Methods\\ CUB Dataset}}  \\ 
\midrule

{\multirow{2}{*}{\bf \shortstack{Dataset\\Pret. on}}}~ &~~~~&
{\multirow{2}{*}{\bf \shortstack{Arch\\ Type}}}~ &~~~~&
{\multirow{2}{*}{\bf Backbone}}~ &~~~~&
\multicolumn{3}{@{\hskip 0.11in}c}{\bf tfVAEGAN} &~~~~& 
\multicolumn{3}{@{\hskip 0.11in}c}{\bf CADA-VAE} &~~~~& 
\multicolumn{3}{@{\hskip 0.11in}c}{\bf CE} \\

\cmidrule{7-9}\cmidrule{11-13}\cmidrule{15-17}

&& && && \textit{Seen} & \textit{Novel} & \textit{Harm.} 
&& \textit{Seen} & \textit{Novel} & \textit{Harm.} 
&& \textit{Seen} & \textit{Novel} & \textit{Harm.} \\

\midrule

\multirow{17}{*}{I-1k} & &
\multirow{12}{*}{CNN} & &


RN101 &&
57.08 & 42.88 & 48.97 &&
58.27 & 49.71 & 53.65  &&
60.09 & \textbf{49.05} & 54.01  \\ 

&& &&\cellcolor{gray!18}RN101+FT &\cellcolor{gray!18}&
72.44 & 53.66 & 61.65 &&
76.45 & 57.53 & 65.65  &&
\cellcolor{gray!18}\textbf{76.71} &\cellcolor{gray!18} 48.81 &\cellcolor{gray!18} \textbf{59.66}  \\  

&& &&RN50 &&
49.29 & 42.35 & 45.55 &&
45.88 & 38.95 & 42.13  &&
42.79 & 35.46 & 38.78  \\ 

&& &&RN152 &&
50.23 & 44.48 & 47.18 &&
47.58 & 41.64 & 44.41  &&
45.77 & 35.52 & 40.00  \\ 

&& &&GoogleNet &&
38.54 & 33.34 & 35.76 &&
33.84 & 30.20 & 31.92  &&
33.72 & 26.05 & 29.39  \\ 

&& &&VGG16 &&
36.67 & 38.46 & 37.54 &&
37.12 & 35.38 & 36.23  &&
35.00 & 37.84 & 36.36  \\  

&& &&Alexnet &&
21.48 & 32.52 & 25.87 &&
22.36 & 24.31 & 23.29  &&
23.73 & 28.62 & 25.95  \\  

&& &&Shufflenet &&
51.62 & 43.83 & 47.41 &&
43.14 & 38.56 & 40.72  &&
48.95 & 37.69 & 42.59  \\ 

&& &&Inceptionv3 &&
54.01 & 50.41 & 52.15 &&
50.32 & 39.87 & 44.49  &&
54.80 & 38.45 & 45.19  \\  

&& &&Inceptionv3$_{\text{adv}}$ &&
62.81 & 41.34 & 49.86 &&
50.28 & 37.90 & 43.22  &&
54.83 & 35.41 & 43.03  \\  

\cmidrule{5-17}
&& &&RN50-MOCO$^{\dag}$ &&
41.88 & 29.13 & 34.36 &&
27.40 & 22.39 & 24.64  &&
34.01 & 24.09 & 28.21  \\  

&& &&RN50-DINO$^{\dag}$ &&
64.11 & 53.84 & 58.53 &&
55.05 & 47.59 & 51.05  &&
62.45 & 45.13 & 52.39  \\

\cmidrule{3-17}

&& MLP&&MLP-Mixer && 
18.64 & 15.87 & 17.15 &&
11.74 & 18.45 & 14.35  &&
10.99 & 10.94 & 10.97  \\  

\cmidrule{3-17}

&& \multirow{3}{*}{ViT} &&ViT$_{\text{large}}$&&
\textbf{80.34} & 54.34 & 64.83 &&
61.23 & 53.31 & 56.99  &&
70.22 & 43.07 & 53.39  \\

&& &&DeiT$_{\text{base}}$ && 
73.29 & 49.44 & 59.05 &&
60.39 & 50.05 & 54.74  &&
55.68 & 38.68 & 45.65  \\  

&& &&ViTB16-DINO$^{\dag}$&& 
76.82 & 57.94 & 66.06 &&
71.95 & 55.37 & 62.58  &&
61.29 & 45.47 & 52.21  \\

\midrule

\multirow{5}{*}{I-21k}
&& MLP && 
MLP-Mixer$_{\text{L16}}$ & &
30.91 & 28.77 & 29.80 &&
28.68 & 25.19 & 26.82  &&
17.46 & 20.76 & 18.96  \\

\cmidrule{3-17}

&& \multirow{3}{*}{ViT} && ViT$_{\text{base}}$ & &
74.16 & 71.13 & 72.61 &&
74.46 & 60.77 & 66.93  &&
61.01 & 51.25 & 55.71  \\

&& && ViT$_{\text{large}}$ & &
76.95 & 61.56 & 68.40 &&
72.54 & 58.94 & 65.04  &&
67.16 & 46.94 & 55.26  \\ 

&& && ViT$_{\text{huge}}$ & &
75.15 & 62.76 & 68.40 &&
70.53 & 60.50 & 65.13  &&
49.37 & 43.76 & 46.40  \\

&& &&\cellcolor{gray!18} ViT$_{\text{huge}}$+FT &\cellcolor{gray!18} &
\cellcolor{gray!18}78.32 &\cellcolor{gray!18} \textbf{76.26} &\cellcolor{gray!18} \textbf{77.27} &\cellcolor{gray!18}&
\cellcolor{gray!18}\textbf{77.99} &\cellcolor{gray!18} \textbf{74.46} &\cellcolor{gray!18} \textbf{76.18}  &&
70.87&	44.66 &	54.79 \\


\bottomrule
\end{tabularx}
%\vspace{-0.05in}
\caption{Results of Generative Based Methods for the CUB dataset using different features extracted from a diverse set of architecture types pretrained on ImageNet-1k (I-1k) and ImageNet-21k (I-21k). These backbones were trained via: supervised and self-supervised (${\dag}$) learning. The bold numbers correspond to the highest scores per column, and the shaded rows correspond to the most performant image feature per method. +FT indicates the features were fine-tuned with the seen classes from the training set. The most performant visual features are extracted from a ViT$_{\text{huge}}$ pretrained on ImageNet-21k and fine-tuned with the seen classes, using the tfVAEGAN method.
}
\label{tab:cub_generative_CNN}
% \vspace{-0.1in}
\end{table*}


\begin{table*}[!htbp]
\newcolumntype{Y}{>{\raggedright\arraybackslash}X}
\newcolumntype{Z}{>{\centering\arraybackslash}X}
\centering
\footnotesize
\setlength\tabcolsep{1pt}
\renewcommand{\arraystretch}{1.2}

\begin{tabularx}{\textwidth}{l c l c l c YYY c YYY c YYY}
\toprule

\multicolumn{17}{@{\hskip 0.11in}c}{\bf \shortstack{Generative Based GZSL Methods\\ SUN Dataset}}  \\ 
\midrule

{\multirow{2}{*}{\bf \shortstack{Dataset\\Pret. on}}}~ &~~~~&
{\multirow{2}{*}{\bf \shortstack{Arch\\ Type}}}~ &~~~~&
{\multirow{2}{*}{\bf Backbone}}~ &~~~~&
\multicolumn{3}{@{\hskip 0.11in}c}{\bf tfVAEGAN} &~~~~& 
\multicolumn{3}{@{\hskip 0.11in}c}{\bf CADA-VAE} &~~~~& 
\multicolumn{3}{@{\hskip 0.11in}c}{\bf CE} \\

\cmidrule{7-9}\cmidrule{11-13}\cmidrule{15-17}

&& && && \textit{Seen} & \textit{Novel} & \textit{Harm.} 
&& \textit{Seen} & \textit{Novel} & \textit{Harm.} 
&& \textit{Seen} & \textit{Novel} & \textit{Harm.} \\

\midrule

\multirow{17}{*}{I-1k} & &
\multirow{12}{*}{CNN} & &

\cellcolor{gray!18}RN101 &\cellcolor{gray!18}&
38.95 & 45.62 & 42.03  & ~ &
34.15 & 48.96 & 40.23  &\cellcolor{gray!18} ~ &
\cellcolor{gray!18}51.24 &\cellcolor{gray!18} \textbf{55.83} &\cellcolor{gray!18} \textbf{53.44}  \\ 

&& &&RN101+FT &&
35.08 & 38.06 & 36.51  & ~ &
39.84 & 51.60 & 44.97  & ~ &
29.07 & 38.75 & 33.22  \\ 

&& &&RN50 &&
34.61 & 45.07 & 39.15  & ~ &
34.07 & 41.53 & 37.43  & ~ & 23.95 & 42.36 & 30.60  \\

&& &&RN152 &&
35.35 & 45.97 & 39.97  & ~ &
37.05 & 40.00 & 38.47  & ~ & 26.20 & 43.33 & 32.66  \\ 

&& &&GoogleNet &&
27.17 & 38.26 & 31.78  & ~ &
24.96 & 36.32 & 29.59  & ~ &
20.43 & 38.96 & 26.80  \\ 

&& &&VGG16 &&
25.04 & 28.61 & 26.71  & ~ &
31.32 & 37.85 & 34.27  & ~ &
24.11 & 36.81 & 29.13  \\ 

&& &&Alexnet &&
25.27 & 34.72 & 29.25  & ~ &
16.51 & 23.06 & 19.24  & ~ &
13.84 & 30.90 & 19.12  \\ 

&& &&Shufflenet &&
31.59 & 42.71 & 36.32  & ~ &
30.62 & 37.64 & 33.77  & ~ &
24.92 & 37.50 & 29.94  \\

&& &&Inceptionv3 &&
30.00 & 32.15 & 31.04  & ~ &
32.64 & 38.26 & 35.23  & ~ &
26.09 & 37.43 & 30.74  \\

&& &&Inceptionv3$_{\text{adv}}$ &&
33.37 & 45.42 & 38.47  & ~ &
32.02 & 40.83 & 35.89  & ~ &
27.09 & 39.44 & 32.12  \\ 

\cmidrule{5-17}
&& &&RN50-MOCO$^{\dag}$ &&
37.44 & 42.99 & 40.02  & ~ &
35.08 & 38.13 & 36.54  & ~ &
30.62 & 44.24 & 36.19  \\ 

&& &&RN50-DINO$^{\dag}$ &&
42.60 & 46.67 & 44.54  & ~ &
41.16 & 46.39 & 43.62  & ~ &
29.38 & 55.56 & 38.43  \\

\cmidrule{3-17}

&& MLP&&MLP-Mixer && 
24.96 & 32.22 & 28.13  & ~ &
6.82 & 12.78 & 8.89  & ~ &
8.64 & 8.33 & 8.49  \\ 

\cmidrule{3-17}

&& \multirow{3}{*}{ViT} &&\cellcolor{gray!18}ViT$_{\text{large}}$&\cellcolor{gray!18}&
\cellcolor{gray!18}55.12 &\cellcolor{gray!18} \textbf{64.93} &\cellcolor{gray!18} \textbf{59.62}  &\cellcolor{gray!18} ~ &
\cellcolor{gray!18}\textbf{52.64} &\cellcolor{gray!18} \textbf{61.32} &\cellcolor{gray!18} \textbf{56.65}  & ~ &
50.70 & 52.78 & 51.72  \\

&& &&DeiT$_{\text{base}}$ && 
34.34 & 44.72 & 38.85  & ~ &
37.44 & 44.51 & 40.67  & ~ &
36.36 & 37.57 & 36.95  \\

&& &&ViTB16-DINO$^{\dag}$&& 
42.64 & 52.71 & 47.14  & ~ &
42.52 & 51.18 & 46.45  & ~ &
40.39 & 45.56 & 42.82  \\

\midrule

\multirow{5}{*}{I-21k}
&& MLP && 
MLP-Mixer$_{\text{L16}}$ & &
24.22 & 43.03 & 28.30  & ~ &
24.77 & 29.65 & 26.99  & ~ &
23.84 & 20.00 & 21.75  \\

\cmidrule{3-17}

&& \multirow{3}{*}{ViT} && ViT$_{\text{base}}$ & &
54.19 & 58.75 & 56.38  & ~ &
52.02 & 60.14 & 55.78  & ~ &
\textbf{51.53} & 50.74 & 51.13  \\

&& && ViT$_{\text{large}}$ & &
\textbf{57.13} & 61.32 & 59.15  & ~ &
52.83 & 60.69 & 56.49  & ~ &
50.12 & 50.69 & 50.40  \\

&& && ViT$_{\text{huge}}$ & &
43.37 & 53.61 & 47.95  & ~ &
47.64 & 51.18 & 49.34  & ~ &
49.79 & 16.05 & 24.27  \\

&& && ViT$_{\text{huge}}$+FT & &
44.26 & 54.24 & 48.75  & ~ &
44.99 & 55.35 & 49.64  & ~ &
6.86 & 53.61 & 12.16  \\


\bottomrule
\end{tabularx}
%\vspace{-0.05in}
\caption{Results of Generative Based Methods for the SUN dataset using different features extracted from a diverse set of architecture types pretrained on ImageNet-1k (I-1k) and ImageNet-21k (I-21k). These backbones were trained via: supervised and self-supervised (${\dag}$) learning. The bold numbers correspond to the highest scores per column, and the shaded rows correspond to the most performant image feature per method. +FT indicates the features were fine-tuned with the seen classes from the training set. Surprisingly, the CE method does not seem to get any significant advantage from any of the ViT features, and overall, the most performant visual features are extracted from a ViT$_{\text{large}}$ pretrained on ImageNet-1k using the tfVAEGAN method.
}
\label{tab:sun_generative_CNN}
% \vspace{-0.1in}
\end{table*}

\begin{table*}[!htbp]
\newcolumntype{Y}{>{\raggedright\arraybackslash}X}
\newcolumntype{Z}{>{\centering\arraybackslash}X}
\centering
\footnotesize
\setlength\tabcolsep{1pt}
\renewcommand{\arraystretch}{1.2}

\begin{tabularx}{\textwidth}{l c l c l c YYY c YYY c YYY}
\toprule

\multicolumn{17}{@{\hskip 0.11in}c}{\bf \shortstack{Generative Based GZSL Methods\\ AWA2 Dataset}}  \\ 
\midrule

{\multirow{2}{*}{\bf \shortstack{Dataset\\Pret. on}}}~ &~~~~&
{\multirow{2}{*}{\bf \shortstack{Arch\\ Type}}}~ &~~~~&
{\multirow{2}{*}{\bf Backbone}}~ &~~~~&
\multicolumn{3}{@{\hskip 0.11in}c}{\bf tfVAEGAN} &~~~~& 
\multicolumn{3}{@{\hskip 0.11in}c}{\bf CADA-VAE} &~~~~& 
\multicolumn{3}{@{\hskip 0.11in}c}{\bf CE} \\

\cmidrule{7-9}\cmidrule{11-13}\cmidrule{15-17}

&& && && \textit{Seen} & \textit{Novel} & \textit{Harm.} 
&& \textit{Seen} & \textit{Novel} & \textit{Harm.} 
&& \textit{Seen} & \textit{Novel} & \textit{Harm.} \\

\midrule

\multirow{17}{*}{I-1k} & &
\multirow{12}{*}{CNN} & &

RN101 &&
75.48 & 59.56 & 66.58  & ~ &
75.95 & 54.76 & 63.87  & ~ &
69.26 & 56.03 & 61.95  \\

&& &&RN101+FT &&
84.81 & 58.44 & 69.20  & ~ &
77.74 & 59.95 & 69.14  & ~ &
83.43 & 47.66 & 60.66  \\ 

&& &&RN50 &&
79.91 & 55.83 & 65.73  & ~ &
74.56 & 59.48 & 67.80  & ~ &
71.72 & 46.65 & 56.53  \\

&& &&RN152 &&
84.48 & 60.01 & 70.17  & ~ &
\textbf{88.88} & 57.69 & 70.48  & ~ &
76.74 & 40.25 & 52.8  \\

&& &&GoogleNet &&
69.08 & 55.17 & 61.35  & ~ &
71.51 & 53.22 & 62.22  & ~ &
73.36 & 47.63 & 57.76  \\

&& &&VGG16 &&
78.95 & 52.54 & 63.09  & ~ &
74.17 & 58.92 & 67.26  & ~ &
62.37 & 46.85 & 53.51  \\ 

&& &&Alexnet &&
64.31 & 41.11 & 50.16  & ~ &
61.98 & 40.80 & 49.60  & ~ &
57.13 & 39.39 & 46.63  \\  

&& &&Shufflenet &&
69.53 & 55.38 & 61.66  & ~ &
68.07 & 54.09 & 61.84  & ~ &
71.87 & 50.11 & 59.05  \\

&& &&Inceptionv3 &&
84.00 & 58.45 & 68.94  & ~ &
78.97 & 61.77 & 70.86  & ~ &
74.69 & 52.09 & 61.38  \\ 

&& &&Inceptionv3$_{\text{adv}}$ &&
85.48 & 59.21 & 69.96  & ~ &
82.15 & 53.43 & 65.23  & ~ &
66.96 & 53.73 & 59.62  \\  

\cmidrule{5-17}
&& &&RN50-MOCO$^{\dag}$ &&
68.51 & 51.75 & 58.96  & ~ &
65.52 & 55.53 & 62.02  & ~ &
64.77 & 40.92 & 50.15  \\  

&& &&RN50-DINO$^{\dag}$ &&
74.58 & 60.00 & 66.50  & ~ &
73.36 & 55.23 & 64.30  & ~ &
74.22 & 45.46 & 56.38  \\

\cmidrule{3-17}

&& MLP&&MLP-Mixer && 
28.14 & 25.27 & 26.63  & ~ &
14.01 & 41.56 & 27.90  & ~ &
22.93 & 20.68 & 21.75  \\

\cmidrule{3-17}

&& \multirow{3}{*}{ViT} &&\cellcolor{gray!18}ViT$_{\text{large}}$&\cellcolor{gray!18}&
90.14 & 68.28 & 77.70  & ~ &
85.55 & 70.62 & 79.25  &\cellcolor{gray!18} ~ &
\cellcolor{gray!18}81.37 &\cellcolor{gray!18} \textbf{69.14} &\cellcolor{gray!18} \textbf{74.75}  \\

&& &&DeiT$_{\text{base}}$ && 
84.63 & 51.89 & 64.33  & ~ &
77.83 & 59.04 & 68.50  & ~ &
79.35 & 50.62 & 61.81  \\ 

&& &&ViTB16-DINO$^{\dag}$&& 
77.64 & 57.77 & 66.24  & ~ &
75.69 & 63.90 & 71.25  & ~ &
81.66 & 54.65 & 65.48  \\

\midrule

\multirow{5}{*}{I-21k}
&& MLP && 
MLP-Mixer$_{\text{L16}}$ & &
72.63 & 42.47 & 53.60  & ~ &
70.15 & 51.01 & 60.12  & ~ &
61.51 & 39.01 & 47.74  \\

\cmidrule{3-17}

&& \multirow{3}{*}{ViT} && ViT$_{\text{base}}$ & &
54.19 & 58.75 & 56.38  & ~ &
84.48 & 67.42 & 76.67  & ~ &
77.80 & 49.54 & 60.53  \\

&& &&\cellcolor{gray!18} ViT$_{\text{large}}$ &\cellcolor{gray!18} &
\cellcolor{gray!18}\textbf{91.05} &\cellcolor{gray!18} \textbf{63.58} &\cellcolor{gray!18} \textbf{74.87}  &\cellcolor{gray!18} ~ &
\cellcolor{gray!18}88.69 &\cellcolor{gray!18} \textbf{70.75} &\cellcolor{gray!18} \textbf{80.40}  & ~ &
\textbf{78.32} & 59.21 & 67.44  \\

&& && ViT$_{\text{huge}}$ & &
88.85 & 60.79 & 72.19  & ~ &
85.68 & 60.95 & 72.25  & ~ &
75.57 & 53.24 & 62.47  \\

&& && ViT$_{\text{huge}}$+FT & &
68.23 & 61.63 & 64.76  & ~ &
80.69 & 60.76 & 69.32  & ~ &
75.23 & 60.10 & 66.82  \\


\bottomrule
\end{tabularx}
%\vspace{-0.05in}
\caption{Results of Generative Based Methods for the AWA2 dataset using different features extracted from a diverse set of architecture types pretrained on ImageNet-1k (I-1k) and ImageNet-21k (I-21k). These backbones were trained via: supervised and self-supervised (${\dag}$) learning. The bold numbers correspond to the highest scores per column, and the shaded rows correspond to the most performant image feature per method. +FT indicates the features were fine-tuned with the seen classes from the training set. The most performant visual features are extracted from a ViT$_{\text{huge}}$ pretrained on ImageNet-21k and fine-tuned with the seen classes, using the CADA-VAE method. More interestingly, the features from a ViT$_{\text{large}}$ pretrained on ImageNet-1k seem competitive with the features from a ViT$_{\text{large}}$ pretrained on ImageNet-21k for the CE and tfVAEGAN methods respectively.
}
\label{tab:awa2_generative_CNN}
% \vspace{-0.1in}
\end{table*}