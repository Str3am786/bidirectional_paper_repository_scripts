\section{Conclusion}

In this paper, we provide strong empirical evidence that indicates that:

\begin{compactitem}
\item Using Transformer based architectures provides superior feature representation capabilities while not violating the zero-shot principle of being pre-trained on unseen classes.

\item The feature representations extracted from unimodal architectures that were pre-trained on larger datasets (e.g., ImageNet-21k) do not necessarily boost GZSL performance.

\item Using Convolutional based architectures pre-trained without labels, using contrastive learning and self-distillation, provides better feature representations for GZSL than models trained using supervised learning, with known labels and cross-entropy loss alone.

\item Fine-tuning does not significantly impact the performance on Transformer based unimodal backbones but may boost the performence on multimodal backbones.

\item Multimodal architectures trained on internet-scale large data (CLIP) still benefit from generative based GZSL methods to achieve state-of-the art performance in CUB and SUN, which are fine-grained datasets. This may indicate that feature representations from CLIP are more suitable for GZSL when there is less inter-class correlation among data samples.

\item Fine-tuning a CLIP model using prompts including the class names and attributes 
% with the training samples 
from the seen categories also boosts the ranking performance of the unseen classes.
\end{compactitem}

% \vspace{-0.4in}
In summary, our work provides an update on GZSL methods in the era of large-scale multi-modal pre-training, and re-evaluates in this context the progress that has been made so far in this area.
We release a well-documented codebase that both replicates our findings and provides a modular framework for further feature representation explorations to the GZSL task with recent large pre-trained models.