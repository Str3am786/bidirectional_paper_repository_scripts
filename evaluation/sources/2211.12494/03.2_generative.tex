\subsection{Generative-based Methods}


\textit{Generalized Zero- and Few-Shot Learning via Aligned Variational Autoencoders (\textbf{CADA-VAE})}~\cite{CADA_VAE} (2019) use two variational autoencoders (VAEs~\cite{VAEs}) to align the visual and semantic features by learning a shared latent space between both modalities. Then, these VAEs are used to synthesize a large number of seen and unseen features, which are then used to train a classifier.

\textit{Latent Embedding Feedback and Discriminative
Features for Zero-Shot Classification (\textbf{tfVAEGAN})}~\cite{tfvaegan} (2020) propose to use a feedback module in a model that combines a variational autoencoder (VAE~\cite{VAEs}) and a generative adversarial network (GAN~\cite{GANs}) to modulate the latent representation of the generator. They propose to enforce semantic consistency by introducing a feedback loop from the semantic embedding decoder. They propose to use both synthesized features and latent embeddings during classification.


% \qnote{should the CE method in the "embedding" category? why is it "generative"?} 
% \pnote{for now I'm letting it sit here, I'll try to add the contrastive approach as a new "family" of GZSL methods; the authors of CE define it as a hybrid method}

More recently, hybrid methods such as \textit{Contrastive Embedding for Generalized Zero-Shot Learning (\textbf{CE})}~\cite{CE} (2021) have emerged. This method proposes to integrate the generation model with the embedding model to map 
the real and the synthetic samples produced by the generation model into an embedding space. To do this, they leverage a contrastive loss that learns to discriminate between one
positive sample and a large number of negative samples from different semantic descriptor classes. They claim that the original visual feature space is suboptimal for GZSL classification since it lacks discriminative information, which they aim to learn using the contrastive objective.

