\vspace{-0.05in}
\section{Generalized Zero-Shot Learning}

In the GZSL setting, we define $S$ as the set of seen classes within $N_s$ categories and $Y_s$ as their corresponding labels. We also define $U$ as the set of unseen classes with $N_u$ categories and $Y_u$ as their corresponding labels. Here, $S$ and $U$ have no intersection. 
Each set consists of image-features $x$, class labels $y$ available during training and class-embeddings $c(y)$. 
Thus, $S = \{ {x_{i}^{s}, y_{i}^{s}) \} }^{N_{s}}_{i=1}$ and $U = \{ {x_{i}^{u}, y_{i}^{u}) \} }^{N_{u}}_{i=1}$. 
Additionally, we have access to a set of semantic descriptions of the seen and unseen classes $A = \{ {a_{i}^{s} \} }^{N_{s} + N_{u} }_{i=1}$, which are typically class-embeddings vectors of hand-annotated  attributes. The image features are typically extracted from a feature backbone (i.e. ResNet101 pretrained on ImageNet-1K).
When training, we use $A$ along with the visual features and labels from the seen set (i.e., $ \{ {x_{i}^{s}, y_{i}^{s} \} }^{N_{s}}_{i=1}$) and only the labels from the unseen set (i.e., $ \{ y_{i}^{u} \} ^{N_{u}}_{i=1}$). Finally, the test set contains image-features from both $S$ and $U$, and their corresponding labels.
This section presents and describes all the methods we include in this study grouped by their corresponding family of methods. These can be characterized as: embedding-based, generative-based and disentanglement-based. We also present all the datasets we use in our study. Additional training and computational details are included in the Appendix.

