
\subsection{Unimodal Feature Backbones}




Overall, the pool of networks in this study have been trained using two learning objectives: 
\textit{Supervised Learning} (SL), which aims to learn a function that maps an input to a \textit{known output}, and is typically trained using a \textit{cross-entropy} loss (e.g., Resnet101~\cite{RNs}) or \textit{distillation} (e.g., DeiT~\cite{DeiT}). On the other hand, \textit{Self-Supervised Learning} (SSL), aims to learn a function that maps an input to a \textit{unknown output}; SSL can be accomplished using a \textit{contrastive loss} such as InfoNCE (e.g., MoCo~\cite{MoCo}) or adding \textit{distillation} along with a similarity metric measured with a cross-entropy loss applied over the features of two different random transformations of an input image (e.g., DINO~\cite{DINO}). 
We perform experiments on three broad types of unimodal architectures trained on images only (Imagenet-1k or Imagenet-21k):
\textit{Convolutional Neural Networks} (CNN)~\cite{Alexnet} trained using the images as a whole under SL or SSL, 
\textit{Vision Transformers} (ViT)~\cite{ViT} which takes an image, transforms it in a sequence of image patches and is trained using either SL or SSL; and
\textit{Multi-Layer Perceptron Mixers} (MLP)~\cite{MLPMixer} which also exploit image patches and are trained with supervision.

We chose the most performant models and fine-tuned them using only the training samples from the seen classes. Since the visual features are extracted from a diverse set of architectures trained in the wild, we focus on the \textit{inductive} setting, where there is no access to the unlabeled visual data from the unseen classes. In this way, we mitigate any bias reinforced by additional training of the visual representations. 
This practice has been followed to achieve better results in prior literature (which only uses Resnet101 features); however, there are no available reports for all the methods using these features; thus, we run all methods and report our findings in Section~\ref{sec:results_unimodal}.



\subsection{Multimodal Feature Backbones}

In addition, we study CLIP~\cite{CLIP}, a multi-modal model with a visual encoder and a textual encoder trained with 400 million image/text pairs in a contrastive way. Traditionally, all GZSL methods disregard the attribute values as they are given for granted, and no additional analysis is performed. We instead take a closer look at these semantic features and their corresponding attributes (e.g., color, shape, type of animal, type of place, etc)  and use them to fine-tune several pre-trained CLIP models. 

We perform three experiments with CLIP: 
% \begin{compactitem}
% \item 
(A) We directly evaluate the model using the images and class names without any further pre-training or post-processing by directly looking at the ranking the model yields for each seen and unseen samples,
% \item 
(B) We fine-tune CLIP using the class names, the attribute values and a combination of class names + attribute values, and evaluate its performance, 
% \item 
(C) We extract the visual features from the CNN and ViT based visual backbones available in their public repository, and use the features extracted from (A) and (B) to train the GZSL methods.
We go over our findings in Section~\ref{sec:results_multimodal}.
% \end{compactitem}


%%%% %%%% %%%% %%%% %%%% %%%% %%%% %%%% %%%% %%%% %%%%
%%%% %%%% %%%% %%%% %%%% %%%% %%%% %%%% %%%% %%%% %%%%






