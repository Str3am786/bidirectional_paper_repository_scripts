\section{Preliminaries}
\label{sec:preliminaries}

Here we provide a holistic view of VideoQA by summarizing a common paradigm throughout existing works. Specifically, we denote a variable and its deterministic value by upper-cased (\eg $A$) and lower-cased (\eg $a$) letters, respectively. 

\vspace{5pt}
\noindent\textbf{Modeling}.
Given the video $V$, the VideoQA model $f_{\hat{A}}{(\cdot)}$ answers the question $Q$ by formulating the visual-linguistic alignment: 
\begin{gather}\label{eq:conventional-modeling}\
    \hat{A} = f_{\hat{A}}(V,Q),
\end{gather}
where $\hat{A}$ is the predictive answer. Typically, $f_{\hat{A}}{(\cdot)}$ is a combination of two modules: 
\begin{itemize}[leftmargin=*]
    \item Video-question encoder, which warps up the visual content and linguistic semantics via two encoders: (1) the video encoder capsules the video content by methods like hierarchical design \cite{DBLP:conf/mm/PengYBW21, dang2021hierarchical, le2021hierarchical}, enhanced memory architecture \cite{gao2018motionappearance, fan2019heterogeneous} and structural graph representation  \cite{jiang2020reasoning,huang2020locationaware,DBLP:conf/acl/GuoZJ0L20, Wang_2018_ECCV}; (2) the question encoder embeds the contextual information into linguistic representation through multi-scale semantic integration \cite{jiang2020reasoning, DBLP:conf/acl/SeoKPZ20, 2021} or grammatical dependencies parsing \cite{park2021bridge}.
    
    \item Answer decoder, which abridges the encoded visual-linguistic information via cross-modal interaction methods like graph alignment \cite{ park2021bridge} and progressive attention \cite{DBLP:conf/acl/SeoKPZ20, DBLP:conf/mm/PengYBW21}, then generates the prediction accordingly.
    % \item Answer decoder, which abridges the visual-linguistic information to generate the prediction. Based on the encoded representation, the cross-modal interaction is learned via graph alignment \cite{ park2021bridge} or progressive attention \cite{DBLP:conf/acl/SeoKPZ20, DBLP:conf/mm/PengYBW21}.
\end{itemize}

\vspace{5pt}
\noindent\textbf{Learning}.
\wx{To optimize the video-question encoder and answer decoder, current VideoQA models usually adopt the scheme of empirical risk minimization (ERM) \cite{gao2018motionappearance,le2021hierarchical,jiang2020reasoning,DBLP:conf/mm/PengYBW21}, which measures and minimizes the risk between the ground-truth answer $A$ and predictive answer $\hat{A}$:}
\begin{gather}\label{equ:erm-loss}
    \min\mathcal{L}_{\text{ERM}}(\hat{A}, A).
\end{gather}
In essence, ERM recklessly takes the video content as a whole and enforces the risk deduction over compassion of question and every video frame, which \wx{hardly discovers a reliable interpretation to exhibit the visual-linguistic alignment}.

% In essence, ERM encourages these VideoQA modules to capture the statistical correlations between answer and video content as a whol

% where $\mathcal{L}_{\text{ERM}}$ is the risk function that measure the entropy between the predictive answer $\hat{A}$ and ground-truth answer $A$, which is usually set as cross-entropy loss  \cite{gao2018motionappearance,le2021hierarchical} or hinge loss \cite{fan2019heterogeneous,jiang2020reasoning,DBLP:conf/cvpr/XiaoSYC21}.
% % ERM is consistent with the criterion of mutual information maximization, which maximizes the mutual information between .
% In essence, ERM encourages these VideoQA modules to capture the statistical correlations between the video-question pairs and answers.





