\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[dvipsnames]{xcolor}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\newcommand{\wx}[1]{{\color{green}{#1}}}
\newcommand{\blue}[1]{{\color{blue}{#1}}}
\newcommand{\pink}[1]{{\color{magenta}{#1}}}
% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
% \setcounter{table}{4}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter for `enumiv' to
% the number of references you have in the main paper (here, 6).
%\let\oldthebibliography=\thebibliography
%\let\oldendthebibliography=\endthebibliography
%\renewenvironment{thebibliography}[1]{%
%     \oldthebibliography{#1}%
%     \setcounter{enumiv}{6}%
%}{\oldendthebibliography}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{7878} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}
\input{preamble}
\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Invariant Grounding for Video Question Answering}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}
\appendix

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW

We gratefully thank all the reviewers for their valuable and constructive comments. We are encouraged that they find our topic interesting and important (R2, R3, R4, R6), our idea novel and insightful (R2, R3, R4, R6), our method effective (R2, R3, R4), and our representation clear and enjoyable-to-read (R1, R4). We receive a \emph{strong accept}, two \emph{weak accept}, and a \emph{weak reject}. We will carefully revise our paper by taking into account all the suggestions. Please find the point-to-point responses as follows.

% Thank all reviewers for their efforts and \wx{insightful comments}. We are glad that all reviewers acknowledge our novelty and motivation. We receive a \textit{strong accept}, two \textit{weak accept}, and a \textit{weak reject}.


% We thank the review from R4, but R4 mainly focuses on the expression within a single sentence or table, rather than the gist of our principle. Since we are  revisiting VideoQA from a brand-new angle, one may find these delicate causal concepts cumbersome, while others like R2 enjoy reading. We apologize for the roughness and appreciate R4's efforts. However, according to the CVPR reviewer's guideline, ``we recommend you embrace the novel, brave concept'', and the presentation details should not be the reason for rejection. Nevertheless, we will polish the language toward general satisfaction. 


\vspace{2pt}
\noindent
$\triangleright$\pink{Reviewer \#2.} \blue{Q1: The sampling strategy of complement stratification (\cf Line 457).}
A1: Thanks. For simplicity, we randomly sample the complement stratification from a memory bank to pair the grounded causal scene, to perform the causal interventions.
\blue{Q2: Visualization of causal and complement scenes predicted by Equation (9).}
A2: Thanks for the suggestions. In Figure 5 of our paper, we have showcased the visualizations, where the causal scenes are highlighted and the rests are the complement scenes. Furthermore, we have offered analyses in Section 5.3, which validate the explainability of our IGV method.


\vspace{2pt}
\noindent
$\triangleright$\pink{Reviewer \#3.}
\blue{Q1: Optimization of the scene intervener module.}
Thanks. Actually, the scene intervener module is non-parametric, \ie no model parameters are needed to optimize. It is the combination of two components: (1) a complement collector, which collects the complement estimations of videos appearing in the previous batch, and (2) a complement stratification sampler, which randomly yields complement stratification to intervene the videos in the current batch.
With the improvements of causal grounding, this module will be updated with high-quality complements.
We will clarify this point in the final paper.
\blue{Q2: Effect of the causal grounding result on the performance.}
Great catch! The modules of causal grounding and scene intervener aim for the shared goal of distinguishing the causal scenes from the complement scenes, thus playing the cooperative game and mutually improving each other.
That is, with the improvements of causal grounding, the scene intervener module will capture high-quality complements; in turn, under the guidance of the scene intervener, the causal grounding module will shape the grounding.
% In the final paper, we will showcase the progressive improvements of these two modules via qualitative and quantitative studies.

\vspace{2pt}
\noindent
$\triangleright$\pink{Reviewer \#4.}
\blue{Q1: Relation \& Difference between causal look and statistical learning.}
Great catch! In general, causal inference and statistical inference are distinct \cite{pearl2009causal}.
At the core of statistical inference (\eg regression, estimation, hypothesis test) is to assess the distribution from samples. As long as the distribution remains the same, statistical learning techniques (\eg CNN) perform well.
Causal inference goes one step further: its goal is to infer not only probabilities under static conditions, but also the causal relationship under changing conditions (\eg intervention, counterfactual).
Specifically, statistical analysis cannot measure one property of distribution that ought to vary when another property is modified. In contrast, causality identifies relationships that remain invariant when conditions change (\eg generalization ability).
Meanwhile, causal inference and statistical inference are also highly connected --- solving causal problems requires the standard statistical language with a paradigmatic shift.
\blue{Q2: Relation \& Difference between intervened and adversarial scenes.}
Good point! The scene intervention can be viewed as causality-guided data augmentation, where the intervened complements can result from either causal intervention or adversarial learning.
Here we embraced the former so that the framework can be trained in a simple, end-to-end, and effective fashion.
\blue{Performance on TGIF-QA.}
Thanks. We present the performance on TGIF-QA as follows:
\vspace{-10pt}
\input{tab/tgif-rebuttal}

\vspace{9pt}
\noindent
$\triangleright$\pink{Reviewer \#6.}
\blue{Q1: Typos.}
A1: Thanks for bringing them to us. We sincerely appreciate and promise to revise thoroughly.
\blue{Q2: Frame- \& Region-level Grounding.}
A2: Thanks. In Section 7 of our paper (\cf Line 858), we have stated the frame-level grounding's limitations and listed the region-level grounding as our future work.
Here are more clarifications: our current work aims to present and verify a novel training paradigm for VideoQA in general, instead of the effect of feature granularity; moreover, as most of the SOTA VideoQA models adopt the frame-level reasoning, our IGV stays with this tendency for adaptability and consistency.
\blue{Q3: Shared video encoder with grounding indicator.}
A3: Thanks. To reduce the computation cost, we use the shared video encoder in the grounding indicator. Difference encoders might be more effective, which is for our future exploration.
\blue{Q4: Baseline method in Table 4.}
A4: Sorry for the confusion. The baseline indicates the backbone SOTAs without our IGV strategy. We update Table 4 as: 
\vspace{-15pt}
% \input{tab/ablation-loss-rebuttal} 
\begin{table}[ht]
\small
  \centering
  \caption{Study of IGV loss components}
   \vspace{-10pt}
    \resizebox{0.99\linewidth}{!}{
    \begin{tabular}{l|cc|cc}
    \toprule
    \multirow{2}[1]{*}{Variants} & \multicolumn{2}{c}{MSVD-QA} & \multicolumn{2}{c}{MSRVTT-QA} \\
          & Our Backbone   & Co-Mem & Our Backbone   & Co-Mem \\
    \midrule
    \midrule
    Baseline & 36.1  & 34.6  & 36.3  & 35.3 \\
    $\Lapl_{\hat{c}}$     & 36.0    & 33.3  & 36.7  & 36.0 \\
    $\Lapl_{\hat{c}}+\Lapl_{\hat{t}}$   & 37.4  & 36.1  & 37.8  & 36.8 \\
    $\Lapl_{\hat{c}}+\Lapl_{v^{*}}$   & 38.2  & 36.3  & 37.4  & 36.2 \\
    $\Lapl_{\hat{c}}+\Lapl_{\hat{t}}+\Lapl_{v^{*}}$ & \textbf{40.8} & \textbf{37.7} & \textbf{38.3} & \textbf{37.3} \\
    \bottomrule
    \end{tabular}
    }
  \label{tab:ablation-loss}%
  \vspace{-15pt}
\end{table}%

% \vspace{2pt}
% \noindent
% $\triangleright$\pink{Reviewer \#2.} \blue{Q1. The sampling strategy in Line 457 is not explained. Is it randomly sampled?}
% A1: Thanks. Yes, the complement stratification is randomly sampled. 
% \blue{Q2. Visualizing or statistically analyzing the causal and complement scenes predicted by Formula 9 could strengthen the explainability of this method.}
% A2: Good suggestion, we have exactly such visualization in Figure 5, where the highlighted part is the causal scene and the rest is the complement scene.


% \vspace{2pt}
% \noindent
% $\triangleright$\pink{Reviewer \#3.} \blue{Q. As the scene intervener relies on the causal grounding result. I wonder how to optimize this module? Would the causal grounding result affect the performance? If the causal grounding module is not well-trained, the scene intervener might not synthesize reliable videos. I imagine the training of the model is not very straightforward.}
% A: Good question. The grounding result is fundamental to our framework. To achieve that, we carefully designed our training objective with two auxiliary loss Eq 12-13 that enables the grounding indicator trained in an end-to-end fashion.  

% \vspace{2pt}
% \noindent
% $\triangleright$\pink{Reviewer \#4.} \blue{Q1. Is causal look another statistical learning?
% This paper argues that ERM blindly captures all statistical relations. To deal with this problem, they adopt a causal look, which analyzes data from a causal point of view. Thus, the proposed method also belongs to statistical learning. Please explain the difference between them.}

% \blue{Q2. Whether Complement Scene is a technical enhancement? I think the operation of changing the complement scene is similar to adversarial learning since “intervened videos” is a lot like a generated adversarial example.}
% A: Good point, the 'intervened videos' can be derived via either causal intervention or adversarial training, we embraced the former so that the framework can be trained in an end-to-end fashion.

% \blue{Q. How about the performance in TGIF-QA dataset?
% TGIF-QA dataset also is a large-scale videoqa, which is widely adopted in existing work and has more challenging. Please report the performance in TGIF-QA dataset}
% A: We have followed your suggestions to conduct experiments on three subtasks of TGIF-QA, here is the result:
% \input{tab/tgif-rebuttal}
% we didn't include TGIF-QA initially, because 1) recent analysis \cite{DBLP:conf/mm/PengYBW21} shows the severe answer biases in TGIF-QA (the "distractor" is not distracting enough, 90\% accuracy can be achieved by only using the candidate answers). 2) the video is trimmed to a very short length, which is not practical in real-world scenarios.

% \vspace{2pt}
% \noindent
% $\triangleright$\pink{Reviewer \#6.} \blue{The paper writing is poor and hard to read. It contains many grammatical errors,
% e.g. We then analyze ERM's suffering from the spurious correlations.But only part of the scenes are critical to answering the question of interest, while the rest hardly offers information relevant to the question.
% A in-depth comprehension.}
% A: Thanks for the careful review, we apologize for the roughness. We will do grammar checks thoroughly and  polish the language.

% \blue{Though well-motivated, the framework is not well-designed. The model is better to attend to related regions in related video frames. However, the proposed method only attends to related video frames without region-level attention in video frames.}
% Thanks, (this agrees with our discussion/ as discussed) in Section 7 (L858), we do anticipate fine-grained region-level features will strengthen the current IGV design, and this will be part of the future work. However, in this work, we aim to present and verify a novel training paradigm for VideoQA in general, instead of the effect of feature granularity. On the other hand, most of the VideoQA SoTAs adopt frame-level reasoning, so IGV stays with this tendency for adaptability and consistency.

% \blue{Why the video encoder is shared with the grounding indicator?}
% To save computation cost, it's not imperative.
% \blue{What is the baseline method in Table 4?}
% Sorry for the confusion, the baseline indicates backbone SoTAs without enhancement from invariant grounding strategy (identical to Table 3), where the IGV indicates Our Backbone. We revise Table 4 as below for your reference. 
% \input{tab/ablation-loss-rebuttal} 





%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{macros,main}
}

\end{document}
