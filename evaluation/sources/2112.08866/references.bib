@article{Ardizzone2018,
  author    = {Lynton Ardizzone and
               Jakob Kruse and
               Sebastian J. Wirkert and
               Daniel Rahner and
               Eric W. Pellegrini and
               Ralf S. Klessen and
               Lena Maier{-}Hein and
               Carsten Rother and
               Ullrich K{\"{o}}the},
  title     = {{Analyzing Inverse Problems with Invertible Neural Networks}},
  journal   = {CoRR},
  volume    = {abs/1808.04730},
  year      = {2018},
  archivePrefix = {arXiv},
  eprint    = {1808.04730}
}

@article{papamakarios2016fast,
  title={Fast $\varepsilon$-free inference of simulation models with bayesian conditional density estimation},
  author={Papamakarios, George and Murray, Iain},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}


@article{gabry_visualization_2019,
	title = {Visualization in {Bayesian} workflow},
	volume = {182},
	number = {2},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt, Michael and Gelman, Andrew},
	year = {2019},
	note = {Publisher: Wiley Online Library},
	pages = {389--402},
}


@article{hermans2021averting,
  title={Averting a crisis in simulation-based inference},
  author={Hermans, Joeri and Delaunoy, Arnaud and Rozet, Fran{\c{c}}ois and Wehenkel, Antoine and Louppe, Gilles},
  journal={arXiv preprint arXiv:2110.06581},
  year={2021}
}

@misc{Ardizzone2019,
	title={{Guided Image Generation with Conditional Invertible Neural Networks}}, 
	author={Lynton Ardizzone and Carsten Lüth and Jakob Kruse and Carsten Rother and Ullrich Köthe},
	year={2019},
	eprint={1907.02392},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@article{Barnard2000,
	author = {Barnard, John and McCulloch, Robert and Meng, Xiao-Li},
	year = {2000},
	month = {10},
	pages = {1281-1311},
	title = {{Modelling Covariance Matrices in Terms of Standard Deviations and Correlations, with Application To Shrinkage}},
	volume = {10},
	journal = {Statistica Sinica}
}

@article{Beaumont2002,
	doi = {10.1093/genetics/162.4.2025},
	year = {2002},
	month = dec,
	publisher = {Oxford University Press ({OUP})},
	volume = {162},
	number = {4},
	pages = {2025--2035},
	author = {Mark A Beaumont and Wenyang Zhang and David J Balding},
	title = {{Approximate Bayesian Computation in Population Genetics}},
	journal = {Genetics}
}

@Book{Berger1985,
	author = {Berger, James},
	title = {{Statistical decision theory and Bayesian analysis}},
	publisher = {Springer-Verlag},
	year = {1985},
	address = {New York},
	isbn = {0-387-96098-8}
}

@article{betancourt2017,
  title={{A conceptual introduction to Hamiltonian Monte Carlo}},
  author={Betancourt, Michael},
  journal={arXiv preprint},
  year={2017}
}


@INPROCEEDINGS{Denker1991,
	author = {John Denker and Yann Lecun},
	title = {{Transforming Neural-Net Output Levels to Probability Distributions}},
	booktitle = {Advances in Neural Information Processing Systems 3},
	year = {1991},
	pages = {853--859},
	publisher = {Morgan Kaufmann}
}

@book{Goodfellow2016,
	title={{Deep Learning}},
	author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
	publisher={MIT Press},
	year={2016}
}

@article{Nowak2016,
  title={{Entropy-based experimental design for optimal model discrimination in the geosciences}},
  author={Nowak, Wolfgang and Guthke, Anneli},
  journal={Entropy},
  volume={18},
  number={11},
  pages={409-434},
  year={2016},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{Schoniger2015,
  title={{Finding the right balance between groundwater model complexity and experimental effort via Bayesian model selection}},
  author={Sch{\"o}niger, Anneli and Illman, Walter A and W{\"o}hling, Thomas and Nowak, Wolfgang},
  journal={Journal of Hydrology},
  volume={531},
  pages={96--110},
  year={2015},
  publisher={Elsevier}
}


@article{Gretton2012,
    author = {Gretton, A and Borgwardt, K. and Rasch, Malte and Schölkopf, Bernhard and Smola, AJ},
    year = {2012},
    month = {03},
    pages = {723-773},
    title = {{A Kernel Two-Sample Test}},
    volume = {13},
    journal = {The Journal of Machine Learning Research}
}

@BOOK{Jaynes2003,
	AUTHOR = {Jaynes, E. T.},
	YEAR = {2003},
	TITLE = {{Probability Theory -- The Logic of Science}},
	EDITION = {},
	PUBLISHER = {Cambridge University Press},
	ADDRESS = {Cambridge}
}

@misc{Kendall2017,
	title={{What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?}}, 
	author={Alex Kendall and Yarin Gal},
	year={2017},
	eprint={1703.04977},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@inproceedings{Kingma2018,
	author = {Kingma, Durk P and Dhariwal, Prafulla},
	booktitle = {{Advances in Neural Information Processing Systems}},
	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Glow: Generative Flow with Invertible 1x1 Convolutions},
	volume = {31},
	year = {2018}
}



@Book{Lehmann1998,
	author = {Lehmann, E. L.},
	title = {{Theory of point estimation}},
	publisher = {Springer},
	year = {1998},
	address = {New York},
	isbn = {0-387-98502-6}
}



@book{Mardia1979,
  address = {London},
  author = {Mardia, {Kantilal Varichand} and Kent, {John T.} and Bibby, {John M.}},
  keywords = {Multivariate_analysis},
  publisher = {Acad. Press},
  series = {Probability and mathematical statistics},
  title = {{Multivariate analysis}},
  year = 1979
}

@BOOK{McElreath2016,
	AUTHOR = {McElreath, Richard},
	YEAR = {2016},
	TITLE = {{Statistical Rethinking -- A Bayesian Course with Examples in {R} and {Stan}}},
	PUBLISHER = {CRC Press/Taylor \& Francis Group}
}

@article{Muandet2017,
    doi = {10.1561/2200000060},
    year = {2017},
    publisher = {Now Publishers},
    volume = {10},
    number = {1-2},
    pages = {1--141},
    author = {Krikamol Muandet and Kenji Fukumizu and Bharath Sriperumbudur and Bernhard Sch\"{o}lkopf},
    title = {{Kernel Mean Embedding of Distributions: A Review and Beyond}},
    journal = {Foundations and Trends{\textregistered} in Machine Learning}
}

@misc{Murphy2007,
    author = {Murphy, Kevin},
    year = {2007},
    month = {11},
    title = {{Conjugate Bayesian analysis of the Gaussian distribution}}
}

@book{Oshana2006,
	author = {Oshana, Robert},
	doi = {10.1016/b978-0-7506-7759-2.x5000-5},
	year = {2006},
	publisher = {Elsevier},
	title = {{DSP} Software Development Techniques for Embedded and Real-Time Systems}}
}


@article{Pritchard1999,
	doi = {10.1093/oxfordjournals.molbev.a026091},
	year = 1999,
	publisher = {Oxford University Press ({OUP})},
	volume = {16},
	number = {12},
	pages = {1791--1798},
	author = {J. K. Pritchard and M. T. Seielstad and A. Perez-Lezaun and M. W. Feldman},
	title = {{Population growth of human Y chromosomes: a study of Y chromosome microsatellites}},
	journal = {Molecular Biology and Evolution}
}

@article{Voss2004,
  doi = {10.3758/bf03196893},
  year = {2004},
  month = oct,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {32},
  number = {7},
  pages = {1206--1220},
  author = {Andreas Voss and Klaus Rothermund and Jochen Voss},
  title = {{Interpreting the parameters of the diffusion model: An empirical validation}},
  journal = {Memory {\&} Cognition}
}

@article{Radev2019,
	doi = {10.1111/bmsp.12159},
	year = {2019},
	month = feb,
	publisher = {Wiley},
	volume = {73},
	number = {1},
	pages = {23--43},
	author = {Stefan T. Radev and Ulf K. Mertens and Andreas Voss and Ullrich K\"{o}the},
	title = {{Towards end-to-end likelihood-free inference with convolutional neural networks}},
	journal = {British Journal of Mathematical and Statistical Psychology}
}

@article{bayesflow,
  title={{BayesFlow: Learning complex stochastic models with invertible neural networks}},
  author={Radev, Stefan T and Mertens, Ulf K and Voss, Andreas and Ardizzone, Lynton and K{\"o}the, Ullrich},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2020},
  publisher={IEEE}
}

@book{Raiffa1961,
  title={{Applied Statistical Decision Theory}},
  author={Raiffa, H. and Raiffa, F.P.R.P.M.E.H. and Schlaifer, R.},
  series={Harvard Business School Publications},
  year={1961},
  publisher={Division of Research, Graduate School of Business Adminitration, Harvard University}
}


@article{Ratcliff2008,
  doi = {10.1162/neco.2008.12-06-420},
  year = {2008},
  month = apr,
  publisher = {{MIT} Press - Journals},
  volume = {20},
  number = {4},
  pages = {873--922},
  author = {Roger Ratcliff and Gail McKoon},
  title = {{The Diffusion Decision Model: Theory and Data for Two-Choice Decision Tasks}},
  journal = {Neural Computation}
}

@misc{Rustamov2020,
	title={{Closed-form Expressions for Maximum Mean Discrepancy with Applications to Wasserstein Auto-Encoders}}, 
	author={Raif M. Rustamov},
	year={2020},
	eprint={1901.03227},
	archivePrefix={arXiv},
	primaryClass={stat.ML}
}

@article {Tavare1997,
	author = {Tavar{\'e}, Simon and Balding, David J. and Griffiths, R. C. and Donnelly, Peter},
	title = {{Inferring Coalescence Times From DNA Sequence Data}},
	volume = {145},
	number = {2},
	pages = {505--518},
	year = {1997},
	publisher = {Genetics},
	journal = {Genetics}
}

@misc{Tensorflow2016,  
	title={{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},   
	author={Martin Abadi and Ashish Agarwal and Paul Barham and Eugene Brevdo and Zhifeng Chen and Craig Citro and Greg S. Corrado and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Ian Goodfellow and Andrew Harp and Geoffrey Irving and Michael Isard and Yangqing Jia and Rafal Jozefowicz and Lukasz Kaiser and Manjunath Kudlur and Josh Levenberg and Dan Mane and Rajat Monga and Sherry Moore and Derek Murray and Chris Olah and Mike Schuster and Jonathon Shlens and Benoit Steiner and Ilya Sutskever and Kunal Talwar and Paul Tucker and Vincent Vanhoucke and Vijay Vasudevan and Fernanda Viegas and Oriol Vinyals and Pete Warden and Martin Wattenberg and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},  
	year={2016},  
	eprint={1603.04467},  
	archivePrefix={arXiv},  
	primaryClass={cs.DC} 
}



@ARTICLE{Timmer1995,
    author = {{Timmer}, J. and {Koenig}, M.},
    title = {{On generating power law noise}},
    journal = {Astronomy and Astrophysics},
    year = {1995},
    volume = {300},
    pages = {707--710}
}


@article{diffusion,
  title={{Connectionist and diffusion models of reaction time.}},
  author={Ratcliff, Roger and Van Zandt, Trisha and McKoon, Gail},
  journal={Psychological review},
  volume={106},
  number={2},
  pages={261},
  year={1999},
  publisher={American Psychological Association}
}

@incollection{compartment,
  title={{Compartmental models in epidemiology}},
  author={Brauer, Fred},
  booktitle={Mathematical epidemiology},
  pages={19--79},
  year={2008},
  publisher={Springer}
}

@article{bayesflow_agent,
  title={{Estimation of agent-based models using Bayesian deep learning approach of BayesFlow}},
  author={Shiono, Takashi},
  journal={Journal of Economic Dynamics and Control},
  volume={125},
  pages={104082},
  year={2021},
  publisher={Elsevier}
}

@article{bayesflow_qcd,
  title={{Measuring QCD Splittings with Invertible Networks}},
  author={Bieringer, Sebastian and Butter, Anja and Heimel, Theo and H{\"o}che, Stefan and K{\"o}the, Ullrich and Plehn, Tilman and Radev, Stefan T},
  journal={SciPost Physics Proceedings},
  volume={10},
  number={6},
  year={2021},
  publisher={Stichting SciPost}
}

@article{bayes_ppc,
  title={{Visualization in Bayesian workflow}},
  author={Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt, Michael and Gelman, Andrew},
  journal={Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  volume={182},
  number={2},
  pages={389--402},
  year={2019},
  publisher={Wiley Online Library}
}

@inproceedings{apt,
  title={{Automatic posterior transformation for likelihood-free inference}},
  author={Greenberg, David and Nonnenmacher, Marcel and Macke, Jakob},
  booktitle={International Conference on Machine Learning},
  pages={2404--2414},
  year={2019},
  organization={PMLR}
}

@article{outbreak,
  title={{OutbreakFlow: Model-based Bayesian inference of disease outbreak dynamics with invertible neural networks and its application to the COVID-19 pandemics in Germany}},
  author={Radev, Stefan T and Graw, Frederik and Chen, Simiao and Mutters, Nico T and Eichel, Vanessa M and B{\"a}rnighausen, Till and K{\"o}the, Ullrich},
  journal={PLOS Computational Biology},
  volume={17},
  number={10},
  pages={e1009472},
  year={2021},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{bissiri2016general,
  title={A general framework for updating belief distributions},
  author={Bissiri, Pier Giovanni and Holmes, Chris C and Walker, Stephen G},
  journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume={78},
  number={5},
  pages={1103--1130},
  year={2016},
  publisher={Wiley Online Library}
}

@article{vehtari_survey_2012,
	title = {A survey of {Bayesian} predictive methods for model assessment, selection and comparison},
	volume = {6},
	doi = {10.1214/12-SS102},
	number = {none},
	urldate = {2022-08-02},
	journal = {Statistics Surveys},
	author = {Vehtari, Aki and Ojanen, Janne},
	month = jan,
	year = {2012},
	file = {Full Text:/Users/marvin/Zotero/storage/DKWZNBYG/Vehtari and Ojanen - 2012 - A survey of Bayesian predictive methods for model .pdf:application/pdf},
}

@article{burkner_approximate_2020,
	title = {Approximate leave-future-out cross-validation for {Bayesian} time series models},
	volume = {90},
	doi = {10.1080/00949655.2020.1783262},
	number = {14},
	urldate = {2022-08-02},
	journal = {Journal of Statistical Computation and Simulation},
	author = {Bürkner, Paul-Christian and Gabry, Jonah and Vehtari, Aki},
	month = sep,
	year = {2020},
	note = {arXiv:1902.06281 [stat]},
	keywords = {Statistics - Methodology},
	pages = {2499--2523},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/GW9S6CW9/Bürkner et al. - 2020 - Approximate leave-future-out cross-validation for .pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/FXXMWICC/1902.html:text/html},
}

@article{pudlo2016reliable,
  title={Reliable ABC model choice via random forests},
  author={Pudlo, Pierre and Marin, Jean-Michel and Estoup, Arnaud and Cornuet, Jean-Marie and Gautier, Mathieu and Robert, Christian P},
  journal={Bioinformatics},
  volume={32},
  number={6},
  pages={859--866},
  year={2016},
  publisher={Oxford University Press}
}

@article{Carpenter2017,
title={{Stan: A probabilistic programming language}},
author={Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
journal={Journal of statistical software},
volume={76},
number={1},
year={2017},
publisher={Columbia Univ., New York, NY (United States); Harvard Univ., Cambridge, MA (United States)}
}

@misc{Stan2022,
title = {{The Stan Core Library}},
author = {{Stan Development Team}},
note = {Version 2.30.0},
year = {2022}
}

@article{covid_germany,
  title={{Inferring change points in the spread of COVID-19 reveals the effectiveness of interventions}},
  author={Dehning, Jonas and Zierenberg, Johannes and Spitzner, F Paul and Wibral, Michael and Neto, Joao Pinheiro and Wilczek, Michael and Priesemann, Viola},
  journal={Science},
  volume={369},
  number={6500},
  year={2020},
  publisher={American Association for the Advancement of Science}
}

@article{mms_genbayes,
  title={{Diagnosing model misspecification and performing generalized Bayes' updates via probabilistic classifiers}},
  author={Thomas, Owen and Corander, Jukka},
  journal={arXiv preprint arXiv:1912.05810},
  year={2019}
}

@article{knoblauch2019generalized,
  title={Generalized variational inference: Three arguments for deriving new posteriors},
  author={Knoblauch, Jeremias and Jewson, Jack and Damoulas, Theodoros},
  journal={arXiv preprint arXiv:1904.02063},
  year={2019}
}

@article{giummole2019objective,
  title={Objective Bayesian inference with proper scoring rules},
  author={Giummol{\`e}, Federica and Mameli, Valentina and Ruli, Erlis and Ventura, Laura},
  journal={Test},
  volume={28},
  number={3},
  pages={728--755},
  year={2019},
  publisher={Springer}
}

@inproceedings{contrastive,
  title={{On contrastive learning for likelihood-free inference}},
  author={Durkan, Conor and Murray, Iain and Papamakarios, George},
  booktitle={International Conference on Machine Learning},
  pages={2771--2781},
  year={2020},
  organization={PMLR}
}

@article{butter2022machine,
  title={Machine Learning and LHC Event Generation},
  author={Butter, Anja and Plehn, Tilman and Schumann, Steffen and Badger, Simon and Caron, Sascha and Cranmer, Kyle and Di Bello, Francesco Armando and Dreyer, Etienne and Forte, Stefano and Ganguly, Sanmay and others},
  journal={arXiv preprint arXiv:2203.07460},
  year={2022}
}

@article{masegosa2020learning,
  title={Learning under model misspecification: Applications to variational and ensemble methods},
  author={Masegosa, Andres},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5479--5491},
  year={2020}
}

@article{bayes_lstm,
  title={{Flexible statistical inference for mechanistic models of neural dynamics}},
  author={Lueckmann, Jan-Matthis and Goncalves, Pedro J and Bassetto, Giacomo and {\"O}cal, Kaan and Nonnenmacher, Marcel and Macke, Jakob H},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{bayesian_miss,
  title={{Inconsistency of Bayesian inference for misspecified linear models, and a proposal for repairing it}},
  author={Gr{\"u}nwald, Peter and Van Ommen, Thijs and others},
  journal={Bayesian Analysis},
  volume={12},
  number={4},
  pages={1069--1103},
  year={2017},
  publisher={International Society for Bayesian Analysis}
}

@article{lstm,
  title={{Learning to forget: Continual prediction with LSTM}},
  author={Gers, Felix A and Schmidhuber, J{\"u}rgen and Cummins, Fred},
  journal={Neural computation},
  volume={12},
  number={10},
  pages={2451--2471},
  year={2000},
  publisher={MIT Press}
}

@inproceedings{conv,
  title={{Fully convolutional networks for semantic segmentation}},
  author={Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3431--3440},
  year={2015}
}

@article{invariant,
  title={{Probabilistic Symmetries and Invariant Neural Networks.}},
  author={Bloem-Reddy, Benjamin and Teh, Yee Whye},
  journal={J. Mach. Learn. Res.},
  volume={21},
  pages={90--1},
  year={2020}
}


@book{gelman1995bayesian,
  title={{Bayesian data analysis}},
  author={Gelman, Andrew and Carlin, John B and Stern, Hal S and Rubin, Donald B},
  year={1995},
  publisher={Chapman and Hall/CRC}
}

@article{lavin2021simulation,
  title={Simulation intelligence: TWe use sums of Gaussian kernels with different widths $\sigma_i$ as an established and flexible universal kernel \cite{Muandet2017}.owards a new generation of scientific methods},
  author={Lavin, Alexander and Zenil, Hector and Paige, Brooks and Krakauer, David and Gottschlich, Justin and Mattson, Tim and Anandkumar, Anima and Choudry, Sanjay and Rocki, Kamil and Baydin, At{\i}l{\i}m G{\"u}ne{\c{s}} and others},
  journal={arXiv preprint arXiv:2112.03235},
  year={2021}
}

@article{frontier,
  title={{The frontier of simulation-based inference}},
  author={Cranmer, Kyle and Brehmer, Johann and Louppe, Gilles},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={48},
  pages={30055--30062},
  year={2020},
  publisher={National Acad Sciences}
}

@inproceedings{snle,
  title={{Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows}},
  author={Papamakarios, George and Sterratt, David and Murray, Iain},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={837--848},
  year={2019},
  organization={PMLR}
}

@article{nuts,
  title={{The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo.}},
  author={Hoffman, Matthew D and Gelman, Andrew},
  journal={Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={1593--1623},
  year={2014}
}

@article{snpla,
  title={{Sequential Neural Posterior and Likelihood Approximation}},
  author={Wiqvist, Samuel and Frellsen, Jes and Picchini, Umberto},
  journal={arXiv preprint arXiv:2102.06522},
  year={2021}
}

@inproceedings{ratios,
  title={{Likelihood-free mcmc with amortized approximate ratio estimators}},
  author={Hermans, Joeri and Begy, Volodimir and Louppe, Gilles},
  booktitle={International Conference on Machine Learning},
  pages={4239--4248},
  year={2020},
  organization={PMLR}
}

@article{amortized_bmc,
  title={{Amortized bayesian model comparison with evidential deep learning}},
  author={Radev, Stefan T and D'Alessandro, Marco and Mertens, Ulf K and Voss, Andreas and K{\"o}the, Ullrich and B{\"u}rkner, Paul-Christian},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2021},
  publisher={IEEE}
}

@misc{Radev2020bayesflow-cognition,
Author = {Stefan T. Radev and Andreas Voss and Eva Marie Wieschen and Paul-Christian Buerkner},
Title = {{Amortized Bayesian Inference for Models of Cognition}},
Year = {2020},
Eprint = {arXiv:2005.03899},
}

@article{Stine1989,
  doi = {10.1177/0049124189018002003},
  year = {1989},
  month = nov,
  publisher = {{SAGE} Publications},
  volume = {18},
  number = {2-3},
  pages = {243--291},
  author = {Robert Stine},
  title = {{An Introduction to Bootstrap Methods}},
  journal = {Sociological Methods {\&} Research}
}

@article{info_geo,
  title={{Pulling back information geometry}},
  author={Arvanitidis, Georgios and Gonz{\'a}lez-Duque, Miguel and Pouplin, Alison and Kalatzis, Dimitris and Hauberg, S{\o}ren},
  journal={arXiv preprint arXiv:2106.05367},
  year={2021}
}

@article{gonccalves2020training,
  title={{Training deep neural density estimators to identify mechanistic models of neural dynamics}},
  author={Gon{\c{c}}alves, Pedro J and Lueckmann, Jan-Matthis and Deistler, Michael and Nonnenmacher, Marcel and {\"O}cal, Kaan and Bassetto, Giacomo and Chintaluri, Chaitanya and Podlaski, William F and Haddad, Sara A and Vogels, Tim P and others},
  journal={Elife},
  volume={9},
  pages={e56261},
  year={2020},
  publisher={eLife Sciences Publications Limited}
}

@misc{Tolstikhin2017,
Author = {Ilya Tolstikhin and Olivier Bousquet and Sylvain Gelly and Bernhard Schoelkopf},
Title = {Wasserstein Auto-Encoders},
Year = {2017},
Eprint = {arXiv:1711.01558},
}

@article{frazier_model_2020,
	title = {Model misspecification in approximate {Bayesian} computation: consequences and diagnostics},
	volume = {82},
	shorttitle = {Model misspecification in approximate {Bayesian} computation},
	doi = {10.1111/rssb.12356},
	language = {en},
	number = {2},
	urldate = {2022-04-06},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Frazier, David T. and Robert, Christian P. and Rousseau, Judith},
	month = apr,
	year = {2020},
	pages = {421--444}
}


@article{frazier_robust_2021,
	title = {Robust {Approximate} {Bayesian} {Inference} {With} {Synthetic} {Likelihood}},
	volume = {30},
	doi = {10.1080/10618600.2021.1875839},
	language = {en},
	number = {4},
	urldate = {2022-04-06},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Frazier, David T. and Drovandi, Christopher},
	month = oct,
	year = {2021},
	pages = {958--976}
}

@article{zhang_convergence_2020,
	title = {Convergence rates of variational posterior distributions},
	volume = {48},
	doi = {10.1214/19-AOS1883},
	number = {4},
	urldate = {2022-04-06},
	journal = {The Annals of Statistics},
	author = {Zhang, Fengshuo and Gao, Chao},
	month = aug,
	year = {2020},
}

@article{ramesh2022gatsbi,
  title={GATSBI: Generative Adversarial Training for Simulation-Based Inference},
  author={Ramesh, Poornima and Lueckmann, Jan-Matthis and Boelts, Jan and Tejero-Cantero, {\'A}lvaro and Greenberg, David S and Gon{\c{c}}alves, Pedro J and Macke, Jakob H},
  journal={arXiv preprint arXiv:2203.06481},
  year={2022}
}

@article{burkner2022some,
  title={Some models are useful, but how do we know which ones? Towards a unified Bayesian model taxonomy},
  author={B{\"u}rkner, Paul-Christian and Scholz, Maximilian and Radev, Stefan},
  journal={arXiv preprint arXiv:2209.02439},
  year={2022}
}

@inproceedings{lueckmann2021benchmarking,
  title={Benchmarking simulation-based inference},
  author={Lueckmann, Jan-Matthis and Boelts, Jan and Greenberg, David and Goncalves, Pedro and Macke, Jakob},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={343--351},
  year={2021},
  organization={PMLR}
}

@book{vandervaart2000asymptotic,
	title = {Asymptotic statistics},
	volume = {3},
	publisher = {Cambridge university press},
	author = {van der Vaart, Aad W},
	year = {2000},
}

@article{papamakarios2017masked,
  title={Masked autoregressive flow for density estimation},
  author={Papamakarios, George and Pavlakou, Theo and Murray, Iain},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{nguyen2019sensitivity,
  title={Sensitivity of Optimal Estimation satellite retrievals to misspecification of the prior mean and covariance, with application to OCO-2 retrievals},
  author={Nguyen, Hai and Cressie, Noel and Hobbs, Jonathan},
  journal={Remote Sensing},
  volume={11},
  number={23},
  pages={2770},
  year={2019},
  publisher={MDPI}
}

@article{tejero2020sbi,
  title={SBI--A toolkit for simulation-based inference},
  author={Tejero-Cantero, Alvaro and Boelts, Jan and Deistler, Michael and Lueckmann, Jan-Matthis and Durkan, Conor and Gon{\c{c}}alves, Pedro J and Greenberg, David S and Macke, Jakob H},
  journal={arXiv preprint arXiv:2007.09114},
  year={2020}
}

@article{lotfi2022bayesian,
  title={Bayesian Model Selection, the Marginal Likelihood, and Generalization},
  author={Lotfi, Sanae and Izmailov, Pavel and Benton, Gregory and Goldblum, Micah and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:2202.11678},
  year={2022}
}

@article{pacchiardi2022likelihood,
  title={Likelihood-Free Inference with Generative Neural Networks via Scoring Rule Minimization},
  author={Pacchiardi, Lorenzo and Dutta, Ritabrata},
  journal={arXiv preprint arXiv:2205.15784},
  year={2022}
}

@article{holmes2017assigning,
  title={Assigning a value to a power likelihood in a general Bayesian model},
  author={Holmes, Chris C and Walker, Stephen G},
  journal={Biometrika},
  volume={104},
  number={2},
  pages={497--503},
  year={2017},
  publisher={Oxford University Press}
}

@article{loaiza2021focused,
  title={Focused Bayesian prediction},
  author={Loaiza-Maya, Ruben and Martin, Gael M and Frazier, David T},
  journal={Journal of Applied Econometrics},
  volume={36},
  number={5},
  pages={517--543},
  year={2021},
  publisher={Wiley Online Library}
}


@article{alquier_concentration_2019,
	title = {Concentration of tempered posteriors and of their variational approximations},
	abstract = {While Bayesian methods are extremely popular in statistics and machine learning, their application to massive datasets is often challenging, when possible at all. Indeed, the classical MCMC algorithms are prohibitively slow when both the model dimension and the sample size are large. Variational Bayesian methods aim at approximating the posterior by a distribution in a tractable family. Thus, MCMC are replaced by an optimization algorithm which is orders of magnitude faster. VB methods have been applied in such computationally demanding applications as including collaborative filtering, image and video processing, NLP and text processing... However, despite very nice results in practice, the theoretical properties of these approximations are usually not known. In this paper, we propose a general approach to prove the concentration of variational approximations of fractional posteriors. We apply our theory to two examples: matrix completion, and Gaussian VB.},
	urldate = {2022-04-06},
	journal = {arXiv:1706.09293 [cs, math, stat]},
	author = {Alquier, Pierre and Ridgway, James},
	month = apr,
	year = {2019},
	note = {arXiv: 1706.09293},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory},
}

@article{dong_interactive_2020,
	title = {An interactive web-based dashboard to track {COVID}-19 in real time},
	volume = {20},
	doi = {10.1016/S1473-3099(20)30120-1},
	language = {en},
	number = {5},
	urldate = {2022-05-18},
	journal = {The Lancet Infectious Diseases},
	author = {Dong, Ensheng and Du, Hongru and Gardner, Lauren},
	month = may,
	year = {2020},
	pages = {533--534},
}



@misc{cannon_investigating_2022,
	title = {Investigating the {Impact} of {Model} {Misspecification} in {Neural} {Simulation}-based {Inference}},
	abstract = {Aided by advances in neural density estimation, considerable progress has been made in recent years towards a suite of simulation-based inference (SBI) methods capable of performing flexible, black-box, approximate Bayesian inference for stochastic simulation models. While it has been demonstrated that neural SBI methods can provide accurate posterior approximations, the simulation studies establishing these results have considered only well-specified problems -- that is, where the model and the data generating process coincide exactly. However, the behaviour of such algorithms in the case of model misspecification has received little attention. In this work, we provide the first comprehensive study of the behaviour of neural SBI algorithms in the presence of various forms of model misspecification. We find that misspecification can have a profoundly deleterious effect on performance. Some mitigation strategies are explored, but no approach tested prevents failure in all cases. We conclude that new approaches are required to address model misspecification if neural SBI algorithms are to be relied upon to derive accurate scientific conclusions.},
	urldate = {2022-10-14},
	publisher = {arXiv},
	author = {Cannon, Patrick and Ward, Daniel and Schmon, Sebastian M.},
	month = sep,
	year = {2022},
	note = {arXiv:2209.01845 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/UWN94SGD/Cannon et al. - 2022 - Investigating the Impact of Model Misspecification.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/9VAAAA43/2209.html:text/html},
}

@misc{leclercq_simulation-based_2022,
	title = {Simulation-based inference of {Bayesian} hierarchical models while checking for model misspecification},
	abstract = {This paper presents recent methodological advances to perform simulation-based inference (SBI) of a general class of Bayesian hierarchical models (BHMs), while checking for model misspecification. Our approach is based on a two-step framework. First, the latent function that appears as second layer of the BHM is inferred and used to diagnose possible model misspecification. Second, target parameters of the trusted model are inferred via SBI. Simulations used in the first step are recycled for score compression, which is necessary to the second step. As a proof of concept, we apply our framework to a prey-predator model built upon the Lotka-Volterra equations and involving complex observational processes.},
	urldate = {2022-10-14},
	publisher = {arXiv},
	author = {Leclercq, Florent},
	month = sep,
	year = {2022},
	note = {arXiv:2209.11057 [astro-ph, q-bio, stat]},
	keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Mathematics - Statistics Theory, Quantitative Biology - Populations and Evolution, Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/RSTMJJ2R/Leclercq - 2022 - Simulation-based inference of Bayesian hierarchica.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/2LVVDHQB/2209.html:text/html},
}



@misc{schmon_generalized_2021,
	title = {Generalized {Posteriors} in {Approximate} {Bayesian} {Computation}},
	abstract = {Complex simulators have become a ubiquitous tool in many scientific disciplines, providing high-fidelity, implicit probabilistic models of natural and social phenomena. Unfortunately, they typically lack the tractability required for conventional statistical analysis. Approximate Bayesian computation (ABC) has emerged as a key method in simulation-based inference, wherein the true model likelihood and posterior are approximated using samples from the simulator. In this paper, we draw connections between ABC and generalized Bayesian inference (GBI). First, we re-interpret the accept/reject step in ABC as an implicitly defined error model. We then argue that these implicit error models will invariably be misspecified. While ABC posteriors are often treated as a necessary evil for approximating the standard Bayesian posterior, this allows us to re-interpret ABC as a potential robustification strategy. This leads us to suggest the use of GBI within ABC, a use case we explore empirically.},
	urldate = {2022-10-14},
	publisher = {arXiv},
	author = {Schmon, Sebastian M. and Cannon, Patrick W. and Knoblauch, Jeremias},
	month = feb,
	year = {2021},
	note = {arXiv:2011.08644 [stat]},
	keywords = {Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology},
	annote = {Comment: Accepted at Advances in Approximate Bayesian Inference, AABI 2020},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/9ZYPYWRE/Schmon et al. - 2021 - Generalized Posteriors in Approximate Bayesian Com.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/92SRRKRQ/2011.html:text/html},
}

@misc{matsubara_robust_2022,
	title = {Robust {Generalised} {Bayesian} {Inference} for {Intractable} {Likelihoods}},
	abstract = {Generalised Bayesian inference updates prior beliefs using a loss function, rather than a likelihood, and can therefore be used to confer robustness against possible mis-specification of the likelihood. Here we consider generalised Bayesian inference with a Stein discrepancy as a loss function, motivated by applications in which the likelihood contains an intractable normalisation constant. In this context, the Stein discrepancy circumvents evaluation of the normalisation constant and produces generalised posteriors that are either closed form or accessible using standard Markov chain Monte Carlo. On a theoretical level, we show consistency, asymptotic normality, and bias-robustness of the generalised posterior, highlighting how these properties are impacted by the choice of Stein discrepancy. Then, we provide numerical experiments on a range of intractable distributions, including applications to kernel-based exponential family models and non-Gaussian graphical models.},
	urldate = {2022-10-14},
	publisher = {arXiv},
	author = {Matsubara, Takuo and Knoblauch, Jeremias and Briol, François-Xavier and Oates, Chris J.},
	month = jan,
	year = {2022},
	note = {arXiv:2104.07359 [math, stat]},
	keywords = {Mathematics - Statistics Theory, Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology},
}



@article{dellaporta_robust_2022,
	title = {Robust {Bayesian} {Inference} for {Simulator}-based {Models} via the {MMD} {Posterior} {Bootstrap}},
	copyright = {Creative Commons Attribution 4.0 International},
	doi = {10.48550/ARXIV.2202.04744},
	abstract = {Simulator-based models are models for which the likelihood is intractable but simulation of synthetic data is possible. They are often used to describe complex real-world phenomena, and as such can often be misspecified in practice. Unfortunately, existing Bayesian approaches for simulators are known to perform poorly in those cases. In this paper, we propose a novel algorithm based on the posterior bootstrap and maximum mean discrepancy estimators. This leads to a highly-parallelisable Bayesian inference algorithm with strong robustness properties. This is demonstrated through an in-depth theoretical study which includes generalisation bounds and proofs of frequentist consistency and robustness of our posterior. The approach is then assessed on a range of examples including a g-and-k distribution and a toggle-switch model.},
	urldate = {2022-10-14},
	author = {Dellaporta, Charita and Knoblauch, Jeremias and Damoulas, Theodoros and Briol, François-Xavier},
	year = {2022},
	keywords = {FOS: Computer and information sciences, Methodology (stat.ME), Machine Learning (cs.LG), Machine Learning (stat.ML)}
}


@misc{pacchiardi_score_2022,
	title = {Score {Matched} {Neural} {Exponential} {Families} for {Likelihood}-{Free} {Inference}},
	abstract = {Bayesian Likelihood-Free Inference (LFI) approaches allow to obtain posterior distributions for stochastic models with intractable likelihood, by relying on model simulations. In Approximate Bayesian Computation (ABC), a popular LFI method, summary statistics are used to reduce data dimensionality. ABC algorithms adaptively tailor simulations to the observation in order to sample from an approximate posterior, whose form depends on the chosen statistics. In this work, we introduce a new way to learn ABC statistics: we first generate parameter-simulation pairs from the model independently on the observation; then, we use Score Matching to train a neural conditional exponential family to approximate the likelihood. The exponential family is the largest class of distributions with fixed-size sufficient statistics; thus, we use them in ABC, which is intuitively appealing and has state-of-the-art performance. In parallel, we insert our likelihood approximation in an MCMC for doubly intractable distributions to draw posterior samples. We can repeat that for any number of observations with no additional model simulations, with performance comparable to related approaches. We validate our methods on toy models with known likelihood and a large-dimensional time-series model.},
	urldate = {2022-10-14},
	publisher = {arXiv},
	author = {Pacchiardi, Lorenzo and Dutta, Ritabrata},
	month = jan,
	year = {2022},
	note = {arXiv:2012.10903 [stat]},
	keywords = {Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/XABNHKGX/Pacchiardi and Dutta - 2022 - Score Matched Neural Exponential Families for Like.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/8Y5HTM66/2012.html:text/html},
}



@inproceedings{ruff_deep_2018,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Deep {One}-{Class} {Classification}},
	volume = {80},
	abstract = {Despite the great advances made by deep learning in many machine learning problems, there is a relative dearth of deep learning approaches for anomaly detection. Those approaches which do exist involve networks trained to perform a task other than anomaly detection, namely generative models or compression, which are in turn adapted for use in anomaly detection; they are not trained on an anomaly detection based objective. In this paper we introduce a new anomaly detection method—Deep Support Vector Data Description—, which is trained on an anomaly detection based objective. The adaptation to the deep regime necessitates that our neural network and training procedure satisfy certain properties, which we demonstrate theoretically. We show the effectiveness of our method on MNIST and CIFAR-10 image benchmark datasets as well as on the detection of adversarial examples of GTSRB stop signs.},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ruff, Lukas and Vandermeulen, Robert and Goernitz, Nico and Deecke, Lucas and Siddiqui, Shoaib Ahmed and Binder, Alexander and Müller, Emmanuel and Kloft, Marius},
	editor = {Dy, Jennifer and Krause, Andreas},
	month = jul,
	year = {2018},
	pages = {4393--4402},
}

@article{pang_deep_2022,
	title = {Deep {Learning} for {Anomaly} {Detection}: {A} {Review}},
	volume = {54},
	shorttitle = {Deep {Learning} for {Anomaly} {Detection}},
	doi = {10.1145/3439950},
	abstract = {Anomaly detection, a.k.a. outlier detection or novelty detection, has been a lasting yet active research area in various research communities for several decades. There are still some unique problem complexities and challenges that require advanced approaches. In recent years, deep learning enabled anomaly detection, i.e., deep anomaly detection, has emerged as a critical direction. This paper surveys the research of deep anomaly detection with a comprehensive taxonomy, covering advancements in three high-level categories and 11 fine-grained categories of the methods. We review their key intuitions, objective functions, underlying assumptions, advantages and disadvantages, and discuss how they address the aforementioned challenges. We further discuss a set of possible future opportunities and new perspectives on addressing the challenges.},
	number = {2},
	urldate = {2022-10-14},
	journal = {ACM Computing Surveys},
	author = {Pang, Guansong and Shen, Chunhua and Cao, Longbing and Hengel, Anton van den},
	month = mar,
	year = {2022},
	note = {arXiv:2007.02500 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	pages = {1--38},
	annote = {Comment: Survey paper, 36 pages, 180 references, 2 figures, 4 tables},
}



@misc{delaunoy_towards_2022,
	title = {Towards {Reliable} {Simulation}-{Based} {Inference} with {Balanced} {Neural} {Ratio} {Estimation}},
	abstract = {Modern approaches for simulation-based inference rely upon deep learning surrogates to enable approximate inference with computer simulators. In practice, the estimated posteriors' computational faithfulness is, however, rarely guaranteed. For example, Hermans et al. (2021) show that current simulation-based inference algorithms can produce posteriors that are overconfident, hence risking false inferences. In this work, we introduce Balanced Neural Ratio Estimation (BNRE), a variation of the NRE algorithm designed to produce posterior approximations that tend to be more conservative, hence improving their reliability, while sharing the same Bayes optimal solution. We achieve this by enforcing a balancing condition that increases the quantified uncertainty in small simulation budget regimes while still converging to the exact posterior as the budget increases. We provide theoretical arguments showing that BNRE tends to produce posterior surrogates that are more conservative than NRE's. We evaluate BNRE on a wide variety of tasks and show that it produces conservative posterior surrogates on all tested benchmarks and simulation budgets. Finally, we emphasize that BNRE is straightforward to implement over NRE and does not introduce any computational overhead.},
	urldate = {2022-10-14},
	publisher = {arXiv},
	author = {Delaunoy, Arnaud and Hermans, Joeri and Rozet, François and Wehenkel, Antoine and Louppe, Gilles},
	month = aug,
	year = {2022},
	note = {arXiv:2208.13624 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/VHQB976T/Delaunoy et al. - 2022 - Towards Reliable Simulation-Based Inference with B.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/43ACWZ9R/2208.html:text/html},
}

@misc{ward_robust_2022,
	title = {Robust {Neural} {Posterior} {Estimation} and {Statistical} {Model} {Criticism}},
	abstract = {Computer simulations have proven a valuable tool for understanding complex phenomena across the sciences. However, the utility of simulators for modelling and forecasting purposes is often restricted by low data quality, as well as practical limits to model fidelity. In order to circumvent these difficulties, we argue that modellers must treat simulators as idealistic representations of the true data generating process, and consequently should thoughtfully consider the risk of model misspecification. In this work we revisit neural posterior estimation (NPE), a class of algorithms that enable black-box parameter inference in simulation models, and consider the implication of a simulation-to-reality gap. While recent works have demonstrated reliable performance of these methods, the analyses have been performed using synthetic data generated by the simulator model itself, and have therefore only addressed the well-specified case. In this paper, we find that the presence of misspecification, in contrast, leads to unreliable inference when NPE is used naively. As a remedy we argue that principled scientific inquiry with simulators should incorporate a model criticism component, to facilitate interpretable identification of misspecification and a robust inference component, to fit 'wrong but useful' models. We propose robust neural posterior estimation (RNPE), an extension of NPE to simultaneously achieve both these aims, through explicitly modelling the discrepancies between simulations and the observed data. We assess the approach on a range of artificially misspecified examples, and find RNPE performs well across the tasks, whereas naively using NPE leads to misleading and erratic posteriors.},
	urldate = {2022-10-14},
	publisher = {arXiv},
	author = {Ward, Daniel and Cannon, Patrick and Beaumont, Mark and Fasiolo, Matteo and Schmon, Sebastian M.},
	month = oct,
	year = {2022},
	note = {arXiv:2210.06564 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
	annote = {Comment: Accepted at NeurIPS 2022},
}



@article{von_krause_mental_2022,
	title = {Mental speed is high until age 60 as revealed by analysis of over a million participants},
	volume = {6},
	doi = {10.1038/s41562-021-01282-7},
	language = {en},
	number = {5},
	urldate = {2022-10-14},
	journal = {Nature Human Behaviour},
	author = {von Krause, Mischa and Radev, Stefan T. and Voss, Andreas},
	month = may,
	year = {2022},
	pages = {700--708},
}


@techreport{ghaderi-kangavari_general_2022,
	type = {preprint},
	title = {A general integrative neurocognitive modeling framework to jointly describe {EEG} and decision-making on single trials},
	abstract = {Despite advances in techniques for exploring reciprocity in brain-behavior relations, few studies focus on building neurocognitive models that describe both human EEG and behavioral modalities at the single-trial level. Here, we introduce a new integrative joint modeling framework for the simultaneous description of single-trial EEG measures and cognitive modeling parameters of decision making. The new framework can be utilized for the evaluation of research questions as well as the prediction of both data types concurrently. In the introduced joint models, we formalized how single-trial N200 latencies and Centro-parietal positivities (CPP) are predicted by changing single-trial parameters of various drift-diffusion models (DDMs). These models do not have clear closed-form likelihoods and are not easy to fit using Markov chain Monte Carlo (MCMC) methods because nuisance parameters on single trials are shared in both behavior and neural activity. We trained deep neural networks to learn the Bayesian posterior distributions of unobserved neurocognitive parameters based on model simulations. We then used parameter recovery assessment and model misspecification to ascertain how robustly the models’ parameters can be estimated. Moreover, we fit the models to three different real datasets to test their applicability. Our results show that the single-trial integrative models can recover their latent parameters. Finally, we provide some pieces of evidence that single-trial integrative joint models are superior to traditional integrative models. The current single-trial paradigm and the likelihood-free approach for parameter recovery can inspire scientists and modelers to conveniently develop new neuro-cognitive models for other neural measures and to evaluate them appropriately.},
	urldate = {2022-10-14},
	institution = {PsyArXiv},
	author = {Ghaderi-Kangavari, Amin and Rad, Jamal Amani and Nunez, Michael D.},
	month = aug,
	year = {2022},
	doi = {10.31234/osf.io/pqv2c},
	file = {Submitted Version:/Users/marvin/Zotero/storage/CZIANQ2Z/Ghaderi-Kangavari et al. - 2022 - A general integrative neurocognitive modeling fram.pdf:application/pdf},
}


@inproceedings{lueckmann_benchmarking_2021,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Benchmarking {Simulation}-{Based} {Inference}},
	volume = {130},
	abstract = {Recent advances in probabilistic modelling have led to a large number of simulation-based inference algorithms which do not require numerical evaluation of likelihoods. However, a public benchmark with appropriate performance metrics for such ’likelihood-free’ algorithms has been lacking. This has made it difficult to compare algorithms and identify their strengths and weaknesses. We set out to fill this gap: We provide a benchmark with inference tasks and suitable performance metrics, with an initial selection of algorithms including recent approaches employing neural networks and classical Approximate Bayesian Computation methods. We found that the choice of performance metric is critical, that even state-of-the-art algorithms have substantial room for improvement, and that sequential estimation improves sample efficiency. Neural network-based approaches generally exhibit better performance, but there is no uniformly best algorithm. We provide practical advice and highlight the potential of the benchmark to diagnose problems and improve algorithms. The results can be explored interactively on a companion website. All code is open source, making it possible to contribute further benchmark tasks and inference algorithms.},
	booktitle = {Proceedings of {The} 24th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Lueckmann, Jan-Matthis and Boelts, Jan and Greenberg, David and Goncalves, Pedro and Macke, Jakob},
	editor = {Banerjee, Arindam and Fukumizu, Kenji},
	month = apr,
	year = {2021},
	pages = {343--351},
}


@book{berger_likelihood_1988,
	address = {Hayward, Calif},
	edition = {2nd ed},
	series = {Lecture notes-monograph series},
	title = {The likelihood principle},
	isbn = {978-0-940600-13-3},
	number = {v. 6},
	publisher = {Institute of Mathematical Statistics},
	author = {Berger, James O. and Wolpert, Robert L.},
	year = {1988},
	keywords = {Mathematical statistics, Probabilities, Estimation theory},
}


@article{jones-todd_identifying_2019,
	title = {Identifying prognostic structural features in tissue sections of colon cancer patients using point pattern analysis: {Point} pattern analysis of colon cancer tissue sections},
	volume = {38},
	shorttitle = {Identifying prognostic structural features in tissue sections of colon cancer patients using point pattern analysis},
	doi = {10.1002/sim.8046},
	language = {en},
	number = {8},
	urldate = {2022-10-26},
	journal = {Statistics in Medicine},
	author = {Jones-Todd, Charlotte M. and Caie, Peter and Illian, Janine B. and Stevenson, Ben C. and Savage, Anne and Harrison, David J. and Bown, James L.},
	month = apr,
	year = {2019},
	pages = {1421--1441},
}


@misc{sailynoja_graphical_2021,
	title = {Graphical {Test} for {Discrete} {Uniformity} and its {Applications} in {Goodness} of {Fit} {Evaluation} and {Multiple} {Sample} {Comparison}},
	abstract = {Assessing goodness of fit to a given distribution plays an important role in computational statistics. The Probability integral transformation (PIT) can be used to convert the question of whether a given sample originates from a reference distribution into a problem of testing for uniformity. We present new simulation and optimization based methods to obtain simultaneous confidence bands for the whole empirical cumulative distribution function (ECDF) of the PIT values under the assumption of uniformity. Simultaneous confidence bands correspond to such confidence intervals at each point that jointly satisfy a desired coverage. These methods can also be applied in cases where the reference distribution is represented only by a finite sample. The confidence bands provide an intuitive ECDF-based graphical test for uniformity, which also provides useful information on the quality of the discrepancy. We further extend the simulation and optimization methods to determine simultaneous confidence bands for testing whether multiple samples come from the same underlying distribution. This multiple sample comparison test is especially useful in Markov chain Monte Carlo convergence diagnostics. We provide numerical experiments to assess the properties of the tests using both simulated and real world data and give recommendations on their practical application in computational statistics workflows.},
	urldate = {2022-11-03},
	publisher = {arXiv},
	author = {Säilynoja, Teemu and Bürkner, Paul-Christian and Vehtari, Aki},
	month = nov,
	year = {2021},
	note = {arXiv:2103.10522 [stat]},
	keywords = {Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/7767VHYE/Säilynoja et al. - 2021 - Graphical Test for Discrete Uniformity and its App.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/I8JRJL8V/2103.html:text/html},
}


@misc{talts_validating_2020,
	title = {Validating {Bayesian} {Inference} {Algorithms} with {Simulation}-{Based} {Calibration}},
	abstract = {Verifying the correctness of Bayesian computation is challenging. This is especially true for complex models that are common in practice, as these require sophisticated model implementations and algorithms. In this paper we introduce {\textbackslash}emph\{simulation-based calibration\} (SBC), a general procedure for validating inferences from Bayesian algorithms capable of generating posterior samples. This procedure not only identifies inaccurate computation and inconsistencies in model implementations but also provides graphical summaries that can indicate the nature of the problems that arise. We argue that SBC is a critical part of a robust Bayesian workflow, as well as being a useful tool for those developing computational algorithms and statistical software.},
	urldate = {2022-11-03},
	publisher = {arXiv},
	author = {Talts, Sean and Betancourt, Michael and Simpson, Daniel and Vehtari, Aki and Gelman, Andrew},
	month = oct,
	year = {2020},
	note = {arXiv:1804.06788 [stat]},
	keywords = {Statistics - Methodology},
	annote = {Comment: 19 pages, 13 figures},
}


@misc{bergamin_model-agnostic_2022,
	title = {Model-agnostic out-of-distribution detection using combined statistical tests},
	abstract = {We present simple methods for out-of-distribution detection using a trained generative model. These techniques, based on classical statistical tests, are model-agnostic in the sense that they can be applied to any differentiable generative model. The idea is to combine a classical parametric test (Rao's score test) with the recently introduced typicality test. These two test statistics are both theoretically well-founded and exploit different sources of information based on the likelihood for the typicality test and its gradient for the score test. We show that combining them using Fisher's method overall leads to a more accurate out-of-distribution test. We also discuss the benefits of casting out-of-distribution detection as a statistical testing problem, noting in particular that false positive rate control can be valuable for practical out-of-distribution detection. Despite their simplicity and generality, these methods can be competitive with model-specific out-of-distribution detection algorithms without any assumptions on the out-distribution.},
	publisher = {arXiv},
	author = {Bergamin, Federico and Mattei, Pierre-Alexandre and Havtorn, Jakob D. and Senetaire, Hugo and Schmutz, Hugo and Maaløe, Lars and Hauberg, Søren and Frellsen, Jes},
	month = mar,
	year = {2022},
}
