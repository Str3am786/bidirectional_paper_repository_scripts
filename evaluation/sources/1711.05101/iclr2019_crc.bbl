\begin{thebibliography}{28}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aitchison(2018)]{aitchison18}
Laurence Aitchison.
\newblock A unified theory of adaptive stochastic gradient descent as
  {B}ayesian filtering.
\newblock \emph{arXiv:1507.02030}, 2018.

\bibitem[Chrabaszcz et~al.(2017)Chrabaszcz, Loshchilov, and
  Hutter]{chrabaszcz2017downsampled}
Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter.
\newblock {A downsampled variant of ImageNet as an alternative to the CIFAR
  datasets}.
\newblock \emph{arXiv:1707.08819}, 2017.

\bibitem[Cubuk et~al.(2018)Cubuk, Zoph, Mane, Vasudevan, and
  Le]{cubuk2018autoaugment}
Ekin~D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc~V Le.
\newblock Autoaugment: Learning augmentation policies from data.
\newblock \emph{arXiv preprint arXiv:1805.09501}, 2018.

\bibitem[Dinh et~al.(2017)Dinh, Pascanu, Bengio, and Bengio]{dinh2017sharp}
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio.
\newblock Sharp minima can generalize for deep nets.
\newblock \emph{arXiv:1703.04933}, 2017.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{The Journal of Machine Learning Research}, 12:\penalty0
  2121--2159, 2011.

\bibitem[Gastaldi(2017)]{gastaldi2017shake}
Xavier Gastaldi.
\newblock {Shake-Shake regularization}.
\newblock \emph{arXiv preprint arXiv:1705.07485}, 2017.

\bibitem[Hanson \& Pratt(1988)Hanson and Pratt]{hanson1988comparing}
Stephen~Jos{\'e} Hanson and Lorien~Y Pratt.
\newblock Comparing biases for minimal network construction with
  back-propagation.
\newblock In \emph{Proceedings of the 1st International Conference on Neural
  Information Processing Systems}, pp.\  177--185, 1988.

\bibitem[Huang et~al.(2017)Huang, Li, Pleiss, Liu, Hopcroft, and
  Weinberger]{SnapshotICLR2017}
Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John~E Hopcroft, and Kilian~Q
  Weinberger.
\newblock Snapshot ensembles: Train 1, get m for free.
\newblock \emph{arXiv:1704.00109}, 2017.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv:1609.04836}, 2016.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Diederik Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv:1412.6980}, 2014.

\bibitem[Krizhevsky(2009)]{krizhevsky2009learning}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Li et~al.(2017)Li, Xu, Taylor, and Goldstein]{li2017visualizing}
Hao Li, Zheng Xu, Gavin Taylor, and Tom Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock \emph{arXiv preprint arXiv:1712.09913}, 2017.

\bibitem[Loshchilov \& Hutter(2016)Loshchilov and Hutter]{loshchilov2016sgdr}
Ilya Loshchilov and Frank Hutter.
\newblock {SGDR}: stochastic gradient descent with warm restarts.
\newblock \emph{arXiv:1608.03983}, 2016.

\bibitem[Martens \& Grosse(2015)Martens and Grosse]{martens2015optimizing}
James Martens and Roger Grosse.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature.
\newblock In \emph{International conference on machine learning}, pp.\
  2408--2417, 2015.

\bibitem[Radford et~al.(2015)Radford, Metz, and
  Chintala]{radford2015unsupervised}
Alec Radford, Luke Metz, and Soumith Chintala.
\newblock Unsupervised representation learning with deep convolutional
  generative adversarial networks.
\newblock \emph{arXiv:1511.06434}, 2015.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, and
  Sutskever]{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
\newblock Improving language understanding by generative pre-training.
\newblock \emph{URL https://s3-us-west-2. amazonaws.
  com/openai-assets/research-covers/language-unsupervised/language\_
  understanding\_paper. pdf}, 2018.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{reddi2018iclr}
Sashank~J. Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of adam and beyond.
\newblock \emph{International Conference on Learning Representations}, 2018.

\bibitem[Smith(2016)]{smith2016}
Leslie~N Smith.
\newblock Cyclical learning rates for training neural networks.
\newblock \emph{arXiv:1506.01186v3}, 2016.

\bibitem[Tieleman \& Hinton(2012)Tieleman and Hinton]{tieleman2012lecture}
Tijmen Tieleman and Geoffrey Hinton.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its
  recent magnitude.
\newblock \emph{COURSERA: Neural networks for machine learning}, 4\penalty0
  (2):\penalty0 26--31, 2012.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5998--6008, 2017.

\bibitem[V{\"o}lker et~al.(2018)V{\"o}lker, Hammer, Schirrmeister, Behncke,
  Fiederer, Schulze-Bonhage, Marusi{\v{c}}, Burgard, and
  Ball]{volker2018intracranial}
Martin V{\"o}lker, Ji{\v{r}}{\'\i} Hammer, Robin~T Schirrmeister, Joos Behncke,
  Lukas~DJ Fiederer, Andreas Schulze-Bonhage, Petr Marusi{\v{c}}, Wolfram
  Burgard, and Tonio Ball.
\newblock Intracranial error detection via deep learning.
\newblock \emph{arXiv preprint arXiv:1805.01667}, 2018.

\bibitem[Wang et~al.(2018)Wang, Yuan, Yu, and Jian]{2018arXiv180406559W}
Jianfeng Wang, Ye~Yuan, Gang Yu, and Sun Jian.
\newblock Sface: An efficient network for face detection in large scale
  variations.
\newblock \emph{arXiv preprint arXiv:1804.06559}, 2018.

\bibitem[Wilson et~al.(2017)Wilson, Roelofs, Stern, Srebro, and
  Recht]{wilson2017marginal}
Ashia~C Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin
  Recht.
\newblock The marginal value of adaptive gradient methods in machine learning.
\newblock \emph{arXiv:1705.08292}, 2017.

\bibitem[Xu et~al.(2015)Xu, Ba, Kiros, Cho, Courville, Salakhudinov, Zemel, and
  Bengio]{xu2015show}
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan
  Salakhudinov, Rich Zemel, and Yoshua Bengio.
\newblock Show, attend and tell: Neural image caption generation with visual
  attention.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2048--2057, 2015.

\bibitem[Yang et~al.(2016)Yang, Luo, Loy, and Tang]{yang2016wider}
Shuo Yang, Ping Luo, Chen-Change Loy, and Xiaoou Tang.
\newblock Wider face: A face detection benchmark.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  5525--5533, 2016.

\bibitem[Zhang et~al.(2018)Zhang, Wang, Xu, and Grosse]{zhang2018three}
Guodong Zhang, Chaoqi Wang, Bowen Xu, and Roger Grosse.
\newblock Three mechanisms of weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1810.12281}, 2018.

\bibitem[Zhang et~al.(2017)Zhang, Ma, Li, and Wu]{zhang2017normalized}
Zijun Zhang, Lin Ma, Zongpeng Li, and Chuan Wu.
\newblock Normalized direction-preserving adam.
\newblock \emph{arXiv:1709.04546}, 2017.

\bibitem[Zoph et~al.(2017)Zoph, Vasudevan, Shlens, and Le]{zoph-arxiv17b}
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc~V. Le.
\newblock Learning transferable architectures for scalable image recognition.
\newblock In \emph{arXiv:1707.07012 [cs.CV]}, 2017.

\end{thebibliography}
