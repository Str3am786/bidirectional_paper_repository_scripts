\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.2}{Decoupling the Weight Decay from the Gradient-based Update}{}% 2
\BOOKMARK [1][-]{section.3}{Justification of Decoupled Weight Decay via a View of Adaptive Gradient Methods as Bayesian Filtering}{}% 3
\BOOKMARK [1][-]{section.4}{Experimental Validation}{}% 4
\BOOKMARK [2][-]{subsection.4.1}{Evaluating Decoupled Weight Decay With Different Learning Rate Schedules}{section.4}% 5
\BOOKMARK [2][-]{subsection.4.2}{Decoupling the Weight Decay and Initial Learning Rate Parameters}{section.4}% 6
\BOOKMARK [2][-]{subsection.4.3}{Better Generalization of AdamW}{section.4}% 7
\BOOKMARK [2][-]{subsection.4.4}{AdamWR with Warm Restarts for Better Anytime Performance}{section.4}% 8
\BOOKMARK [2][-]{subsection.4.5}{Use of AdamW on other datasets and architectures}{section.4}% 9
\BOOKMARK [1][-]{section.5}{Conclusion and Future Work}{}% 10
\BOOKMARK [1][-]{section.6}{Acknowledgments}{}% 11
\BOOKMARK [1][-]{appendix.A}{Formal Analysis of Weight Decay vs L2 Regularization}{}% 12
\BOOKMARK [1][-]{appendix.B}{Additional Practical Improvements of Adam}{}% 13
\BOOKMARK [2][-]{subsection.B.1}{Normalized Weight Decay}{appendix.B}% 14
\BOOKMARK [2][-]{subsection.B.2}{Adam with Cosine Annealing and Warm Restarts}{appendix.B}% 15
\BOOKMARK [1][-]{appendix.C}{An Example Setting of the Schedule Multiplier}{}% 16
\BOOKMARK [1][-]{appendix.D}{Additional Results}{}% 17
