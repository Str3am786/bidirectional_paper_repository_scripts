\textbf{Data Augmentation}: It is a process of augmenting newly generated
data to the existing training set for improving the model's robustness.
It can be performed by a minor alteration to the existing data. For
example, in computer vision data augmentation is used to enhance deep
learning models by \textit{flipping, color spacing, injecting noise},\textit{
random erasing} to reduce the bias in the classifier to favor more
frequently presented training examples \cite{hernandez2018data,devries2017improved}.
It can also be performed by generating synthetic data to act as a
regularizer and reduce over-fitting while training machine learning
models \cite{shorten2019survey}. Some algorithms such as \textit{data
wrapping,} SMOTE and MaxUp modify real-world examples to create augmented
datasets \cite{baird1992document,chawla2002smote,gong2021maxup}.
However, these methods are exclusively useful for either specific
kinds of data. For example, image recognition dataset or to improve
the performance of a particular algorithm like AGCN \cite{walawalkar2020attentive}.

\textbf{Counterfactual Augmented Data (CAD)}: Another popular method
is to augment data is by using counterfactual reasoning to improve
the generalization of the model. CAD can be generated by using existing
machine learning algorithms by matching closely related samples within
the training set, for example, POLYJUICE to generate text and\textit{
counterfactual image generation} for generating images by using generative
adversarial networks \cite{wu2021polyjuice,neal2018open}. Generating
diverse sets of realistic counterfactuals has proven to improve the
model's training efficiency and overall results \cite{mothilal2020explaining}.
For example, in classification problems, the models trained on CAD
were not sensitive to spurious features unlike modified data \cite{kaushik2019learning,chang2021towards}.
While, in discrimination and fairness literature counterfactual data
substitution and CAD helped to mitigate gender bias by replacing duplicate
text and handling conditional discrimination respectively \cite{maudslay2019s,vzliobaite2011handling}.
However, counterfactually augmented data does not always generalize
better than unaugmented datasets of the same size and may also hurt
the model's robustness \cite{huang2020counterfactually}. There is
a significant gap to explore on the quantity and quality of counterfactual
data needed to be augmented on original dataset by an effective learning
process such that, the model generalizes better and is robust across
various environments.

\textbf{Active learning: }It is a process that learns by an interaction
between oracle and learner agent, it resolves the problem of costly
data labeling in the learning process to improve the obtained model
by making it efficient \cite{cohn1996active,nissim2016improving}.
It can also be implemented on existing classification and predictive
algorithms to optimize a model's performance when compared with state-of-the-art
methods \cite{collet2014active}. For example, in classification problems,
logistic regression yielded remarkably better results by implementing
the simplest suggested active learning method \cite{lewis1994sequential,settles_12_active,yang2018benchmark}.
There are lots of effective approaches such as margin-based methods
\cite{ducoffe2018adversarial} and uncertainty sampling-based methods
to optimize this process \cite{gal2017deep,settles2007multiple}.
By using the uncertainty sampling-based learning process we can measure
how certain a probabilistic classifier's prediction is and, obtain
counterfactual versions of uncertain samples from the \textit{region
of uncertainty} to improve the model's transportability and robustness.
