\begin{thebibliography}{27}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[{Arjovsky} \& {Bottou}(2017){Arjovsky} and
  {Bottou}]{towardPrincipledGAN}
M.~{Arjovsky} and L.~{Bottou}.
\newblock {Towards Principled Methods for Training Generative Adversarial
  Networks}.
\newblock \emph{ArXiv e-prints}, January 2017.

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and Bottou]{wgan}
Martin Arjovsky, Soumith Chintala, and L{\'e}on Bottou.
\newblock {W}asserstein generative adversarial networks.
\newblock In Doina Precup and Yee~Whye Teh (eds.), \emph{Proceedings of the
  34th International Conference on Machine Learning}, volume~70 of
  \emph{Proceedings of Machine Learning Research}, pp.\  214--223,
  International Convention Centre, Sydney, Australia, 06--11 Aug 2017. PMLR.

\bibitem[Arora et~al.(2017)Arora, Ge, Liang, Ma, and
  Zhang]{equiAndGeneralization}
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi~Zhang.
\newblock Generalization and equilibrium in generative adversarial nets
  ({GAN}s).
\newblock In Doina Precup and Yee~Whye Teh (eds.), \emph{Proceedings of the
  34th International Conference on Machine Learning}, volume~70 of
  \emph{Proceedings of Machine Learning Research}, pp.\  224--232,
  International Convention Centre, Sydney, Australia, 06--11 Aug 2017. PMLR.

\bibitem[Arora et~al.(2018)Arora, Risteski, and Zhang]{doGANLearnDist}
Sanjeev Arora, Andrej Risteski, and Yi~Zhang.
\newblock Do {GAN}s learn the distribution? some theory and empirics.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock {ImageNet: A Large-Scale Hierarchical Image Database}.
\newblock In \emph{CVPR09}, 2009.

\bibitem[Fedus et~al.(2018)Fedus, Rosca, Lakshminarayanan, Dai, Mohamed, and
  Goodfellow]{manypatheq}
William Fedus, Mihaela Rosca, Balaji Lakshminarayanan, Andrew~M. Dai, Shakir
  Mohamed, and Ian Goodfellow.
\newblock Many paths to equilibrium: {GAN}s do not need to decrease a
  divergence at every step.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{gan}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In Z.~Ghahramani, M.~Welling, C.~Cortes, N.~D. Lawrence, and K.~Q.
  Weinberger (eds.), \emph{Advances in Neural Information Processing Systems
  27}, pp.\  2672--2680. Curran Associates, Inc., 2014.

\bibitem[Gulrajani et~al.(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and
  Courville]{wgangp}
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron~C
  Courville.
\newblock Improved training of wasserstein gans.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett (eds.), \emph{Advances in Neural Information
  Processing Systems 30}, pp.\  5767--5777. Curran Associates, Inc., 2017.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter]{ttur}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
  Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash
  equilibrium.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett (eds.), \emph{Advances in Neural Information
  Processing Systems 30}, pp.\  6626--6637. Curran Associates, Inc., 2017.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{batchnorm}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In Francis Bach and David Blei (eds.), \emph{Proceedings of the 32nd
  International Conference on Machine Learning}, volume~37 of \emph{Proceedings
  of Machine Learning Research}, pp.\  448--456, Lille, France, 07--09 Jul
  2015. PMLR.

\bibitem[Karras et~al.(2018)Karras, Aila, Laine, and Lehtinen]{progressiveGAN}
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
\newblock Progressive growing of {GAN}s for improved quality, stability, and
  variation.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock \emph{CoRR}, abs/1412.6980, 2014.

\bibitem[Lucas et~al.(2018)Lucas, Tallec, Ollivier, and
  Verbeek]{mixedSymmetricGAN}
Thomas Lucas, Corentin Tallec, Yann Ollivier, and Jakob Verbeek.
\newblock Mixed batches and symmetric discriminators for {GAN} training.
\newblock In Jennifer Dy and Andreas Krause (eds.), \emph{Proceedings of the
  35th International Conference on Machine Learning}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pp.\  2844--2853,
  Stockholmsmässan, Stockholm Sweden, 10--15 Jul 2018. PMLR.

\bibitem[Mescheder et~al.(2018)Mescheder, Geiger, and
  Nowozin]{whichGANConverge}
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin.
\newblock Which training methods for {GAN}s do actually converge?
\newblock In Jennifer Dy and Andreas Krause (eds.), \emph{Proceedings of the
  35th International Conference on Machine Learning}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pp.\  3478--3487,
  Stockholmsmässan, Stockholm Sweden, 10--15 Jul 2018. PMLR.

\bibitem[Miyato et~al.(2018)Miyato, Kataoka, Koyama, and Yoshida]{spectralNorm}
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.
\newblock Spectral normalization for generative adversarial networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Mroueh \& Sercu(2017)Mroueh and Sercu]{fishergan}
Youssef Mroueh and Tom Sercu.
\newblock Fisher gan.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett (eds.), \emph{Advances in Neural Information
  Processing Systems 30}, pp.\  2513--2523. Curran Associates, Inc., 2017.

\bibitem[Mroueh et~al.(2018)Mroueh, Li, Sercu, Raj, and Cheng]{sobolevgan}
Youssef Mroueh, Chun-Liang Li, Tom Sercu, Anant Raj, and Yu~Cheng.
\newblock Sobolev {GAN}.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Nagarajan \& Kolter(2017)Nagarajan and Kolter]{ganStable}
Vaishnavh Nagarajan and J.~Zico Kolter.
\newblock Gradient descent gan optimization is locally stable.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett (eds.), \emph{Advances in Neural Information
  Processing Systems 30}, pp.\  5585--5595. Curran Associates, Inc., 2017.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{pytorch}
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
  DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
\newblock Automatic differentiation in pytorch.
\newblock 2017.

\bibitem[Petzka et~al.(2018)Petzka, Fischer, and Lukovnikov]{wganlp}
Henning Petzka, Asja Fischer, and Denis Lukovnikov.
\newblock On the regularization of wasserstein {GAN}s.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[{Qi}(2017)]{lsgan}
G.-J. {Qi}.
\newblock {Loss-Sensitive Generative Adversarial Networks on Lipschitz
  Densities}.
\newblock \emph{ArXiv e-prints}, January 2017.

\bibitem[Radford et~al.(2015)Radford, Metz, and Chintala]{dcgan}
Alec Radford, Luke Metz, and Soumith Chintala.
\newblock Unsupervised representation learning with deep convolutional
  generative adversarial networks.
\newblock \emph{CoRR}, abs/1511.06434, 2015.

\bibitem[Roth et~al.(2017)Roth, Lucchi, Nowozin, and Hofmann]{stabilizingGAN}
Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, and Thomas Hofmann.
\newblock Stabilizing training of generative adversarial networks through
  regularization.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett (eds.), \emph{Advances in Neural Information
  Processing Systems 30}, pp.\  2018--2028. Curran Associates, Inc., 2017.

\bibitem[Salimans et~al.(2016)Salimans, Goodfellow, Zaremba, Cheung, Radford,
  Chen, and Chen]{improvedGAN}
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford,
  Xi~Chen, and Xi~Chen.
\newblock Improved techniques for training gans.
\newblock In D.~D. Lee, M.~Sugiyama, U.~V. Luxburg, I.~Guyon, and R.~Garnett
  (eds.), \emph{Advances in Neural Information Processing Systems 29}, pp.\
  2234--2242. Curran Associates, Inc., 2016.

\bibitem[Srivastava et~al.(2017)Srivastava, Valkoz, Russell, Gutmann, and
  Sutton]{veegan}
Akash Srivastava, Lazar Valkoz, Chris Russell, Michael~U. Gutmann, and Charles
  Sutton.
\newblock Veegan: Reducing mode collapse in gans using implicit variational
  learning.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett (eds.), \emph{Advances in Neural Information
  Processing Systems 30}, pp.\  3308--3318. Curran Associates, Inc., 2017.

\bibitem[Wu et~al.(2018)Wu, Huang, Thoma, Acharya, and
  Van~Gool]{wassersteinDiv}
Jiqing Wu, Zhiwu Huang, Janine Thoma, Dinesh Acharya, and Luc Van~Gool.
\newblock Wasserstein divergence for gans.
\newblock In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair
  Weiss (eds.), \emph{Computer Vision -- ECCV 2018}, pp.\  673--688, Cham,
  2018. Springer International Publishing.
\newblock ISBN 978-3-030-01228-1.

\bibitem[Zhang et~al.(2018)Zhang, Liu, Zhou, Xu, and He]{disGenTradoff}
Pengchuan Zhang, Qiang Liu, Dengyong Zhou, Tao Xu, and Xiaodong He.
\newblock On the discrimination-generalization tradeoff in {GAN}s.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\end{thebibliography}
