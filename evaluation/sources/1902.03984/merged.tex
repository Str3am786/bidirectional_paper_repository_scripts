\documentclass{article} % For LaTeX2e
\usepackage{iclr2019_conference,times}
\usepackage{amsfonts}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}


\usepackage{url}
\def\UrlBreaks{\do\/\do-}
\usepackage{breakurl}
\usepackage[breaklinks]{hyperref}
\usepackage{graphicx}
\usepackage{mathtools}

\usepackage{subfiles}
\usepackage{float}
\usepackage{subfig}
\usepackage{physics}

\usepackage[linesnumbered]{algorithm2e}

\usepackage{amsthm}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%


\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
%\newtheorem{observation}{Observation}

\usepackage[disable]{todonotes}
%\usepackage{todonotes}

\iclrfinalcopy


%\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\title{Improving Generalization and Stability of Generative Adversarial Networks}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Hoang Thanh-Tung \\
\texttt{hoangtha@deakin.edu.au} \And
Truyen Tran \\
\texttt{truyen.tran@deakin.edu.au} \And
Svetha Venkatesh \\
\texttt{svetha.venkatesh@deakin.edu.au}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Generative Adversarial Networks (GANs) are one of the most popular tools for learning complex high dimensional distributions. However, generalization properties of GANs have not been well understood. In this paper, we analyze the generalization of GANs in practical settings. We show that discriminators trained on discrete datasets with the original GAN loss have poor generalization capability and do not approximate the theoretically optimal discriminator. We propose a zero-centered gradient penalty for improving the generalization of the discriminator by pushing it toward the optimal discriminator. The penalty guarantees the generalization and convergence of GANs. Experiments on synthetic and large scale datasets verify our theoretical analysis.
\end{abstract}

\section{Introduction}
GANs \citep{gan} are one of the most popular tools for modeling high dimensional data. The original GAN is, however, highly unstable and often suffers from mode collapse. Much of recent researches has focused on improving the stability of GANs  \citep{dcgan, wgan, ttur, spectralNorm, progressiveGAN}. On the theoretical aspect, \cite{ganStable} proved that gradient based training of the original GAN is locally stable. \cite{ttur} further proved that GANs trained with Two Timescale Update Rule (TTUR) converge to local equilibria. However, the generalization of GANs at local equilibria is not discussed in depth in these papers.

\cite{equiAndGeneralization} showed that the generator can win by remembering a polynomial number of training examples. The result implies that a low capacity discriminator cannot detect the lack of diversity. Therefore, it cannot teach the generator to approximate the target distribution. In section \ref{generalization}, we discuss the generalization capability of high capacity discriminators. We show that high capacity discriminators trained with the original GAN loss tends to overfit to the mislabeled samples in training dataset, guiding the generator toward collapsed equilibria (i.e. equilibria where the generator has mode collapse). 

\cite{doGANLearnDist} proposed to measure the generalization capability of GAN by estimating the number of modes in the model distribution using the birthday paradox. Experiments on several datasets showed that the number of modes in the model distribution is several times greater than the number of training examples. The author concluded that although GANs might not be able to learn distributions, they do exhibit some level of generalization. Our analysis shows that poor generalization comes from the mismatch between discriminators trained on discrete finite datasets and the theoretically optimal discriminator. We propose a zero-centered gradient penalty for improving the generalization capability of (high capacity) discriminators. Our zero-centered gradient penalty pushes the discriminator toward the optimal one, making GAN to converge to equilibrium with good generalization capability. 

%We further show that the original GAN loss does not drive the discriminator toward the theoretically optimal discriminator.

%In this paper, we discuss the generalization capability of GANs and derive methods for improving it. Our method makes GANs convergent and generalizable. 

Our contributions are as follow:
\begin{enumerate}
\item We show that discriminators trained with the original GAN loss have poor generalization capability. Poor generalization in the discriminator prevents the generator from learning the target distribution. 
\item We show that the original GAN objective encourages gradient exploding in the discriminator. Gradient exploding in the discriminator can lead to mode collapse in the generator. %Gradient penalties and weight normalization methods prevent gradient exploding in the discriminator and improve its generalization capability.
\item We propose a zero-centered gradient penalty (0-GP) for improving the generalization capability of the discriminator. We show that non-zero centered GP and the zero-centered GP proposed in \cite{whichGANConverge} cannot make the discriminator generalize. Our 0-GP helps GANs to converge to generalizable equilibria. Theoretical results are verified on real world datasets.
\item We show that 0-GP helps the discriminator to distribute its capacity more equally between regions of the space, effectively preventing mode collapse. Experiments on synthetic and real world datasets verify that 0-GP can prevent mode collapse. GANs with 0-GP is much more robust to changes in hyper parameters, optimizers, and network architectures than the original GAN and GANs with other gradient penalties.
%\item We show that GAN trained with 1-centered GP (GAN-1-GP) \citep{manypatheq} approximates the Wasserstein GAN. Thus, GAN-1-GP is minimizing the Wasserstein distance at every update. %Conversely, WGAN without Lipschitz property behaves similarly to the original GAN. That explains the success of GP in stabilizing GAN.
\end{enumerate}

Table \ref{tab:compare} compares the key properties of our 0-GP with one centered GP (1-GP) \citep{wgangp} and zero centered GP on real/fake samples only (0-GP-sample) \citep{whichGANConverge}.

\subsection*{Notations}
%In this paper, we use the following notations:

\begin{tabular}{ll}
\hline \\
%$\mathbb{P}(e)$ & the probablity of event $e$ \\
%$\mathbb{P}_g$ & the model distribution \\
$p_r$ & the target distribution \\
$p_g$ & the model distribution \\
$p_z$ & the noise distribution \\
$d_x$ & the dimensionality of a data sample (real or fake) \\
$d_z$ & the dimensionality of a noise sample \\
%$D(\cdot)$ & a discriminator, a function from $\mathbb{R}^{d_x}$ to $[0, 1]$ \\
%$D(\cdot; \bm \theta_D)$ & a discriminator with parameter $\bm \theta_D$ \\
%$G(\cdot)$ & a generator, a function from $\mathbb{R}^{d_z}$ to $\mathbb{R}^{d_x}$\\
%$G(\cdot; \bm \theta_G)$ & a generator with parameter $\bm \theta_G$ \\
$supp(p)$ & the support of distribution $p$ \\
$\bm x \sim p_r$ & a real sample \\
$\bm z \sim p_z$ & a noise vector drawn from the noise distribution $p_z$ \\ 
$\bm{y} = G(\bm z)$ & a generated sample \\
%$D^*$ & the theoretically optimal discriminator \\
%$\hat{D}^*$ & the empirically optimal discriminator \\
$\mathcal{D}_r = \left\lbrace \bm{x}_1, ..., \bm x_n \right\rbrace$ & the set of $n$ real samples \\
$\mathcal{D}_g^{(t)} = \left\lbrace \bm{y}_1^{(t)}, ..., \bm y_m^{(t)} \right\rbrace$ & the set of $m$ generated samples at step $t$ \\ 
$\mathcal{D}^{(t)} = \mathcal{D}_r \cup \mathcal{D}_g^{(t)}$ & the training dataset at step $t$ \\
%$\norm{\bm v}$ & the Euclidean norm of $\bm v$ \\
\hline
\end{tabular}

\section{Related works}
Gradient penalties are widely used in GANs literature. There are a plethora of works on using gradient penalty to improve the stability of GANs \citep{whichGANConverge, wgangp, wganlp, stabilizingGAN, lsgan}. However, these works mostly focused on making the training of GANs stable and convergent. Our work aims to improve the generalization capability of GANs via gradient regularization.

\cite{doGANLearnDist} showed that the number of modes in the model distribution grows linearly with the size of the discriminator. The result implies that higher capacity discriminators are needed for better approximation of the target distribution. \cite{disGenTradoff} studied the tradeoff between generalization and discrimination in GANs. The authors showed that generalization is guaranteed if the discriminator set is small enough. In practice, rich discriminators are usually used for better discriminative power. Our GP makes rich discriminators generalizable while remaining discriminative.

Although less mode collapse is not exactly the same as generalization, the ability to produce more diverse samples implies better generalization. There are a large number of papers on preventing mode collapse in GANs. 
\cite{dcgan, improvedGAN} introduced a number of empirical tricks to help stabilizing GANs. \cite{towardPrincipledGAN} showed the importance of divergences in GAN training, leading to the introduction of Wasserstein GAN \citep{wgan}. The use of weak divergence is further explored by \cite{fishergan, sobolevgan}. 
\cite{mixedSymmetricGAN} advocated the use of mixed-batches, mini-batches of real and fake data, to smooth out the loss surface. The method exploits the distributional information in a mini-batch to prevent mode collapse.
VEEGAN \citep{veegan} uses an inverse of the generator to map the data to the prior distribution. The mismatch between the inverse mapping and the prior is used to detect mode collapse. If the generator can remember the entire training set, then the inverse mapping can be arbitrarily close the the prior distribution. It suggests that VEEGAN might not be able to help GAN to generalize outside of the training dataset. Our method helps GANs to discover unseen regions of the target distribution, significantly improve the diversity of generated samples.
\section{Background}
In the original GAN, the discriminator $D$ maximizes the following objective
\begin{equation}
\label{eqn:ganloss}
\mathcal{L} = \mathbb{E}_{\bm x \sim p_r} [\log(D(\bm x))] + \mathbb{E}_{\bm z \sim p_z} [\log(1 - D(G(\bm z)))]
\end{equation}
\cite{gan} showed that if the density functions $p_g$ and $p_r$ are known, then for a fixed generator $G$ the optimal discriminator is 
\begin{equation}
D^*(\bm v) = \frac{p_r(\bm v)}{p_r(\bm v) + p_g(\bm v)}, \forall \bm v \in supp(p_r) \ \cup\ supp(p_g) \label{eqn:optimD}
\end{equation}
In the beginning of the training, $p_g$ is very different from $p_r$ so we have
$ p_r(\bm x) \gg p_g(\bm x), \text{ for } \bm x \in \mathcal{D}_r $ and
$ p_g(\bm y) \gg p_r(\bm y), \text{ for } \bm y \in \mathcal{D}_g $.
Therefore, in the beginning of the training
$D^*(\bm x) \approx 1, \text{ for } \bm x \in \mathcal{D}_r $ and 
$D^*(\bm y) \approx 0, \text{ for } \bm y \in \mathcal{D}_g $.
As the training progresses, the generator will bring $p_g$ closer to $p_r$. The game reaches the global equilibrium when $p_r = p_g$. At the global equilibrium,
$ D^*(\bm v) = \frac{1}{2}, \forall \bm v \in supp(p_r) \ \cup\ supp(p_g)$.
One important result of the original paper is that, if the discriminator is optimal at every step of the GAN algorithm, then $p_g$ converges to $p_r$.

In practice, density functions are not known and the optimal discriminator is approximated by optimizing the classification performance of a parametric discriminator $D(\cdot; \bm \theta_D)$ on a discrete finite dataset $\mathcal{D} = \mathcal{D}_r \cup \mathcal{D}_g$. We call a discriminator trained on a discrete finite dataset an empirical discriminator. The empirically optimal discriminator is denoted by $\hat{D}^*$. 

\cite{equiAndGeneralization} defined generalization of a divergence  $d$ as follow: A divergence $d$ is said to have generalization error $\epsilon$ if 
\begin{equation}
\abs{d(\mathcal{D}_g, \mathcal{D}_r) - d(p_g, p_r)} \le \epsilon
\label{eqn:generalization}
\end{equation}
A discriminator $D$ defines a divergence between two distributions. The performance of a discriminator with good generalization capability on the training dataset should be similar to that on the entire data space. In practice, generalization capability of $D$ can be estimated by measuring the difference between its performance on the training dataset and a held-out dataset.
\section{Generalization capability of discriminators}
\label{generalization}
\begin{table}
\centering
\begin{tabular}{|p{2cm}|p{4cm}|p{2cm}|p{2cm}|p{2cm}|}
\hline
GP & Formula & Improve generalization & Prevent grad expoding & Convergence guarantee \\
\hline
\hline
Our 0-GP & $\lambda \mathbb{E}_{\bm v \in \mathcal{C}}[\norm{(\nabla D)_{\bm v}}^2]$, $\mathcal{C}$ from $\bm y$ to $\bm x$ & \cmark & \cmark & \cmark \\
\hline
1-GP & $\lambda \mathbb{E}_{\tilde{\bm x}}[(\norm{(\nabla D)_{\tilde{\bm x}}} - 1)^2]$, where $\tilde{\bm x} = \alpha \bm x + (1 - \alpha) \bm y$ & \xmark & \cmark & \xmark \\
\hline
0-GP-sample & $\lambda \mathbb{E}_{\bm v \in \mathcal{D}}[\norm{(\nabla D)_{\bm v}}^2]$ & \xmark & \xmark & \cmark \\
\hline
\end{tabular}
\caption{Summary of different gradient penalties}
\label{tab:compare}
\end{table}

\subsection{The empirically optimal discriminator does not approximate the theoretically optimal discriminator}
It has been observed that if the discriminator is too good at discriminating real and fake samples, the generator cannot learn effectively \citep{gan, towardPrincipledGAN}. The phenomenon suggests that $\hat{D}^*$ does not well approximate $D^*$, and does not guarantee the convergence of $p_g$ to $p_r$. In the following, we clarify the mismatch between $\hat{D}^*$ and $D^*$, and its implications.

\begin{proposition}
The two datasets $\mathcal{D}_r$ and $\mathcal{D}_g^{(t)}$ are disjoint with probability $1$ regardless of how close the two distributions $p_r$ and $p_g^{(t)}$ are.
\label{prop:disjoint}
\end{proposition}
\begin{proof}
See appendix \ref{appx:disjoint}.
\end{proof}

$\mathcal{D}_r$ and $\mathcal{D}_g^{(t)}$ are disjoint with probability 1 even when $p_g$ and $p_r$ are exactly the same. $\hat{D}^*$ perfectly classifies the real and the fake datasets, and
$\hat{D}^*(\bm x) = 1, \forall \bm x \in \mathcal{D}_r$
, $\hat{D}^*(\bm y) = 0, \forall \bm y \in \mathcal{D}_g^{(t)}$.
The value of $\hat{D}^*$ on $\mathcal{D}^{(t)}$ does not depend on the distance between the two distributions and does not reflect the learning progress. 
The value of $\hat{D}^*$ on the training dataset approximates that of $D^*$ in the beginning of the learning process but not when the two distributions are close.
When trained using gradient descent on a discrete finite dataset with the loss in Eqn. \ref{eqn:ganloss}, the discriminator $D$ is pushed toward $\hat{D}^*$, not $D^*$. This behavior does not depend on the size of training set (see Fig. \ref{fig:opDNoGP}, \ref{fig:opDNoGPlarge}), implying that \emph{the original GAN is not guaranteed to converge to the target distribution even when given enough data}.

\begin{figure}
\centering
\subfloat[] {\includegraphics[width=0.16\textwidth]{figs/optimalD_gradpen_center_0.00_alpha_None_lambda_0.00/optimal_discriminator_10000} \label{fig:opDNoGP}}
\subfloat[] {\includegraphics[width=0.16\textwidth]{figs/optimalD_linear_gradpen_center_0.00_alpha_None_lambda_0.00_nsamples_100/optimal_discriminator_10000} \label{fig:opDNoGPlarge}}
\subfloat[] {\includegraphics[width=0.16\textwidth]{figs/optimalD_gradpen_center_1.00_alpha_None_lambda_1.00/optimal_discriminator_10000} \label{fig:opD1GP}} 
\subfloat[] {\includegraphics[width=0.16\textwidth]{figs/optimalD_gradpen_center_0.00_alpha_1.0_lambda_1.00/optimal_discriminator_10000} \label{fig:opD0real}}
\subfloat[] {\includegraphics[width=0.16\textwidth]{figs/optimalD_gradpen_center_0.00_alpha_None_lambda_1.00/optimal_discriminator_10000} \label{fig:opD0GP}} 
\subfloat[] {\includegraphics[width=0.16\textwidth]{figs/theoretically_optimalD} \label{fig:opDTheory}} 

\caption{Value surfaces of discriminators trained for 10,000 iterations with different gradient penalties, on samples from two Gaussian distributions. The discriminator is a 2 hidden layer MLP with 64 hidden neurons.%, tanh activation function at hidden layers, sigmoid activation function at output layer. 
\protect\subref{fig:opDNoGP} No GP. \protect\subref{fig:opDNoGPlarge} No GP with more samples. \protect\subref{fig:opD1GP} One-centered GP (1-GP) with $\lambda = 1$. \protect\subref{fig:opD0real} Zero-centered GP on real/fake samples only (0-GP-sample) with $\lambda = 1$. \protect\subref{fig:opD0GP} Our zero-centered GP with $\lambda = 1$. \protect\subref{fig:opDTheory} Theoretically optimal discriminator computed using Eqn. \ref{eqn:optimD}.}
\label{fig:opD}
\end{figure}

\subsection{Empirical discriminators have poor generalization capability}
%Gradient based training of a parametric discriminator $D(\cdot; \bm \theta_D)$ pushes it toward $\hat{D}^*$ which does not have the same theoretical guarantee as $D^*$. 
When the generator gets better, generated samples are more similar to samples from the target distribution. However, regardless of their quality, generated samples are still labeled as fake in Eqn. \ref{eqn:ganloss}. The training dataset $\mathcal{D}$ is a bad dataset as it contains many mislabeled examples. A discriminator trained on such dataset will overfit to the mislabeled examples and has poor generalization capability. It will misclassify unseen samples and cannot teach the generator to generate these samples. 

Figure \ref{fig:opDNoGP} and \ref{fig:opDNoGPlarge} demonstrate the problem on a synthetic dataset consisting of samples from two Gaussian distributions. The discriminator in Fig. \ref{fig:opDNoGP} overfits to the small dataset and does not generalize to new samples in Fig. \ref{fig:opDNoGPlarge}. Although the discriminator in Fig. \ref{fig:opDNoGPlarge} was trained on a larger dataset which is sufficient to characterize the two distributions, it still overfits to the data and its value surface is very different from that of the theoretically optimal discriminator in Fig. \ref{fig:opDTheory}. 


An overfitted discriminator does not guide the model distribution toward target distribution but toward the real samples in the dataset. This explains why the original GAN usually exhibits mode collapse behavior. Finding the empirically optimal discriminator using gradient descent usually requires many iterations. Heuristically, overfitting can be alleviated by limiting the number of discriminator updates per generator update.
\cite{gan} recommended to update the discriminator once every generator update. In the next subsection, we show that limiting the number of discriminator updates per generator update prevents the discriminator from overfitting.

%\begin{figure}
%\centering
%\includegraphics[width=0.5\textwidth]{figs/overfitting}
%\caption{The test loss and test accuracy of the discriminator trained with different gradient penalties on real samples in test data of MNIST dataset. The discriminator is a 2 hidden layer MLP with 512 hidden neurons.}
%\end{figure}

\subsubsection{$\epsilon$-optimal discriminators}
$\hat{D}^*$ is costly to find and maintain. 
We consider here a weaker notion of optimality which can be achieved in practical settings.
\begin{definition}[$\epsilon$-optimal discriminator]
\label{def:epsOptimD}
Given two disjoint datasets $\mathcal{D}_r$ and $\mathcal{D}_g$, and a number $\epsilon > 0$, a discriminator $D$ is $\epsilon$-optimal if 
\begin{eqnarray*}
D(\bm x) & \ge & \frac{1}{2} + \frac{\epsilon}{2}, \forall \bm x \in \mathcal{D}_r \\
D(\bm y) & \le & \frac{1}{2} - \frac{\epsilon}{2}, \forall \bm y \in \mathcal{D}_g \\
\end{eqnarray*}
\end{definition}
As observed in \cite{gan}, $\hat{D}^*$ does not generate usable gradient for the generator. Goodfellow et al. proposed the non-saturating loss for the generator to circumvent this vanishing gradient problem. For an $\epsilon$-optimal discriminator, if $\epsilon$ is relatively small, then the gradient of the discriminator w.r.t. fake datapoints might not vanish and can be used to guide the model distribution toward the target distribution. 
\begin{proposition}
Given two disjoint datasets $\mathcal{D}_r$ and $\mathcal{D}_g$, and a number $\epsilon > 0$, an $\epsilon$-optimal discriminator $D_{\epsilon}$ exists and can be constructed as a one hidden layer MLP with $\mathcal{O}(d_x(m + n))$ parameters. \label{prop:optimD}
\end{proposition}
\begin{proof}
See appendix \ref{appx:constructOptimalD}. 
\end{proof}
Because deep networks are more powerful than shallow ones, the size of a deep $\epsilon$-optimal discriminator can be much smaller than $\mathcal{O}(d_x(m + n))$.
%Real world datasets usually contains hundreds of thousands to a few millions datapoints whose dimensionality ranges from hundreds to thousands. 
From the formula, the size of a shallow $\epsilon$-optimal discriminator for real world datasets ranges from a few to hundreds of millions parameters. That is comparable to the size of discriminators used in practice. \cite{towardPrincipledGAN} showed that even when the generator can generate realistic samples, a discriminator that can perfectly classify real and fake samples can be found easily using gradient descent. The experiment verified that $\epsilon$-optimal discriminator can be found using gradient descent in practical settings.

We observe that the norm of the gradient w.r.t. the discriminator's parameters decreases as fakes samples approach real samples. If the discriminator's learning rate is fixed, then the number of gradient descent steps that the discriminator has to take to reach $\epsilon$-optimal state should increase. 
\begin{proposition}
Alternating gradient descent with the same learning rate for discriminator and generator, and fixed number of discriminator updates per generator update (Fixed-Alt-GD) cannot maintain the (empirical) optimality of the discriminator. 
\end{proposition}
Fixed-Alt-GD decreases the discriminative power of the discriminator to improve its generalization capability. The proof for linear case is given in appendix \ref{appx:maintainOptimal}. 

In GANs trained with Two Timescale Update Rule (TTUR) \citep{ttur}, the ratio between the learning rate of the discriminator and that of the generator goes to infinity as the iteration number goes to infinity. Therefore, the discriminator can learn much faster than the generator and might be able to maintain its optimality throughout the learning process.

\todo[inline]{Discussing the TTUR behavior is beyond the scope of this paper but we note several things: 1. TTUR is not correctly implemented in practice, the learning rate ratio is usually fixed during training. 2. Maintaining optimality does not mean that a fake sample cannot approach a real sample. If the non-saturating loss is used then the generator can move fake datapoints closer to real datapoints even though the discriminator has overfitted. Fig. \ref{fig:gradField0GPTTUR20k} show that the fake samples collapse exactly some real samples and the gradient w.r.t. these datapoints vanishes. This does not contradict our analysis. We only state that TTUR might be able to maintain the optimality. TTUR makes the generator stays where it is, i.e. makes the game to come to a local equilibrium. 3. Maximizing the discriminative power for 2 fixed datasets always result in gradient exploding. The weight has to go to infinity in order to make $D(x) = 1$, $D(y) = 0$.}

\subsubsection{Gradient exploding in $\epsilon$-optimal discriminators}\label{gradExploding}
Let's consider a simplified scenario where the real and the fake datasets each contains a single datapoint:  $\mathcal{D}_r = \left\lbrace \bm x \right\rbrace$, $\mathcal{D}_g^{(t)} = \left\lbrace \bm y^{(t)} \right\rbrace$.
Updating the generator according to the gradient from the discriminator will push $\bm y^{(t)}$ toward $\bm x$. The absolute value of directional derivative of $D$ in the direction $\bm u = \bm x - \bm y^{(t)}$, at $\bm x$ is 
\[ \abs{(\nabla_{\bm u}D)_{\bm x}}  = \lim_{\bm y^{(t)} \xrightarrow{\bm u} \bm x}{\frac{\abs{D(\bm x) - D(\bm y^{(t)})}}{\norm{\bm x - \bm y^{(t)}}}} \]
If $D$ is always $\epsilon$-optimal, then $\abs{D(\bm x) - D(\bm y^{(t)})} \ge \epsilon, \forall t \in \mathbb{N}$, and
\[ \abs{(\nabla_{\bm u}D)_{\bm x}}  \ge \lim_{\bm y^{(t)} \xrightarrow{\bm u} \bm x}{\frac{\epsilon}{\norm{\bm x - \bm y^{(t)}}}} = \infty \]

The directional derivate of the $\epsilon$-optimal discriminator explodes as the fake datapoint approaches the real datapoint. 
%$\frac{\abs{D(\bm x) - D(\bm y^{(t)})}}{\norm{\bm x - \bm y^{(t)}}}$ can be interpreted as the average value of the directional derivatives w.r.t. datapoints on the line segment connecting $\bm x$ and $\bm y^{(t)}$. There must be some datapoint $\bm v$ on the line segment satisfies $\abs{(\nabla_{\bm u} D)_{\bm v}} \ge \frac{\epsilon}{\norm{\bm x - \bm y^{(t)}}}$. 
Directional derivative exploding implies gradient exploding at datapoints on the line segment connecting $\bm x$ and $\bm y^{(t)}$.
If in the next iteration, the generator produces a sample in a region where the gradient explodes, then the gradient w.r.t. the generator's parameters explodes.

Let's consider the following line integral
\begin{equation}\label{eqn:lineInt}
\int_{\mathcal{C}}{(\nabla D)_{\bm v} \cdot d\bm s} = D(\bm x) - D(\bm y^{(t)}) 
\end{equation}
where $\mathcal{C}$ is the line segment from $\bm y^{(t)}$ to $\bm x$. As the model distribution gets closer to the target distribution, the length of $\mathcal{C}$ should be non increasing. Therefore, maximizing $D(\bm x) - D(\bm y^{(t)})$, or the discriminative power of $D$, leads to the maximization of the directional derivative of $D$ in the direction $d \bm s$. The original GAN loss makes $D$ to maximize its discriminative power, encouraging gradient exploding to occur.
\todo[inline]{The original GAN loss actually encourages gradient exploding on every line connecting every pair of samples. That explain the situation in Fig. \ref{fig:gradField0GPTTUR10k}. Because of the saturated regions in sigmoid function, the gradient near training examples will vanish exponentially fast while the gradient near decision boundary will grow linearly. That result in the accumulation of gradient near the decision boundary.}

Gradient exploding happens in the discriminator trained with TTUR in Fig. \ref{fig:gradFieldNoGPTTUR} and \ref{fig:gradFieldNoGPTTUR10k}. Because TTUR can help the discriminator to maintain its optimality, gradient exploding happens and persists throughout the training process. Without TTUR, the discriminator cannot maintain its optimality so gradient exploding can happen sometimes during the training but does not persist (Fig. \ref{fig:gradFieldNoGP} and \ref{fig:gradFieldNoGP10k}). 
Because of the saturated regions in the sigmoid function used in neural network based discriminators, the gradient w.r.t. datapoints in the training set could vanishes. However, gradient exploding must happen at some datapoints on the path between a pair of samples, where the sigmoid function does not saturate. In Fig. \ref{fig:opDNoGP}, gradient exploding happens near the decision boundary. 
\todo[inline]{Discuss the difference between linear loss and cross entropy loss. For linear loss, the gradient is the same everywhere on the line segment. For cross entropy loss + sigmoid activation function, gradient explodes only for points near the decision boundary, and gradients near training samples will vanish.}

In practice, $\mathcal{D}_r$ and $\mathcal{D}_g$ contain many datapoints and the generator is updated using the average of gradients of the discriminator w.r.t. fake datapoints in the mini-batch. If a fake datapoint $\bm y_0$ is very close to a real datapoint $\bm x_0$, the gradient $(\nabla D)_{\bm y_0}$ might explode. When the average gradient is computed over the mini-batch, $(\nabla D)_{\bm y_0}$ outweighs other gradients. The generator updated with this average gradient will move many fake datapoints in the direction of $(\nabla D)_{\bm y_0}$, toward $\bm x_0$, making mode collapse visible. 

%The fraction $\frac{\abs{D(\bm x) - D(\bm y^{(t)})}}{\norm{\bm x - \bm y^{(t)}}}$ can be interpreted as the average value of the directional derivative. 

\begin{figure}
\centering
\subfloat[] {\includegraphics[width=.19\textwidth]{figs/gan_8Gaussians_gradfield_center_0.00_alpha_None_lambda_0.00_lrg_0.00300_lrd_0.00300_nhidden_512_scale_10.00_optim_SGD/fig_01000.pdf} \label{fig:gradFieldNoGP}} 
\subfloat[] {\includegraphics[width=.19\textwidth]{figs/gan_8Gaussians_gradfield_center_0.00_alpha_None_lambda_0.00_lrg_0.00300_lrd_0.00300_nhidden_512_scale_10.00_optim_SGD/fig_10000.pdf} \label{fig:gradFieldNoGP10k}} 
\subfloat[] {\includegraphics[width=.19\textwidth]{figs/gan_8Gaussians_gradfield_center_0.00_alpha_None_lambda_0.00_lrg_0.00300_lrd_0.00900_nhidden_512_scale_10.00_optim_SGD/fig_01000.pdf} \label{fig:gradFieldNoGPTTUR}}
\subfloat[] {\includegraphics[width=.19\textwidth]{figs/gan_8Gaussians_gradfield_center_0.00_alpha_None_lambda_0.00_lrg_0.00300_lrd_0.00900_nhidden_512_scale_10.00_optim_SGD/fig_10000.pdf} \label{fig:gradFieldNoGPTTUR10k}} \\
\subfloat[] {\includegraphics[width=.19\textwidth]{figs/gan_8Gaussians_gradfield_center_0.00_alpha_None_lambda_10.00_lrg_0.00300_lrd_0.00300_nhidden_512_scale_10.00_optim_SGD/fig_10000.pdf} \label{fig:gradField0GP10k}} 
\subfloat[] {\includegraphics[width=.19\textwidth]{figs/gan_8Gaussians_gradfield_center_0.00_alpha_None_lambda_10.00_lrg_0.00300_lrd_0.00900_nhidden_512_scale_10.00_optim_SGD/fig_10000.pdf} \label{fig:gradField0GPTTUR10k}}
\subfloat[] {\includegraphics[width=.19\textwidth]{figs/gan_8Gaussians_gradfield_center_0.00_alpha_None_lambda_10.00_lrg_0.00300_lrd_0.00900_nhidden_512_scale_10.00/fig_20000.pdf} \label{fig:gradField0GPTTUR20k}}
\subfloat[] {\includegraphics[width=.19\textwidth]{figs/gan_8Gaussians_gradfield_center_1.00_alpha_None_lambda_10.00_lrg_0.00300_lrd_0.00300_nhidden_512_scale_10.00_optim_SGD/fig_10000.pdf} \label{fig:gradField1GP10k}}
\subfloat[] {\includegraphics[width=.19\textwidth]{figs/gan_8Gaussians_gradfield_center_0.00_alpha_1.0_lambda_10.00_lrg_0.00300_lrd_0.00300_nhidden_512_scale_10.00_optim_SGD/fig_10000.pdf} \label{fig:gradField0real10k}}

\caption{Gradient w.r.t. the input of the discriminator of a GAN trained with different gradient penalties. The vector associated with a datapoint $\bm v$ points in the direction that increases the value of $\log\left(D(\bm v)\right)$ the fastest. The discriminator is a 2 hidden layer MLP with 512 hidden neurons. The discriminator is updated once every generator update. SGD is used for optimization.
\protect\subref{fig:gradFieldNoGP}, \protect\subref{fig:gradFieldNoGP10k} No GP, iter. 1000 and 10,000. 
\protect\subref{fig:gradFieldNoGPTTUR},
\protect\subref{fig:gradFieldNoGPTTUR10k} No GP with TTUR, iter. 1,000 and 10,000.
\protect\subref{fig:gradField0GP10k} Our 0-GP with $\lambda = 10$, iter. 10,000. 
\protect\subref{fig:gradField0GPTTUR10k}, \protect\subref{fig:gradField0GPTTUR20k} Our 0-GP with TTUR and $\lambda = 10$, iter. 10,000 and 20,000. 
\protect\subref{fig:gradField1GP10k} 1-GP with $\lambda = 10$, iter. 10,000. 
\protect\subref{fig:gradField0real10k} 0-GP-sample with $\lambda = 10$, iter. 10,000. 
}
\label{fig:gradField}
\end{figure}

\section{Improving generalization capability of empirical discriminators}
\label{improve}
Although the theoretically optimal discriminator $D^*$ is generalizable, the original GAN loss does not push empirical discriminators toward $D^*$.
We aim to improve the generalization capability of empirical discriminators by pushing them toward $D^*$. %We begin by analyzing the properties of $D^*$.
\subsection{Pushing empirical discriminators toward $D^*$}
For any input $\bm v \in supp(p_r) \ \cup\ supp(p_g)$, the value of $D^*(\bm v)$ goes to $\frac{1}{2}$ and the gradient $(\nabla D)_{\bm v}$ goes to $\bm 0$ as $p_g$ approaches $p_r$. %Consequently, the gradient $(\nabla D^*)_{\bm v}$ goes to $\bm 0$. 
Consider again the line integral in Eqn. \ref{eqn:lineInt}. As $D^*(\bm x)$ and $D^*(\bm y)$ approach $\frac{1}{2}$ for all $\bm x \in supp(p_r)$ and $\bm y \in supp(p_g)$, we have
\begin{equation}
 D^*(\bm x) - D^*(\bm y) = \int_{\mathcal{C}}{(\nabla D^*)_{\bm v} \cdot d\bm s} \rightarrow 0 \label{eqn:0lineInt}
\end{equation}

for all pairs of $\bm x$ and $\bm y$ and all paths $\mathcal{C}$ from $\bm y$ to $\bm x$.
That means, the discriminative power of $D^*$ must decrease as the two distributions become more similar.

To push an empirical discriminator $D$ toward $D^*$, we force $D$ to satisfy two requirements:
\begin{enumerate}
\item $(\nabla D)_{\bm v} \rightarrow \bm 0, \forall \ \bm v \in supp(p_r) \ \cup\ supp(p_g)$
\item $D(\bm x) - D(\bm y) = \int_{\mathcal{C}}{(\nabla D)_{\bm v} \cdot d\bm s} \rightarrow 0, \forall \ \bm x \sim p_r, \bm y \sim p_g, \mathcal{C} \text{ from } \bm y \text{ to } \bm x$
\end{enumerate} 
\subsection{Zero-centered gradient penalty}
The first requirement can be implemented by sampling some datapoints $\bm v \in supp(p_r) \ \cup\ supp(p_g)$ and force $(\nabla D)_{\bm v}$ to be $\bm 0$.
The second requirement can be implemented by sampling pairs of real and fake datapoints $(\bm x, \bm y)$ and force $D(\bm x) - D(\bm y)$ to be 0. The two requirements can be added to the discriminator's objective as follows
\begin{equation*}
\hat{\mathcal{L}} = \mathcal{L} - \lambda_1 \mathbb{E}_{\bm v}[\norm{(\nabla D)_{\bm v}}^2] - \lambda_2 \mathbb{E}_{\bm x, \bm y}[\left( D(\bm x) - D(\bm y) \right)^2]
\end{equation*}
where $\mathcal{L}$ is the objective in Eqn. \ref{eqn:ganloss}.
However, as discussed in section \ref{gradExploding}, an $\epsilon$-optimal discriminator can have zero gradient on the training dataset and have gradient exploding outside of the training dataset. The gradient norm could go to infinity even when $D(\bm x) - D(\bm y)$ is small. Regulating the difference between $D(\bm x)$ and $D(\bm y)$ is not an efficient way to prevent gradient exploding. 
%If $\lambda_2$ is finite, then a discriminator trained with $\hat{\mathcal{L}}$ can be $\epsilon$-optimal and can have gradient exploding. 
%The relationship between $\lambda_2$ and $\epsilon$ is discussed in depth in appendix \ref{appx:valueRegularization}.

We want to prevent gradient exploding on every path in $supp(p_r) \ \cup\ supp(p_g)$. Because $(\nabla D^*)_{\bm v} \rightarrow \bm 0$ for all $\bm v \in supp(p_r)\ \cup\ supp(p_g)$ as $p_g$ approach $p_r$, we could push the gradient w.r.t. every datapoint on every path $\mathcal{C} \in supp(p_r)\ \cup\ supp(p_g)$ toward $\bm 0$. We note that, if $(\nabla D)_{\bm v} \rightarrow \bm 0, \forall \ \bm v \in \mathcal{C}$ then $\int_{\mathcal{C}}{(\nabla D)_{\bm v} \cdot d\bm s} \rightarrow 0$. Therefore, the two requirements can be enforced by a single zero-centered gradient penalty of the form 
\begin{equation*}
\lambda \mathbb{E}_{\bm v \in \mathcal{C}}[\norm{(\nabla D)_{\bm v}}^2]
\end{equation*}
The remaining problem is how to find the path $\mathcal{C}$ from a fake to a real sample which lies inside $supp(p_r)\ \cup\ supp(p_g)$. Because we do not have access to the full supports of $p_r$ and $p_g$, and the supports of two distributions could be disjoint in the beginning of the training process, finding a path which lies completely inside the support is infeasible. 

In the current implementation, we approximate $\mathcal{C}$ with the straight line connecting a pair of samples, although there is no guarantee that all datapoints on that straight line are in $supp(p_r)\ \cup\ supp(p_g)$. That results in the following objective
\begin{equation}
\mathcal{L}_{0-GP} = \mathcal{L} - \lambda \mathbb{E}_{\tilde{\bm x}}[\norm{(\nabla D)_{\tilde{\bm x}}}^2] \label{eqn:l0gp}
\end{equation}
where $\tilde{\bm x} = \alpha \bm x + (1 - \alpha) \bm y$, $\bm x \sim p_r$, $\bm y \sim p_g$, and $\alpha \sim \mathcal{U}(0, 1)$ \footnote{\cite{wassersteinDiv} independently proposed the Wasserstein divergence for WGAN which uses a gradient penalty of similar form. Although the two penalties have similar approximate form, they have different motivations and addresses different problems in GANs.}. We describe a more sophisticated way of finding a better path in appendix \ref{appx:pathFinding}.

The larger $\lambda$ is, the stronger $(\nabla D)_{\tilde{\bm x}}$ is pushed toward $\bm 0$. If $\lambda$ is 0, then the discriminator will only focus on maximizing its discriminative power. If $\lambda$ approaches infinity, then the discriminator has maximum generalization capability and no discriminative power. $\lambda$ controls the tradeoff between discrimination and generalization in the discriminator. 

\subsection{Generalization capability of different gradient penalties} \label{gradPenGeneralization}
\cite{whichGANConverge} proposed to force the gradient w.r.t. datapoints in the real and/or fake dataset(s) to be $\bm 0$ to make the training of GANs convergent. %While the penalty helps GANs to converge, it does not help to push $D$ toward $D^*$.
%\cite{towardPrincipledGAN} proved that if $supp(p_r) \cap supp(p_g) = \emptyset$, then a perfect discriminator $\tilde{D}^*$ exists and satisfies
%\begin{eqnarray*}
%\tilde{D}^*(\bm x) & = & 1, \forall \bm x \sim p_r \\
%\tilde{D}^*(\bm y) & = & 0, \forall \bm y \sim p_g \\
%(\nabla \tilde{D}^*)_v & = & \bm 0, \forall \bm v \in supp(p_r)\ \cup\ supp(p_g)
%\end{eqnarray*} 
In section \ref{generalization}, we showed that for discrete training dataset, an empirically optimal discriminator $\hat{D}^*$ always exists and could be found by gradient descent.
Although %$\tilde{D}^*$ and 
$(\nabla \hat{D}^*)_{\bm v} = \bm 0, \forall \ \bm v \in \mathcal{D}$, $\hat{D}^*$ does not satisfy the requirement in Eqn. \ref{eqn:0lineInt} and have gradient exploding when some fake datapoints approach a real datapoint. 
The discriminators in Fig. \ref{fig:opDNoGP}, \ref{fig:opDNoGPlarge}, \ref{fig:opD0real}, \ref{fig:gradFieldNoGPTTUR} and \ref{fig:gradFieldNoGPTTUR10k} have vanishingly small gradients on datapoints in the training dataset and very large gradients outside. They have poor generalization capability and cannot teach the generator to generate unseen real datapoints.
Therefore, \emph{zero-centered gradient penalty on samples from $p_r$ and $p_g$ only cannot help improving the generalization of the discriminator}. 

\emph{Non-zero centered GPs do not push an empirical discriminator toward $D^*$ because the gradient does not converge to $\bm 0$}. A commonly used non-zero centered GP is the one-centered GP (1-GP) \citep{wgangp} which has the following form
\begin{equation}
\lambda \mathbb{E}_{\tilde{\bm x}}[(\norm{(\nabla D)_{\tilde{\bm x}}} - 1)^2]
\end{equation} 
where $\tilde{\bm x} = \alpha \bm x + (1 - \alpha) \bm y$, $\bm x \sim p_r$, $\bm y \sim p_g$, and $\alpha \sim \mathcal{U}(0, 1)$. Although the initial goal of 1-GP was to enforce Lipschitz constraint on the discriminator \footnote{\cite{wganlp} pointed out that 1-GP is based on the wrong intuition that the gradient of the optimal critic must be 1 everywhere under $p_r$ and $p_g$. The corrected GP is based on the definition of Lipschitzness.}, \cite{manypatheq} found that 1-GP prevents gradient exploding, making the original GAN more stable.
1-GP forces the norm of gradients w.r.t. datapoints on the line segment connecting $\bm x$ and $\bm y$ to be 1. If all gradients on the line segment have norm $1$, then the line integral in Eqn. \ref{eqn:lineInt} could be as large as  $\norm{\bm x - \bm y}$. Because the distance between random samples grows with the dimensionality, in high dimensional space $\norm{\bm x - \bm y}$ is greater than 1 with high probability. The discriminator could maximize the value of the line integral without violating the Lipschitz constraint. The discriminator trained with 1-GP, therefore, can overfit to the training data and have poor generalization capability. 

%\subsection{Discrimination - Generalization tradeoff}

\subsection{Convergence analysis for zero-centered gradient penalty}
\cite{whichGANConverge} showed that zero-centered GP on real and/or fake samples (0-GP-sample) makes GANs convergent. The penalty is based on the convergence analysis for the Dirac GAN, an 1-dimensional linear GAN which learns the Dirac distribution. The intuition is that when $p_g$ is the same as $p_r$, the gradient of the discriminator w.r.t. the fake datapoints (which are also real datapoints) should be $\bm 0$ so that generator will not move away when being updated using this gradient. If the gradient from the discriminator is not $\bm 0$, then the generator will oscillate around the equilibrium.

Our GP forces the gradient w.r.t. all datapoints on the line segment between a pair of samples (including the two endpoints) to be $\bm 0$. As a result, our GP also prevents the generator from oscillating. Therefore, our GP has the same convergence guarantee as the 0-GP-sample.
\subsection{Zero-centered gradient penalty improves capacity distribution}
Discriminators trained with the original GAN loss tends to focus on the region of the where fake samples are close to real samples, ignoring other regions. The phenomenon can be seen in Fig. \ref{fig:gradFieldNoGP}, \ref{fig:gradFieldNoGP10k}, \ref{fig:gradFieldNoGPTTUR}, \ref{fig:gradFieldNoGPTTUR10k}, \ref{fig:gradField1GP10k} and \ref{fig:gradField0real10k}. Gradients in the region where fake samples are concentrated are large while gradients in other regions, including regions where real samples are located, are very small. The generator cannot discover and generate real datapoints in regions where the gradient vanishes.

When trained with the objective in Eqn. \ref{eqn:l0gp}, the discriminator will have to balance between maximizing $\mathcal{L}$ and minimizing the GP. For finite $\lambda$, the GP term will not be exactly 0. Let $\gamma = \lambda \mathbb{E}_{\tilde{\bm x}}[\norm{(\nabla D)_{\tilde{\bm x}}}^2]$. Among discriminators with the same value of $\gamma$, gradient descent will find the discriminator that maximizes $\mathcal{L}$. As discussed in section \ref{gradExploding}, maximizing $\mathcal{L}$ leads to the maximization of norms of gradients on the path from $\bm y$ to $\bm x$. The discriminator should maximize the value
$ \eta = \mathbb{E}_{\tilde{\bm x}}[\norm{(\nabla D)_{\tilde{\bm x}}}] $.
If $\gamma$ is fixed then $\eta$ is maximized when $\norm{\nabla D_{\tilde{\bm x}^{(i)}}} = \norm{\nabla D_{\tilde{\bm x}^{(j)}}}, \forall \ i, j$ (Cauchy-Schwarz inequality). Therefore, our zero-centered GP encourages the gradients at different regions of the real data space to have the same norm. The capacity of $D$ is distributed more equally between regions of the real data space, effectively reduce mode collapse. The effect can be seen in Fig. \ref{fig:gradField0GP10k} and \ref{fig:gradField0GPTTUR10k}.

1-GP encourages $\abs{\norm{\nabla D_{\tilde{\bm x}^{(i)}}} - 1} = \abs{\norm{\nabla D_{\tilde{\bm x}^{(j)}}} - 1}, \forall \ i, j$. That allows gradient norms to be smaller than $1$ in some regions and larger than $1$ in some other regions. The problem can be seen in Fig. \ref{fig:gradField1GP10k}.
\section{Experiments}
The code is made available at \url{https://github.com/htt210/GeneralizationAndStabilityInGANs}.
\subsection{Zero-centered gradient penalty prevents overfitting}
To test the effectiveness of gradient penalties in preventing overfitting, we designed a dataset with real and fake samples coming from two Gaussian distributions and trained a MLP based discriminator on that dataset. The result is shown in Fig. \ref{fig:opD}. As predicted in section \ref{gradPenGeneralization}, 0-GP-sample does not help to improve generalization.
%In Fig. \ref{fig:opDNoGP}, the discriminator without GP overfits to the dataset and has very sharp decision boundary. The value surface of this discriminator is very different from that of the theoretically optimal discriminator. 
%Fig. \ref{fig:opD0real} shows that zero-centered GP on real and/or fake samples only \citep{whichGANConverge} does not help improving generalization. The value surface of the discriminator in Fig. \ref{fig:opD0real} is very similar to that of the unregularized discriminator. 
1-GP helps to improve generalization. The value surface in Fig. \ref{fig:opD1GP} is smoother than that in Fig. \ref{fig:opDNoGP}. However, as discussed in section \ref{gradPenGeneralization}, 1-GP cannot help much in higher dimensional space where the pair-wise distances are large. 
The discriminator trained with our 0-GP has the best generalization capability, with a value surface which is the most similar to that of the theoretically optimal one.

We increased the number of discriminator updates per generator update to $5$ to see the effect of GPs in preventing overfitting. On the MNIST dataset, GAN without GP and with other GPs cannot learn anything after 10,000 iterations. GAN with our 0-GP can still learn normally and start produce recognizable digits after only 1,000 iterations. 
\todo[inline]{Increasing the number of critic iteration to 20 makes WGAN-GP converge more slowly (Fig. \ref{fig:vswgangp} in appendix \ref{appx:result}). }
The result confirms that our GP is effective in preventing overfitting in the discriminator.

\subsection{Zero-centered gradient penalty improves generalization and robustness of GANs}
\subsubsection*{Synthetic data}
We tested different gradient penalties on a number of synthetic datasets to compare their effectiveness. The first dataset is a mixture of 8 Gaussians. 
The dataset is scaled up by a factor of 10 to simulate the situation in high dimensional space where random samples are far from each other. The result is shown in Fig. \ref{fig:gradField}. GANs with other gradient penalties all fail to learn the distribution and exhibit mode collapse problem to different extents. 
GAN with our 0-GP (GAN-0-GP) can successfully learn the distribution. Furthermore, GAN-0-GP can generate datapoints on the circle, demonstrating good generalization capability. The original GAN collapses to some disconnected modes and cannot perform smooth interpolation between modes: small change in the input result in large, unpredictable change in the output.  GAN with zero-centered GP on real/fake samples only also exhibits the same "mode jumping" behavior. The behavior suggests that these GANs tend to remember the training dataset and have poor generalization capability. Fig. \ref{fig:interpolation} in appendix \ref{appx:result} demonstrates the problem on MNIST dataset.

We observe that GAN-0-GP behaves similar to Wasserstein GAN as it first learns the overall structure of the distribution and then focuses on the modes. An evolution sequence of GAN-0-GP is shown in Fig. \ref{fig:0gpEvolution} in appendix \ref{appx:result}. Results on other synthetic datasets are shown in appendix \ref{appx:result}.

\subsubsection*{MNIST dataset}
The result on MNIST dataset is shown in Fig. \ref{fig:mnist}. After 1,000 iterations, all other GANs exhibit mode collapse or cannot learn anything. GAN-0-GP is robust to changes in hyper parameters such as learning rate and optimizers. When Adam is initialized with large $\beta_1$, e.g. $0.9$, GANs with other GPs cannot learn anything after many iterations. %GAN-0-GP can successfully learn to generate recognizable digits after 1,000 iterations and can generate high quality digits after 10,000 iterations. When trained for more iterations, the original GAN collapses. GAN with 0-GP-sample can produce samples of lower quality than GAN-0-GP. 
More samples are given in appendix \ref{appx:result}.

We observe that higher value of $\lambda$ improves the diversity of generated samples. For $\lambda = 50$, we observe some similar looking samples in the generated data. This is consistent with our conjecture that larger $\lambda$ leads to better generalization. %We also observe that GAN-0-GP is more robust to changes in hyper parameters, optimizers than GANs with other gradient penalties. Result are shown in appendix \ref{appx:result}.

\subsubsection*{ImageNet}
When trained on ImangeNet \citep{imagenet}, GAN-0-GP can produce high quality samples from all 1,000 classes. We compared our method with GAN with 0-GP-sample and WGAN-GP. GAN-0-GP-sample is able to produce samples of state of the art quality without using progressive growing trick \citep{progressiveGAN}. The result in Fig. \ref{fig:imagenet} shows that our method consistently outperforms GAN-0-GP-sample. GAN-0-GP and GAN-0-GP-sample outperform WGAN-GP by a large margin. Image samples are given in appendix \ref{appx:result}.

\section{Conclusion}
In this paper, we clarify the reason behind the poor generalization capability of GAN. We show that the original GAN loss does not guide the discriminator and the generator toward a generalizable equilibrium. We propose a zero-centered gradient penalty which pushes empirical discriminators toward the optimal discriminator with good generalization capability. Our gradient penalty provides better generalization and convergence guarantee than other gradient penalties. Experiments on diverse datasets verify that our method significantly improves the generalization and stability of GANs.


\begin{figure}
\centering
\subfloat[] {\includegraphics[width=0.18\textwidth]{figs/gan_mnist_center_0.00_alpha_None_lambda_0.00_lrg_0.00030_lrd_0.00030_nhidden_512_nz_50_ncritic_1/mnist_iter_001000.png} \label{fig:noGPMNIST}}
\subfloat[] {\includegraphics[width=0.18\textwidth]{figs/gan_mnist_center_0.00_alpha_1.0_lambda_100.00_lrg_0.00030_lrd_0.00030_nhidden_512_nz_50_ncritic_1/mnist_iter_001000.png} \label{fig:0realMNIST}}
\subfloat[] {\includegraphics[width=0.18\textwidth]{figs/gan_mnist_center_1.00_alpha_None_lambda_100.00_lrg_0.00030_lrd_0.00030_nhidden_512_nz_50_ncritic_1/mnist_iter_001000.png} \label{fig:1GPMNIST}}
\subfloat[] {\includegraphics[width=0.18\textwidth]{figs/gan_mnist_center_0.00_alpha_None_lambda_100.00_lrg_0.00030_lrd_0.00030_nhidden_512_nz_50_ncritic_1/mnist_iter_001000.png} \label{fig:0GPMNIST}}
\subfloat[] {\includegraphics[width=0.18\textwidth]{figs/gan_mnist_center_0.00_alpha_None_lambda_100.00_lrg_0.00030_lrd_0.00030_nhidden_512_nz_50_ncritic_1/mnist_iter_010000.png} \label{fig:0GPMNIST10k}}

\caption{Result on MNIST. The networks have the same architectures with networks used in synthetic experiment. Batch normalization \citep{batchnorm} was not used. Adam optimizer \citep{adam} with $\beta_1 = 0.5, \beta_2 = 0.9$ was used.
\protect\subref{fig:noGPMNIST} No GP, iter. 1,000.
\protect\subref{fig:0realMNIST} 0-GP-sample, $\lambda = 100$, iter. 1,000.
\protect\subref{fig:1GPMNIST} 1-GP, $\lambda = 100$, iter. 1,000.
\protect\subref{fig:0GPMNIST},
\protect\subref{fig:0GPMNIST10k} 0-GP, $\lambda = 100$, iter. 1,000 and 10,000.
}
\label{fig:mnist}
\end{figure}


\begin{figure}
\centering
\includegraphics[width=.5\textwidth]{figs/inception_plot.pdf}
\caption{Inception score \citep{improvedGAN} on ImageNet of GAN-0-GP, GAN-0-GP-sample, and WGAN-GP. The code for this experiment is adapted from \cite{whichGANConverge}. We used $\lambda=10$ for all GANs as recommended by Mescheder et al. The critic in WGAN-GP was updated 5 times per generator update. To improve convergence, we used TTUR with learning rates of 0.0001 and 0.0003 for the generator and discriminator, respectively.}
\label{fig:imagenet}
\end{figure}


%----------------------------------------------------------------------------
%
%Note sections
%
%\section{Generalization capability of GANs}
%\label{generalization}
%There are a number of works on improving the stability of GAN. However, not so many works on generalization properties of GAN and how to improve it. Our work aims to do that.
%
%The original GAN pushes the value of the empirically optimal D toward 0 and 1 but it should be pushed to $\frac{p_r(x)}{p_r(x) + p_g(x)}$. That value is, however, not computable/be estimated from data. WGAN distance can be estimated from data. That leads to the paper: A Two-update Computation of the Exact GAN Wasserstein Distance \cite{twoupdatewgan}.
%
%It has been observed that if the discriminator is too good, $G$ cannot learn anything. That contrast with the proof in the original GAN which requires the discriminator to be perfect at each update. That is because the prefect discriminator for two empirical distributions is not the same as the perfect discriminator for two continuous distributions. In other words, the $D$ trained on the training dataset does not generalize to the actual distribution. The minimax loss forces $D$ to be not generalizable. In the next subsections, we discuss the causes and effects of this.
%
%One important conclusion: the original GAN cannot learn distribution. WGAN can learn the distribution given enough data and capacity.
%
%\subsection{The perfect discriminator}
%\label{perfectDisc}
%\citet{equiAndGeneralization, doGANLearnDist} showed that low capacity discriminator cannot detect lack of diversity. We show here the problem with rich discriminator: too powerful discriminator can perfectly classify reals and fakes, resulting in unusable gradients and gradient exploding.
%
%For discrete training dataset, regardless of how large, we can always construct a optimal discriminator of $\Omega (nd)$ parameters. Zero measures, optimal discriminator with polynomial number of parameters. Note that this does not contradict theorem 3.1 in \citet{equiAndGeneralization} where they showed that neural net distance generalizes. The proof assume Lipschitzness over the $D$ and $G$ and the number of samples depends on the Lipschitz constant $L$ and $L_\phi$. As we show in the next subsection, $L$ and $L_\phi$ can explode (go to infinity) and that implies that to make the networks generalize, we need infinite number of training examples.
%
%The optimal discriminator is not generalizable. The divergence calculated by the discriminator on empirical distributions does not track the true divergence between real and fake distributions.
%
%The output of the empirically optimal discriminator is not $\frac{p_r}{p_r + p_G}$, but is 1 or 0.
%
%Note that \cite{equiAndGeneralization} showed that $G$ can win even though the two distributions are arbitrarily far from each other. Our paper show that the discriminator can be optimal regardless of the size of the training dataset. That is a different aspect, although the conclusion is the same: GAN cannot learn distribution.
%
%The main take away of this part is: the empirically optimal discriminator is not the same as the truly optimal discriminator, i.e. the divergence calculated by the empirically optimal discriminator between two empirical distributions does not track the divergence between two distribution.
%
%----------------
%
%
%\subsection{Gradient exploding in optimal discriminators}
%The purpose of this part is to show that the empirically optimal discriminator is not a good discriminator and can have gradient exploding. It is very sensitive to change in the input. The training the original GAN makes D more and more sensitive to small changes in the inputs. Gradient regularization is needed to make it more robust to changes in inputs. 
%
%\citet{manypatheq} claimed that although the minimax loss for $G$ has vanishing gradient, the non-saturated loss can still produce usable gradient and can guide $p_G$ toward $p_r$. We note that if the discriminator is optimal then $D(G(z)) = 0, \forall z \sim p_z$ and the gradient for non saturated $G$ is infinity, which is also not usable. Training a optimal discriminator might not be feasible in practice so we consider a much more realistic $\epsilon$-optimal discriminator $D_\epsilon$. 
%
%Show how gradient exploding happen in $D_\epsilon$.
%
%Note that for gradient exploding to happen, it only requires a single fake datapoint being close enough to a real datapoint. That could happen easily in practice. Show examples on synthetic data and MNIST.
%
%To stabilize GAN, therefore, we need to regularize the gradient w.r.t. discriminator inputs. We also need to regularize $G$ so that it does not cause gradient exploding in $D$. TTUR prevent $G$ from learning to fast so it will not cause gradient exploding in $D$.
%
%\subsubsection{Gradient exploding and mode collapse}
%Draw the scenarios where mode collapse can happen.
%\begin{itemize}
%\item D is too good at detecting some fake samples: $D(y) < \epsilon \ll 1$ for some $y = G(z) \sim p_G$. That cause the gradient of $\mathcal{L}_G = -\log(D(y)$ to explode. When the gradient is averaged over a mini-batch, the gradient corresponds to $y$ outweights others. As a result, $G$ focuses on improving the quality for $y$ only, i.e. $G$ move the samples toward $x$ - the direction that improve $y$ the most. That cause mode collapse to happen.
%\item G is too good at simulating some real datapoint $x$, i.e. $D(y) = D(G(z)) \approx 1$. That make the loss for $\mathcal{L}_D$ to explode. $D$ focuses its capacity on separating $x$ and $y$, ignoring other datapoints. That further decrease the generalization of $D$. The gradient w.r.t. $x$ outweights others, making $D$ forgets other datapoints.
%\end{itemize}
%
%The original GAN has no mechanism to prevent this from happening and suffer from severe mode collapse. 
%
%Proofs of convergence for GAN assumes Lipschitzness, i.e. gradient exploding never occurs. That leads to some conclusions that contradict normal observations:
%\begin{itemize}
%\item GAN converges exponentially but WGAN does not converge \cite{ganStable}. This paper assumes that $D$ generalizes but this does not hold in practical setup where we can only access the empirical distribution.
%\end{itemize}
%
%TTUR is the best paper on GAN after WGAN. TTUR actually proposes a mechanism for preventing gradient exploding at zero cost.
%\subsection{Comparing generalization capability of WGAN and GAN}
%Can we show that the empirical W-1 distance tracks the true W-1 distance? Yes, it has been shown in \cite{equiAndGeneralization}.
%
%Optimal critic in WGAN is generalizable, the error bound is only the sample variance (which could be large for high dimensional data), which decreases as the number of training examples increases. The optimal WGAN can converge to $\epsilon$-equilibrium. The optimal GAN does not. We can improve generalizability of GAN by regularization but regularizers introduce bias-variance trade off. In other words, we cannot know how much regularization is enough. Commonly, a validation dataset is used to estimate generalization capability. Using validation dataset in GAN training is expensive: we need to validate the discriminator one every generator iteration. For WGAN, we know that more accurate critic implies better model distribution. TTUR is better than the heuristic in WGAN: train $D$ for $n$ updates, train $G$ for 1 update.
%
%The generalization bound of GAN depends on the Lipschitz constant (theorem 3.1 \cite{equiAndGeneralization})
%
%We note that adding the Lipschitz constraint improve the generalization capability of GAN in some cases (absolutely continuous distributions?) but it may hurts in some other cases (because $|D(x) - D(y)| < L || x - y ||$, that means $D(y) > 0$ for all $y$ that satisfy $L || x - y || < 1$. This is not true as well. Example: true distribution consists of two disjoint regions, points out side of these region should have $D(y) = 0$ but due to Lipschitz constraint, $D(y) > 0$ for $y$ close to the region boundaries). The Lipschitz constant is a hyper parameter to be tunned and no single value is optimal for all distributions. In conclusion, theoretically, GAN cannot learn distribution, we cannot know the exact amount of regularization to make GAN converge to the true distribution. WGAN, on the other hands, has a known Lipschitz constant of 1 and can converge theoretically.
%
%Can we characterize the generalization capability of WGAN by the variance of the W-1 distance and the $\epsilon$ in $\epsilon$-equilibrium?
%
%\section{GAN-GP is minimizing Wasserstein distance}
%\[\mathbb{E}[\log(D(x))] + \mathbb{E}[\log(1 - D(y))] \approx \mathbb{E}[D(x) - 1] - \mathbb{E}[D(y)] = \tilde{\mathcal{W}}\]
%for accurate enough discriminator. 
%If $x$ and $y$ are normalized, i.e. they lie inside $d$-dimensional unit ball then $|D(x) - D(y)| \le || x- y || < 2\sqrt{d}$ which means $\tilde{\mathcal{W}}$ can approximate $\mathcal{W}$ reasonably well. Also $|\sup(D)(x) - \inf(D)(y)| \le || x - y ||$ means that the co-domain of $D(x) \in [a, b]$ where $b - a < 2 \sqrt{d}$. 
%%\[\tilde{\mathcal{W}} > \frac{1}{2} \mathcal{W}\]
%
%That have the form of Wasserstein distance but $D$ is not warranted to be Lipschitz. GAN-GP introduce Lipschitzness to the original GAN and turn the original GAN to WGAN-GP. That mean GAN-GP indeed minimizing the Wasserstein distance and the claim that "GAN do not need to minimize a divergence at every step" is false.
%
%Show the evolution of the distribution on synthetic data: GAN-GP behaves like WGAN: it first captures the overall structure of the distribution then focuses on the mode. GAN does not do this. It often collapses to a few modes. When it can learn the distribution by accident, it do it in a different way.
%\section{Improving generalization capability of GAN}
%Our method balance between two extreme: a perfect discriminative model and a perfectly generalizable model. 
%
%Analysis in previous section suggest that $D(x)$ must converge to $D(y)$ as $x$ goes to $y$. The original GAN objective prevent this to happen. To improve the generalizability of GAN, we must limits the gradient of D w.r.t. its inputs. There are two commonly used methods: gradient penalties and weight normalization.
%
%Note: replay to counter catastrophic forgetting, 1 iter D 1 iter G is a form of early stopping. 
%\subsection{Gradient penalties}
%Zero centered GP. Unlike 1 centered GP, this guarantee convergence and helps distribute the capacity equally over the space between real and fake distribution.
%
%When 1-centered GP is used, many random fixed points (local optima) are created over the space. 0-centered gradpen, the only fixed points are true data. Experiment with swissroll dataset confirm this.
%
%0-centered gradpen, in theory have better generalization. 0-centered gradpen converge to true distribution with carefully tunned weights.
%
%Show that gradient tends to 0 as two distribution get closer to each other.
%
%Disadvantages: 0-centered gradpen converges more slowly than 1-centered gradpen. This is expected as the gradient is smaller.
%
%Remark: TTUR is the best invention, it improves the performance of everything.
%%\subsection{Weight normalization}
%%L2 penalty, spectral normalization
%
%\section{Experiments}
%Show WGAN-1-GP is non-convergent: it moves between modes during training. Our 0-GP is convergent for any kind of GAN: a fake datapoint converges to a fixed real datapoint.
%
%
%0-GP is more robust to change in optimizers. 0-GP works with various settings of Adam. 1-GP requires more careful tuning.
%
%\section{Related works}
%
%\section{Conclusion}
\bibliography{hgan}
\bibliographystyle{iclr2019_conference}

\appendix
\section{Proof for Proposition \ref{prop:disjoint}}
\label{appx:disjoint}
For continuous random variable $\bm V$, $\mathbb{P}(\bm  V= \bm v) = 0$ for any $\bm v$. The probability of finding a noise vector $\bm z$ such that $G(\bm z)$ is exactly equal to a real datapoint $\bm x \in \mathcal{D}_r$ via random sampling is 0. Therefore, the probability of a real datapoint $\bm x_i$ being in the fake dataset $\mathcal{D}_g$ is 0. Similarly, the probability of any fake datapoint being in the real dataset is 0. 
\begin{eqnarray}
\mathbb{P}(\bm x \in \mathcal{D}_g^{(t)}) & = & 0, \forall \bm x \in \mathcal{D}_r, t \in \mathbb{N} \nonumber \\
\mathbb{P}(\bm y \in \mathcal{D}_r) & = & 0, \forall \bm y \in \mathcal{D}_g^{(t)}, t \in \mathbb{N} \nonumber\\
\mathbb{P}(\mathcal{D}_r \cap \mathcal{D}_g^{(t)} = \emptyset) & = & 1, \forall t \in \mathbb{N} \label{eqn:disjoint}
\end{eqnarray}
Furthermore, due to the curse of dimensionality, the probability of sampling a datapoint which is close to another datapoint in high dimensional space also decrease exponentially. The distances between datapoints are larger in higher dimensional space. That suggests that it is easier to separate $\mathcal{D}_r$ and $\mathcal{D}_g^{(t)}$ in higher dimensional space.
\section{Constructing $\epsilon$-optimal discriminators}
\label{appx:constructOptimalD}

To make the construction process simpler, let's assume that samples are normalized:
\[ \norm{\bm x_i} = \norm{\bm y_j} = 1, \forall \bm x_i \in \mathcal{D}_r, \bm y_j \in \mathcal{D}_g \]
Let's use the following new notations for real and fake samples:
\begin{eqnarray*}
\mathcal{D} & = & \mathcal{D}_r \cup \mathcal{D}_g = \left\lbrace \bm v_1, ..., \bm v_{m + n} \right\rbrace \\
\bm v_i & = & 
\begin{cases}
\bm x_i \text{, for } i = 1, ..., n \\
\bm y_{i - n} \text{, for } i = n + 1, ..., n + m
\end{cases}
\end{eqnarray*}
We construct the $\epsilon$-optimal discriminator $D$ as a MLP with 1 hidden layer. Let $\bm W_1 \in \mathbb{R}^{(m + n) \times d_x}$ and $\bm W_2 \in \mathbb{R}^{m + n}$ be the weight matrices of $D$. The total number of parameters in $D$ is $d_x(m + n) + (m + n) = \mathcal{O}(d_x(m + n))$. We set the value of $\bm W_1$ as
\begin{eqnarray*}
\bm W_1 & = & k 
\left[
\begin{matrix}
\bm v_1^\top \\
\vdots \\
\bm v_{n + m}^\top
\end{matrix}
\right] 
\end{eqnarray*}
and $\bm W_2$ as
\begin{eqnarray*}
W_{2, i} & = & \frac{1}{2} + \frac{\epsilon}{2} + \alpha \text{, for } i = 1, ..., n \\
W_{2, i} & = & \frac{1}{2} - \frac{\epsilon}{2} - \alpha \text{, for } i = n + 1, ..., n + m \\
\alpha & > & 0
\end{eqnarray*}



Given an input $\bm v \in \mathcal{D}$, the output is computed as:
\[ D(\bm v) = \bm W_2^\top \sigma(\bm W_1 \bm v) \]
where $\sigma$ is the softmax function. Let $\bm a = \bm W_1 \bm v$, we have
\[ a_i = k \bm v_i^\top \bm v = 
\begin{cases}
k, & \text{ if } \bm v = \bm v_i \\
< k, & \text{ if } \bm v \neq \bm v_i
\end{cases}
 \]

As $k \rightarrow \infty$, $\sigma(\bm W_1 \bm v_i)$ becomes a one-hot vector with the $i$-th element being 1, all other elements being 0. Thus, for large enough $k$, for any $\bm v_j \in \mathcal{D}$, the output of the network is
\[ D(\bm v_j) = \bm W_2^\top \sigma(\bm W_1 \bm v_j) \approx W_{2, j} = 
\begin{cases}
> \frac{1}{2} + \frac{\epsilon}{2}, \text{ for } j = 1, ..., n \\
< \frac{1}{2} - \frac{\epsilon}{2}, \text{ for } j = n + 1, ..., n + m
\end{cases}
\]
$D$ is a $\epsilon$-optimal discriminator for dataset $\mathcal{D}$.

\section{Fixed-Alt-GD cannot maintain the optimality of $\epsilon$-discriminators}
\label{appx:maintainOptimal}
Let's consider the case where the real and the fake dataset each contain a single datapoint $\mathcal{D}_r = \left\lbrace \bm x \right\rbrace$, $\mathcal{D}_g^{(t)} = \bm y^{(t)}$, and the discriminator and the generator are linear:
\begin{eqnarray*}
D(\bm v) & = & \bm \theta_D^\top \bm v \\
G(\bm z) & = & \bm \theta_G \bm z
\end{eqnarray*}
and the objective is also linear (Wasserstein GAN's objective):
\begin{eqnarray}
 \mathcal{L_{\mathcal{W}}} & = & \mathbb{E}_{\bm x \in \mathcal{D}_r}[D(\bm x)] - \mathbb{E}_{\bm y \in \mathcal{D}_g^{(t)}}[D(\bm y)] \nonumber \\
 & = & D(\bm x) - D(\bm y^{(t)}) \nonumber 
\end{eqnarray}
The same learning rate $\alpha$ is used for $D$ and $G$.

At step $t$, the discriminator is $\epsilon$-optimal
\begin{eqnarray}
D(\bm x) - D(\bm y^{(t)}) & = & \bm \theta_D^\top \left( \bm x - \bm y^{(t)} \right) \ge \epsilon  \label{eqn:linearEpsOptimal} \\
\norm{\bm \theta_D} & \ge & \frac{\epsilon}{\norm{\bm x - \bm y^{(t)}}} \label{eqn:thetaNormLowerBound}
\end{eqnarray}
The gradients w.r.t. $\bm \theta_D$ and $\bm \theta_G$ are
\begin{eqnarray}
\frac{\partial \mathcal{L}}{\partial \bm \theta_D} & = & \bm x - \bm y^{(t)} \\
\frac{\partial \mathcal{L}}{\partial \bm \theta_G} & = & \frac{\partial \mathcal{L}}{\partial \bm y^{(t)}} \times \bm z^\top \nonumber \\
& = & \bm \theta_D \times \bm z^\top \label{eqn:gradThetaG}
\end{eqnarray}
If the learning rate $\alpha$ is small enough, $\norm{\bm x - \bm y^{(t)}}$ should decrease as $t$ increases. As the empirical fake distribution converges to the empirical real distribution, $\norm{\bm x - \bm y^{(t)}} \rightarrow 0$. The norm of gradient w.r.t. $\bm \theta_D$, therefore, decreases as $t$ increases and vanishes when the two empirical distributions are the same. 
From Eqn. \ref{eqn:thetaNormLowerBound}, we see that, in order to maintain $D$'s $\epsilon$-optimality when $\norm{\bm x - \bm y^{(t)}}$ decreases, $\norm{\bm \theta_D}$ has to increase. 
From Eqn. \ref{eqn:thetaNormLowerBound} and \ref{eqn:gradThetaG}, we see that the gradient w.r.t. $\bm \theta_G$ grows as the two empirical distributions are more similar. As $\norm{\bm x - \bm y^{(t)}} \rightarrow 0$,
\begin{eqnarray}
\frac{\norm{\frac{\partial \mathcal{L}_{\mathcal{W}}}{\partial \bm \theta_D}}}{\norm{\frac{\partial \mathcal{L}_{\mathcal{W}}}{\partial \bm \theta_G}}} \rightarrow 0
\end{eqnarray}

Because the same learning rate $\alpha$ is used for both $G$ and $D$, $G$ will learn much faster than $D$. Furthermore, because $\norm{\bm x - \bm y^{(t)}}$ decreases as $t $ increases, the difference
\[ \frac{\epsilon}{\norm{\bm x - \bm y^{(t + 1)}}} - \frac{\epsilon}{\norm{\bm x - \bm y^{(t)}}} \]
increases with $t$. The number of gradient steps that $D$ has to take to reach the next $\epsilon$-optimal state increases, and goes to infinity as $\norm{\bm x - \bm y^{(t)}} \rightarrow 0$. Therefore, gradient descent with fixed number of updates to $\bm \theta_D$ cannot maintain the optimality of $D$.

The derivation for the objective in Eqn. \ref{eqn:ganloss} is similar.

%\section{The relationship between $\lambda_2$ and $\epsilon$} \label{appx:valueRegularization}

\section{Results on different datasets}\label{appx:result}

\begin{figure}[h!]
\centering
\subfloat[Iter. 0] {\includegraphics[width=0.19\textwidth]{figs/gan_8Gaussians_gradfield_center_0.00_alpha_None_lambda_100.00_lrg_0.00300_lrd_0.00900_nhidden_512_scale_10.00_optim_SGD/fig_00000.pdf}}
\subfloat[Iter. 1,000] {\includegraphics[width=0.19\textwidth]{figs/gan_8Gaussians_gradfield_center_0.00_alpha_None_lambda_100.00_lrg_0.00300_lrd_0.00900_nhidden_512_scale_10.00_optim_SGD/fig_01000.pdf}}
\subfloat[Iter. 2,000] {\includegraphics[width=0.19\textwidth]{figs/gan_8Gaussians_gradfield_center_0.00_alpha_None_lambda_100.00_lrg_0.00300_lrd_0.00900_nhidden_512_scale_10.00_optim_SGD/fig_02000.pdf}}
\subfloat[Iter. 5,000] {\includegraphics[width=0.19\textwidth]{figs/gan_8Gaussians_gradfield_center_0.00_alpha_None_lambda_100.00_lrg_0.00300_lrd_0.00900_nhidden_512_scale_10.00_optim_SGD/fig_05000.pdf}}
\subfloat[Iter. 20,000] {\includegraphics[width=0.19\textwidth]{figs/gan_8Gaussians_gradfield_center_0.00_alpha_None_lambda_100.00_lrg_0.00300_lrd_0.00900_nhidden_512_scale_10.00_optim_SGD/fig_20000.pdf}}
\caption{Evolution of GAN-0-GP with $\lambda = 100$ on 8 Gaussians dataset.}
\label{fig:0gpEvolution}
\end{figure}

\begin{figure}[h!]
\centering
\subfloat[GAN-0-GP] {\includegraphics[width=0.19\textwidth]{figs/gan_swissroll_gradfield_center_0.00_alpha_None_lambda_0.10_lrg_0.00010_lrd_0.00010_nhidden_512/fig_10000.pdf}}
\subfloat[GAN-1-GP] {\includegraphics[width=0.19\textwidth]{figs/gan_swissroll_gradfield_center_1.00_alpha_None_lambda_0.10_lrg_0.00010_lrd_0.00010_nhidden_512/fig_10000.pdf}}

\caption{GANs trained with different gradient penalty on swissroll dataset. Although GAN-1-GP is able to learn the distribution, the gradient field has bad pattern. GAN-1-GP is more sensitive to change in hyper parameters and optimizers. GAN-1-GP fails to learn the scaled up version of the distribution.}
\end{figure}

\begin{figure}[h!]
\centering
\subfloat[] {\includegraphics[width=0.19\textwidth]{figs/gan_mnist_center_0.00_alpha_None_lambda_0.00_lrg_0.00030_lrd_0.00030_nhidden_512_nz_50_ncritic_1/mnist_iter_010000.png} \label{fig:noGPMNIST10k}}
\subfloat[] {\includegraphics[width=0.19\textwidth]{figs/gan_mnist_center_0.00_alpha_1.0_lambda_100.00_lrg_0.00030_lrd_0.00030_nhidden_512_nz_50_ncritic_1/mnist_iter_010000.png} \label{fig:0realMNIST10k}}
\subfloat[] {\includegraphics[width=0.19\textwidth]{figs/gan_mnist_center_1.00_alpha_None_lambda_100.00_lrg_0.00030_lrd_0.00030_nhidden_512_nz_50_ncritic_1/mnist_iter_010000.png} \label{fig:1GPMNIST10k}}
\subfloat[] {\includegraphics[width=0.19\textwidth]{figs/gan_mnist_center_0.00_alpha_None_lambda_100.00_lrg_0.00030_lrd_0.00030_nhidden_512_nz_50_ncritic_1/mnist_iter_010000.png} \label{fig:0GPMNIST10kappx}}

\caption{Result on MNIST. Adam was initialized with $\beta_1 = 0.5, \beta_2 = 0.9$. 
\protect\subref{fig:noGPMNIST10k} No GP, iteration 10,000.
\protect\subref{fig:0realMNIST10k} Zero-centered GP on real samples only with $\lambda = 100$, iteration 10,000.
\protect\subref{fig:1GPMNIST10k} One-centered GP with $\lambda = 100$, iteration 10,000.
\protect\subref{fig:0GPMNIST10kappx} Our zero-centered GP with $\lambda = 100$, iteration 10,000.
}
\label{fig:mnist10kappx}
\end{figure}

\begin{figure}[h!]
\centering
\subfloat[] {\includegraphics[width=0.19\textwidth]{figs/gan_mnist_center_0.00_alpha_1.0_lambda_100.00_lrg_0.00030_lrd_0.00030_nhidden_512_nz_50_ncritic_1_beta1_0.90_beta2_0.99/mnist_iter_010000.png} \label{fig:largeAdam0realMNIST10k}}
\subfloat[] {\includegraphics[width=0.19\textwidth]{figs/gan_mnist_center_0.00_alpha_None_lambda_100.00_lrg_0.00030_lrd_0.00030_nhidden_512_nz_50_ncritic_1_beta1_0.90_beta2_0.99/mnist_iter_010000.png} \label{fig:largeAdam0GPMNIST10k}}

\caption{Result on MNIST. Adam was initialized with $\beta_1 = 0.9, \beta_2 = 0.99$. 
\protect\subref{fig:largeAdam0realMNIST10k} Zero-centered GP on real samples only with $\lambda = 100$, iteration 10,000.
\protect\subref{fig:largeAdam0GPMNIST10k} Our zero-centered GP with $\lambda = 100$, iteration 10,000.
}
\label{fig:mnist10k}
\end{figure}

\begin{figure}[h!]
\centering
\subfloat[] {\includegraphics[width=0.5\textwidth]{figs/gan_mnist_center_0.00_alpha_1.0_lambda_100.00_lrg_0.00030_lrd_0.00030_nhidden_512_nz_50_ncritic_1/mnist_interpolation_iter_050000.png} \label{fig:inter0real}} 
\subfloat[] {\includegraphics[width=0.5\textwidth]{figs/gan_mnist_center_0.00_alpha_None_lambda_100.00_lrg_0.00030_lrd_0.00030_nhidden_512_nz_50_ncritic_1/mnist_interpolation_iter_065000.png} \label{fig:inter0gp}}

\caption{Linear latent space interpolation between two random samples. \protect\subref{fig:inter0real} GAN with 0-GP-sample cannot perform smooth interpolation between modes. Small changes in input latent variable result in big difference in the output (red boxes). The result suggest that 0-GP-sample makes GANs to remember the training dataset and do not generalize to the region between samples in the training dataset. \protect\subref{fig:inter0gp} GAN with our 0-GP can perform smooth interpolation between modes. The behavior implies that GANs with our 0-GP have better generalization.}
\label{fig:interpolation}
\end{figure}

\begin{figure}[h!]
\centering
\subfloat[All categories] {\includegraphics[width=.7\textwidth]{figs/all}} \\
\subfloat[Welsh springer spaniel] {\includegraphics[width=.7\textwidth]{figs/dog}} \\

\caption{Samples from GAN-0-GP trained on ImageNet.}
\end{figure}

\section{Implementation details}
We used Pytorch \citep{pytorch} for development.

\subsection{Synthetic and MNIST datasets}
Generator architecture in synthetic and MNIST experiments
%\begin{table}
\begin{center}
\begin{tabular}{|c|}
\hline
Fully connected layer $2 \times nhidden$ \\
\hline
$\downarrow$ \\
\hline
ReLU \\
\hline
$\downarrow$ \\
\hline
Fully connected layer $nhidden \times nhidden$ \\
\hline
$\downarrow$ \\
\hline
ReLU \\
\hline 
$\downarrow$ \\
\hline
Fully connected layer $nhidden \times nhidden$ \\
\hline
$\downarrow$ \\
\hline
Fully connected layer $nhidden \times 2$ \\
\hline
\end{tabular}
%\end{table}
\end{center}

Discriminator architecture in synthetic and MNIST experiments
\begin{center}
\begin{tabular}{|c|}
\hline
Fully connected layer $2 \times nhidden$ \\
\hline
$\downarrow$ \\
\hline
ReLU \\
\hline
$\downarrow$ \\
\hline
Fully connected layer $nhidden \times nhidden$ \\
\hline
$\downarrow$ \\
\hline
ReLU \\
\hline 
$\downarrow$ \\
\hline
Fully connected layer $nhidden \times nhidden$ \\
\hline
$\downarrow$ \\
\hline
Fully connected layer $nhidden \times 1$ \\
\hline
$\downarrow$ \\
\hline
Sigmoid \\
\hline
\end{tabular}
%\end{table}
\end{center}


Hyper parameters for synthetic and MNIST experiments
%\begin{table}
\begin{center}
\begin{tabular}{|l|l|}
\hline
Learning rate & 0.003 for both $G$ and $D$ \\
\hline
Learning rate TTUR & 0.003 for $G$, 0.009 for $D$ \\
\hline
%Optimizer & plain SGD \\
%\hline
\end{tabular}
\end{center}

\subsection{ImageNet}
The entire ImageNet dataset with all 1000 classes was used in the experiment. Because of our hardware limits, we used images of size $64 \times 64$. We used the code from \cite{whichGANConverge}, available at \url{https://github.com/LMescheder/GAN_stability}, for our experiment. Generator and Discriminator are ResNets, each contains 5 residual blocks. All GANs in our experiment have the same architectures and hyper parameters. The configuration for WGAN-GP5 is as follows.

\begin{verbatim}
generator:
  name: resnet2
  kwargs:
    nfilter: 32
    nfilter_max: 512
    embed_size: 128
discriminator:
  name: resnet2
  kwargs:
    nfilter: 32
    nfilter_max: 512
    embed_size: 128
z_dist:
  type: gauss
  dim: 128
training:
  out_dir: ../output/imagenet_wgangp5_TTUR
  gan_type: wgan
  reg_type: wgangp
  reg_param: 10.
  batch_size: 64
  nworkers: 32
  take_model_average: true
  model_average_beta: 0.999
  model_average_reinit: false
  monitoring: tensorboard
  sample_every: 1000
  sample_nlabels: 20
  inception_every: 10000
  save_every: 900
  backup_every: 100000
  restart_every: -1
  optimizer: adam
  lr_g: 0.0001
  lr_d: 0.0003
  lr_anneal: 1.
  lr_anneal_every: 150000
  d_steps: 5
  equalize_lr: false
\end{verbatim}

\section{Finding a better path between a pair of samples}
\label{appx:pathFinding}
Because the set of real data is unlikely to be convex, a linear interpolation between two datapoints is unlikely to be in the set. For example, the weighted average of two real images is often not a real image. If the generator is good enough then its output is likely to be on the real data manifold. A path $\mathcal{C}$ from $\bm y$ to $\bm x$ can be found by transforming the straight line between the two corresponding latent codes, to the data space using the generator $G$. To get the latent code of real datapoints, we use an encoder $E$ which is trained to map real data to normally distributed latent codes. The pseudo code of the path finding algorithm is shown in Alg. \ref{alg:pathFinding}.

\begin{algorithm}
\caption{Path finding algorithm}
\label{alg:pathFinding}
\KwData{an encoder $E$; a pair of samples $\bm x, \bm y = G(\bm z)$; }
 \KwResult{interpolated datapoint $\tilde{\bm x}$}
 Get the latent code of $\bm x$: $\bm z_x = E(\bm x)$ \\
 Calculate the interpolated latent code: $\tilde{\bm z} = \alpha \bm z_x + (1 - \alpha) \bm z$ \\
 Generate the interpolated datapoint: $\tilde{\bm x} = G(\tilde{\bm z})$
\end{algorithm}
\end{document}

%%%%% NEW MATH DEFINITIONS %%%%%

\usepackage{amsmath,amsfonts,bm}

% Mark sections of captions for referring to divisions of figures
\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

% Highlight a newly defined term
\newcommand{\newterm}[1]{{\bf #1}}


% Figure reference, lower-case.
\def\figref#1{figure~\ref{#1}}
% Figure reference, capital. For start of sentence
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
% Section reference, lower-case.
\def\secref#1{section~\ref{#1}}
% Section reference, capital.
\def\Secref#1{Section~\ref{#1}}
% Reference to two sections.
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
% Reference to three sections.
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
% Reference to an equation, lower-case.
\def\eqref#1{equation~\ref{#1}}
% Reference to an equation, upper case
\def\Eqref#1{Equation~\ref{#1}}
% A raw reference to an equation---avoid using if possible
\def\plaineqref#1{\ref{#1}}
% Reference to a chapter, lower-case.
\def\chapref#1{chapter~\ref{#1}}
% Reference to an equation, upper case.
\def\Chapref#1{Chapter~\ref{#1}}
% Reference to a range of chapters
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
% Reference to an algorithm, lower-case.
\def\algref#1{algorithm~\ref{#1}}
% Reference to an algorithm, upper case.
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
% Reference to a part, lower case
\def\partref#1{part~\ref{#1}}
% Reference to a part, upper case
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


% Random variables
\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
% rm is already a command, just don't name any random variables m
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

% Random vectors
\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

% Elements of random vectors
\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

% Random matrices
\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

% Elements of random matrices
\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

% Vectors
\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

% Elements of vectors
\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

% Matrix
\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

% Tensor
\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


% Graph
\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

% Sets
\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
% Don't use a set called E, because this would be the same as our symbol
% for expectation.
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

% Entries of a matrix
\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

% entries of a tensor
% Same font as tensor, without \bm wrapper
\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

% The true underlying data generating distribution
\newcommand{\pdata}{p_{\rm{data}}}
% The empirical distribution defined by the training set
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
% The model distribution
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
% Stochastic autoencoder distributions
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} % Laplace distribution

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
% Wolfram Mathworld says $L^2$ is for function spaces and $\ell^2$ is for vectors
% But then they seem to use $L^2$ for vectors throughout the site, and so does
% wikipedia.
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} % See usage in notation.tex. Chosen to match Daphne's book.

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak

