@article{local-competition,
 author = {Panousis, Konstantinos P and Chatzis, Sotirios and Theodoridis, Sergios},
 journal = {arXiv preprint arXiv:1805.07624},
 title = {Nonparametric Bayesian Deep Networks with Local Competition},
 year = {2018}
}

@inproceedings{peng-collaborative,
 author = {Peng, Hanyu and Wu, Jiaxiang and Chen, Shifeng and Huang, Junzhou},
 booktitle = {International Conference on Machine Learning},
 pages = {5113--5122},
 title = {Collaborative Channel Pruning for Deep Networks},
 year = {2019}
}

@article{eigenDamage,
 author = {Wang, Chaoqi and Grosse, Roger and Fidler, Sanja and Zhang, Guodong},
 journal = {arXiv preprint arXiv:1905.05934},
 title = {EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis},
 year = {2019}
}

@inproceedings{SNIP,
  author    = {Namhoon Lee and
               Thalaiyasingam Ajanthan and
               Philip H. S. Torr},
  title     = {Snip: single-Shot Network Pruning based on Connection sensitivity},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
               New Orleans, LA, USA, May 6-9, 2019},
  publisher = {OpenReview.net},
  year      = {2019},
  url       = {https://openreview.net/forum?id=B1VZqjAcYX},
}

@article{lottery-ticket-followup,
 author = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M and Carbin, Michael},
 journal = {arXiv preprint arXiv:1903.01611},
 title = {The Lottery Ticket Hypothesis at Scale},
 year = {2019}
}

@inproceedings{rethinking-net-pruning,
  author    = {Zhuang Liu and
               Mingjie Sun and
               Tinghui Zhou and
               Gao Huang and
               Trevor Darrell},
  title     = {Rethinking the Value of Network Pruning},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
               New Orleans, LA, USA, May 6-9, 2019},
  publisher = {OpenReview.net},
  year      = {2019},
  url       = {https://openreview.net/forum?id=rJlnB3C5Ym},
}

@article{net-trim-v2,
 author = {Aghasi, Alireza and Abdi, Afshin and Romberg, Justin},
 journal = {arXiv preprint arXiv:1806.06457},
 title = {Fast Convex Pruning of Deep Neural Networks},
 year = {2018}
}

@article{mit-coreset-pruning,
 author = {Baykal, Cenk and Liebenwein, Lucas and Gilitschenski, Igor and Feldman, Dan and Rus, Daniela},
 journal = {arXiv preprint arXiv:1804.05345},
 title = {Data-dependent coresets for compressing neural networks with applications to generalization bounds},
 year = {2018}
}

@article{samsung-winograd-sparse,
 author = {Choi, Yoojin and El-Khamy, Mostafa and Lee, Jungwon},
 journal = {arXiv preprint arXiv:1902.08192},
 title = {Jointly Sparse Convolutional Neural Networks in Dual Spatial-Winograd Domains},
 year = {2019}
}

@misc{google-state-of-sparsity,
 archiveprefix = {arXiv},
 author = {Trevor Gale and Erich Elsen and Sara Hooker},
 eprint = {1902.09574},
 primaryclass = {cs.LG},
 title = {The State of Sparsity in Deep Neural Networks},
 year = {2019}
}

@article{autopruner,
 author = {Luo, Jian-Hao and Wu, Jianxin},
 journal = {arXiv preprint arXiv:1805.08941},
 title = {Autopruner: An end-to-end trainable filter pruning method for efficient deep model inference},
 year = {2018}
}

@inproceedings{synaptic-strength,
 author = {Lin, Chen and Zhong, Zhao and Wei, Wu and Yan, Junjie},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {10149--10158},
 title = {Synaptic Strength For Convolutional Neural Network},
 year = {2018}
}

@article{pcas,
 author = {Yamamoto, Kohei and Maeno, Kurato},
 journal = {arXiv preprint arXiv:1806.05382},
 title = {Pcas: Pruning channels with attention statistics},
 year = {2018}
}

@article{balanced-sparsity,
 author = {Yao, Zhuliang and Cao, Shijie and Xiao, Wencong},
 journal = {arXiv preprint arXiv:1811.00206},
 title = {Balanced Sparsity for Efficient DNN Inference on GPU},
 year = {2018}
}

@inproceedings{zhuang-discriminative-channel,
 author = {Zhuang, Zhuangwei and Tan, Mingkui and Zhuang, Bohan and Liu, Jing and Guo, Yong and Wu, Qingyao and Huang, Junzhou and Zhu, Jinhui},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {875--886},
 title = {Discrimination-aware channel pruning for deep neural networks},
 year = {2018}
}

@inproceedings{lottery-ticket,
  author    = {Jonathan Frankle and
               Michael Carbin},
  title     = {The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
               New Orleans, LA, USA, May 6-9, 2019},
  publisher = {OpenReview.net},
  year      = {2019},
  url       = {https://openreview.net/forum?id=rJl-b3RcF7}
}

@inproceedings{uiuc-coreset-pruning,
 author = {Dubey, Abhimanyu and Chatterjee, Moitreya and Ahuja, Narendra},
 booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
 pages = {454--470},
 title = {Coreset-based neural network compression},
 year = {2018}
}

@inproceedings{sss,
 author = {Huang, Zehao and Wang, Naiyan},
 booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
 pages = {304--320},
 title = {Data-driven sparse structure selection for deep neural networks},
 year = {2018}
}

@article{sparse-evolutionary,
 author = {Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H and Gibescu, Madeleine and Liotta, Antonio},
 journal = {Nature communications},
 number = {1},
 pages = {2383},
 publisher = {Nature Publishing Group},
 title = {Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science},
 volume = {9},
 year = {2018}
}

@inproceedings{extreme-net-compress,
 author = {Peng, Bo and Tan, Wenming and Li, Zheyang and Zhang, Shun and Xie, Di and Pu, Shiliang},
 booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
 pages = {300--316},
 title = {Extreme network compression via filter group approximation},
 year = {2018}
}

@article{apple-pfa,
 author = {Suau, Xavier and Zappella, Luca and Apostoloff, Nicholas},
 title = {NETWORK COMPRESSION USING CORRELATION ANALYSIS OF LAYER RESPONSES},
 year = {2018}
}

@inproceedings{soft-filter-pruning,
 author = {He, Y and Kang, G and Dong, X and Fu, Y and Yang, Y},
 booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
 title = {Soft filter pruning for accelerating deep convolutional neural networks},
 year = {2018}
}

@inproceedings{amc-automl-han,
 author = {He, Yihui and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Li, Li-Jia and Han, Song},
 booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
 pages = {784--800},
 title = {Amc: Automl for model compression and acceleration on mobile devices},
 year = {2018}
}

@article{spectral-pruning,
 author = {Suzuki, Taiji and Abe, Hiroshi and Murata, Tomoya and Horiuchi, Shingo and Ito, Kotaro and Wachi, Tokuma and Hirai, So and Yukishima, Masatoshi and Nishimura, Tomoaki},
 journal = {arXiv preprint arXiv:1808.08558},
 title = {Spectral-Pruning: Compressing deep neural network via spectral analysis},
 year = {2018}
}

@inproceedings{learning-compression,
 author = {Carreira-Perpin{\'a}n, Miguel A and Idelbayev, Yerlan},
 booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
 pages = {8532--8541},
 title = {“Learning-Compression” Algorithms for Neural Net Pruning},
 year = {2018}
}

@article{smallify,
 author = {Leclerc, Guillaume and Vartak, Manasi and Fernandez, Raul Castro and Kraska, Tim and Madden, Samuel},
 journal = {arXiv preprint arXiv:1806.03723},
 title = {Smallify: Learning Network Size while Training},
 year = {2018}
}

@inproceedings{nisp,
 author = {Yu, Ruichi and Li, Ang and Chen, Chun-Fu and Lai, Jui-Hsin and Morariu, Vlad I and Han, Xintong and Gao, Mingfei and Lin, Ching-Yung and Davis, Larry S},
 booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
 pages = {9194--9203},
 title = {Nisp: Pruning networks using neuron importance score propagation},
 year = {2018}
}

@article{dai-info-bottleneck,
 author = {Dai, Bin and Zhu, Chen and Wipf, David},
 journal = {arXiv preprint arXiv:1802.10399},
 title = {Compressing neural networks using the variational information bottleneck},
 year = {2018}
}

@inproceedings{huang-prune-filters,
 author = {Huang, Qiangui and Zhou, Kevin and You, Suya and Neumann, Ulrich},
 booktitle = {2018 IEEE Winter Conference on Applications of Computer Vision (WACV)},
 organization = {IEEE},
 pages = {709--718},
 title = {Learning to prune filters in convolutional neural networks},
 year = {2018}
}

@inproceedings{ding-auto-balanced,
 author = {Ding, Xiaohan and Ding, Guiguang and Han, Jungong and Tang, Sheng},
 booktitle = {Thirty-Second AAAI Conference on Artificial Intelligence},
 title = {Auto-balanced filter pruning for efficient convolutional neural networks},
 year = {2018}
}

@article{rethinking-smaller-norm,
 author = {Ye, Jianbo and Lu, Xin and Lin, Zhe and Wang, James Z},
 journal = {arXiv preprint arXiv:1802.00124},
 title = {Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers},
 year = {2018}
}

@article{lstm-group-lasso,
 author = {Wen, Wei and He, Yuxiong and Rajbhandari, Samyam and Zhang, Minjia and Wang, Wenhan and Liu, Fang and Hu, Bin and Chen, Yiran and Li, Hai},
 journal = {arXiv preprint arXiv:1709.05027},
 title = {Learning intrinsic sparse structures within long short-term memory},
 year = {2017}
}

@inproceedings{net-trim,
 author = {Aghasi, Alireza and Abdi, Afshin and Nguyen, Nam and Romberg, Justin},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {3177--3186},
 title = {Net-trim: Convex pruning of deep neural networks with performance guarantee},
 year = {2017}
}

@inproceedings{compression-aware-training,
 author = {Alvarez, Jose M and Salzmann, Mathieu},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {856--867},
 title = {Compression-aware training of deep networks},
 year = {2017}
}

@article{openai-block-sparse,
 author = {Gray, Scott and Radford, Alec and Kingma, Diederik P},
 journal = {arXiv preprint arXiv:1711.09224},
 title = {Gpu kernels for block-sparse weights},
 year = {2017}
}

@inproceedings{runtime-neural-pruning,
 author = {Lin, Ji and Rao, Yongming and Lu, Jiwen and Zhou, Jie},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {2181--2191},
 title = {Runtime neural pruning},
 year = {2017}
}

@article{hard-concrete,
 author = {Louizos, Christos and Welling, Max and Kingma, Diederik P},
 journal = {arXiv preprint arXiv:1712.01312},
 title = {Learning Sparse Neural Networks through $ L\_0 $ Regularization},
 year = {2017}
}

@inproceedings{bayesian-compression,
 author = {Louizos, Christos and Ullrich, Karen and Welling, Max},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {3288--3298},
 title = {Bayesian compression for deep learning},
 year = {2017}
}

@inproceedings{structured-log-normal,
 author = {Neklyudov, Kirill and Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry P},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {6775--6784},
 title = {Structured bayesian pruning via log-normal multiplicative noise},
 year = {2017}
}

@article{block-sparse-rnns,
 author = {Narang, Sharan and Undersander, Eric and Diamos, Gregory},
 journal = {arXiv preprint arXiv:1711.02782},
 title = {Block-sparse recurrent neural networks},
 year = {2017}
}

@inproceedings{channel-lasso-lstsq,
 author = {He, Yihui and Zhang, Xiangyu and Sun, Jian},
 booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
 pages = {1389--1397},
 title = {Channel pruning for accelerating very deep neural networks},
 year = {2017}
}

@inproceedings{network-slimming,
 author = {Liu, Zhuang and Li, Jianguo and Shen, Zhiqiang and Huang, Gao and Yan, Shoumeng and Zhang, Changshui},
 booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
 pages = {2736--2744},
 title = {Learning efficient convolutional networks through network slimming},
 year = {2017}
}

@inproceedings{more-is-less,
 author = {Dong, Xuanyi and Huang, Junshi and Yang, Yi and Yan, Shuicheng},
 booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
 pages = {5840--5848},
 title = {More is less: A more complicated network with less inference complexity},
 year = {2017}
}

@inproceedings{thinet-channel-norms,
 author = {Luo, Jian-Hao and Wu, Jianxin and Lin, Weiyao},
 booktitle = {Proceedings of the IEEE international conference on computer vision},
 pages = {5058--5066},
 title = {Thinet: A filter level pruning method for deep neural network compression},
 year = {2017}
}

@inproceedings{sze-energy-aware,
 author = {Yang, Tien-Ju and Chen, Yu-Hsin and Sze, Vivienne},
 booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
 pages = {5687--5695},
 title = {Designing energy-efficient convolutional neural networks using energy-aware pruning},
 year = {2017}
}

@inproceedings{sparse-variational-dropout,
 author = {Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
 booktitle = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
 organization = {JMLR. org},
 pages = {2498--2507},
 title = {Variational dropout sparsifies deep neural networks},
 year = {2017}
}

@inproceedings{babu-training-sparse,
 author = {Srinivas, Suraj and Subramanya, Akshayvarun and Venkatesh Babu, R},
 booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
 pages = {138--145},
 title = {Training sparse neural networks},
 year = {2017}
}

@article{nvidia-taylor-pruning,
 author = {Molchanov, Pavlo and Tyree, Stephen and Karras, Tero and Aila, Timo and Kautz, Jan},
 journal = {arXiv preprint arXiv:1611.06440},
 title = {Pruning convolutional neural networks for resource efficient inference},
 year = {2016}
}

@article{pruning-filters,
 author = {Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
 journal = {arXiv preprint arXiv:1608.08710},
 title = {Pruning filters for efficient convnets},
 year = {2016}
}

@article{google-interchannel,
 author = {Changpinyo, Soravit and Sandler, Mark and Zhmoginov, Andrey},
 journal = {arXiv preprint arXiv:1702.06257},
 title = {The power of sparsity in convolutional neural networks},
 year = {2017}
}

@article{welling-slow-quantize+prune,
 author = {Ullrich, Karen and Meeds, Edward and Welling, Max},
 journal = {arXiv preprint arXiv:1702.04008},
 title = {Soft weight-sharing for neural network compression},
 year = {2017}
}

@inproceedings{learning-num-neurons,
 author = {Alvarez, Jose M and Salzmann, Mathieu},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {2270--2278},
 title = {Learning the number of neurons in deep networks},
 year = {2016}
}

@inproceedings{perforated-cnns,
 author = {Figurnov, Mikhail and Ibraimova, Aizhan and Vetrov, Dmitry P and Kohli, Pushmeet},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {947--955},
 title = {Perforatedcnns: Acceleration through elimination of redundant convolutions},
 year = {2016}
}

@inproceedings{net-surgery,
 author = {Guo, Yiwen and Yao, Anbang and Chen, Yurong},
 booktitle = {Advances In Neural Information Processing Systems},
 pages = {1379--1387},
 title = {Dynamic network surgery for efficient dnns},
 year = {2016}
}

@inproceedings{ssl,
 author = {Wen, Wei and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
 booktitle = {Advances in neural information processing systems},
 pages = {2074--2082},
 title = {Learning structured sparsity in deep neural networks},
 year = {2016}
}

@article{babu-generalized-dropout,
 author = {Srinivas, Suraj and Babu, R Venkatesh},
 journal = {arXiv preprint arXiv:1611.06791},
 title = {Generalized dropout},
 year = {2016}
}

@article{course-pruning,
 author = {Anwar, Sajid and Sung, Wonyong},
 journal = {arXiv preprint arXiv:1610.09639},
 title = {Compact deep convolutional neural networks with coarse pruning},
 year = {2016}
}

@inproceedings{convnet-tensor-decomp,
 author = {Wang, Peisong and Cheng, Jian},
 booktitle = {Proceedings of the 24th ACM international conference on Multimedia},
 organization = {ACM},
 pages = {541--545},
 title = {Accelerating convolutional neural networks for mobile applications},
 year = {2016}
}

@article{babu-learning-architecture,
 author = {Srinivas, Suraj and Babu, R Venkatesh},
 journal = {arXiv preprint arXiv:1511.05497},
 title = {Learning neural network architectures using backpropagation},
 year = {2015}
}

@article{net-trimming-apoz,
 author = {Hu, Hengyuan and Peng, Rui and Tai, Yu-Wing and Tang, Chi-Keung},
 journal = {arXiv preprint arXiv:1607.03250},
 title = {Network trimming: A data-driven neuron pruning approach towards efficient deep architectures},
 year = {2016}
}

@article{pan-dropneuron,
 author = {Pan, Wei and Dong, Hao and Guo, Yike},
 journal = {arXiv preprint arXiv:1606.07326},
 title = {Dropneuron: Simplifying the structure of deep neural networks},
 year = {2016}
}

@article{group-sparse-dnns,
 author = {Scardapane, Simone and Comminiello, Danilo and Hussain, Amir and Uncini, Aurelio},
 journal = {Neurocomputing},
 pages = {81--89},
 publisher = {Elsevier},
 title = {Group sparse regularization for deep neural networks},
 volume = {241},
 year = {2017}
}

@inproceedings{lempitsky-fast-convnets,
 author = {Lebedev, Vadim and Lempitsky, Victor},
 booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
 pages = {2554--2564},
 title = {Fast convnets using group-wise brain damage},
 year = {2016}
}

@article{samsung-vbmf-tucker,
 author = {Kim, Yong-Deok and Park, Eunhyeok and Yoo, Sungjoo and Choi, Taelim and Yang, Lu and Shin, Dongjun},
 journal = {arXiv preprint arXiv:1511.06530},
 title = {Compression of deep convolutional neural networks for fast and low power mobile applications},
 year = {2015}
}

@article{divnet,
 author = {Mariet, Zelda and Sra, Suvrit},
 journal = {arXiv preprint arXiv:1511.05077},
 title = {Diversity networks: Neural network compression using determinantal point processes},
 year = {2015}
}

@inproceedings{han-prune-quant-huff,
  author    = {Song Han and
               Huizi Mao and
               William J. Dally},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Deep Compression: Compressing Deep Neural Network with Pruning, Trained
               Quantization and Huffman Coding},
  booktitle = {4th International Conference on Learning Representations, {ICLR} 2016,
               San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year      = {2016},
  url       = {http://arxiv.org/abs/1510.00149},
}

@article{tai-low-rank,
 author = {Tai, Cheng and Xiao, Tong and Zhang, Yi and Wang, Xiaogang and others},
 journal = {arXiv preprint arXiv:1511.06067},
 title = {Convolutional neural networks with low-rank regularization},
 year = {2015}
}

@inproceedings{learning-both,
 author = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
 booktitle = {Advances in neural information processing systems},
 pages = {1135--1143},
 title = {Learning both weights and connections for efficient neural network},
 year = {2015}
}

@article{zhang-accel-very-deep,
 author = {Zhang, Xiangyu and Zou, Jianhua and He, Kaiming and Sun, Jian},
 journal = {IEEE transactions on pattern analysis and machine intelligence},
 number = {10},
 pages = {1943--1955},
 publisher = {IEEE},
 title = {Accelerating very deep convolutional networks for classification and detection},
 volume = {38},
 year = {2015}
}

@article{face-prune,
 author = {Polyak, Adam and Wolf, Lior},
 journal = {IEEE Access},
 pages = {2163--2175},
 publisher = {IEEE},
 title = {Channel-level acceleration of deep face representations},
 volume = {3},
 year = {2015}
}

@article{babu-data-free-pruning,
 author = {Srinivas, Suraj and Babu, R Venkatesh},
 journal = {arXiv preprint arXiv:1507.06149},
 title = {Data-free parameter pruning for deep neural networks},
 year = {2015}
}

@inproceedings{liu-sparse-conv,
 author = {Liu, Baoyuan and Wang, Min and Foroosh, Hassan and Tappen, Marshall and Pensky, Marianna},
 booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
 pages = {806--814},
 title = {Sparse convolutional neural networks},
 year = {2015}
}

@article{lempitsky-cp-decomp,
 author = {Lebedev, Vadim and Ganin, Yaroslav and Rakhuba, Maksim and Oseledets, Ivan and Lempitsky, Victor},
 journal = {arXiv preprint arXiv:1412.6553},
 title = {Speeding-up convolutional neural networks using fine-tuned cp-decomposition},
 year = {2014}
}

@article{memory-bounded-convnets,
 author = {Collins, Maxwell D and Kohli, Pushmeet},
 journal = {arXiv preprint arXiv:1412.1442},
 title = {Memory bounded deep convolutional networks},
 year = {2014}
}

@inproceedings{exploit-linear-structure,
 author = {Denton, Emily L and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
 booktitle = {Advances in neural information processing systems},
 pages = {1269--1277},
 title = {Exploiting linear structure within convolutional networks for efficient evaluation},
 year = {2014}
}

@inproceedings{he-reshaping,
 author = {He, Tianxing and Fan, Yuchen and Qian, Yanmin and Tan, Tian and Yu, Kai},
 booktitle = {2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
 organization = {IEEE},
 pages = {245--249},
 title = {Reshaping deep neural network for fast decoding by node-pruning},
 year = {2014}
}

@inproceedings{jaderberg-low-rank-conv,
 author = {Jaderberg, Max and Vedaldi, Andrea and Zisserman, Andrew},
 booktitle = {Proceedings of the British Machine Vision Conference. BMVA Press},
 title = {Speeding up Convolutional Neural Networks with Low Rank Expansions},
 year = {2014}
}

@inproceedings{early-brain-damage,
 author = {Tresp, Volker and Neuneier, Ralph and Zimmermann, Hans-Georg},
 booktitle = {Advances in neural information processing systems},
 pages = {669--675},
 title = {Early brain damage},
 year = {1997}
}

@inproceedings{optimal-brain-surgeon,
 author = {Hassibi, Babak and Stork, David G and Wolff, Gregory J},
 booktitle = {IEEE international conference on neural networks},
 organization = {IEEE},
 pages = {293--299},
 title = {Optimal brain surgeon and general network pruning},
 year = {1993}
}

@inproceedings{optimal-brain-damage,
 author = {LeCun, Yann and Denker, John S and Solla, Sara A},
 booktitle = {Advances in neural information processing systems},
 pages = {598--605},
 title = {Optimal brain damage},
 year = {1990}
}

@article{vgg,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@inproceedings{alexnet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}

@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{resnet2,
  title={Identity mappings in deep residual networks},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={European conference on computer vision},
  pages={630--645},
  year={2016},
  organization={Springer}
}
@article{wrn,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}
@inproceedings{googlenet,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}
@inproceedings{densenet,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4700--4708},
  year={2017}
}
@inproceedings{squeezeExcite,
  title={Squeeze-and-excitation networks},
  author={Hu, Jie and Shen, Li and Sun, Gang},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7132--7141},
  year={2018}
}
@inproceedings{xception,
  title={Xception: Deep learning with depthwise separable convolutions},
  author={Chollet, Francois},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1251--1258},
  year={2017}
}
@article{maybeFirstDwSepConvNets,
  title={Rigid-motion scattering for image classification},
  author={Sifre, Laurent and Mallat, St{\'e}phane},
  journal={Ph. D. dissertation},
  year={2014},
  publisher={Citeseer}
}
@inproceedings{nasnet,
  title={Learning transferable architectures for scalable image recognition},
  author={Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={8697--8710},
  year={2018}
}
@article{gpipe,
  title={Gpipe: Efficient training of giant neural networks using pipeline parallelism},
  author={Huang, Yanping and Cheng, Yonglong and Chen, Dehao and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Chen, Zhifeng},
  journal={arXiv preprint arXiv:1811.06965},
  year={2018}
}
@article{efficientnet,
  title={EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
  author={Tan, Mingxing and Le, Quoc V},
  journal={arXiv preprint arXiv:1905.11946},
  year={2019}
}
@misc{vggCifarTorch,
  title={92.45\% on CIFAR-10 in Torch},
  author={Zagoruyko, Sergey},
  howpublished={\url{https://torch.ch/blog/2015/07/30/cifar.html}},
  note={Accessed: 2019-07-22},
  year={2015},
  month={7}
}
@misc{caffenet,
  title={What's the advantage of the reference CaffeNet in comparison with the AlexNet?},
  author={},
  howpublished={\url{https://github.com/BVLC/caffe/issues/4202}},
  note={Accessed: 2019-07-22},
  year={2016},
  month={5}
}

% author={Karayev, Sergey and Shelhamer, Evan and Donahue, Jeff},
@misc{lenet-5-caffe,
  title={Training LeNet on MNIST with Caffe},
  author={Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
  howpublished={\url{https://caffe.berkeleyvision.org/gathered/examples/mnist.html}},
  note={Accessed: 2019-07-22},
  year={2016},
  month={5}
}
@misc{lenet-5-proto1,
  title={lenet},
  author={Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
  howpublished={\url{https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet.prototxt}},
  year={2015},
  month={2},
}
@misc{lenet-5-proto2,
  title={lenet-train-test},
  author={Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
  howpublished={\url{https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet_train_test.prototxt}},
  year={2015},
  month={2},
}

@article{lenet,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick and others},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Taipei, Taiwan}
}
@article{snappy,
  title={Snappy: A fast compressor/decompressor},
  author={Gunderson, SH},
  journal={code. google. com/p/snappy},
  year={2015}
}
@article{fastJL,
author = {Ailon, Nir and Chazelle, Bernard},
doi = {10.1137/060673096},
journal = {SIAM Journal on Computing (SICOMP)},
keywords = {approximate nearest neighbors,dimension reduction,random matrices},
number = {1},
pages = {302--322},
title = {{The Fast Johnson-Lindenstrauss Transform and Approximate Nearest Neighbors}},
volume = {39},
year = {2009}
}
.
@article{bengioKmeansMips,
archivePrefix = {arXiv},
arxivId = {1507.05910},
author = {Auvolat, Alex and Chandar, Sarath and Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua},
doi = {10.475/123},
eprint = {1507.05910},
isbn = {9781450335423},
issn = {0146-4833},
journal = {ICLR},
pages = {10},
title = {{Clustering is Efficient for Approximate Maximum Inner Product Search}},
url = {http://arxiv.org/abs/1507.05910},
year = {2016}
}

@article{otq,
author = {Babenko, Artem and Lempitsky, Victor},
isbn = {9781467369640},
journal = {CVPR},
pages = {1--9},
title = {{Tree Quantization for Large-Scale Similarity Search and Classification}},
url = {papers3://publication/uuid/F4762974-BB97-4208-B035-508945A90EFC},
year = {2015}
}

@article{wtaDnn,
archivePrefix = {arXiv},
arxivId = {1504.07488},
author = {Bakhtiary, Amir H. and Lapedriza, Agata and Masip, David},
doi = {10.3233/978-1-61499-578-4-173},
eprint = {1504.07488},
isbn = {9781614995777},
issn = {09226389},
journal = {Frontiers in Artificial Intelligence and Applications},
keywords = {Deep Convolutional Net,Large Neural Layer,Winner Takes All Hashing},
pages = {173--182},
title = {{Speeding Up Neural Networks for Large Scale Classification using WTA Hashing}},
volume = {277},
year = {2015}
}

@article{jegouBeyondSrp,
author = {Balu, Raghavendran and Furon, Teddy and J{\'{e}}gou, Herv{\'{e}}},
isbn = {9781479928934},
journal = {IEEE International Conference on Acoustic , Speech and Signal Processing (ICASSP)},
number = {3},
pages = {6934--6938},
title = {{Beyond "Project and Sign" For Cosine Estimation with Binary Codes}},
year = {2014}
}

@article{bengioHierarchicalMemNets,
archivePrefix = {arXiv},
arxivId = {1605.07427},
author = {Chandar, Sarath and Ahn, Sungjin and Larochelle, Hugo and Vincent, Pascal and Tesauro, Gerald and Bengio, Yoshua},
eprint = {1605.07427},
journal = {Arxiv},
pages = {1--10},
title = {{Hierarchical Memory Networks}},
url = {http://arxiv.org/abs/1605.07427},
year = {2016}
}

@article{E2LSH,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Datar, M. and Immorlica, N. and Indyk, Piotr and Mirrokni, V.S.},
doi = {10.1145/997817.997857},
eprint = {arXiv:1011.1669v3},
isbn = {1581138857},
issn = {0899-7667},
journal = {Proceedings of the Twentieth Annual Symposium on Computational Geometry},
keywords = {approximate nearest neighbor,locally sensitive hashing,sublinear algorithm,{\^{o}} -stable distributions},
pages = {253--262},
pmid = {25052830},
title = {{Locality-Sensitive Hashing Scheme Based on p-Stable Distributions}},
url = {http://portal.acm.org/citation.cfm?id=997857},
year = {2004}
}

@inproceedings{learningWeightsConnections,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1135--1143},
  year={2015}
}

@article{antisparseCoding,
author = {J{\'{e}}gou, Herv{\'{e}} and Furon, Teddy and Fuchs, Jean-Jacques},
isbn = {9781467300469},
journal = {ICASSP - 37th International Conference on Acoustics, Speech, and Signal Processing},
pages = {2029--2032},
title = {{Anti-sparse coding for approximate nearest neighbor search}},
year = {2012}
}

@article{grvq,
archivePrefix = {arXiv},
arxivId = {1609.05345},
author = {Liu, Shicong and Shao, Junru and Lu, Hongtao},
doi = {10.1109/ICME.2016.7552944},
eprint = {1609.05345},
isbn = {9781467372589},
issn = {1945788X},
journal = {Proceedings - IEEE International Conference on Multimedia and Expo},
keywords = {Large Scale Data,Nearest Neighbor Search,Similarity Search,Vector Quantization},
title = {{Generalized Residual Vector Quantization for Large Scale Data}},
volume = {2016-Augus},
year = {2016}
}

@article{acdc,
archivePrefix = {arXiv},
arxivId = {1511.05946},
author = {Moczulski, Marcin and Denil, Misha and Appleyard, Jeremy and de Freitas, Nando},
eprint = {1511.05946},
isbn = {5996014123031},
journal = {ICLR},
number = {2},
pages = {1--11},
title = {{ACDC: A Structured Efficient Linear Layer}},
year = {2016}
}
% url = {http://arxiv.org/abs/1511.05946},

@article{shrivastavaDnnHashing,
archivePrefix = {arXiv},
arxivId = {1602.08194},
author = {Spring, Ryan and Shrivastava, Anshumali},
eprint = {1602.08194},
journal = {arXiv:1602.08194},
pages = {9},
title = {{Scalable and Sustainable Deep Learning via Randomized Hashing}},
year = {2016}
}
% url = {http://arxiv.org/abs/1602.08194},

@article{compressiveMining,
archivePrefix = {arXiv},
arxivId = {1405.5873},
author = {Vlachos, Michail and Freris, Nikolaos M. and Kyrillidis, Anastasios},
doi = {10.1007/s00778-014-0360-3},
eprint = {1405.5873},
issn = {0949877X},
journal = {VLDB Journal},
keywords = {Compressive sensing,Convex optimization,Data compression,Fourier,Waterfilling algorithm,Wavelets},
number = {1},
pages = {1--24},
title = {{Compressive mining: Fast and optimal data mining in the compressed domain}},
volume = {24},
year = {2015}
}

@article{gpuNNcvpr,
author = {Wieschollek, Patrick and Wang, Oliver and Sorkine-Hornung, Alexander and Lensch, Hendrik P. A.},
doi = {10.1109/CVPR.2016.223},
isbn = {9781467388511},
issn = {10636919},
journal = {CVPR},
pages = {2027--2035},
title = {{Efficient Large-Scale Approximate Nearest Neighbor Search on the GPU}},
year = {2016}
}

@article{adaptiveFastfood,
archivePrefix = {arXiv},
arxivId = {1412.7149},
author = {Yang, Zichao and Moczulski, Marcin and Denil, Misha and de Freitas, Nando and Smola, Alex and Song, Le and Wang, Ziyu},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1412.7149},
isbn = {9780874216561},
issn = {0717-6163},
journal = {ICCV},
pmid = {15003161},
title = {{Deep Fried Convnets}},
url = {http://arxiv.org/abs/1412.7149},
year = {2014}
}

@article{circulantBinaryEmbedding,
archivePrefix = {arXiv},
arxivId = {arXiv:1405.3162v1},
author = {Yu, Felix X and Kumar, Sanjiv and Gong, Yunchao and Chang, Shih-Fu},
eprint = {arXiv:1405.3162v1},
isbn = {9781634393973},
issn = {9781634393973},
journal = {ICML},
number = {x},
pages = {9},
title = {{Circulant Binary Embedding}},
volume = {32},
year = {2014}
}

@article{cq,
abstract = {Proceedings of the International Conference on Machine Learning 2014},
author = {Zhang, Ting and Du, Chao and Wang, Jingdong},
journal = {Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
keywords = {Composite quantization,compact code,hashing,nea},
pages = {838--846},
title = {{Composite Quantization for Approximate Nearest Neighbor Search}},
volume = {32},
year = {2014}
}


@inproceedings{IMI,
  title={The inverted multi-index},
  author={Babenko, Artem and Lempitsky, Victor},
  booktitle={Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on},
  pages={3069--3076},
  year={2012},
  organization={IEEE}
}

@inproceedings{isohash,
  title={Isotropic hashing},
  author={Kong, Weihao and Li, Wu-Jun},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1646--1654},
  year={2012}
}

@article{itq,
  title={Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval},
  author={Gong, Yunchao and Lazebnik, Svetlana and Gordo, Albert and Perronnin, Florent},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={35},
  number={12},
  pages={2916--2929},
  year={2013},
  publisher={IEEE}
}

@inproceedings{lopq,
  title={Locally optimized product quantization for approximate nearest neighbor search},
  author={Kalantidis, Yannis and Avrithis, Yannis},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2321--2328},
  year={2014}
}

@article{learningToHashSurvey,
  title={Learning to hash for indexing big data—a survey},
  author={Wang, Jun and Liu, Wei and Kumar, Sanjiv and Chang, Shih-Fu},
  journal={Proceedings of the IEEE},
  volume={104},
  number={1},
  pages={34--57},
  year={2016},
  publisher={IEEE}
}

@article{lossAwareDnnBinarization,
  title={Loss-aware Binarization of Deep Networks},
  author={Hou, Lu and Yao, Quanming and Kwok, James T},
  journal={arXiv preprint arXiv:1611.01600},
  year={2016}
}

@inproceedings{McqGpu,
  title={Solving Multi-codebook Quantization in the GPU},
  author={Martinez, Julieta and Hoos, Holger H and Little, James J},
  booktitle={European Conference on Computer Vision},
  pages={638--650},
  year={2016},
  organization={Springer}
}

@inproceedings{lsq,
  title={Revisiting additive quantization},
  author={Martinez, Julieta and Clement, Joris and Hoos, Holger H and Little, James J},
  booktitle={European Conference on Computer Vision},
  pages={137--153},
  year={2016},
  organization={Springer}
}

@inproceedings{minimalLossHashing,
  title={Minimal loss hashing for compact binary codes},
  author={Norouzi, Mohammad and Fleet, David J.},
  booktitle={Proceedings of the 28th international conference on machine learning (ICML-11)},
  pages={353--360},
  year={2011}
}

@inproceedings{cartesianKmeans,
  title={Cartesian k-means},
  author={Norouzi, Mohammad and Fleet, David J},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3017--3024},
  year={2013}
}

@inproceedings{crosspolytopeLSH,
  title={Practical and optimal LSH for angular distance},
  author={Andoni, Alexandr and Indyk, Piotr and Laarhoven, Thijs and Razenshteyn, Ilya and Schmidt, Ludwig},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1225--1233},
  year={2015}
}

@inproceedings{McqLutOptimization,
  title={Optimized distances for binary code ranking},
  author={Wang, Jianfeng and Shen, Heng Tao and Yan, Shuicheng and Yu, Nenghai and Li, Shipeng and Wang, Jingdong},
  booktitle={Proceedings of the 22nd ACM international conference on Multimedia},
  pages={517--526},
  year={2014},
  organization={ACM}
}

@article{opq,
  title={Optimized product quantization},
  author={Ge, Tiezheng and He, Kaiming and Ke, Qifa and Sun, Jian},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={36},
  number={4},
  pages={744--755},
  year={2014},
  publisher={IEEE}
}

@incollection{orthogonalRandomFeatures,
  title = {Orthogonal Random Features},
  author = {Yu, Felix X and Suresh, Ananda Theertha and Choromanski, Krzysztof M and Holtmann-Rice, Daniel N and Kumar, Sanjiv},
  booktitle = {Advances in Neural Information Processing Systems 29},
  editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
  pages = {1975--1983},
  year = {2016},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/6246-orthogonal-random-features.pdf}
}

@article{pairQ,
  title={Pairwise Quantization},
  author={Babenko, Artem and Arandjelovi{\'c}, Relja and Lempitsky, Victor},
  journal={arXiv preprint arXiv:1606.01550},
  year={2016}
}

@inproceedings{polysemous,
  title={Polysemous codes},
  author={Douze, Matthijs and J{\'e}gou, Herv{\'e} and Perronnin, Florent},
  booktitle={European Conference on Computer Vision},
  pages={785--801},
  year={2016},
  organization={Springer}
}

@article{pq,
  title={Product quantization for nearest neighbor search},
  author={Jegou, Herve and Douze, Matthijs and Schmid, Cordelia},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={33},
  number={1},
  pages={117--128},
  year={2011},
  publisher={IEEE}
}

@inproceedings{jain2016approximate,
  title={Approximate search with quantized sparse representations},
  author={Jain, Himalaya and P{\'e}rez, Patrick and Gribonval, R{\'e}mi and Zepeda, Joaquin and J{\'e}gou, Herv{\'e}},
  booktitle={European Conference on Computer Vision},
  pages={681--696},
  year={2016},
  organization={Springer}
}

@article{hashingSimilaritySurvey,
  title={Hashing for similarity search: A survey},
  author={Wang, Jingdong and Shen, Heng Tao and Song, Jingkuan and Ji, Jianqiu},
  journal={arXiv preprint arXiv:1408.2927},
  year={2014}
}

@inproceedings{spectralHashing,
  title={Spectral hashing},
  author={Weiss, Yair and Torralba, Antonio and Fergus, Rob},
  booktitle={Advances in neural information processing systems},
  pages={1753--1760},
  year={2009}
}

@article{stackedQuantizers,
  title={Stacked quantizers for compositional vector compression},
  author={Martinez, Julieta and Hoos, Holger H and Little, James J},
  journal={arXiv preprint arXiv:1411.2173},
  year={2014}
}

@article{structuredSpinners,
  title={Structured adaptive and random spinners for fast machine learning computations},
  author={Bojarski, Mariusz and Choromanska, Anna and Choromanski, Krzysztof and Fagan, Francois and Gouy-Pailler, Cedric and Morvan, Anne and Sakr, Nouri and Sarlos, Tamas and Atif, Jamal},
  journal={arXiv preprint arXiv:1610.06209},
  year={2016}
}

@inproceedings{superbitLSH,
  title={Super-bit locality-sensitive hashing},
  author={Ji, Jianqiu and Li, Jianmin and Yan, Shuicheng and Zhang, Bo and Tian, Qi},
  booktitle={Advances in Neural Information Processing Systems},
  pages={108--116},
  year={2012}
}

@article{surveyLearningToHash,
  title={A survey on learning to hash},
  author={Wang, Jingdong and Zhang, Ting and Song, Jingkuan and Sebe, Nicu and Shen, Heng Tao},
  journal={arXiv preprint arXiv:1606.00185},
  year={2016}
}

@article{simdpq,
  title={Cache locality is not enough: high-performance nearest neighbor search with product quantization fast scan},
  author={Andr{\'e}, Fabien and Kermarrec, Anne-Marie and Le Scouarnec, Nicolas},
  journal={Proceedings of the VLDB Endowment},
  volume={9},
  number={4},
  pages={288--299},
  year={2015},
  publisher={VLDB Endowment}
}

@inproceedings{xboxReductions,
  title={Speeding up the xbox recommender system using a euclidean transformation for inner-product spaces},
  author={Bachrach, Yoram and Finkelstein, Yehuda and Gilad-Bachrach, Ran and Katzir, Liran and Koenigstein, Noam and Nice, Nir and Paquet, Ulrich},
  booktitle={Proceedings of the 8th ACM Conference on Recommender systems},
  pages={257--264},
  year={2014},
  organization={ACM}
}
@inproceedings{NOIMI,
  title={Efficient indexing of billion-scale datasets of deep descriptors},
  author={Babenko, Artem and Lempitsky, Victor},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2055--2063},
  year={2016}
}

@inproceedings{aq,
  title={Additive quantization for extreme vector compression},
  author={Babenko, Artem and Lempitsky, Victor},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={931--938},
  year={2014}
}

@article{jlIsTight,
  title={The Johnson-Lindenstrauss lemma is optimal for linear dimensionality reduction},
  author={Larsen, Kasper Green and Nelson, Jelani},
  journal={arXiv preprint arXiv:1411.2404},
  year={2014}
}
@inproceedings{hashnet,
  title={Compressing Neural Networks with the Hashing Trick.},
  author={Chen, Wenlin and Wilson, James T and Tyree, Stephen and Weinberger, Kilian Q and Chen, Yixin},
  booktitle={ICML},
  pages={2285--2294},
  year={2015}
}
@article{diversityNets,
  title={Diversity networks},
  author={Mariet, Zelda and Sra, Suvrit},
  journal={arXiv preprint arXiv:1511.05077},
  year={2015}
}
@inproceedings{rnnDarkKnowledge,
  title={Recurrent neural network training with dark knowledge transfer},
  author={Tang, Zhiyuan and Wang, Dong and Zhang, Zhiyong},
  booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on},
  pages={5900--5904},
  year={2016},
  organization={IEEE}
}
@article{jl,
  title={An elementary proof of a theorem of Johnson and Lindenstrauss},
  author={Dasgupta, Sanjoy and Gupta, Anupam},
  journal={Random Structures \& Algorithms},
  volume={22},
  number={1},
  pages={60--65},
  year={2003},
  publisher={Wiley Online Library}
}
@misc{eigen,
  author = {Gael Guennebaud and Benoit Jacob and others},
  title = {Eigen v3},
  howpublished = {http://eigen.tuxfamily.org},
  year = {2010}
 }
@inproceedings{googleMips,
  title={Quantization based fast inner product search},
  author={Guo, Ruiqi and Kumar, Sanjiv and Choromanski, Krzysztof and Simcha, David},
  booktitle={Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  pages={482--490},
  year={2016}
}
@article{hypersphericalCap,
  title={Concise formulas for the area and volume of a hyperspherical cap},
  author={Li, Shengqiao},
  journal={Asian Journal of Mathematics and Statistics},
  volume={4},
  number={1},
  pages={66--70},
  year={2011}
}
@misc{dlmf,
         key = "{\relax DLMF}",
       title = "{\it NIST Digital Library of Mathematical Functions}",
howpublished = "http://dlmf.nist.gov/, Release 1.0.14 of 2016-12-21",
         url = "http://dlmf.nist.gov/",
        note = "F.~W.~J. Olver, A.~B. {Olde Daalhuis}, D.~W. Lozier, B.~I. Schneider,
                R.~F. Boisvert, C.~W. Clark, B.~R. Miller and B.~V. Saunders, eds."}
@inproceedings{sift,
  title={Object recognition from local scale-invariant features},
  author={Lowe, David G},
  booktitle={Computer vision, 1999. The proceedings of the seventh IEEE international conference on},
  volume={2},
  pages={1150--1157},
  year={1999},
  organization={Ieee}
}

@inproceedings{xnorNet,
  title={Xnor-net: Imagenet classification using binary convolutional neural networks},
  author={Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
  booktitle={European Conference on Computer Vision},
  pages={525--542},
  year={2016},
  organization={Springer}
}
@article{ternaryNets,
  title={Ternary weight networks},
  author={Li, Fengfu and Zhang, Bo and Liu, Bin},
  journal={arXiv preprint arXiv:1605.04711},
  year={2016}
}
@article{dorefaNet,
  title={DoReFa-Net: Training low bitwidth convolutional neural networks with low bitwidth gradients},
  author={Zhou, Shuchang and Wu, Yuxin and Ni, Zekun and Zhou, Xinyu and Wen, He and Zou, Yuheng},
  journal={arXiv preprint arXiv:1606.06160},
  year={2016}
}
@article{squeezeNet,
  title={SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and 0.5 MB model size},
  author={Iandola, Forrest N and Han, Song and Moskewicz, Matthew W and Ashraf, Khalid and Dally, William J and Keutzer, Kurt},
  journal={arXiv preprint arXiv:1602.07360},
  year={2016}
}
@article{enet,
  title={Enet: A deep neural network architecture for real-time semantic segmentation},
  author={Paszke, Adam and Chaurasia, Abhishek and Kim, Sangpil and Culurciello, Eugenio},
  journal={arXiv preprint arXiv:1606.02147},
  year={2016}
}
@article{benDnnExpressive,
  title={On the expressive power of deep neural networks},
  author={Raghu, Maithra and Poole, Ben and Kleinberg, Jon and Ganguli, Surya and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1606.05336},
  year={2016}
}
@article{cormodePrivCount,
  title={Constrained Differential Privacy for Count Data},
  author={Cormode, Graham and Kulkarni, Tejas and Srivastava, Divesh},
  journal={arXiv preprint arXiv:1710.00608},
  year={2017}
}
@article{countMinSketch,
  title={An improved data stream summary: the count-min sketch and its applications},
  author={Cormode, Graham and Muthukrishnan, Shan},
  journal={Journal of Algorithms},
  volume={55},
  number={1},
  pages={58--75},
  year={2005},
  publisher={Elsevier}
}
@inproceedings{prio,
  title={Prio: Private, Robust, and Scalable Computation of Aggregate Statistics.},
  author={Corrigan-Gibbs, Henry and Boneh, Dan},
  booktitle={NSDI},
  pages={259--282},
  year={2017}
}
@article{decoupledBackprop,
  title={Decoupled neural interfaces using synthetic gradients},
  author={Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1608.05343},
  year={2016}
}
@article{rnnNoBacktrack,
  title={Training recurrent networks online without backtracking},
  author={Ollivier, Yann and Tallec, Corentin and Charpiat, Guillaume},
  journal={arXiv preprint arXiv:1507.07680},
  year={2015}
}
@article{oneShotMemory,
  title={One-shot learning with memory-augmented neural networks},
  author={Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
  journal={arXiv preprint arXiv:1605.06065},
  year={2016}
}
@article{learningRareEvents,
  title={Learning to remember rare events},
  author={Kaiser, {\L}ukasz and Nachum, Ofir and Roy, Aurko and Bengio, Samy},
  journal={arXiv preprint arXiv:1703.03129},
  year={2017}
}
@inproceedings{visualizingConvnets,
  title={Visualizing and understanding convolutional networks},
  author={Zeiler, Matthew D and Fergus, Rob},
  booktitle={European conference on computer vision},
  pages={818--833},
  year={2014},
  organization={Springer}
}
@article{convnetSaliency,
  title={Deep inside convolutional networks: visualising image classification models and saliency maps (2014)},
  author={Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1312.6034},
  year={2013}
}
@article{attention,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}
@inproceedings{kddExplain,
  title={Why should i trust you?: Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={1135--1144},
  year={2016},
  organization={ACM}
}
@inproceedings{ytCatFace,
  title={Building high-level features using large scale unsupervised learning},
  author={Le, Quoc V},
  booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on},
  pages={8595--8598},
  year={2013},
  organization={IEEE}
}
@article{AISysChallenges,
  title={A Berkeley View of Systems Challenges for AI},
  author={Stoica, Ion and Song, Dawn and Popa, Raluca Ada and Patterson, David A and Mahoney, Michael W and Katz, Randy H and Joseph, Anthony D and Jordan, Michael and Hellerstein, Joseph M and Gonzalez, Joseph and others},
  year={2017}
}
@article{rememberTooMuch,
  title={Machine Learning Models that Remember Too Much},
  author={Song, Congzheng and Ristenpart, Thomas and Shmatikov, Vitaly},
  journal={arXiv preprint arXiv:1709.07886},
  year={2017}
}
@inproceedings{modelInversion,
  title={Model inversion attacks that exploit confidence information and basic countermeasures},
  author={Fredrikson, Matt and Jha, Somesh and Ristenpart, Thomas},
  booktitle={Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security},
  pages={1322--1333},
  year={2015},
  organization={ACM}
}
@inproceedings{membershipInference,
  title={Membership inference attacks against machine learning models},
  author={Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
  booktitle={Security and Privacy (SP), 2017 IEEE Symposium on},
  pages={3--18},
  year={2017},
  organization={IEEE}
}
@inproceedings{privstats,
  title={Privacy and accountability for location-based aggregate statistics},
  author={Popa, Raluca Ada and Blumberg, Andrew J and Balakrishnan, Hari and Li, Frank H},
  booktitle={Proceedings of the 18th ACM conference on Computer and communications security},
  pages={653--666},
  year={2011},
  organization={ACM}
}
@article{papernotSemi,
  title={Semi-supervised knowledge transfer for deep learning from private training data},
  author={Papernot, Nicolas and Abadi, Mart{\'\i}n and Erlingsson, {\'U}lfar and Goodfellow, Ian and Talwar, Kunal},
  journal={arXiv preprint arXiv:1610.05755},
  year={2016}
}
@article{intriguing,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  journal={arXiv preprint arXiv:1312.6199},
  year={2013}
}
@inproceedings{papernotBlackBox,
  title={Practical black-box attacks against machine learning},
  author={Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z Berkay and Swami, Ananthram},
  booktitle={Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security},
  pages={506--519},
  year={2017},
  organization={ACM}
}
@inproceedings{extract,
  title={EXTRACT: Strong Examples from Weakly-Labeled Sensor Data},
  author={Blalock, Davis W and Guttag, John V},
  booktitle={Proceedings of IEEE ICDM},
  pages={799--804},
  year={2016},
  organization={IEEE}
}
@inproceedings{bolt,
  title={Bolt: Accelerated Data Mining with Fast Vector Compression},
  author={Blalock, Davis W and Guttag, John V},
  booktitle={Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={727--735},
  year={2017},
  organization={ACM}
}
@article{synapto,
  title={Adaptive synaptogenesis constructs neural codes that benefit discrimination},
  author={Thomas, Blake T and Blalock, Davis W and Levy, William B},
  journal={PLoS computational biology},
  volume={11},
  number={7},
  year={2015},
  publisher={Public Library of Science}
}
@article{sparsitron,
  title={Learning graphical models using multiplicative weights},
  author={Klivans, Adam and Meka, Raghu},
  journal={arXiv preprint arXiv:1706.06274},
  year={2017}
}
@inproceedings{breslerIsing,
  title={Efficiently learning Ising models on arbitrary graphs},
  author={Bresler, Guy},
  booktitle={Proceedings of the forty-seventh annual ACM symposium on Theory of computing},
  pages={771--782},
  year={2015},
  organization={ACM}
}
@article{diffPrivBook,
  title={The algorithmic foundations of differential privacy},
  author={Dwork, Cynthia and Roth, Aaron and others},
  journal={Foundations and Trends{\textregistered} in Theoretical Computer Science},
  volume={9},
  number={3--4},
  pages={211--407},
  year={2014},
  publisher={Now Publishers, Inc.}
}
@article{cnnGenerativeModel,
  title={A probabilistic theory of deep learning},
  author={Patel, Ankit B and Nguyen, Tan and Baraniuk, Richard G},
  journal={arXiv preprint arXiv:1504.00641},
  year={2015}
}

@article{deepRecSurvey2017,
  title={Deep learning based recommender system: A survey and new perspectives},
  author={Zhang, Shuai and Yao, Lina and Sun, Aixin},
  journal={arXiv preprint arXiv:1707.07435},
  year={2017}
}
@inproceedings{hogwild,
  title={Hogwild: A lock-free approach to parallelizing stochastic gradient descent},
  author={Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
  booktitle={Advances in neural information processing systems},
  pages={693--701},
  year={2011}
}
@inproceedings{wideAndDeep,
  title={Wide \& deep learning for recommender systems},
  author={Cheng, Heng-Tze and Koc, Levent and Harmsen, Jeremiah and Shaked, Tal and Chandra, Tushar and Aradhye, Hrishi and Anderson, Glen and Corrado, Greg and others},
  booktitle={Proceedings of the 1st Workshop on Deep Learning for Recommender Systems},
  pages={7--10},
  year={2016},
  organization={ACM}
}
@inproceedings{eie,
  title={EIE: efficient inference engine on compressed deep neural network},
  author={Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A and Dally, William J},
  booktitle={Proceedings of the 43rd International Symposium on Computer Architecture},
  pages={243--254},
  year={2016},
  organization={IEEE Press}
}
@article{szeEfficient,
  title={Efficient processing of deep neural networks: A tutorial and survey},
  author={Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel},
  journal={arXiv preprint arXiv:1703.09039},
  year={2017}
}
@inproceedings{tpu,
  title={In-datacenter performance analysis of a tensor processing unit},
  author={Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and others},
  booktitle={Proceedings of the 44th Annual International Symposium on Computer Architecture},
  pages={1--12},
  year={2017},
  organization={ACM}
}
@misc{wiredDeepHardware,
  author = {Cade Metz},
  title = {The Race To Build An AI Chip For Everything Just Got Real},
  howpublished = {Available at \url{https://www.wired.com/2017/04/race-make-ai-chips-everything-heating-fast/}},
  year={2017},
 }
@inproceedings{ortizHeart,
  title={Heart sound classification based on temporal alignment techniques},
  author={Ortiz, Jos{\'e} Javier Gonz{\'a}lez and Phoo, Cheng Perng and Wiens, Jenna},
  booktitle={Computing in Cardiology Conference (CinC), 2016},
  pages={589--592},
  year={2016},
  organization={IEEE}
}

@inbook{murphyCramerRao,
  pages = {201--202},
  title={Machine learning: a Probabilistic Perspective},
  author={Murphy, Kevin P},
  chapter = {6},
  year = {2012},
  publisher={MIT press},
}

@inproceedings{metaTrain,
  title={Learning from Small Sample Sets by Combining Unsupervised Meta-Training with CNNs},
  author={Wang, Yu-Xiong and Hebert, Martial},
  booktitle={Advances in Neural Information Processing Systems},
  pages={244--252},
  year={2016}
}
@inproceedings{matchingNets,
  title={Matching networks for one shot learning},
  author={Vinyals, Oriol and Blundell, Charles and Lillicrap, Tim and Wierstra, Daan and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3630--3638},
  year={2016}
}

@inproceedings{optimalBrainDamage,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John S and Solla, Sara A},
  booktitle={Advances in neural information processing systems},
  pages={598--605},
  year={1990}
}

@article{deepCompression,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={arXiv preprint arXiv:1510.00149},
  year={2015}
}

@incollection{targetPropOrig,
  title={Learning process in an asymmetric threshold network},
  author={Le Cun, Yann},
  booktitle={Disordered systems and biological organization},
  pages={233--240},
  year={1986},
  publisher={Springer}
}
@article{ojasRule,
  title={Simplified neuron model as a principal component analyzer},
  author={Oja, Erkki},
  journal={Journal of mathematical biology},
  volume={15},
  number={3},
  pages={267--273},
  year={1982},
  publisher={Springer}
}
@inproceedings{prelu,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}
@article{batchnorm,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  journal={arXiv preprint arXiv:1502.03167},
  year={2015}
}
@article{sparseMixExpert,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}
@inproceedings{caffe,
  title={Caffe: Convolutional architecture for fast feature embedding},
  author={Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
  booktitle={Proceedings of the 22nd ACM international conference on Multimedia},
  pages={675--678},
  year={2014},
  organization={ACM}
}

@misc{unreproducible0,
  title={Accuracy of Resnet50 is much higher than reported!},
  author={Crall, Jon},
  howpublished={\url{https://github.com/kuangliu/pytorch-cifar/issues/45}},
  note = {Accessed: 2019-07-22},
  year={2018}
}
@misc{unreproducible1,
  title={Validating ResNet50},
  author={Jogeshwar, Anjali},
  howpublished={\url{https://github.com/keras-team/keras/issues/8672}},
  note={Accessed: 2019-07-22},
  year={2017},
  month={12}
}
@misc{unreproducible2,
  title={How to reproduce ImageNet validation results},
  author={Robinson, Caleb},
  howpublished={\url{http://calebrob.com/ml/imagenet/ilsvrc2012/2018/10/22/imagenet-benchmarking.html}},
  note={Accessed: 2019-07-22},
  year={2018},
  month={10}
}
@misc{unreproducibleCurtis,
  title={Towards Reproducibility: Benchmarking Keras and PyTorch},
  author={Northcutt, Curtis},
  howpublished={https://l7.curtisnorthcutt.com/towards-reproducibility-benchmarking-keras-pytorch},
  note={Accessed: 2019-07-22},
  year={2019},
  month={2}
}
@misc{unreproducible3,
  title={Keras doesn't reproduce Caffe example code accuracy},
  author={Nola, David},
  howpublished={\url{https://github.com/keras-team/keras/issues/4444}},
  note={Accessed: 2019-07-22},
  year={2016},
  month={11}
}
@misc{unreproducible4,
  title={Keras Exported Model shows very low accuracy in Tensorflow Serving},
  author={},
  howpublished={\url{https://github.com/keras-team/keras/issues/7848}},
  note={Accessed: 2019-07-22},
  year={2017},
  month={9}
}

@misc{kerasBnWeird,
  title={Change BN layer to use moving mean/var if frozen},
  author={Vryniotis, Vasilis},
  howpublished={\url{https://github.com/keras-team/keras/pull/9965}},
  note={Accessed: 2019-07-22},
  year={2018},
  month={4}
}

@misc{pytorchModels,
  title={Pretrained models for Pytorch},
  author={Cadene, Remi},
  howpublished={\url{https://github.com/Cadene/pretrained-models.pytorch}},
  note={Accessed: 2019-07-22},
  year={2017},
  month={4}
}

@article{pytorch,
  title={Automatic differentiation in pytorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year={2017}
}
@misc{keras,
  title={Keras},
  author={Chollet, Francois and others},
  howpublished={\url{https://keras.io}},
  year={2015}
}
@inproceedings{tensorflow,
  title={TensorFlow: A System for Large-Scale Machine Learning.},
  author={Abadi, Martin and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and others},
  booktitle={OSDI},
  volume={16},
  pages={265--283},
  year={2016}
}
@inproceedings{deepFaceRecognition,
  title={Deep face recognition.},
  author={Parkhi, Omkar M and Vedaldi, Andrea and Zisserman, Andrew and others},
  booktitle={bmvc},
  volume={1},
  number={3},
  pages={6},
  year={2015}
}
@article{wavernn,
  title={Efficient neural audio synthesis},
  author={Kalchbrenner, Nal and Elsen, Erich and Simonyan, Karen and Noury, Seb and Casagrande, Norman and Lockhart, Edward and Stimberg, Florian and Oord, Aaron van den and Dieleman, Sander and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1802.08435},
  year={2018}
}

@misc{scholarTopVenues,
  title={Top publications},
  author={Google},
  howpublished={\url{https://scholar.google.com/citations?view_op=top_venues&vq=eng}},
  note={Accessed: 2019-07-22},
  year={2019},
}

@misc{slimmingVggSrc,
  title={vgg.lua},
  author={Liu, Zhuang},
  howpublished={\url{https://github.com/liuzhuang13/slimming/blob/master/models/vgg.lua}},
  note={Accessed: 2019-07-22},
  year={2017},
  month={8},
  day={22},
}
@misc{mnist-page,
  title={The MNIST database of handwritten digits},
  author={LeCun, Yann and Cortes, Corinna and Burges, Christopher},
  year={1998},
  note={Accessed: 2019-09-6}
}
https://github.com/liuzhuang13/slimming/blob/master/models/vgg.lua
@article{luigi,
  title={Benchmark analysis of representative deep neural network architectures},
  author={Bianco, Simone and Cadene, Remi and Celona, Luigi and Napoletano, Paolo},
  journal={IEEE Access},
  volume={6},
  pages={64270--64277},
  year={2018},
  publisher={IEEE}
}
@article{imagenetGeneralize,
  title={Do ImageNet Classifiers Generalize to ImageNet?},
  author={Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
  journal={arXiv preprint arXiv:1902.10811},
  year={2019}
}
@inproceedings{whyBnDropoutSucks,
  title={Understanding the disharmony between dropout and batch normalization by variance shift},
  author={Li, Xiang and Chen, Shuo and Hu, Xiaolin and Yang, Jian},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2682--2690},
  year={2019}
}
@inproceedings{understandingBatchNorm,
  title={Understanding batch normalization},
  author={Bjorck, Nils and Gomes, Carla P and Selman, Bart and Weinberger, Kilian Q},
  booktitle={Advances in Neural Information Processing Systems},
  pages={7694--7705},
  year={2018}
}
@article{madryBatchNorm,
  title={How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift)},
  author={Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
  journal={arXiv preprint arXiv:1805.11604},
  year={2018}
}

@inproceedings{messer_choosing_1998,
	address = {Southampton},
	title = {Choosing an {Optimal} {Neural} {Network} {Size} to {Aid} a {Search} through a {Large} {Image} {Database}},
	isbn = {978-1-901725-04-9},
	url = {http://www.bmva.org/bmvc/1998/papers/d105/h105.htm},
	doi = {10.5244/C.12.24},
	language = {en},
	urldate = {2019-07-18},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 1998},
	publisher = {British Machine Vision Association},
	author = {Messer, K. and Kittler, J.},
	year = {1998},
	pages = {24.1--24.10},
	file = {Messer and Kittler - 1998 - Choosing an Optimal Neural Network Size to Aid a S.pdf:/Users/davis/Zotero/storage/IA9TNMIE/Messer and Kittler - 1998 - Choosing an Optimal Neural Network Size to Aid a S.pdf:application/pdf}
}

@article{janowsky_pruning_1989,
	title = {Pruning versus clipping in neural networks},
	volume = {39},
	issn = {0556-2791},
	url = {https://link.aps.org/doi/10.1103/PhysRevA.39.6600},
	doi = {10.1103/PhysRevA.39.6600},
	language = {en},
	number = {12},
	urldate = {2019-07-18},
	journal = {Physical Review A},
	author = {Janowsky, Steven A.},
	month = jun,
	year = {1989},
	pages = {6600--6603},
	file = {Janowsky - 1989 - Pruning versus clipping in neural networks.pdf:/Users/davis/Zotero/storage/B9H9NMMQ/Janowsky - 1989 - Pruning versus clipping in neural networks.pdf:application/pdf}
}

@inproceedings{hassibi_optimal_1993,
	address = {San Francisco, CA, USA},
	title = {Optimal {Brain} {Surgeon} and general network pruning},
	isbn = {978-0-7803-0999-9},
	url = {http://ieeexplore.ieee.org/document/298572/},
	doi = {10.1109/ICNN.1993.298572},
	abstract = {We investigate the use of inforination from all second order derivatives of the error function to perform network pruning (i.e., renioving unimportant weights fi*oma trained network) in order to improve generalization, siniplify networks, reduce hardware or storage requirements, increase the speed of further training. and in soiiie cases enable rule extraction. Our method, Optimal Brain Surgeon (OBS), is significantly better than magnitude-based methods and Optinial Brain Damage, which often remove the wrong weights. OBS permits pruning of more weights than other methods (for the same error on the training set), and thus yields better generalizatioii on test data. Crucial to OBS is a recursion relation for calculating the inverse Hessian matrix H-' from training data and structural information of the net. OBS permits a 76\%, a 62\%, and a 90\% reduction in weights over backpropagation with weight decay on three benchmark MONK'S problems. Of OBS, Optimal Brain Damage, and a magnitude-based method, only OBS deletes the correct weights from a trained XOR network in every case. Finally, whereas Sejnowski and Rosenberg used 18,000 weights in their NETtalk network, we used OBS to prune a network to just 1,560weights, yielding better generalization.},
	language = {en},
	urldate = {2019-07-18},
	booktitle = {{IEEE} {International} {Conference} on {Neural} {Networks}},
	publisher = {IEEE},
	author = {Hassibi, B. and Stork, D.G. and Wolff, G.J.},
	year = {1993},
	pages = {293--299},
	file = {Hassibi et al. - 1993 - Optimal Brain Surgeon and general network pruning.pdf:/Users/davis/Zotero/storage/DJM6KICJ/Hassibi et al. - 1993 - Optimal Brain Surgeon and general network pruning.pdf:application/pdf}
}

@article{reed_pruning_1993,
	title = {Pruning algorithms-a survey},
	volume = {4},
	issn = {10459227},
	url = {http://ieeexplore.ieee.org/document/248452/},
	doi = {10.1109/72.248452},
	abstract = {A rule of thumb for obtaininggood generalizationin systems trained by examples is that one should use the smallest system that will fit the data. Unfortunately, it usually is not obvious what size is best; a system that is too small will not be able to learn the data while one that is just big enough may learn very slowly and be very sensitive to initial conditions and learning parameters. This paper is a survey of neural network pruning algorithms.The approachtaken by the methodsdescribed here is 5! to train a network that is larger than necessary and then remove Y the parts that are not needed.},
	language = {en},
	number = {5},
	urldate = {2019-07-18},
	journal = {IEEE Transactions on Neural Networks},
	author = {Reed, R.},
	month = sep,
	year = {1993},
	pages = {740--747},
	file = {Reed - 1993 - Pruning algorithms-a survey.pdf:/Users/davis/Zotero/storage/FNI865HI/Reed - 1993 - Pruning algorithms-a survey.pdf:application/pdf}
}

@article{castellano_iterative_1997,
	title = {An iterative pruning algorithm for feedforward neural networks},
	volume = {8},
	issn = {10459227},
	url = {http://ieeexplore.ieee.org/document/572092/},
	doi = {10.1109/72.572092},
	language = {en},
	number = {3},
	urldate = {2019-07-18},
	journal = {IEEE Transactions on Neural Networks},
	author = {Castellano, G. and Fanelli, A.M. and Pelillo, M.},
	month = may,
	year = {1997},
	pages = {519--531},
	file = {Castellano et al. - 1997 - An iterative pruning algorithm for feedforward neu.pdf:/Users/davis/Zotero/storage/D7KNMRHH/Castellano et al. - 1997 - An iterative pruning algorithm for feedforward neu.pdf:application/pdf}
}

@article{pearlmutter_fast_1994,
	title = {Fast {Exact} {Multiplication} by the {Hessian}},
	volume = {6},
	issn = {0899-7667, 1530-888X},
	url = {http://www.mitpressjournals.org/doi/10.1162/neco.1994.6.1.147},
	doi = {10.1162/neco.1994.6.1.147},
	language = {en},
	number = {1},
	urldate = {2019-07-18},
	journal = {Neural Computation},
	author = {Pearlmutter, Barak A.},
	month = jan,
	year = {1994},
	pages = {147--160},
	file = {Pearlmutter - 1994 - Fast Exact Multiplication by the Hessian.pdf:/Users/davis/Zotero/storage/426QNJ84/Pearlmutter - 1994 - Fast Exact Multiplication by the Hessian.pdf:application/pdf}
}

@article{bishop_training_1995,
	title = {Training with {Noise} is {Equivalent} to {Tikhonov} {Regularization}},
	volume = {7},
	issn = {0899-7667, 1530-888X},
	url = {http://www.mitpressjournals.org/doi/10.1162/neco.1995.7.1.108},
	doi = {10.1162/neco.1995.7.1.108},
	language = {en},
	number = {1},
	urldate = {2019-07-18},
	journal = {Neural Computation},
	author = {Bishop, Chris M.},
	month = jan,
	year = {1995},
	pages = {108--116},
	file = {Bishop - 1995 - Training with Noise is Equivalent to Tikhonov Regu.pdf:/Users/davis/Zotero/storage/RKM5XYAA/Bishop - 1995 - Training with Noise is Equivalent to Tikhonov Regu.pdf:application/pdf}
}

@article{mozer_using_1989,
	title = {Using {Relevance} to {Reduce} {Network} {Size} {Automatically}},
	volume = {1},
	issn = {0954-0091, 1360-0494},
	url = {https://www.tandfonline.com/doi/full/10.1080/09540098908915626},
	doi = {10.1080/09540098908915626},
	language = {en},
	number = {1},
	urldate = {2019-07-18},
	journal = {Connection Science},
	author = {Mozer, Michael C. and Smolensky, Paul},
	month = jan,
	year = {1989},
	pages = {3--16},
	file = {Mozer and Smolensky - 1989 - Using Relevance to Reduce Network Size Automatical.pdf:/Users/davis/Zotero/storage/4FV4V9K8/Mozer and Smolensky - 1989 - Using Relevance to Reduce Network Size Automatical.pdf:application/pdf}
}

@inproceedings{tresp_early_1997,
	title = {Early brain damage},
	booktitle = {Advances in neural information processing systems},
	author = {Tresp, Volker and Neuneier, Ralph and Zimmermann, Hans-Georg},
	year = {1997},
	pages = {669--675},
	file = {Tresp et al. - Early Brain Damage.pdf:/Users/davis/Zotero/storage/CEVR3DQN/Tresp et al. - Early Brain Damage.pdf:application/pdf}
}

@inproceedings{lecun_optimal_1990,
	title = {Optimal brain damage},
	booktitle = {Advances in neural information processing systems},
	author = {LeCun, Yann and Denker, John S and Solla, Sara A},
	year = {1990},
	pages = {598--605},
	file = {LeCun et al. - Optimal Brain Damage.pdf:/Users/davis/Zotero/storage/SZ2FTBXN/LeCun et al. - Optimal Brain Damage.pdf:application/pdf}
}

@inproceedings{mozer_skeletonization:_1989,
	title = {Skeletonization: {A} technique for trimming the fat from a network via relevance assessment},
	booktitle = {Advances in neural information processing systems},
	author = {Mozer, Michael C and Smolensky, Paul},
	year = {1989},
	pages = {107--115},
	file = {Mozer and Smolensky - Skeletonization A Technique for Trimming the Fat .pdf:/Users/davis/Zotero/storage/I4D9W8BB/Mozer and Smolensky - Skeletonization A Technique for Trimming the Fat .pdf:application/pdf}
}

@techreport{lawrence_what_1998,
	title = {What size neural network gives optimal generalization? {Convergence} properties of backpropagation},
	author = {Lawrence, Steve and Giles, C Lee and Tsoi, Ah Chung},
	year = {1998},
	file = {Lawrence et al. - What Size Neural Network Gives Optimal Generalizat.pdf:/Users/davis/Zotero/storage/7IS2WNQ3/Lawrence et al. - What Size Neural Network Gives Optimal Generalizat.pdf:application/pdf}
}

@article{karnin_simple_1990,
	title = {A simple procedure for pruning back-propagation trained neural networks},
	volume = {1},
	number = {2},
	journal = {IEEE transactions on neural networks},
	author = {Karnin, Ehud D},
	year = {1990},
	pages = {239--242},
	file = {00080236.pdf:/Users/davis/Zotero/storage/JD5E24V5/00080236.pdf:application/pdf}
}
@techreport{cifarDsets,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  institution={Citeseer}
}
@inproceedings{imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}
@article{places,
  title={Places: A 10 million Image Database for Scene Recognition},
  author={Zhou, Bolei and Lapedriza, Agata and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2017},
  publisher={IEEE}
}

@article{mnist,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick and others},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Taipei, Taiwan}
}
@misc{flac,
  title={Flac-free lossless audio codec},
  author={Coalson, Josh},
  note={\url{http://flac.sourceforge.net}},
  year={2008}
}
@misc{lemireMicrobenchmarks,
  title={Microbenchmarking calls for idealized conditions},
  author={Lemire, Daniel},
  note={\url{https://lemire.me/blog/2018/01/16/microbenchmarking-calls-for-idealized-conditions/}},
  year={2018}
}
@misc{alac,
  title={Apple Lossless Audio Codec},
  author={Apple},
  note={\url{https://github.com/macosforge/alac}},
  year={2011}
}
@article{vorbis,
  title={Ogg Vorbis},
  author={Moffitt, Jack},
  journal={Linux journal},
  volume={2001},
  number={81es},
  pages={9},
  year={2001},
  publisher={Belltown Media}
}
@misc{shorten,
  title={SHORTEN: Simple lossless and near-lossless waveform compression},
  author={Robinson, Tony},
  year={1994},
  publisher={University of Cambridge, Department of Engineering}
}
@misc{aac,
  title={Information technology -- Generic coding of moving pictures and associated audio information -- Part 7: Advanced Audio Coding (AAC)},
  note={\url{https://www.iso.org/standard/43345.html}},
  year={2006},
  publisher={International Standards Organization}
}
@techreport{opus,
  title={Definition of the Opus audio codec},
  author={Valin, Jean-Marc and Vos, Koen and Terriberry, Timothy},
  year={2012}
}
@article{blosc,
  title={Why modern CPUs are starving and what can be done about it},
  author={Alted, Francesc},
  journal={Computing in Science \& Engineering},
  volume={12},
  number={2},
  year={2010},
  publisher={IEEE}
}
@misc{lz4,
  title={LZ4--extremely fast compression},
  author={Collet, Yann},
  note={\url{https://github.com/Cyan4973/lz4}},
  year={2017}
}
@misc{zigzag,
  title={Protocol Buffers Encoding},
  author={Google},
  note={\url{https://developers.google.com/protocol-buffers/docs/encoding#types}},
  year={2001}
}
@misc{zstd,
  title={Zstandard - Fast real-time compression algorithm},
  author={Collet, Yann},
  note={\url{https://facebook.github.io/zstd/}},
  year={2017}
}
@misc{gzip,
  title={The gzip home page},
  author={Gailly, Jean-Loup and Adler, Mark},
  note={\url{https://www.gzip.org/}},
  year={2003}
}
% This one is about doing DEFLATE with 10x less memory (but probably worse
% compression)
@misc{libslz,
  title={Stateless ZIP library - SLZ},
  author={Tarreau, Willy},
  note={\url{http://www.libslz.org}},
  year={2015}
}
@article{deflate,
  title={DEFLATE compressed data format specification version 1.3},
  author={Deutsch, L Peter},
  year={1996}
}
@article{lz77,
  title={A universal algorithm for sequential data compression},
  author={Ziv, Jacob and Lempel, Abraham},
  journal={IEEE Transactions on information theory},
  volume={23},
  number={3},
  pages={337--343},
  year={1977},
  publisher={IEEE}
}
@article{lz78,
  title={Compression of individual sequences via variable-rate coding},
  author={Ziv, Jacob and Lempel, Abraham},
  journal={IEEE transactions on Information Theory},
  volume={24},
  number={5},
  pages={530--536},
  year={1978},
  publisher={IEEE}
}
@article{lzo,
  title={LZO-a real-time data compression library},
  author={Oberhumer, MFXJ},
  journal={http://www.oberhumer.com/opensource/lzo/},
  year={2008}
}
@misc{snappy,
  title={Snappy: A fast compressor/decompressor},
  author={Gunderson, SH},
  note={\url{https://code.google.com/p/snappy}},
  year={2015}
}
@article{simple8b,
  title={Index compression using 64-bit words},
  author={Anh, Vo Ngoc and Moffat, Alistair},
  journal={Software: Practice and Experience},
  volume={40},
  number={2},
  pages={131--147},
  year={2010},
  publisher={Wiley Online Library}
}
@misc{fse,
  title={Finite State Entropy},
  author={Collet, Yann},
  note={\url{https://github.com/Cyan4973/FiniteStateEntropy}},
}
@misc{bitshuf,
  title={Bitshuffle: Filter for improving compression of typed binary data.},
  author={Alted, F},
  note={\url{https://github.com/kiyo-masui/bitshuffle}},
  year={2017}
}
@misc{quark,
  title={Intel Quark Microcontrollers},
  note={\url{https://www.intel.com/content/www/us/en/embedded/products/quark/overview.html}},
  year={2017}
}
@misc{cc2540,
  title={2.4-GHz Bluetooth® low energy System-on-Chip},
  author={Texas Instruments},
  note={\url{http://www.ti.com/lit/ds/symlink/cc2540.pdf}},
  year={2013}
}
@misc{cc2640,
  title={CC2640 SimpleLink Bluetooth Wireless MCU},
  author={Texas Instruments},
  note={\url{http://www.ti.com/lit/ds/swrs176b/swrs176b.pdf}},
  year={2016}
}
@misc{sdcard,
  title={Transcend microSDHC Card series},
  note={\url{https://cdn-shop.adafruit.com/datasheets/TS16GUSDHC6.pdf}},
  year={2017}
}
% ^ writes take hundreds of mW, basically

@misc{digikeyADCs,
  title={Digi-Key Electronics},
  note={\url{https://www.digikey.com/products/en/integrated-circuits-ics/data-acquisition-analog-to-digital-converters-adc/700?k=adc \& k= \& pkeyword=adc \& pv1989=0}},
  year={2017}
}

@inproceedings{marlin,
  title={Marlin: A High Throughput Variable-to-Fixed Codec Using Plurally Parsable Dictionaries},
  author={Martinez, Manuel and Haurilet, Monica and Stiefelhagen, Rainer and Serra-Sagrist{\`a}, Joan},
  booktitle={Data Compression Conference (DCC), 2017},
  pages={161--170},
  year={2017},
  organization={IEEE}
}
@inproceedings{mobileCompression,
  title={Performance and energy consumption of lossless compression/decompression utilities on mobile computing platforms},
  author={Milenkovic, Aleksandar and Dzhagaryan, Armen and Burtscher, Martin},
  booktitle={Modeling, Analysis \& Simulation of Computer and Telecommunication Systems (MASCOTS), 2013 IEEE 21st International Symposium on},
  pages={254--263},
  year={2013},
  organization={IEEE}
}
@article{ampds,
    Author = {Stephen Makonin and Bradley Ellert and Ivan V. Bajic and Fred Popowich},
    Title = {{Electricity, water, and natural gas consumption of a residential house in Canada from 2012 to 2014}},
    Journal = {Scientific Data},
    Volume = {3},
    Number = {160037},
    Pages = {1--12},
    Year = {2016}
}
@article{eegCS,
  title={Compressed sensing of multichannel EEG signals: the simultaneous cosparsity and low-rank optimization},
  author={Liu, Yipeng and De Vos, Maarten and Van Huffel, Sabine},
  journal={IEEE Transactions on Biomedical Engineering},
  volume={62},
  number={8},
  pages={2055--2061},
  year={2015},
  publisher={IEEE}
}
@inproceedings{simdCodecsEval,
  title={In Vacuo and In Situ Evaluation of SIMD Codecs},
  author={Trotman, Andrew and Lin, Jimmy},
  booktitle={Proceedings of the 21st Australasian Document Computing Symposium on ZZZ},
  pages={1--8},
  year={2016},
  organization={ACM}
}
@inproceedings{pfor,
  title={Super-scalar RAM-CPU cache compression},
  author={Zukowski, Marcin and Heman, Sandor and Nes, Niels and Boncz, Peter},
  booktitle={Data Engineering, 2006. ICDE'06. Proceedings of the 22nd International Conference on},
  pages={59--59},
  year={2006},
  organization={IEEE}
}
@inproceedings{gipfeli,
  title={Gipfeli-high speed compression algorithm},
  author={Lenhardt, Rastislav and Alakuijala, Jyrki},
  booktitle={Data Compression Conference (DCC), 2012},
  pages={109--118},
  year={2012},
  organization={IEEE}
}
@article{groupSimd,
  title={A general simd-based approach to accelerating compression algorithms},
  author={Zhao, Wayne Xin and Zhang, Xudong and Lemire, Daniel and Shan, Dongdong and Nie, Jian-Yun and Yan, Hongfei and Wen, Ji-Rong},
  journal={ACM Transactions on Information Systems (TOIS)},
  volume={33},
  number={3},
  pages={15},
  year={2015},
  publisher={ACM}
}
@inproceedings{kGamma,
  title={Fast integer compression using SIMD instructions},
  author={Schlegel, Benjamin and Gemulla, Rainer and Lehner, Wolfgang},
  booktitle={Proceedings of the Sixth International Workshop on Data Management on New Hardware},
  pages={34--40},
  year={2010},
  organization={ACM}
}
@inproceedings{varintG8IU,
  title={SIMD-based decoding of posting lists},
  author={Stepanov, Alexander A and Gangolli, Anil R and Rose, Daniel E and Ernst, Ryan J and Oberoi, Paramjit S},
  booktitle={Proceedings of the 20th ACM international conference on Information and knowledge management},
  pages={317--326},
  year={2011},
  organization={ACM}
}
@article{golomb,
  title={Run-length encodings},
  author={Golomb, Solomon},
  journal={IEEE transactions on information theory},
  volume={12},
  number={3},
  pages={399--401},
  year={1966},
  publisher={IEEE}
}
@article{rice,
  title={Some practical universal noiseless coding techniques, part 3, module PSl14, K+},
  author={Rice, Robert F},
  year={1991}
}
@inproceedings{bbp,
  title={High Speed Lossless Image Compression},
  author={Siedelmann, Hendrik and Wender, Alexander and Fuchs, Martin},
  booktitle={German Conference on Pattern Recognition},
  pages={343--355},
  year={2015},
  organization={Springer}
}
@techreport{zlib,
  title={Zlib compressed data format specification version 3.3},
  author={Deutsch, Peter and Gailly, Jean-Loup},
  year={1996}
}
@article{dwarf,
  title={DWARF debugging information format, version 4},
  author={DWARF Debugging Information Format Committee and others},
  journal={Free Standards Group},
  year={2010}
}
@techreport{brotli,
  title={Brotli compressed data format},
  author={Alakuijala, Jyrki and Szabadka, Zoltan},
  year={2016}
}

@inproceedings{iotCompressCrap,
  title={IoT data compression: Sensor-agnostic approach},
  author={Ukil, Arijit and Bandyopadhyay, Soma and Pal, Arpan},
  booktitle={Data Compression Conference (DCC), 2015},
  pages={303--312},
  year={2015},
  organization={IEEE}
}
@inproceedings{iotSignals,
  title={Signal Characteristics on Sensor Data Compression in IoT-An Investigation},
  author={Bose, Tulika and Bandyopadhyay, Soma and Kumar, Sudhir and Bhattacharyya, Abhijan and Pal, Arpan},
  booktitle={Sensing, Communication, and Networking (SECON), 2016 13th Annual IEEE International Conference on},
  pages={1--6},
  year={2016},
  organization={IEEE}
}
@article{sensorTransforms,
  title={An evaluation of model-based approaches to sensor data compression},
  author={Hung, Nguyen Quoc Viet and Jeung, Hoyoung and Aberer, Karl},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={25},
  number={11},
  pages={2434--2447},
  year={2013},
  publisher={IEEE}
}
@inproceedings{spaceCompress,
  title={Adaptive compression schemes for housekeeping data},
  author={Me{\ss}, Jan-Gerd and Schmidt, Robert and Fey, G{\"o}rschwin},
  booktitle={Aerospace Conference, 2017 IEEE},
  pages={1--12},
  year={2017},
  organization={IEEE}
}
@inproceedings{lachCompress,
  title={Adaptive lossless compression in wireless body sensor networks},
  author={Arrabi, Saad and Lach, John},
  booktitle={Proceedings of the Fourth International Conference on Body Area Networks},
  pages={19},
  year={2009},
  organization={ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)}
}
@article{socialFMRI,
  title={Social fMRI: Investigating and shaping social mechanisms in the real world},
  author={Aharony, Nadav and Pan, Wei and Ip, Cory and Khayal, Inas and Pentland, Alex},
  journal={Pervasive and Mobile Computing},
  volume={7},
  number={6},
  pages={643--659},
  year={2011},
  publisher={Elsevier}
}
@article{guttagAnanthaEEG,
  title={A micro-power EEG acquisition SoC with integrated feature extraction processor for a chronic seizure detection system},
  author={Verma, Naveen and Shoeb, Ali and Bohorquez, Jose and Dawson, Joel and Guttag, John and Chandrakasan, Anantha P},
  journal={IEEE Journal of Solid-State Circuits},
  volume={45},
  number={4},
  pages={804--816},
  year={2010},
  publisher={IEEE}
}

@article{fastpfor,
  title={Decoding billions of integers per second through vectorization},
  author={Lemire, Daniel and Boytsov, Leonid},
  journal={Software: Practice and Experience},
  volume={45},
  number={1},
  pages={1--29},
  year={2015},
  publisher={Wiley Online Library}
}
@inproceedings{floatCompress,
  title={Fast lossless compression of scientific floating-point data},
  author={Ratanaworabhan, Paruj and Ke, Jian and Burtscher, Martin},
  booktitle={Data Compression Conference, 2006. DCC 2006. Proceedings},
  pages={133--142},
  year={2006},
  organization={IEEE}
}
@article{mafisc,
  title={Reducing the hpc-datastorage footprint with mafisc—multidimensional adaptive filtering improved scientific data compression},
  author={H{\"u}bbe, Nathanael and Kunkel, Julian},
  journal={Computer Science-Research and Development},
  volume={28},
  number={2-3},
  pages={231--239},
  year={2013},
  publisher={Springer}
}
@article{tsSegErrGuarantee,
  title={Indexable online time series segmentation with error bound guarantee},
  author={Qi, Jianzhong and Zhang, Rui and Ramamohanarao, Kotagiri and Wang, Hongzhi and Wen, Zeyi and Wu, Dan},
  journal={World Wide Web},
  volume={18},
  number={2},
  pages={359--401},
  year={2015},
  publisher={Springer}
}
@inproceedings{empiricalIntCompress,
  title={Lightweight data compression algorithms: An experimental survey},
  author={Damme, Patrick and Habich, Dirk and Hildebrand, J and Lehner, Wolfgang},
  booktitle={Proc. EDBT},
  year={2017}
}
@article{simdScan,
  title={SIMD-scan: ultra fast in-memory table scan using on-chip vector processing units},
  author={Willhalm, Thomas and Popovici, Nicolae and Boshmaf, Yazan and Plattner, Hasso and Zeier, Alexander and Schaffner, Jan},
  journal={Proceedings of the VLDB Endowment},
  volume={2},
  number={1},
  pages={385--394},
  year={2009},
  publisher={VLDB Endowment}
}
@article{tsCompressGrid,
  title={A time-series compression technique and its application to the smart grid},
  author={Eichinger, Frank and Efros, Pavel and Karnouskos, Stamatis and B{\"o}hm, Klemens},
  journal={The VLDB Journal},
  volume={24},
  number={2},
  pages={193--218},
  year={2015},
  publisher={Springer}
}
@inproceedings{mp3+aac,
  title={MP3 and AAC explained},
  author={Brandenburg, Karlheinz},
  booktitle={Audio Engineering Society Conference: 17th International Conference: High-Quality Audio Coding},
  year={1999},
  organization={Audio Engineering Society}
}
@article{bsnChallenges,
  title={Body area sensor networks: Challenges and opportunities},
  author={Hanson, Mark A and Powell Jr, Harry C and Barth, Adam T and Ringgenberg, Kyle and Calhoun, Benton H and Aylor, James H and Lach, John},
  journal={Computer},
  volume={42},
  number={1},
  year={2009},
  publisher={IEEE}
}
@inproceedings{epenthesis,
  title={Time series epenthesis: Clustering time series streams requires ignoring some data},
  author={Rakthanmanon, Thanawin and Keogh, Eamonn J and Lonardi, Stefano and Evans, Scott},
  booktitle={Data Mining (ICDM), 2011 IEEE 11th International Conference on},
  pages={547--556},
  year={2011},
  organization={IEEE}
}
@article{ans,
  title={Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding},
  author={Duda, Jarek},
  journal={arXiv preprint arXiv:1311.2540},
  year={2013}
}

@inproceedings{librosa,
author = {McFee, Brian and Raffel, Colin and Liang, Dawen and Ellis, Daniel PW and McVicar, Matt and Battenberg, Eric and Nieto, Oriol},
title = {{librosa: Audio and Music Signal Analysis in Python}},
booktitle = {Proceedings of the 14th Python in Science Conference},
year = {2015}
}

@misc{ucrTimeSeries,
title={The UCR Time Series Classification Archive},
author={ Chen, Yanping and Keogh, Eamonn and Hu, Bing and Begum, Nurjahan and Bagnall, Anthony and Mueen, Abdullah and Batista, Gustavo},
year={2015},
month={July},
note = {\url{www.cs.ucr.edu/~eamonn/time_series_data/}}
}
@inproceedings{pamap,
  title={Towards global aerobic activity monitoring},
  author={Reiss, Attila and Stricker, Didier},
  booktitle={Proceedings of the 4th International Conference on PErvasive Technologies Related to Assistive Environments},
  pages={12},
  year={2011},
  organization={ACM}
}
@inproceedings{msrc,
author = {Fothergill, Simon and Mentis, Helena M and Kohli, Pushmeet and Nowozin, Sebastian},
title = {{Instructing people for training gestural interactive systems}},
booktitle = {CHI},
year = {2012},
editor = {Konstan, Joseph A and Chi, Ed H and H{\"o}{\"o}k, Kristina},
pages = {1737--1746},
publisher = {ACM}
}
@article{uci_gas,
  title={Reservoir computing compensates slow response of chemosensor arrays exposed to fast varying gas concentrations in continuous monitoring},
  author={Fonollosa, Jordi and Sheik, Sadique and Huerta, Ram{\'o}n and Marco, Santiago},
  journal={Sensors and Actuators B: Chemical},
  volume={215},
  pages={618--629},
  year={2015},
  publisher={Elsevier}
}
@inproceedings{sax,
  title={A symbolic representation of time series, with implications for streaming algorithms},
  author={Lin, Jessica and Keogh, Eamonn and Lonardi, Stefano and Chiu, Bill},
  booktitle={Proceedings of the 8th ACM SIGMOD workshop on Research issues in data mining and knowledge discovery},
  pages={2--11},
  year={2003},
  organization={ACM}
}
@article{isax,
  title={iSAX: disk-aware mining and indexing of massive time series datasets},
  author={Shieh, Jin and Keogh, Eamonn},
  journal={Data Mining and Knowledge Discovery},
  volume={19},
  number={1},
  pages={24--57},
  year={2009},
  publisher={Springer}
}
@inproceedings{isax2,
  title={iSAX 2.0: Indexing and mining one billion time series},
  author={Camerra, Alessandro and Palpanas, Themis and Shieh, Jin and Keogh, Eamonn},
  booktitle={Data Mining (ICDM), 2010 IEEE 10th International Conference on},
  pages={58--67},
  year={2010},
  organization={IEEE}
}
@inproceedings{saxvsm,
  title={Sax-vsm: Interpretable time series classification using sax and vector space model},
  author={Senin, Pavel and Malinchik, Sergey},
  booktitle={Data Mining (ICDM), 2013 IEEE 13th International Conference on},
  pages={1175--1180},
  year={2013},
  organization={IEEE}
}
@article{apca,
  title={Locally adaptive dimensionality reduction for indexing large time series databases},
  author={Keogh, Eamonn and Chakrabarti, Kaushik and Pazzani, Michael and Mehrotra, Sharad},
  journal={ACM Sigmod Record},
  volume={30},
  number={2},
  pages={151--162},
  year={2001},
  publisher={ACM}
}
@article{paa,
  title={Dimensionality reduction for fast similarity search in large time series databases},
  author={Keogh, Eamonn and Chakrabarti, Kaushik and Pazzani, Michael and Mehrotra, Sharad},
  journal={Knowledge and information Systems},
  volume={3},
  number={3},
  pages={263--286},
  year={2001},
  publisher={Springer}
}
@inproceedings{swab,
  title={An online algorithm for segmenting time series},
  author={Keogh, Eamonn and Chu, Selina and Hart, David and Pazzani, Michael},
  booktitle={Data Mining, 2001. ICDM 2001, Proceedings IEEE International Conference on},
  pages={289--296},
  year={2001},
  organization={IEEE}
}
@inproceedings{lemireSegmentation,
  title={A better alternative to piecewise linear time series segmentation},
  author={Lemire, Daniel},
  booktitle={Proceedings of the 2007 SIAM International Conference on Data Mining},
  pages={545--550},
  year={2007},
  organization={SIAM}
}
@article{tsCompressSmartGrid,
  title={A time-series compression technique and its application to the smart grid},
  author={Eichinger, Frank and Efros, Pavel and Karnouskos, Stamatis and B{\"o}hm, Klemens},
  journal={The VLDB Journal},
  volume={24},
  number={2},
  pages={193--218},
  year={2015},
  publisher={Springer}
}
@inproceedings{hotSax,
  title={Hot sax: Efficiently finding the most unusual time series subsequence},
  author={Keogh, Eamonn and Lin, Jessica and Fu, Ada},
  booktitle={Data mining, fifth IEEE international conference on},
  pages={8--pp},
  year={2005},
  organization={Ieee}
}
@inproceedings{fastShapelet,
  title={Fast shapelets: A scalable algorithm for discovering time series shapelets},
  author={Rakthanmanon, Thanawin and Keogh, Eamonn},
  booktitle={Proceedings of the 2013 SIAM International Conference on Data Mining},
  pages={668--676},
  year={2013},
  organization={SIAM}
}
@misc{influxDB,
  title={InfluxDB},
  author={Beckett, S},
  note={\url{https://influxdata.com}},
  year={2017}
}
@misc{akumuli,
  title={Akumuli Time-series database},
  author={Lazin, Eugene},
  note={\url{https://akumuli.org}},
}
@misc{kairosDB,
  title={Kairos DB: Fast Time Series Database on Cassandra.},
  author={Hawkins, Brian},
  note={\url{https://github.com/kairosdb/kairosdb}},
}
@misc{rocksDB,
  title={RocksDB: A Persistent Key-Value Store for Fast Storage Environments.},
  author={Facebook Database Engineering Team},
  note={\url{http://rocksdb.org}},
}
@inproceedings{respawnDB,
  title={Respawn: A distributed multi-resolution time-series datastore},
  author={Buevich, Maxim and Wright, Anne and Sargent, Randy and Rowe, Anthony},
  booktitle={Real-Time Systems Symposium (RTSS), 2013 IEEE 34th},
  pages={288--297},
  year={2013},
  organization={IEEE}
}
@article{openTSDB,
  title={OpenTSDB: The distributed, scalable time series database},
  author={Sigoure, B},
  journal={Proc. OSCON},
  volume={11},
  year={2010}
}
@article{gorilla,
  title={Gorilla: A fast, scalable, in-memory time series database},
  author={Pelkonen, Tuomas and Franklin, Scott and Teller, Justin and Cavallaro, Paul and Huang, Qi and Meza, Justin and Veeraraghavan, Kaushik},
  journal={Proceedings of the VLDB Endowment},
  volume={8},
  number={12},
  pages={1816--1827},
  year={2015},
  publisher={VLDB Endowment}
}
@inproceedings{chronicleDB,
  title={ChronicleDB: A High-Performance Event Store.},
  author={Seidemann, Marc and Seeger, Bernhard},
  booktitle={EDBT},
  pages={144--155},
  year={2017}
}
@inproceedings{berkeleyTreeDB,
  title={BTrDB: Optimizing Storage System Design for Timeseries Processing.},
  author={Andersen, Michael P and Culler, David E},
  booktitle={FAST},
  pages={39--52},
  year={2016}
}
@inproceedings{druid,
  title={Druid: A real-time analytical data store},
  author={Yang, Fangjin and Tschetter, Eric and L{\'e}aut{\'e}, Xavier and Ray, Nelson and Merlino, Gian and Ganguli, Deep},
  booktitle={Proceedings of the 2014 ACM SIGMOD international conference on Management of data},
  pages={157--168},
  year={2014},
  organization={ACM}
}
@inproceedings{littleTable,
  title={LittleTable: a time-series database and its uses},
  author={Rhea, Sean and Wang, Eric and Wong, Edmund and Atkins, Ethan and Storer, Nat},
  booktitle={Proceedings of the 2017 ACM International Conference on Management of Data},
  pages={125--138},
  year={2017},
  organization={ACM}
}
@article{tinydb,
  title={TinyDB: an acquisitional query processing system for sensor networks},
  author={Madden, Samuel R and Franklin, Michael J and Hellerstein, Joseph M and Hong, Wei},
  journal={ACM Transactions on database systems (TODS)},
  volume={30},
  number={1},
  pages={122--173},
  year={2005},
  publisher={ACM}
}
@article{neuralBranchPredictor,
  title={Neural methods for dynamic branch prediction},
  author={Jim{\'e}nez, Daniel A and Lin, Calvin},
  journal={ACM Transactions on Computer Systems (TOCS)},
  volume={20},
  number={4},
  pages={369--397},
  year={2002},
  publisher={ACM}
}
@inproceedings{extract,
  title={EXTRACT: Strong Examples from Weakly-Labeled Sensor Data},
  author={Blalock, Davis W and Guttag, John V},
  booktitle={Data Mining (ICDM), 2016 IEEE 16th International Conference on},
  pages={799--804},
  year={2016},
  organization={IEEE}
}
@article{ecgCompressLossy,
  title={Multichannel ECG compression using multichannel adaptive vector quantization},
  author={Miaou, Shaou-Gang and Yen, Heng-Lin},
  journal={IEEE transactions on biomedical engineering},
  volume={48},
  number={10},
  pages={1203--1207},
  year={2001},
  publisher={IEEE}
}
@inproceedings{nemenyiTest,
  title={Distribution-free multiple comparisons},
  author={Nemenyi, Peter},
  booktitle={Biometrics},
  volume={18},
  number={2},
  pages={263},
  year={1962},
  organization={INTERNATIONAL BIOMETRIC SOC 1441 I ST, NW, SUITE 700, WASHINGTON, DC 20005-2210}
}
@inproceedings{hdfs,
  title={The hadoop distributed file system},
  author={Shvachko, Konstantin and Kuang, Hairong and Radia, Sanjay and Chansler, Robert},
  booktitle={Mass storage systems and technologies (MSST), 2010 IEEE 26th symposium on},
  pages={1--10},
  year={2010},
  organization={IEEE}
}
@article{spark,
  title={Spark: Cluster computing with working sets.},
  author={Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J and Shenker, Scott and Stoica, Ion},
  journal={HotCloud},
  volume={10},
  number={10-10},
  pages={95},
  year={2010}
}
@inproceedings{hive,
  title={Hive-a petabyte scale data warehouse using hadoop},
  author={Thusoo, Ashish and Sarma, Joydeep Sen and Jain, Namit and Shao, Zheng and Chakka, Prasad and Zhang, Ning and Antony, Suresh and Liu, Hao and Murthy, Raghotham},
  booktitle={Data Engineering (ICDE), 2010 IEEE 26th International Conference on},
  pages={996--1005},
  year={2010},
  organization={IEEE}
}
@article{mapreduce,
  title={MapReduce: simplified data processing on large clusters},
  author={Dean, Jeffrey and Ghemawat, Sanjay},
  journal={Communications of the ACM},
  volume={51},
  number={1},
  pages={107--113},
  year={2008},
  publisher={ACM}
}
@inproceedings{mdlIntrinsic,
  title={Discovering the intrinsic cardinality and dimensionality of time series using MDL},
  author={Hu, Bing and Rakthanmanon, Thanawin and Hao, Yuan and Evans, Scott and Lonardi, Stefano and Keogh, Eamonn},
  booktitle={Data Mining (ICDM), 2011 IEEE 11th International Conference on},
  pages={1086--1091},
  year={2011},
  organization={IEEE}
}
@article{cdDiagrams,
  title={Statistical comparisons of classifiers over multiple data sets},
  author={Dem{\v{s}}ar, Janez},
  journal={Journal of Machine learning research},
  volume={7},
  number={Jan},
  pages={1--30},
  year={2006}
}
@article{png,
  title={Png (portable network graphics) specification version 1.0},
  author={Boutell, Thomas},
  year={1997}
}
@article{parquet,
  title={Introducing Parquet: Efficient columnar storage for apache hadoop},
  author={Kestelyn, Justin},
  journal={Cloudera Blog},
  volume={3},
  year={2013}
}
@article{sprintz,
  title={Sprintz: Time Series Compression for the Internet of Things},
  author={Blalock, Davis and Madden, Samuel and Guttag, John},
  journal={Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume={2},
  number={3},
  pages={93},
  year={2018},
  publisher={ACM}
}
@article{zfp,
  title={Fixed-rate compressed floating-point arrays},
  author={Lindstrom, Peter},
  journal={IEEE transactions on visualization and computer graphics},
  volume={20},
  number={12},
  pages={2674--2683},
  year={2014},
  publisher={IEEE}
}
@inproceedings{dfcm,
  title={Fast lossless compression of scientific floating-point data},
  author={Ratanaworabhan, Paruj and Ke, Jian and Burtscher, Martin},
  booktitle={Data Compression Conference (DCC'06)},
  pages={133--142},
  year={2006},
  organization={IEEE}
}

@article{lottery-transfer,
	title = {One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers},
	shorttitle = {One ticket to win them all},
	url = {http://arxiv.org/abs/1906.02773},
	abstract = {The success of lottery ticket initializations [7] suggests that small, sparsiﬁed networks can be trained so long as the network is initialized appropriately. Unfortunately, ﬁnding these “winning ticket” initializations is computationally expensive. One potential solution is to reuse the same winning tickets across a variety of datasets and optimizers. However, the generality of winning ticket initializations remains unclear. Here, we attempt to answer this question by generating winning tickets for one training conﬁguration (optimizer and dataset) and evaluating their performance on another conﬁguration. Perhaps surprisingly, we found that, within the natural images domain, winning ticket initializations generalized across a variety of datasets, including Fashion MNIST, SVHN, CIFAR-10/100, ImageNet, and Places365, often achieving performance close to that of winning tickets generated on the same dataset. Moreover, winning tickets generated using larger datasets consistently transferred better than those generated using smaller datasets. We also found that winning ticket initializations generalize across optimizers with high performance. These results suggest that winning ticket initializations contain inductive biases generic to neural networks more broadly which improve training across many settings and provide hope for the development of better initialization methods.},
	language = {en},
	urldate = {2019-07-19},
	journal = {arXiv:1906.02773 [cs, stat]},
	author = {Morcos, Ari S. and Yu, Haonan and Paganini, Michela and Tian, Yuandong},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.02773},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing}
}

@article{snip-followup,
	title = {A {Signal} {Propagation} {Perspective} for {Pruning} {Neural} {Networks} at {Initialization}},
	url = {http://arxiv.org/abs/1906.06307},
	abstract = {Network pruning is a promising avenue for compressing deep neural networks. A typical approach to pruning starts by training a model and removing unnecessary parameters while minimizing the impact on what is learned. Alternatively, a recent approach shows that pruning can be done at initialization prior to training. However, it remains unclear exactly why pruning an untrained, randomly initialized neural network is effective. In this work, we consider the pruning problem from a signal propagation perspective, formally characterizing initialization conditions that ensure faithful signal propagation throughout a network. Based on singular values of a network’s input-output Jacobian, we ﬁnd that orthogonal initialization enables more faithful signal propagation compared to other initialization schemes, thereby enhancing pruning results on a range of modern architectures and datasets. Also, we empirically study the effect of supervision for pruning at initialization, and show that often unsupervised pruning can be as effective as the supervised pruning. Furthermore, we demonstrate that our signal propagation perspective, combined with unsupervised pruning, can indeed be useful in various scenarios where pruning is applied to non-standard arbitrarily-designed architectures.},
	language = {en},
	urldate = {2019-07-19},
	journal = {arXiv:1906.06307 [cs, stat]},
	author = {Lee, Namhoon and Ajanthan, Thalaiyasingam and Gould, Stephen and Torr, Philip H. S.},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.06307},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition}
}

@article{prune-largest,
	title = {The {Generalization}-{Stability} {Tradeoff} in {Neural} {Network} {Pruning}},
	url = {http://arxiv.org/abs/1906.03728},
	abstract = {Pruning neural network parameters to reduce model size is an area of much interest, but the original motivation for pruning was the prevention of overﬁtting rather than the improvement of computational efﬁciency. This motivation is particularly relevant given the perhaps surprising observation that a wide variety of pruning approaches confer increases in test accuracy, even when parameter counts are drastically reduced. To better understand this phenomenon, we analyze the behavior of pruning over the course of training, ﬁnding that pruning’s effect on generalization relies more on the instability generated by pruning than the ﬁnal size of the pruned model. We demonstrate that even pruning of seemingly unimportant parameters can lead to such instability, allowing our ﬁnding to account for the generalization beneﬁts of modern pruning techniques. Our results ultimately suggest that, counterintuitively, pruning regularizes through instability and mechanisms unrelated to parameter counts.},
	language = {en},
	urldate = {2019-07-19},
	journal = {arXiv:1906.03728 [cs, stat]},
	author = {Bartoldson, Brian R. and Morcos, Ari S. and Barbu, Adrian and Erlebacher, Gordon},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.03728},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{stabilizing-lottery-tix,
	title = {Stabilizing the {Lottery} {Ticket} {Hypothesis}},
	url = {http://arxiv.org/abs/1903.01611},
	abstract = {Pruning is a well-established technique for removing unnecessary structure from neural networks after training to improve the performance of inference. Several recent results have explored the possibility of pruning at initialization time to provide similar beneﬁts during training. In particular, the lottery ticket hypothesis conjectures that typical neural networks contain small subnetworks that can train to similar accuracy in a commensurate number of steps. The evidence for this claim is that a procedure based on iterative magnitude pruning (IMP) reliably ﬁnds such subnetworks retroactively on small vision tasks. However, IMP fails on deeper networks, and proposed methods to prune before training or train pruned networks encounter similar scaling limitations.},
	language = {en},
	urldate = {2019-07-19},
	journal = {arXiv:1903.01611 [cs, stat]},
	author = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M. and Carbin, Michael},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.01611},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition}
}


@article{han-sparse-how,
	title = {Exploring the {Regularity} of {Sparse} {Structure} in {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1705.08922},
	abstract = {Sparsity helps reduce the computational complexity of deep neural networks by skipping zeros. Taking advantage of sparsity is listed as a high priority in the next generation DNN accelerators such as TPU[1]. The structure of sparsity, i.e., the granularity of pruning, affects the efﬁciency of hardware accelerator design as well as the prediction accuracy. Coarse-grained pruning brings more regular sparsity patterns, making it more amenable for hardware acceleration, but more challenging to maintain the same accuracy. In this paper we quantitatively measure the tradeoff between sparsity regularity and the prediction accuracy, providing insights in how to maintain the accuracy while having more structured sparsity pattern. Our experimental results show that coarse-grained pruning can achieve similar sparsity ratio as unstructured pruning given no loss of accuracy. Moreover, due to the index saving effect, coarse-grained pruning is able to obtain better compression ratio than ﬁne-grained sparsity at the same accuracy threshold. Based on the recent sparse convolutional neural network accelerator (SCNN), our experiments further demonstrate that coarse-grained sparsity saves ∼ 2× of the memory references compared with ﬁne-grained sparsity. Since memory reference is more than two orders of magnitude more expensive than arithmetic operations, the regularity of sparse structure leads to more efﬁcient hardware design.},
	language = {en},
	urldate = {2019-07-19},
	journal = {arXiv:1705.08922 [cs, stat]},
	author = {Mao, Huizi and Han, Song and Pool, Jeff and Li, Wenshuo and Liu, Xingyu and Wang, Yu and Dally, William J.},
	month = may,
	year = {2017},
	note = {arXiv: 1705.08922},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{lebedevSpeedingSurvey,
  title={Speeding-up convolutional neural networks: A survey},
  author={Lebedev, Vadim and Lempitsky, Victor},
  journal={Bulletin of the Polish Academy of Sciences. Technical Sciences},
  volume={66},
  number={6},
  year={2018}
}
@inproceedings{sparseSurvey,
  title={A Survey of Sparse-Learning Methods for Deep Neural Networks},
  author={Ma, Rongrong and Niu, Lingfeng},
  booktitle={2018 IEEE/WIC/ACM International Conference on Web Intelligence (WI)},
  pages={647--650},
  year={2018},
  organization={IEEE}
}

@article{melis_state_2017,
	title = {On the {State} of the {Art} of {Evaluation} in {Neural} {Language} {Models}},
	url = {http://arxiv.org/abs/1707.05589},
	abstract = {Ongoing innovations in recurrent neural network architectures have provided a steady inﬂux of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.},
	language = {en},
	urldate = {2019-07-25},
	journal = {arXiv:1707.05589 [cs]},
	author = {Melis, Gábor and Dyer, Chris and Blunsom, Phil},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.05589},
	keywords = {Computer Science - Computation and Language},
}

@article{lipton_troubling_2018,
	title = {Troubling {Trends} in {Machine} {Learning} {Scholarship}},
	url = {http://arxiv.org/abs/1807.03341},
	abstract = {Collectively, machine learning (ML) researchers are engaged in the creation and dissemination of knowledge about data-driven algorithms. In a given paper, researchers might aspire to any subset of the following goals, among others: to theoretically characterize what is learnable, to obtain understanding through empirically rigorous experiments, or to build a working system that has high predictive accuracy. While determining which knowledge warrants inquiry may be subjective, once the topic is fixed, papers are most valuable to the community when they act in service of the reader, creating foundational knowledge and communicating as clearly as possible. Recent progress in machine learning comes despite frequent departures from these ideals. In this paper, we focus on the following four patterns that appear to us to be trending in ML scholarship: (i) failure to distinguish between explanation and speculation; (ii) failure to identify the sources of empirical gains, e.g., emphasizing unnecessary modifications to neural architectures when gains actually stem from hyper-parameter tuning; (iii) mathiness: the use of mathematics that obfuscates or impresses rather than clarifies, e.g., by confusing technical and non-technical concepts; and (iv) misuse of language, e.g., by choosing terms of art with colloquial connotations or by overloading established technical terms. While the causes behind these patterns are uncertain, possibilities include the rapid expansion of the community, the consequent thinness of the reviewer pool, and the often-misaligned incentives between scholarship and short-term measures of success (e.g., bibliometrics, attention, and entrepreneurial opportunity). While each pattern offers a corresponding remedy (don't do it), we also discuss some speculative suggestions for how the community might combat these trends.},
	language = {en},
	urldate = {2019-07-25},
	journal = {arXiv:1807.03341 [cs, stat]},
	author = {Lipton, Zachary C. and Steinhardt, Jacob},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.03341},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
}

@article{sculley_winners_2018,
	title = {Winner's {Curse}? {On} {Pace}, {Progression}, {Progress}, and {Empirical} {Rigor}},
	abstract = {The ﬁeld of ML is distinguished both by rapid innovation and rapid dissemination of results. While the pace of progress has been extraordinary by any measure, in this paper we explore potential issues that we believe to be arising as a result. In particular, we observe that the rate of empirical advancement may not have been matched by consistent increase in the level of empirical rigor across the ﬁeld as a whole. This short position paper highlights examples where progress has actually been slowed as a result, offers thoughts on incentive structures currently at play, and gives suggestions as seeds for discussions on productive change.},
	language = {en},
	author = {Sculley, D and Snoek, Jasper and Rahimi, Ali and Wiltschko, Alex},
	year = {2018},
	pages = {4},
}

@article{dacrema_are_2019,
	title = {Are {We} {Really} {Making} {Much} {Progress}? {A} {Worrying} {Analysis} of {Recent} {Neural} {Recommendation} {Approaches}},
	shorttitle = {Are {We} {Really} {Making} {Much} {Progress}?},
	url = {http://arxiv.org/abs/1907.06902},
	doi = {10.1145/3298689.3347058},
	abstract = {Deep learning techniques have become the method of choice for researchers working on algorithmic aspects of recommender systems. With the strongly increased interest in machine learning in general, it has, as a result, become difficult to keep track of what represents the state-of-the-art at the moment, e.g., for top-n recommendation tasks. At the same time, several recent publications point out problems in today's research practice in applied machine learning, e.g., in terms of the reproducibility of the results or the choice of the baselines when proposing new models. In this work, we report the results of a systematic analysis of algorithmic proposals for top-n recommendation tasks. Specifically, we considered 18 algorithms that were presented at top-level research conferences in the last years. Only 7 of them could be reproduced with reasonable effort. For these methods, it however turned out that 6 of them can often be outperformed with comparably simple heuristic methods, e.g., based on nearest-neighbor or graph-based techniques. The remaining one clearly outperformed the baselines but did not consistently outperform a well-tuned non-neural linear ranking method. Overall, our work sheds light on a number of potential problems in today's machine learning scholarship and calls for improved scientific practices in this area. Source code of our experiments and full results are available at: https://github.com/MaurizioFD/RecSys2019\_DeepLearning\_Evaluation.},
	language = {en},
	urldate = {2019-07-25},
	journal = {arXiv:1907.06902 [cs]},
	author = {Dacrema, Maurizio Ferrari and Cremonesi, Paolo and Jannach, Dietmar},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.06902},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Information Retrieval},
}

@inproceedings{armstrong_improvements_2009,
	address = {Hong Kong, China},
	title = {Improvements that don't add up: ad-hoc retrieval results since 1998},
	isbn = {978-1-60558-512-3},
	shorttitle = {Improvements that don't add up},
	url = {http://portal.acm.org/citation.cfm?doid=1645953.1646031},
	doi = {10.1145/1645953.1646031},
	abstract = {The existence and use of standard test collections in information retrieval experimentation allows results to be compared between research groups and over time. Such comparisons, however, are rarely made. Most researchers only report results from their own experiments, a practice that allows lack of overall improvement to go unnoticed. In this paper, we analyze results achieved on the TREC Ad-Hoc, Web, Terabyte, and Robust collections as reported in SIGIR (1998–2008) and CIKM (2004–2008). Dozens of individual published experiments report effectiveness improvements, and often claim statistical signiﬁcance. However, there is little evidence of improvement in ad-hoc retrieval technology over the past decade. Baselines are generally weak, often being below the median original TREC system. And in only a handful of experiments is the score of the best TREC automatic run exceeded. Given this ﬁnding, we question the value of achieving even a statistically signiﬁcant result over a weak baseline. We propose that the community adopt a practice of regular longitudinal comparison to ensure measurable progress, or at least prevent the lack of it from going unnoticed. We describe an online database of retrieval runs that facilitates such a practice.},
	language = {en},
	urldate = {2019-07-25},
	booktitle = {Proceeding of the 18th {ACM} conference on {Information} and knowledge management - {CIKM} '09},
	publisher = {ACM Press},
	author = {Armstrong, Timothy G. and Moffat, Alistair and Webber, William and Zobel, Justin},
	year = {2009},
	pages = {601},
}

@article{keogh_need_2003,
	title = {On the need for time series data mining benchmarks: a survey and empirical demonstration},
	volume = {7},
	number = {4},
	journal = {Data Mining and knowledge discovery},
	author = {Keogh, Eamonn and Kasetty, Shruti},
	year = {2003},
	pages = {349--371},
}

@article{ding_querying_2008,
	title = {Querying and mining of time series data: experimental comparison of representations and distance measures},
	volume = {1},
	number = {2},
	journal = {Proceedings of the VLDB Endowment},
	author = {Ding, Hui and Trajcevski, Goce and Scheuermann, Peter and Wang, Xiaoyue and Keogh, Eamonn},
	year = {2008},
	pages = {1542--1552},
}
@article{hebbOrig,
  title={The organization of behavior: A neuropsychological theory},
  author={Hebb, Donald O},
  year={1949},
  publisher={John Wiley}
}
@article{bcmOrig,
  title={Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex},
  author={Bienenstock, Elie L and Cooper, Leon N and Munro, Paul W},
  journal={Journal of Neuroscience},
  volume={2},
  number={1},
  pages={32--48},
  year={1982},
  publisher={Soc Neuroscience}
}
@article{synapticPruning1998,
  title={Synaptic pruning in development: a computational account},
  author={Chechik, Gal and Meilijson, Isaac and Ruppin, Eytan},
  journal={Neural computation},
  volume={10},
  number={7},
  pages={1759--1777},
  year={1998},
  publisher={MIT Press}
}
@article{levy1993synaptogenesis,
  title={Adaptive synaptogenesis constructs networks that maintain information and reduce statistical dependence},
  author={Adelsberger-Mangan, Dawn M and Levy, William B},
  journal={Biological cybernetics},
  volume={70},
  number={1},
  pages={81--87},
  year={1993},
  publisher={Springer}
}
@article{mSynaptogenesis,
  title={Adaptive synaptogenesis constructs neural codes that benefit discrimination},
  author={Thomas, Blake T and Blalock, Davis W and Levy, William B},
  journal={PLoS computational biology},
  volume={11},
  number={7},
  pages={e1004299},
  year={2015},
  publisher={Public Library of Science}
}
