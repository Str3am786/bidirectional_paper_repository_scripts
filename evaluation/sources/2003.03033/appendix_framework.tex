%!TEX root = doc.tex
%!TEX output_directory = aux

\section{Experimental Setup} \label{apx:exp}

For reproducibility purposes, \SB{} fixes random seeds for all the dependencies (\texttt{PyTorch}, \texttt{NumPy}, \texttt{Python}).


\subsection{Pruning Methods}

For the reported experiments, we did not prune the classifier layer preceding the softmax.
%
\SB{} supports pruning said layer as an option to all proposed pruning strategies.
%
For both Global and Layerwise Gradient Magnitude Pruning a single minibatch is used to compute the gradients for the pruning.
%
Three independent runs using different random seeds were performed for every CIFAR10 experiment.
%
We found some variance across methods that relied on randomness, such as random pruning or gradient based methods that use a sampled minibatch to compute the gradients with respect to the weights.


\subsection{Finetuning Setup}

Pruning was performed from the pretrained weights and fixed from there forwards.
%
% Reported values are always for the validation set.
%
Early stopping is implemented during finetuning.
%
Thus if the validation accuracy repeatedly decreases after some point we stop the finetuning process to prevent overfitting.
%
% We did not perform any hyperparameter search across finetuning hyperparameters.

All reported CIFAR10 experiments used the following finetuning setup:
\begin{itemize}[leftmargin=4mm]
    \itemsep-2pt
    \vspace{-2mm}
    \item Batch size: 64
    \item Epochs: 30
    \item Optimizer: Adam
    \item Initial Learning Rate: $3 \times 10^{-4}$
    \item Learning rate schedule: Fixed
\end{itemize}
%
All reported ImageNet experiments used the following finetuning setup
\begin{itemize}[leftmargin=4mm]
    \itemsep-2pt
    \vspace{-2mm}
    \item Batch size: 256
    \item Epochs: 20
    \item Optimizer: SGD with Nesterov Momentum (0.9)
    \item Initial Learning Rate: $1 \times 10^{-3}$
    \item Learning rate schedule: Fixed
\end{itemize}
