\begin{thebibliography}{78}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[caf(2016)]{caffenet}
What's the advantage of the reference caffenet in comparison with the alexnet?
\newblock \url{https://github.com/BVLC/caffe/issues/4202}, 5 2016.
\newblock Accessed: 2019-07-22.

\bibitem[unr(2017)]{unreproducible4}
Keras exported model shows very low accuracy in tensorflow serving.
\newblock \url{https://github.com/keras-team/keras/issues/7848}, 9 2017.
\newblock Accessed: 2019-07-22.

\bibitem[Bianco et~al.(2018)Bianco, Cadene, Celona, and Napoletano]{luigi}
Bianco, S., Cadene, R., Celona, L., and Napoletano, P.
\newblock Benchmark analysis of representative deep neural network
  architectures.
\newblock \emph{IEEE Access}, 6:\penalty0 64270--64277, 2018.

\bibitem[Blalock et~al.(2018)Blalock, Madden, and Guttag]{sprintz}
Blalock, D., Madden, S., and Guttag, J.
\newblock Sprintz: Time series compression for the internet of things.
\newblock \emph{Proceedings of the ACM on Interactive, Mobile, Wearable and
  Ubiquitous Technologies}, 2\penalty0 (3):\penalty0 93, 2018.

\bibitem[Changpinyo et~al.(2017)Changpinyo, Sandler, and
  Zhmoginov]{google-interchannel}
Changpinyo, S., Sandler, M., and Zhmoginov, A.
\newblock The power of sparsity in convolutional neural networks.
\newblock \emph{arXiv preprint arXiv:1702.06257}, 2017.

\bibitem[Choi et~al.(2019)Choi, El-Khamy, and Lee]{samsung-winograd-sparse}
Choi, Y., El-Khamy, M., and Lee, J.
\newblock Jointly sparse convolutional neural networks in dual spatial-winograd
  domains.
\newblock \emph{arXiv preprint arXiv:1902.08192}, 2019.

\bibitem[Crall(2018)]{unreproducible0}
Crall, J.
\newblock Accuracy of resnet50 is much higher than reported!
\newblock \url{https://github.com/kuangliu/pytorch-cifar/issues/45}, 2018.
\newblock Accessed: 2019-07-22.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Ding et~al.(2018)Ding, Ding, Han, and Tang]{ding-auto-balanced}
Ding, X., Ding, G., Han, J., and Tang, S.
\newblock Auto-balanced filter pruning for efficient convolutional neural
  networks.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem[Dong et~al.(2017)Dong, Huang, Yang, and Yan]{more-is-less}
Dong, X., Huang, J., Yang, Y., and Yan, S.
\newblock More is less: A more complicated network with less inference
  complexity.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  5840--5848, 2017.

\bibitem[Dubey et~al.(2018)Dubey, Chatterjee, and Ahuja]{uiuc-coreset-pruning}
Dubey, A., Chatterjee, M., and Ahuja, N.
\newblock Coreset-based neural network compression.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pp.\  454--470, 2018.

\bibitem[Figurnov et~al.(2016)Figurnov, Ibraimova, Vetrov, and
  Kohli]{perforated-cnns}
Figurnov, M., Ibraimova, A., Vetrov, D.~P., and Kohli, P.
\newblock Perforatedcnns: Acceleration through elimination of redundant
  convolutions.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  947--955, 2016.

\bibitem[Frankle \& Carbin(2019)Frankle and Carbin]{lottery-ticket}
Frankle, J. and Carbin, M.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.
\newblock URL \url{https://openreview.net/forum?id=rJl-b3RcF7}.

\bibitem[Frankle et~al.(2019)Frankle, Dziugaite, Roy, and
  Carbin]{lottery-ticket-followup}
Frankle, J., Dziugaite, G.~K., Roy, D.~M., and Carbin, M.
\newblock The lottery ticket hypothesis at scale.
\newblock \emph{arXiv preprint arXiv:1903.01611}, 2019.

\bibitem[Gale et~al.(2019)Gale, Elsen, and Hooker]{google-state-of-sparsity}
Gale, T., Elsen, E., and Hooker, S.
\newblock The state of sparsity in deep neural networks, 2019.

\bibitem[Gray et~al.(2017)Gray, Radford, and Kingma]{openai-block-sparse}
Gray, S., Radford, A., and Kingma, D.~P.
\newblock Gpu kernels for block-sparse weights.
\newblock \emph{arXiv preprint arXiv:1711.09224}, 2017.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{learning-both}
Han, S., Pool, J., Tran, J., and Dally, W.
\newblock Learning both weights and connections for efficient neural network.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1135--1143, 2015.

\bibitem[Han et~al.(2016)Han, Mao, and Dally]{han-prune-quant-huff}
Han, S., Mao, H., and Dally, W.~J.
\newblock Deep compression: Compressing deep neural network with pruning,
  trained quantization and huffman coding.
\newblock In Bengio, Y. and LeCun, Y. (eds.), \emph{4th International
  Conference on Learning Representations, {ICLR} 2016, San Juan, Puerto Rico,
  May 2-4, 2016, Conference Track Proceedings}, 2016.
\newblock URL \url{http://arxiv.org/abs/1510.00149}.

\bibitem[Hassibi et~al.(1993)Hassibi, Stork, and Wolff]{optimal-brain-surgeon}
Hassibi, B., Stork, D.~G., and Wolff, G.~J.
\newblock Optimal brain surgeon and general network pruning.
\newblock In \emph{IEEE international conference on neural networks}, pp.\
  293--299. IEEE, 1993.

\bibitem[He et~al.(2016{\natexlab{a}})He, Zhang, Ren, and Sun]{resnet}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016{\natexlab{a}}.

\bibitem[He et~al.(2016{\natexlab{b}})He, Zhang, Ren, and Sun]{resnet2}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{European conference on computer vision}, pp.\  630--645.
  Springer, 2016{\natexlab{b}}.

\bibitem[He et~al.(2017)He, Zhang, and Sun]{channel-lasso-lstsq}
He, Y., Zhang, X., and Sun, J.
\newblock Channel pruning for accelerating very deep neural networks.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  1389--1397, 2017.

\bibitem[He et~al.(2018{\natexlab{a}})He, Kang, Dong, Fu, and
  Yang]{soft-filter-pruning}
He, Y., Kang, G., Dong, X., Fu, Y., and Yang, Y.
\newblock Soft filter pruning for accelerating deep convolutional neural
  networks.
\newblock In \emph{IJCAI International Joint Conference on Artificial
  Intelligence}, 2018{\natexlab{a}}.

\bibitem[He et~al.(2018{\natexlab{b}})He, Lin, Liu, Wang, Li, and
  Han]{amc-automl-han}
He, Y., Lin, J., Liu, Z., Wang, H., Li, L.-J., and Han, S.
\newblock Amc: Automl for model compression and acceleration on mobile devices.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pp.\  784--800, 2018{\natexlab{b}}.

\bibitem[Huang et~al.(2018)Huang, Cheng, Chen, Lee, Ngiam, Le, and Chen]{gpipe}
Huang, Y., Cheng, Y., Chen, D., Lee, H., Ngiam, J., Le, Q.~V., and Chen, Z.
\newblock Gpipe: Efficient training of giant neural networks using pipeline
  parallelism.
\newblock \emph{arXiv preprint arXiv:1811.06965}, 2018.

\bibitem[Huang \& Wang(2018)Huang and Wang]{sss}
Huang, Z. and Wang, N.
\newblock Data-driven sparse structure selection for deep neural networks.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pp.\  304--320, 2018.

\bibitem[Janowsky(1989)]{janowsky_pruning_1989}
Janowsky, S.~A.
\newblock Pruning versus clipping in neural networks.
\newblock \emph{Physical Review A}, 39\penalty0 (12):\penalty0 6600--6603, June
  1989.
\newblock ISSN 0556-2791.
\newblock \doi{10.1103/PhysRevA.39.6600}.
\newblock URL \url{https://link.aps.org/doi/10.1103/PhysRevA.39.6600}.

\bibitem[Jia et~al.(2015{\natexlab{a}})Jia, Shelhamer, Donahue, Karayev, Long,
  Girshick, Guadarrama, and Darrell]{lenet-5-proto1}
Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R.,
  Guadarrama, S., and Darrell, T.
\newblock lenet.
\newblock
  \url{https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet.prototxt},
  2 2015{\natexlab{a}}.

\bibitem[Jia et~al.(2015{\natexlab{b}})Jia, Shelhamer, Donahue, Karayev, Long,
  Girshick, Guadarrama, and Darrell]{lenet-5-proto2}
Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R.,
  Guadarrama, S., and Darrell, T.
\newblock lenet-train-test.
\newblock
  \url{https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet_train_test.prototxt},
  2 2015{\natexlab{b}}.

\bibitem[Jia et~al.(2016)Jia, Shelhamer, Donahue, Karayev, Long, Girshick,
  Guadarrama, and Darrell]{lenet-5-caffe}
Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R.,
  Guadarrama, S., and Darrell, T.
\newblock Training lenet on mnist with caffe.
\newblock \url{https://caffe.berkeleyvision.org/gathered/examples/mnist.html},
  5 2016.
\newblock Accessed: 2019-07-22.

\bibitem[Jogeshwar(2017)]{unreproducible1}
Jogeshwar, A.
\newblock Validating resnet50.
\newblock \url{https://github.com/keras-team/keras/issues/8672}, 12 2017.
\newblock Accessed: 2019-07-22.

\bibitem[Kalchbrenner et~al.(2018)Kalchbrenner, Elsen, Simonyan, Noury,
  Casagrande, Lockhart, Stimberg, Oord, Dieleman, and Kavukcuoglu]{wavernn}
Kalchbrenner, N., Elsen, E., Simonyan, K., Noury, S., Casagrande, N., Lockhart,
  E., Stimberg, F., Oord, A. v.~d., Dieleman, S., and Kavukcuoglu, K.
\newblock Efficient neural audio synthesis.
\newblock \emph{arXiv preprint arXiv:1802.08435}, 2018.

\bibitem[Karnin(1990)]{karnin_simple_1990}
Karnin, E.~D.
\newblock A simple procedure for pruning back-propagation trained neural
  networks.
\newblock \emph{IEEE transactions on neural networks}, 1\penalty0 (2):\penalty0
  239--242, 1990.

\bibitem[Kim et~al.(2015)Kim, Park, Yoo, Choi, Yang, and
  Shin]{samsung-vbmf-tucker}
Kim, Y.-D., Park, E., Yoo, S., Choi, T., Yang, L., and Shin, D.
\newblock Compression of deep convolutional neural networks for fast and low
  power mobile applications.
\newblock \emph{arXiv preprint arXiv:1511.06530}, 2015.

\bibitem[Lebedev \& Lempitsky(2016)Lebedev and
  Lempitsky]{lempitsky-fast-convnets}
Lebedev, V. and Lempitsky, V.
\newblock Fast convnets using group-wise brain damage.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  2554--2564, 2016.

\bibitem[Lebedev et~al.(2014)Lebedev, Ganin, Rakhuba, Oseledets, and
  Lempitsky]{lempitsky-cp-decomp}
Lebedev, V., Ganin, Y., Rakhuba, M., Oseledets, I., and Lempitsky, V.
\newblock Speeding-up convolutional neural networks using fine-tuned
  cp-decomposition.
\newblock \emph{arXiv preprint arXiv:1412.6553}, 2014.

\bibitem[LeCun et~al.(1990)LeCun, Denker, and Solla]{optimal-brain-damage}
LeCun, Y., Denker, J.~S., and Solla, S.~A.
\newblock Optimal brain damage.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  598--605, 1990.

\bibitem[LeCun et~al.(1998{\natexlab{a}})LeCun, Bottou, Bengio, Haffner,
  et~al.]{mnist}
LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., et~al.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998{\natexlab{a}}.

\bibitem[LeCun et~al.(1998{\natexlab{b}})LeCun, Cortes, and Burges]{mnist-page}
LeCun, Y., Cortes, C., and Burges, C.
\newblock The mnist database of handwritten digits, 1998{\natexlab{b}}.
\newblock Accessed: 2019-09-6.

\bibitem[Lee et~al.(2019{\natexlab{a}})Lee, Ajanthan, Gould, and
  Torr]{snip-followup}
Lee, N., Ajanthan, T., Gould, S., and Torr, P. H.~S.
\newblock A {Signal} {Propagation} {Perspective} for {Pruning} {Neural}
  {Networks} at {Initialization}.
\newblock \emph{arXiv:1906.06307 [cs, stat]}, June 2019{\natexlab{a}}.
\newblock URL \url{http://arxiv.org/abs/1906.06307}.
\newblock arXiv: 1906.06307.

\bibitem[Lee et~al.(2019{\natexlab{b}})Lee, Ajanthan, and Torr]{snip}
Lee, N., Ajanthan, T., and Torr, P. H.~S.
\newblock Snip: single-shot network pruning based on connection sensitivity.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net,
  2019{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=B1VZqjAcYX}.

\bibitem[Li et~al.(2016)Li, Kadav, Durdanovic, Samet, and
  Graf]{pruning-filters}
Li, H., Kadav, A., Durdanovic, I., Samet, H., and Graf, H.~P.
\newblock Pruning filters for efficient convnets.
\newblock \emph{arXiv preprint arXiv:1608.08710}, 2016.

\bibitem[Lindstrom(2014)]{zfp}
Lindstrom, P.
\newblock Fixed-rate compressed floating-point arrays.
\newblock \emph{IEEE transactions on visualization and computer graphics},
  20\penalty0 (12):\penalty0 2674--2683, 2014.

\bibitem[Liu et~al.(2017)Liu, Li, Shen, Huang, Yan, and
  Zhang]{network-slimming}
Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., and Zhang, C.
\newblock Learning efficient convolutional networks through network slimming.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  2736--2744, 2017.

\bibitem[Liu et~al.(2019)Liu, Sun, Zhou, Huang, and
  Darrell]{rethinking-net-pruning}
Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T.
\newblock Rethinking the value of network pruning.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.
\newblock URL \url{https://openreview.net/forum?id=rJlnB3C5Ym}.

\bibitem[Louizos et~al.(2017)Louizos, Ullrich, and
  Welling]{bayesian-compression}
Louizos, C., Ullrich, K., and Welling, M.
\newblock Bayesian compression for deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3288--3298, 2017.

\bibitem[Luo et~al.(2017)Luo, Wu, and Lin]{thinet-channel-norms}
Luo, J.-H., Wu, J., and Lin, W.
\newblock Thinet: A filter level pruning method for deep neural network
  compression.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  5058--5066, 2017.

\bibitem[Mariet \& Sra(2015)Mariet and Sra]{divnet}
Mariet, Z. and Sra, S.
\newblock Diversity networks: Neural network compression using determinantal
  point processes.
\newblock \emph{arXiv preprint arXiv:1511.05077}, 2015.

\bibitem[Molchanov et~al.(2017)Molchanov, Ashukha, and
  Vetrov]{sparse-variational-dropout}
Molchanov, D., Ashukha, A., and Vetrov, D.
\newblock Variational dropout sparsifies deep neural networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  2498--2507. JMLR. org, 2017.

\bibitem[Molchanov et~al.(2016)Molchanov, Tyree, Karras, Aila, and
  Kautz]{nvidia-taylor-pruning}
Molchanov, P., Tyree, S., Karras, T., Aila, T., and Kautz, J.
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock \emph{arXiv preprint arXiv:1611.06440}, 2016.

\bibitem[Morcos et~al.(2019)Morcos, Yu, Paganini, and Tian]{lottery-transfer}
Morcos, A.~S., Yu, H., Paganini, M., and Tian, Y.
\newblock One ticket to win them all: generalizing lottery ticket
  initializations across datasets and optimizers.
\newblock \emph{arXiv:1906.02773 [cs, stat]}, June 2019.
\newblock URL \url{http://arxiv.org/abs/1906.02773}.
\newblock arXiv: 1906.02773.

\bibitem[Mozer \& Smolensky(1989{\natexlab{a}})Mozer and
  Smolensky]{mozer_skeletonization:_1989}
Mozer, M.~C. and Smolensky, P.
\newblock Skeletonization: {A} technique for trimming the fat from a network
  via relevance assessment.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  107--115, 1989{\natexlab{a}}.

\bibitem[Mozer \& Smolensky(1989{\natexlab{b}})Mozer and
  Smolensky]{mozer_using_1989}
Mozer, M.~C. and Smolensky, P.
\newblock Using {Relevance} to {Reduce} {Network} {Size} {Automatically}.
\newblock \emph{Connection Science}, 1\penalty0 (1):\penalty0 3--16, January
  1989{\natexlab{b}}.
\newblock ISSN 0954-0091, 1360-0494.
\newblock \doi{10.1080/09540098908915626}.
\newblock URL
  \url{https://www.tandfonline.com/doi/full/10.1080/09540098908915626}.

\bibitem[Nola(2016)]{unreproducible3}
Nola, D.
\newblock Keras doesn't reproduce caffe example code accuracy.
\newblock \url{https://github.com/keras-team/keras/issues/4444}, 11 2016.
\newblock Accessed: 2019-07-22.

\bibitem[Northcutt(2019)]{unreproducibleCurtis}
Northcutt, C.
\newblock Towards reproducibility: Benchmarking keras and pytorch.
\newblock
  https://l7.curtisnorthcutt.com/towards-reproducibility-benchmarking-keras-pytorch,
  2 2019.
\newblock Accessed: 2019-07-22.

\bibitem[Parkhi et~al.(2015)Parkhi, Vedaldi, Zisserman,
  et~al.]{deepFaceRecognition}
Parkhi, O.~M., Vedaldi, A., Zisserman, A., et~al.
\newblock Deep face recognition.
\newblock In \emph{bmvc}, volume~1, pp.\ ~6, 2015.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{pytorch}
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
  Desmaison, A., Antiga, L., and Lerer, A.
\newblock Automatic differentiation in pytorch.
\newblock 2017.

\bibitem[Peng et~al.(2018)Peng, Tan, Li, Zhang, Xie, and
  Pu]{extreme-net-compress}
Peng, B., Tan, W., Li, Z., Zhang, S., Xie, D., and Pu, S.
\newblock Extreme network compression via filter group approximation.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pp.\  300--316, 2018.

\bibitem[Ratanaworabhan et~al.(2006)Ratanaworabhan, Ke, and Burtscher]{dfcm}
Ratanaworabhan, P., Ke, J., and Burtscher, M.
\newblock Fast lossless compression of scientific floating-point data.
\newblock In \emph{Data Compression Conference (DCC'06)}, pp.\  133--142. IEEE,
  2006.

\bibitem[Reed(1993)]{reed_pruning_1993}
Reed, R.
\newblock Pruning algorithms-a survey.
\newblock \emph{IEEE Transactions on Neural Networks}, 4\penalty0 (5):\penalty0
  740--747, September 1993.
\newblock ISSN 10459227.
\newblock \doi{10.1109/72.248452}.
\newblock URL \url{http://ieeexplore.ieee.org/document/248452/}.

\bibitem[Siedelmann et~al.(2015)Siedelmann, Wender, and Fuchs]{bbp}
Siedelmann, H., Wender, A., and Fuchs, M.
\newblock High speed lossless image compression.
\newblock In \emph{German Conference on Pattern Recognition}, pp.\  343--355.
  Springer, 2015.

\bibitem[Simonyan \& Zisserman(2014)Simonyan and Zisserman]{vgg}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Suau et~al.(2018)Suau, Zappella, and Apostoloff]{apple-pfa}
Suau, X., Zappella, L., and Apostoloff, N.
\newblock Network compression using correlation analysis of layer responses.
\newblock 2018.

\bibitem[Suzuki et~al.(2018)Suzuki, Abe, Murata, Horiuchi, Ito, Wachi, Hirai,
  Yukishima, and Nishimura]{spectral-pruning}
Suzuki, T., Abe, H., Murata, T., Horiuchi, S., Ito, K., Wachi, T., Hirai, S.,
  Yukishima, M., and Nishimura, T.
\newblock Spectral-pruning: Compressing deep neural network via spectral
  analysis.
\newblock \emph{arXiv preprint arXiv:1808.08558}, 2018.

\bibitem[Sze et~al.(2017)Sze, Chen, Yang, and Emer]{szeEfficient}
Sze, V., Chen, Y.-H., Yang, T.-J., and Emer, J.
\newblock Efficient processing of deep neural networks: A tutorial and survey.
\newblock \emph{arXiv preprint arXiv:1703.09039}, 2017.

\bibitem[Tan \& Le(2019)Tan and Le]{efficientnet}
Tan, M. and Le, Q.~V.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock \emph{arXiv preprint arXiv:1905.11946}, 2019.

\bibitem[Tresp et~al.(1997)Tresp, Neuneier, and Zimmermann]{early-brain-damage}
Tresp, V., Neuneier, R., and Zimmermann, H.-G.
\newblock Early brain damage.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  669--675, 1997.

\bibitem[Vryniotis(2018)]{kerasBnWeird}
Vryniotis, V.
\newblock Change bn layer to use moving mean/var if frozen.
\newblock \url{https://github.com/keras-team/keras/pull/9965}, 4 2018.
\newblock Accessed: 2019-07-22.

\bibitem[Wang \& Cheng(2016)Wang and Cheng]{convnet-tensor-decomp}
Wang, P. and Cheng, J.
\newblock Accelerating convolutional neural networks for mobile applications.
\newblock In \emph{Proceedings of the 24th ACM international conference on
  Multimedia}, pp.\  541--545. ACM, 2016.

\bibitem[Wen et~al.(2016)Wen, Wu, Wang, Chen, and Li]{ssl}
Wen, W., Wu, C., Wang, Y., Chen, Y., and Li, H.
\newblock Learning structured sparsity in deep neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2074--2082, 2016.

\bibitem[Yamamoto \& Maeno(2018)Yamamoto and Maeno]{pcas}
Yamamoto, K. and Maeno, K.
\newblock Pcas: Pruning channels with attention statistics.
\newblock \emph{arXiv preprint arXiv:1806.05382}, 2018.

\bibitem[Yang et~al.(2017)Yang, Chen, and Sze]{sze-energy-aware}
Yang, T.-J., Chen, Y.-H., and Sze, V.
\newblock Designing energy-efficient convolutional neural networks using
  energy-aware pruning.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  5687--5695, 2017.

\bibitem[Yao et~al.(2018)Yao, Cao, and Xiao]{balanced-sparsity}
Yao, Z., Cao, S., and Xiao, W.
\newblock Balanced sparsity for efficient dnn inference on gpu.
\newblock \emph{arXiv preprint arXiv:1811.00206}, 2018.

\bibitem[Yu et~al.(2018)Yu, Li, Chen, Lai, Morariu, Han, Gao, Lin, and
  Davis]{nisp}
Yu, R., Li, A., Chen, C.-F., Lai, J.-H., Morariu, V.~I., Han, X., Gao, M., Lin,
  C.-Y., and Davis, L.~S.
\newblock Nisp: Pruning networks using neuron importance score propagation.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  9194--9203, 2018.

\bibitem[Zagoruyko(2015)]{vggCifarTorch}
Zagoruyko, S.
\newblock 92.45\% on cifar-10 in torch.
\newblock \url{https://torch.ch/blog/2015/07/30/cifar.html}, 7 2015.
\newblock Accessed: 2019-07-22.

\bibitem[Zhang et~al.(2015)Zhang, Zou, He, and Sun]{zhang-accel-very-deep}
Zhang, X., Zou, J., He, K., and Sun, J.
\newblock Accelerating very deep convolutional networks for classification and
  detection.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 38\penalty0 (10):\penalty0 1943--1955, 2015.

\bibitem[Zhao et~al.(2015)Zhao, Zhang, Lemire, Shan, Nie, Yan, and
  Wen]{groupSimd}
Zhao, W.~X., Zhang, X., Lemire, D., Shan, D., Nie, J.-Y., Yan, H., and Wen,
  J.-R.
\newblock A general simd-based approach to accelerating compression algorithms.
\newblock \emph{ACM Transactions on Information Systems (TOIS)}, 33\penalty0
  (3):\penalty0 15, 2015.

\bibitem[Zukowski et~al.(2006)Zukowski, Heman, Nes, and Boncz]{pfor}
Zukowski, M., Heman, S., Nes, N., and Boncz, P.
\newblock Super-scalar ram-cpu cache compression.
\newblock In \emph{Data Engineering, 2006. ICDE'06. Proceedings of the 22nd
  International Conference on}, pp.\  59--59. IEEE, 2006.

\end{thebibliography}
