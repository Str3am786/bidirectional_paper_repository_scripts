In the previous sections, we have argued that existing work tends to
\begin{itemize}[leftmargin=4mm]
    % \itemsep-1pt
    \itemsep0pt
    % \itemsep1pt
    % \itemsep3pt
    \vspace{-3mm}
    \item make it difficult to identify the exact experimental setup and metrics,
    \item use too few (dataset, architecture) combinations,
    \item report too few points in the tradeoff curve for any given combination, and no measures of central tendency,
    \item omit comparison to many methods that might be state-of-the-art, and
    \item fail to control for confounding variables.
    \vspace{-3mm}
\end{itemize}
% As a result, we do not believe that we can rigorously defend the claim that there even exists a single state-of-the-art method of pruning neural networks, let alone that any particular method is it.
These problems often make it difficult or impossible to assess the relative efficacy of different pruning methods.
% As a result, the vast majority of neural network pruning methods have never been meaningfully compared to one another. % Consequently, it is not clear that this subfield has made progress in the 30+ years of its existence. Note that this is not a statement that no paper has meaningful experiments, but rather that the collective experiments of the whole subfield are inadequate to prove that progress really has been made.
To enable direct comparison between methods in the future, we suggest the following practices:
\begin{itemize}[leftmargin=4mm]
    % \itemsep-1pt
    \itemsep-.5pt
    % \itemsep0pt
    % \itemsep1pt
    % \itemsep3pt
    \vspace{-2mm}
    % \vspace{-2mm}
\item Identify the \textit{exact} sets of architectures, datasets, and metrics used, ideally in a structured way that is not scattered throughout the results section. % Figures and tables should make clear the exact sets of independent and dependent variables being reported (e.g., ``Change in Top-1 Accuracy (\%) vs Compression Ratio for ResNet-50 on ImageNet'', with compression ratio defined explicitly in the text).
\item Use at least three (dataset, architecture) pairs, including modern, large-scale ones. MNIST and toy models do not count. AlexNet, CaffeNet, and Lenet-5 are no longer modern architectures.
% \item Make clear which algorithmic variations count as one's method \textit{per se}.
\item For any given pruned model, report both compression ratio and theoretical speedup. Compression ratio is defined as the original size divided by the new size. Theoretical speedup is defined as the original number of multiply-adds divided by the new number. Note that there is no reason to report only one of these metrics.% since both can easily and cheaply be computed for a given network.
\item For ImageNet and other many-class datasets, report both Top-1 and Top-5 accuracy. There is again no reason to report only one of these.
% \item For any given pruned model, report both Top-1 and (for ImageNet and other many-class datasets) Top-5 accuracy. There is again no reason to report only one of these.
\item Whatever metrics one reports for a given pruned model, also report these metrics for an appropriate control (usually the original model before pruning).
\item Plot the tradeoff curve for a given dataset and architecture, alongside the curves for competing methods. % Consider deferring tables of raw values to an appendix.
\item When plotting tradeoff curves, use at least 5 operating points spanning a range of compression ratios. The set of ratios $\{2, 4, 8, 16, 32\}$ is a good choice.
\item Report and plot means and sample standard deviations, instead of one-off measurements, whenever feasible.
% Given limited resourced, one can perhaps make an exception for ImageNet or Places365 \cite{places}.
\item Ensure that all methods being compared use identical libraries, data loading, and other code to the greatest extent possible.
% \item Consider focusing one's research efforts on controlled experiments to improve our understanding of neural network pruning (or efficiency more generally), rather than on creating a new ``state-of-the-art'' method.
\vspace{-2mm}
\end{itemize}

We also recommend that reviewers demand a much greater level of rigor when evaluating papers that claim to offer a better method of pruning neural networks.
