
Simultaneous localization and mapping (SLAM) is a core computational
component supporting several application scenarios in augmented/virtual
reality and autonomous robotics. As such, benchmarks for assessing the
performance of SLAM reflect the diverse deployment profiles of potential
application scenarios.  
They reflect a wide variety of 
sensors \cite{smith2009new,KITTI,RGBDbenchmark}, 
platforms \cite{zhao2019closednav,delmerico2018benchmark},
motion patterns \cite{urban2017lafida,schubert2018vidataset,burri2016euroc}, 
scene properties \cite{ICLNUIM,maddern20171}, 
and other meaningful characteristics. 
Given that a large portion of benchmarks are empirical, specialized (to
use case) scenarios recorded for evaluation through replay, there can be
a lack of control over important configuration variables related to
performance.
Furthermore, the diversity of software interfaces for the different datasets
and algorithms complicates comprehensive evaluation.
SLAMBench \cite{nardi2015introducing} addresses this last issue through the use of a
common API for evaluating algorithms with an emphasis on analysis of
different computational platforms and run-time parameters.
SLAMBench performance metrics include energy consumption, accuracy, and
computational performance. 
Follow-up work, SLAMBench 2~\cite{bodin2018slambench2}, improves API consistency 
and includes several SLAM implementations modified for compatibility with the
API.  The input stream can be synthetic data generated from simulations
\cite{ICLNUIM,li2018interiornet}. Simulation addresses the earlier point through the creation
of controlled scenarios that can be systematically perturbed or modified.
We hope to advance the practice of benchmarking by providing a meta-analysis
or design of experiments inspired analysis of the SLAM benchmarks and
algorithms with respect to accuracy and computation.  The analysis will
characterize existing benchmarks and identify critical components of the SLAM
pipeline under different benchmark characteristics. 

Our contributions in this direction follows in the subsequent sections.
Section \ref{sec:bench} lists existing benchmarks and briefly describes their
characteristics according to properties known to impact SLAM accuracy.
Analysis of differentiating factors regarding difficulty level is performed
to arrive at dominant factors influencing the difficulty annotation.
Section \ref{sec:timing} reviews time profiling outcomes of SLAM
instantiations in order to determine the time allocation required for
(sufficiently) complete execution of the SLAM pipeline prior to receipt of the 
next frame.  Providing sufficient time for the computations enables
separating the latency factor from the algorithm factor for establishing the
limiting bound of accuracy performance fo SLAM instances relative to existing
benchmarks.
Section \ref{sec:eval} applies 
three prevalent visual SLAM
algorithms and two low-latency counterparts
to a 
balanced
benchmark set as determined by the analysis of Section \ref{sec:bench}. The aim
of the study is to confirm that the qualitative assessment matches the
quantitative outcomes with the latter annotations determined by the
accuracy results.
The outcome distribution will be clustered into four performance classes:
\textit{fail}, \textit{low}, \textit{medium}, and \textit{high}, based on 
clustering the accuracy outcomes into three equal density regions plus
adding a fail category.  Comparison of the resulting decision trees will
establish whether the primary factors impacting performance relative to the
distinct performance categories are consistent or if a different
prioritization is in order.
The described analysis should provide a means to establish where structural
weaknesses of published SLAM methods lie and where future research effort
should be dedicated to maximize impact. %
The emphasis will be on monocular SLAM as improvements to monocular systems
should translate to the same for stereo and visual-inertial implementation
\cite{zhao2019closednav,zhao2019tro}.
We anticipate that the findings will support more systematic study of SLAM
algorithms in this new era of SLAM research, dubbed the ``Robust
Perception Age'' \cite{cadena2016past}.






