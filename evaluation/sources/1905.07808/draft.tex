\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  %


\IEEEoverridecommandlockouts                              %

\overrideIEEEmargins                                      %


\usepackage{graphicx} 

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{cite}
\usepackage{multirow}
\usepackage{algorithmic}
\usepackage[ruled,vlined,lined,linesnumbered,boxed,commentsnumbered]{algorithm2e}
\usepackage{booktabs}
\usepackage[caption=false]{subfig}
\usepackage{color}
\usepackage{balance}
\usepackage{threeparttable}
\usepackage{rotating}
\usepackage{pifont}
\usepackage[dvipsnames]{xcolor}
 
\definecolor{f5}{gray}{1}
\definecolor{f4}{gray}{0.8}
\definecolor{f3}{gray}{0.6}
\definecolor{f2}{gray}{0.4}
\definecolor{f1}{gray}{0.2}
\definecolor{f0}{gray}{0}

\title{\LARGE \bf
Characterizing SLAM Benchmarks and Methods for the Robust Perception Age
}

\author{Wenkai Ye$^{1}$, Yipu Zhao$^{1}$, and Patricio A. Vela$^{1}$%
\thanks{$^{1}$%
Wenkai Ye, Yipu Zhao, and Patricio A. Vela  
are with School of Electrical and Computer Engineering, 
Georgia Institute of Technology, Atlanta, Georgia, USA. 
{\tt\small \{wye35,yzhao347,pvela\}@gatech.edu}. }%
\thanks{
This work was supported in part by the China Scholarship Council 
(CSC Student No: 201606260089) and the National Science Foundation 
(Award \#1816138).
}%
}

\usepackage{url}
\usepackage{slam_macros}


\graphicspath{ {./images/}}

\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

\begin{abstract}
The diversity of SLAM benchmarks affords extensive testing of SLAM
algorithms to understand their performance, individually or in relative
terms. The \textit{ad-hoc} creation of these benchmarks does not necessarily
illuminate the particular weak points of a SLAM algorithm when performance is
evaluated. In this paper, we propose to use a decision tree to identify challenging 
benchmark properties for state-of-the-art SLAM algorithms and important components 
within the SLAM pipeline regarding their ability to handle these challenges.
Establishing what factors of a particular
sequence lead to track failure or degradation relative to these
characteristics is important if we are to arrive at a strong 
understanding for the core computational needs of a robust SLAM algorithm.
Likewise, we argue that it is important to profile the computational
performance of the individual SLAM components for use when benchmarking.
In particular, we advocate the use of time-dilation during ROS bag
playback, or what we refer to as \textit{slo-mo} playback. 
Using \textit{slo-mo} to benchmark SLAM instantiations can provide clues to how
SLAM implementations should be improved at the computational component
level. Three prevalent VO/SLAM algorithms and two low-latency algorithms 
of our own are tested on selected typical sequences, which are generated from 
benchmark characterization, to further demonstrate the benefits achieved from 
computationally efficient components.
\end{abstract}


\section{Introduction}
\input{intro.tex}
\input{bsetsample.tex}

\section{Benchmark Properties}
\label{sec:bench}
\input{benchmarks.tex}

\section{Time Profiling and Time Dilation}
\label{sec:timing}
\input{slomo.tex}

\section{Dataset Properties Influencing Performance}
\label{sec:eval}
\input{factors.tex}

\section{Conclusion}
\input{conc.tex}


\balance 
\bibliographystyle{IEEEtran}
\bibliography{../../full_references}


\end{document}
