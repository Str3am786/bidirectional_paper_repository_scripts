% There are two subsections in the appendix: 1. Section \ref{sec:proof} provides supplementary proofs to our theorem part.
% 2. Section \ref{sec:exp_details} provides more details of our experiment settings.

\subsection{ Proof to Proposition 1} \label{sec:proof1}

\begin{pf}
Notice that $\frac{x}{x+a}$ is increasing w.r.t $x\in \mathbb{R}, x \geq 0$, where $a\in \mathbb{R}, a > 0$ is a constant. Then we have:
\beqn{\scriptsize
\begin{aligned}
\mathcal{L}_{\rm RSM}
&=\expectunder[\substack{
                \{y^-_i\}_{i=1}^M \iidsim p_{\mathsf y}}]{-\log \frac{e^{\frac{1}{\tau}}}{e^{\frac{1}{\tau}} + \sum_ie^{f(x)\T f(y^-_i) / \tau}}}\\
&\leq \expectunder[\substack{
                (x, y) \sim \distnpos \\
                \{y^-_i\}_{i=1}^M \iidsim p_{\mathsf y}}]{-\log \frac{e^{f(x)\T f(y) / \tau}}{e^{f(x)\T f(y) / \tau} + \sum_ie^{f(x)\T f(y^-_i) / \tau}}}= \mathcal{L}_{\rm ASM}.
\end{aligned}
}

On the other hand,

\begin{equation}{\scriptsize
\begin{aligned}
&\mathcal{L}_{\rm ASM} \leq \expectunder[\substack{(x, y) \sim \distnpos \\
\{y^-_i\}_{i=1}^M \iidsim p_{\mathsf y}}]{-\log \left(\frac{e^{{\min}\left(f(x)\T f(y)\right)/\tau}}{e^{{\min}\left(f(x)\T f(y)\right)/\tau} + \sum_ie^{f(x)\T f(y^-_i)/\tau}}\right)}\\
& \leq  \expectunder[\substack{(x, y) \sim \distnpos \\
\{y^-_i\}_{i=1}^M \iidsim p_{\mathsf y}}]{-\log \left(\frac{e^{{\min}\left(f(x)\T f(y)\right)/\tau}}{e^{\frac{1}{\tau}} + \sum_ie^{f(x)\T f(y^-_i) / \tau}}\right)}\\
& \leq\mathcal{L}_{\rm RSM} + \frac{1}{\tau}\left[1-\underset{(x, y) \sim \distnpos}{\min}\left(f(x)\T f(y)\right)\right]. 
\end{aligned}
}\end{equation}
\hfill $\square$
\end{pf}

\subsection{ Proof to Theorem 2} \label{sec:proof2}

\begin{pf}
We follow the outline of Wang's proof~\cite{wang2020understanding}.

\beqn{\scriptsize
\begin{aligned}
&\lim_{M \rightarrow \infty} [\mathcal{L}_{{\rm ASM|}\lambda,\mathsf{x}}(f;\tau,M, p_{\mathsf y}) - \log M] \\ 
    &= -\frac{1}{\tau}\expectunder[\substack{
        (x, y) \sim \distnpos
    }]{f(x)\T f(y)} 
    \\&+ \lim_{M \rightarrow \infty} \expectunder[\substack{
        (x, y) \sim \distnpos \\
        \{y^-_i\}_{i=1}^M \iidsim p_{\mathsf y}
    }]{\log \left(\frac{\lambda}{M} e^{f(x)\T f(y) / \tau} + \frac{1}{M}\sum_i e^{f(x)\T f(y^-_i) / \tau}\right)}\\
    & = -\frac{1}{\tau}\expectunder[\substack{(x, y) \sim \distnpos}]{f(x)\T f(y)} + \expectunder[\substack{
        x \iidsim p_{\mathsf x}
    }]{\log \expectunder[\substack{
        y^- \iidsim p_{\mathsf y}
    }]{e^{f(x)\T f(y^-) / \tau}}}
\end{aligned}
}

\noindent where the last equality is by the S.L.L.N. (Strong Law of Large Numbers) and the Continuous Mapping Theorem.
\\
\\
\indent The convergence speed is derived as follows, where $\lambda \geq 1$ and $-1 \leq f(x)^Tf(y), f(x)^Tf(y_i^-) \leq 1$.
\\
\\
\indent For one side:
\beqn{\scriptsize
\begin{aligned}
&\mathcal{L}_{{\rm ASM|}\lambda,\mathsf{x}}(f;\tau,M, p_{\mathsf y}) - \log M - \lim_{M \rightarrow \infty}[\mathcal{L}_{{\rm ASM|}\lambda,\mathsf{x}}(f;\tau,M, p_{\mathsf y}) - \log M] \\
    & \leq \expectunder[\substack{
        x \iidsim p_{\mathsf x} \\
        \{y^-_i\}_{i=1}^M \iidsim p_{\mathsf y}
    }]{\log \left(\frac{\lambda}{M} e^{1 / \tau} + \frac{1}{M}\sum_i e^{f(x)\T f(y^-_i) / \tau  }\right)}\\ &- \expectunder[\substack{
        x \iidsim p_{\mathsf x}
    }]{\log \expectunder[\substack{
        y^- \iidsim p_{\mathsf y}
    }]{{e^{f(x)\T f(y^-) / \tau}}}}\\
    &\leq \expectunder[\substack{
        x \iidsim p_{\mathsf x}}]{{\log \expectunder[\substack{
        y^- \iidsim p_{\mathsf y}
    }]{\left(\frac{\lambda}{M} e^{1 / \tau} +  e^{f(x)\T f(y^-) / \tau  }\right)}} - \log \expectunder[\substack{
        y^- \iidsim p_{\mathsf y}
    }]{{e^{f(x)\T f(y^-) / \tau}}}}\\
    & \leq \expectunder[\substack{
        x \iidsim p_{\mathsf x}
    }]{\frac{\lambda}{M} e^{2 / \tau}}\\
    &= \frac{\lambda}{M} e^{2 / \tau},
\end{aligned}
}

\noindent where the second inequality follows the Jensen Inequality based on the the concavity of log.
\\
\\
\indent For the other side:
\beqn{ \scriptsize
\begin{aligned}
&\lim_{M \rightarrow \infty}[\mathcal{L}_{{\rm ASM|}\lambda,\mathsf{x}}(f;\tau,M, p_{\mathsf y}) - \log M] - [\mathcal{L}_{{\rm ASM|}\lambda,\mathsf{x}}(f;\tau,M, p_{\mathsf y}) - \log M]\\
&\leq e^{1/\tau}\expectunder[\substack{
        (x, y) \sim \distnpos \\
        \{y^-_i\}_{i=1}^M \iidsim p_{\mathsf y}
    }]{\left| \expectunder[\substack{
        y^- \iidsim p_{\mathsf y}
    }]{e^{f(x)\T f(y^-) / \tau}} - \left(\frac{\lambda}{M} e^{f(x)\T f(y) / \tau} + \frac{1}{M}\sum_i e^{f(x)\T f(y^-_i) / \tau  }\right)\right|}\\
& \leq \frac{\lambda}{M} e^{2 / \tau} + e^{1/\tau} \expectunder[\substack{
        (x, y) \sim \distnpos \\
        \{y^-_i\}_{i=1}^M \iidsim p_{\mathsf y}
    }]{\left| \expectunder[\substack{
        y^- \iidsim p_{\mathsf y}
    }]{e^{f(x)\T f(y^-) / \tau}} - \frac{1}{M}\sum_i e^{f(x)\T f(y^-_i) / \tau  } \right|}\\
& \leq \frac{\lambda}{M} e^{2 / \tau} + \frac{5}{4}M^{-\frac{2}{3}}e^{\frac{1}{\tau}}\left(e^{\frac{1}{\tau}} - e^{-\frac{1}{\tau}}\right),
\end{aligned}
}

\noindent where the first inequality follows an application of Lagrange's mean-value theorem, and the last inequality follows the bound from Chebychevâ€™s inequality, which can refer to ~\cite{wang2020understanding}.
\\
\\
Therefore, The noisy ASM still converges to the same limit of ASM with absolute deviation decaying in $\mathcal{O}(M^{-2/3})$, combing the derivations of both sides above.\hfill$\square$
\end{pf}


\subsection{ Proof to Theorem 3} \label{sec:proof3}

Let $\Omega_{\mathsf x}, \Omega_{\mathsf y}$ be the space of knowledge graph triplets, $n\in\mathbb{N}$. Let ${\{x^-_i:\Omega_{\mathsf x}\to\mathbb{R}^n\}}_{i=1}^M$, ${\{y^-_i:\Omega_{\mathsf y}\to\mathbb{R}^n\}}_{i=1}^M$ be i.i.d random variables with distribution $p_{\mathsf x}, p_{\mathsf y}$. $\mathcal{S}^{d-1}$ denotes the uni-sphere in $\mathbb{R}^n$. If there exists a random variable  $f:\mathbb{R}^n\to\mathcal{S}^{d-1}$ s.t. $f(x_i^-),f(y_i^-)$ satisfy the same distribution on $\mathcal{S}^{d-1}, 1\le i\le M.$, then we have 


\beqn{\scriptsize
\lim_{M \rightarrow \infty}\left|\mathcal{L}_{{\rm RSM|}\lambda,\mathsf{x}}(f;\tau,M,p_{\mathsf x}) - \mathcal{L}_{{\rm RSM|}\lambda,\mathsf{x}}(f;\tau,M,p_{\mathsf y})\right| = 0.
}

\begin{pf}

\begin{equation}{\scriptsize
\begin{aligned}
&\left|\mathcal{L}_{{\rm RSM|}\lambda,\mathsf{x}}(f;\tau,M,p_{\mathsf x}) - \mathcal{L}_{{\rm RSM|}\lambda,\mathsf{x}}(f;\tau,M,p_{\mathsf y})\right|\\
&=\left| \expectunder[\substack{
        \{x^-_i\}_{i=1}^M \iidsim p_{\mathsf x}
    }]{- \log \left(\frac{e^{\frac{1}{\tau}}}{\lambda e^{\frac{1}{\tau}} + \sum_i e^{f(x)\T f(x^-_i) / \tau}}\right)} \right. \\ & \quad \left. - \expectunder[\substack{
        \{y^-_i\}_{i=1}^M \iidsim p_{\mathsf y}
    }]{- \log \left(\frac{e^{\frac{1}{\tau}}}{\lambda e^{\frac{1}{\tau}} + \sum_i e^{f(x)\T f(y^-_i) / \tau}}\right)} \vphantom{\int_1^2} \right|\\
&= \left| \expectunder[\substack{
        \{x^-_i\}_{i=1}^M \iidsim p_{\mathsf x}\\
        \{y^-_i\}_{i=1}^M \iidsim p_{\mathsf y}
    }]{\log \left (\frac{\lambda e^{\frac{1}{\tau}} + \sum_i e^{f(x)\T f(x^-_i) / \tau}}{\lambda e^{\frac{1}{\tau}} + \sum_i e^{f(x)\T f(y^-_i) / \tau}}\right)}\right|\\
& \leq  \expectunder[\substack{
        \{x^-_i\}_{i=1}^M \iidsim p_{\mathsf x}\\
        \{y^-_i\}_{i=1}^M \iidsim p_{\mathsf y}
    }]{\left | \log \left(\frac{\lambda e^{\frac{1}{\tau}} + \sum_i e^{f(x)\T f(x^-_i) / \tau}}{\lambda e^{\frac{1}{\tau}} + \sum_i e^{f(x)\T f(y^-_i) / \tau}}\right) \right |} \\
&= \expectunder[\substack{
        \{x^-_i\}_{i=1}^M \iidsim p_{\mathsf x}\\
        \{y^-_i\}_{i=1}^M \iidsim p_{\mathsf y}
    }]{\left | \log \left( 1 + \frac{ \sum_i e^{f(x)\T f(x^-_i) / \tau} - \sum_i e^{f(x)\T f(y^-_i) / \tau}}{\lambda e^{\frac{1}{\tau}} + \sum_i e^{f(x)\T f(y^-_i) / \tau}}\right) \right |}.
\end{aligned}
}\end{equation}


Let $S = \frac{\lambda e^{\frac{1}{\tau}} + \sum_i e^{f(x)\T f(x^-_i) / \tau}}{\lambda e^{\frac{1}{\tau}} + \sum_i e^{f(x)\T f(y^-_i) / \tau}}$, then


\beqn{\scriptsize
S \geq \frac{\lambda e^{\frac{1}{\tau}} + Me^{-\frac{1}{\tau}}}{\lambda e^{\frac{1}{\tau}} + Me^{\frac{1}{\tau}}} \geq e^{-\frac{2}{\tau}}.
}

Let $T = \frac{ \sum_i e^{f(x)\T f(x^-_i) / \tau} - \sum_i e^{f(x)\T f(y^-_i) / \tau}}{\lambda e^{\frac{1}{\tau}} + \sum_i e^{f(x)\T f(y^-_i) / \tau}}$, therefore


\beqn{\scriptsize
\left| T \right | = \frac{ \frac{1}{M} \left|\sum_i e^{f(x)\T f(x^-_i) / \tau} - \sum_i e^{f(x)\T f(y^-_i) / \tau}\right|}{\frac{1}{M} \left(\lambda e^{\frac{1}{\tau}} + \sum_i e^{f(x)\T f(y^-_i) / \tau}\right)}\leq \frac{e^{\frac{1}{\tau}}-e^{-{\frac{1}{\tau}}}}{\frac{\lambda}{M}e^{\frac{1}{\tau}}+e^{-\frac{1}{\tau}}}.
}

Then $S = 1+T$, therefore 

\beqn{\scriptsize
S \leq 1+ \frac{e^{\frac{1}{\tau}}-e^{-{\frac{1}{\tau}}}}{\frac{\lambda}{M}e^{\frac{1}{\tau}}+e^{-\frac{1}{\tau}}}  <  1+ \frac{e^{\frac{1}{\tau}}-e^{-{\frac{1}{\tau}}}}{e^{-\frac{1}{\tau}}} = 1+e^{\frac{2}{\tau}} - 1 = e^{\frac{2}{\tau}}.
}


Therefore, $\left|\log S\right|<\frac{2}{\tau}$.

By the S.L.L.N., $\lim_{M \rightarrow \infty} T = 0$, therefore, $\lim_{M \rightarrow \infty} \log S = 0$.

Because $\left | \log S \right |$ is bounded, with Dominated Covergence Theorem, the sign of mathematical expectation (i.e. integral) can be exchanged with the sign of limit:


\beqn{\scriptsize
\begin{aligned}
& \lim_{M \rightarrow \infty}\left|\mathcal{L}_{{\rm RSM|}\lambda,\mathsf{x}}(f;\tau,M,p_{\mathsf x}) - \mathcal{L}_{{\rm RSM|}\lambda,\mathsf{x}}(f;\tau,M,p_{\mathsf y})\right| \\ 
&\leq \lim_{M \rightarrow \infty}{\expectunder(\substack{
        \{x^-_i\}_{i=1}^M \iidsim p_{\mathsf x}\\
        \{y^-_i\}_{i=1}^M \iidsim p_{\mathsf y}
    }){\left | \log S \right|}} \\ 
    &= \expectunder[\substack{
        \{x^-_i\}_{i=1}^M \iidsim p_{\mathsf x}\\
        \{y^-_i\}_{i=1}^M \iidsim p_{\mathsf y}
    }]{\lim_{M \rightarrow \infty}{\left | \log S \right|}}\\& = 0.
\end{aligned}
}
\hfill $\square$
\end{pf}


\hide{Wang et al.\cite{wang2020understanding} suggest that under the condition of $p_\mathsf{x}=p_\mathsf{y}$, the encoder $f$ can be attained approximately as the minimizer of the uniform loss.  Specifically, $f$ follows the uniform distribution on the hypersphere. In our framework, the uni-space learning condition ensures us to obtain unified
representation for both KGs; in other words, entity embeddings of KG$_\mathrm{x}$ and KG$_\mathrm{y}$ could be viewed as samples from one single distribution in a larger space, i.e., $p_\mathsf{x}=p_\mathsf{y}$. This in turn allows the existence of $f$ to be more realizable.
To further this thinking, one may obtain a larger number $M$ of negative samples by randomly generating word vectors and neighbors. }

\vspace{-5mm}
\subsection{Details on Implementation} \label{sec:exp_details}

\subsubsection{Dataset}

For both DWY100K\footnote{Can be downloaded from https://github.com/nju-websoft/BootEA} and DBP15K\footnote{Can be downloaded from https://github.com/syxu828/Crosslingula-KG-Matching} datasets we used, we do simple data processing on the original datasets built in BootEA ~\cite{sun2018bootstrapping} and JAPE ~\cite{JAPE} respectively. The process of data processing is as follows:

Firstly, we remove the redundant prefixes of the URLs representing the entities, leaving the meaningful entity names at the end. For example, in DBP15K$_{\text{zh\_en}}$ dataset, there is an entity represented by "http://dbpedia.org/resource/2012\_Summer\_Olympics". We remove the substring in front of "2012\_Summer\_Olympics" to remove the useless part. Then we replace the underscores used to connect words in the entity names with spaces, so that the entities can be represented by their original entity names. In addition, we replace the indices that represent entities in DWY100K$_{\text{dbp\_wd}}$ (e.g., Q123) with strings of entity names. The purpose of this step is to make the entity names as original as possible to let our model better extract the character-level information and the semantic-level information with useful data. Then, we need to map every entity to a unique index in every pair of KGs respectively. The pairs of KGs are the subdatasets of DWY100K and DBP15K: DWY100K$_{\text{dbp\_wd}}$, DWY100K$_{\text{dbp\_yg}}$, DBP15K$_{\text{zh\_en}}$, DBP15K$_{\text{ja\_en}}$ and DBP15K$_{\text{fr\_en}}$. We use DBP15K dataset provided in~\cite{xu2019cross-lingual} and the DWY100K dataset provided in \cite{JAPE} as our original dataset and follow the indices they created in our experiments since they have already done this processing step.

As for obtaining 1-hop neighbors, we treat the KGs as undirected graphs, that means we use the relational triples in the datasets to find all the entities connected to an entity regardless of the direction of the connection.

Finally, we reconstruct Dataset and use DataLoader of Pytorch's torch.utils.data package to packet our data and create batches. Because we do not use any labels in our model for training, we set the indices of the entities as the y data which is usually used to contain the labels in Dataset package. As for the x data which is the training data in Dataset package, we set the entity names of the center entities and the corresponding neighbors with the adjacency matrix of the center entities as the x data.


\subsubsection{Implementation Notes}

Our model is implemented using Python package Pytorch 1.7.1.\footnote{More details can be found in our code \url{https://github.com/THUDM/SelfKG}}
The experiments were conducted on a GNU/Linux server with 8 Tesla V100 SXM2 GPU and 32G GPU RAM mainly, and also 56 Intel(R) Xeon(R) Gold 5120 CPU(2.20GHz), 500G RAM.

% We employ Adam as our optimizer with a relatively small learning rate 1e-6 and gradually reduce our learning rate to maintain the stability of training (learning rate $*=$ 0.5 every 10 epochs).


% To extract the semantic level information, we use LaBSE instead to obtain the entity embeddings as input of following layers:

% \begin{enumerate}
% \item neighbor aggreator layer: input size = LaBSE\_DIM, output size = LaBSE\_DIM, n\_head = 1, attn dropout = 0.3
% \item Fully-connected layer: input size = 2 $*$ LaBSE\_DIM, output size = LaBSE\_DIM
% \end{enumerate}
% where LaBSE\_DIM=768.


% We use neighbor embeddings to obtain the entity embeddings as the input of neighbor aggreator layer, and then concatenate the output with the embedding of center entity as the input of layer 2,  conducting normalization both on layer 1 and 2.

For both experiments on DWY100K and DBP15K, we randomly select 5\% links from the training set in the original datasets as our validation set and evaluate our model's performance both on the validation set and the testing set. We stop the training progress once our model reaches the best performance on the validation set and record Hit@1 and Hit@10 results on the testing set.





\subsubsection{Similarity Search}

In order to evaluate our model on the validation set and the test set efficiently, we apply Faiss\footnote{\url{https://github.com/facebookresearch/faiss}}, a library for efficient similarity search.

In the evaluation period, we apply the IndexFlatL2 as indexer, which is based on $\ell_2$ distance. Once the indices are built, via the kd-tree algorithm used in Faiss, the top 1 and top 10 closest entities in the target KG of every entity in the source KG can be found efficiently.

\subsection{Runtime}

\hhy{On the time efficiency of using large number negative samples in \solution, by leveraging multiple negative queues with Moco~\cite{he2020momentum}, the running time of \solution is significantly reduced even when the sample size is large, making it similar to the common negative sampling method adopted in state-of-the-art baseline methods. Details are discussed in Section~\ref{sec:mnq}.}

\subsection{Limitations}

\hhy{There are mainly two limitations in \solution. Firstly, \solution requires good embeddings to ensure the unified representation for both KGs. As we clarified in Section \ref{sec:ablation}, we confirm that a better pre-trained language model like LaBSE will boost the performance of \solution. This issue is also commonly faced by other embedding-based entity alignment methods. Secondly, \solution still underperforms some supervised state-of-the-art methods. Some of the supervised methods such as BERT-INT\cite{tang2019bert-int} can reach almost an accuracy of 100\% on both DBP15K and DWY100K, which outperforms our self-supervised solution. The gap is expected since supervision does provide much useful information for the alignment task. The ultimate goal of self-supervised methods is to match or even beat supervised methods.}
