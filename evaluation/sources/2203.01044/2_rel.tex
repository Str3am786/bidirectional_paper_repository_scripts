\section{RELATED WORK}
\vpara{Entity alignment.}
Entity alignment, also named entity resolution, ontology alignment, or schema matching, is a fundamental problem in the knowledge graph community~\cite{zeng2021comprehensive} that has been researched for decades. Before the deep learning era, most approaches focus on designing proper similarity factors and Bayesian-based probability estimation. ~\cite{tang2006using} develops the idea of transforming the alignment into minimizing the risk of decision making. RiMOM~\cite{li2008rimom} proposes a multi-strategy ontology alignment framework, which leverages primary similarity factors with the Cartesian product to align concepts unsupervisedly. ~\cite{li2014rule} argues for rule-based linking and design a rule discovery algorithm. ~\cite{zhang2015cosnet} develops an efficient multi-network linking algorithm based on the factor graph model.

Recently, embedding-based methods have drawn people's attention due to their flexibility and effectiveness. TransE~\cite{bordes2013translating} is the very beginning to introduce the embedding method to represent relational data. ~\cite{MTransE} develops the knowledge graph alignment strategy based on TransE. ~\cite{JAPE} argues for a cross-lingual entity alignment task and constructs the dataset from DBpedia. ~\cite{zhang2018mego2vec} proposes to embed entity ego-network to vectors for the alignment. ~\cite{GCN-Align} introduces the GCN to model both the entity and relation in knowledge graphs to perform the alignment. ~\cite{trisedya2019entity} argues that we can use attributes and structure to supervise each other mutually. BERT-INT~\cite{tang2019bert-int} proposes an interactive entity alignment strategy based on BERT and substantially improves the supervised entity alignment performance on public benchmarks. ~\cite{zhang2019oag} designs heterogeneous graph attention networks to perform large-scale entity linking across the open academic graph.

However, most embedding-based methods nowadays rely heavily on supervised data, hindering their application in real web-scale noisy data. As a prior effort, in~\cite{liu2021oag_know} authors present self-supervised pre-training for concept linking but with downstream supervised classification. In this work, we endeavor to investigate the potential of a completely self-supervised approach without using labels to reduce the cost of entity alignment while improving performance.

\vpara{Self-supervised learning.}
Self-supervised learning~\cite{liu2020self}, which learns the co-occurrence relationships in the data without human supervision, is a data-efficient and powerful machine learning paradigm. We can divide them into two categories: generative and contrastive.

Generative self-supervised learning is often related to pre-training. For instance, BERT~\cite{devlin2018bert}, GPT~\cite{radford2019language}, XLNet~\cite{yang2019xlnet} and so on~\cite{raffel2020exploring,du2021all} develop the field of language model pre-training, which boost the development of natural language processing. The contrastive self-supervised learning is recently proposed by MoCo and SimCLR~\cite{he2020momentum,chen2020simple} in computer vision to conduct successful vision pre-training. The core idea of leveraging the instance discrimination and contrastive loss has been proved to be especially useful for downstream
classification tasks.
Self-supervised learning has also been applied to graph pre-training tasks, such as in GCC~\cite{qiu2020gcc}, the authors pre-train the structural representation of subgraphs using contrastive learning and transfer the model to other graphs. ~\cite{you2020graph} proposes adding augmentations to sampled graphs following SimCLR's strategy to promote graph pre-training performance.




\hide{
\vpara{(Semi-)Supervised Entity Alignment.} 
Before the deep learning era, most alignment approaches focus on handcrafted proper similarity factors and Bayesian-based probability estimation~\cite{tang2006using,li2008rimom,zhang2015cosnet}. 
Deep embedding-based entity alignment methods are superior due to their flexibility and effectiveness but are mostly based on supervised or semi-supervised learning.~\cite{MTransE} develops the knowledge graph alignment strategy based on TransE~\cite{bordes2013translating}.~\cite{JAPE} argues for a cross-lingual entity alignment. 
MEgo2Vec~\cite{zhang2018mego2vec} proposes to embed entity ego-network to vectors for the alignment.~\cite{GCN-Align} introduces the GCN to model both the entity and relation in entity alignment.~\cite{trisedya2019entity} describes the mutual supervision between attributes and structure.
BERT-INT~\cite{tang2019bert-int} suggests an interactive entity alignment strategy based on BERT.~\cite{zhang2019oag} performs the large-scale heterogeneous entity linking across the open academic graph. 
Despite their success, supervised methods' dependence on labels hinders their application in real Web-scale noisy data. 

\vpara{Self-supervised Learning.}
As an alternative, self-supervised learning~\cite{liu2020self}, which learns the data co-occurrence relationships without human supervision, is a label-efficient machine learning paradigm. 
The recent boost of contrastive self-supervised learning is advocated by MoCo and SimCLR~\cite{he2020momentum,chen2020simple} in computer vision to conduct successful vision pre-training. 
The core idea of leveraging the instance discrimination and contrastive loss has been proved to be especially useful for downstream classification tasks.
Self-supervised learning has also been applied to graph learning, such as GCC~\cite{qiu2020gcc} and~\cite{you2020graph}. 
}