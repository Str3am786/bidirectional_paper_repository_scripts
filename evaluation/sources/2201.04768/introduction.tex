% Brief motivation as to why we need this paper
Representative \emph{sampling} of collaborative filtering (CF) data is a crucial problem from numerous stand-points and can be performed at various levels: (1) mining hard-negatives while training complex algorithms over massive datasets \cite{eclare, sampling_cf_nn}; (2) down-sampling the item-space to estimate expensive ranking metrics \cite{sampled_metrics, castells_sampling}; and (3) 
% sub-sampling the entire dataset for 
reasons like easy-sharing, fast-experimentation, mitigating the significant environmental footprint of training resource-hungry machine learning models \cite{google_emissions, wu2021sustainable, facebook_emissions, green_ai}. In this paper, we are interested in finding a sub-sample of a dataset which has minimal effects on model utility evaluation \ie an algorithm performing well on the sub-sample should also perform well on the original dataset.

Preserving \emph{exactly} the same levels of performance on sub-sampled data over metrics, such as MSE and AUC, is a challenging problem. A simpler yet useful problem is accurately preserving the \emph{ranking} or relative performance of different algorithms on sub-sampled data. For example, a sampling scheme that has low bias but high variance in preserving metric performance values has less utility than a different sampling scheme with high amounts of bias but low variance, since the overall algorithm ranking is still preserved. 

% What wrong/good people have been doing
Performing 
% careless and 
ad-hoc sampling such as randomly removing interactions, or making dense subsets by removing users \emph{or} items with few interactions \cite{sigir20} can have adverse downstream repercussions. For example, sampling only the head-portion of a dataset---from a fairness and inclusion perspective---is inherently biased against minority-groups and benchmarking algorithms on this biased data is 
% highly 
likely to propagate the 
% original sampling biases. 
original sampling bias.
% On the other hand, 
Alternatively,
simply from 
% the model quality performance view-point, 
a model performance view-point,
accurately retaining the relative performance of different recommendation algorithms on much smaller sub-samples is a challenging research problem in itself.

% What do we actually do in this paper?
Two prominent directions toward better and more representative sampling of CF data are: (1) designing principled sampling strategies, especially for user-item interaction data; and (2) analyzing the performance of different sampling strategies, in order to better grasp which sampling scheme performs ``better'' for which types of data. \emph{In this paper,} we explore both directions through the lens of expediting the recommendation algorithm development cycle by:
\begin{itemize}
    \item Characterizing the efficacy of \emph{sixteen} different sampling strategies in accurately benchmarking various kinds of recommendation algorithms on smaller sub-samples.

    \item Proposing a sampling strategy, \sampler, which dynamically samples the ``toughest'' portion of a CF dataset. \sampler is tailor-designed to handle the inherent data heterogeneity and missing-not-at-random properties in user-item interaction data.
    
    \item Developing \oracle, which analyzes the \emph{performance} of different sampling strategies. Given a dataset sub-sample, \oracle can directly estimate the likelihood of model performance being preserved on that sample. 
\end{itemize}
%
% Give a taste of the findings.
Ultimately, our experiments reveal that \sampler outperforms all other sampling strategies and can accurately benchmark recommendation algorithms with roughly $50-60\%$ of the original dataset size. Furthermore, by employing \oracle to dynamically select the best sampling scheme for a dataset, we are able to preserve model performance with only $10\%$ of the initial data, leading to a net $5.8\times$ training time speedup.
