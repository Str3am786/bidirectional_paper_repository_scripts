% !TEX root = paper.tex
% !TeX spellcheck = en_US

\section{Related Work}
\label{sec:related}

In this section, we introduce Learning to Rank (LtR) and its use in Information Retrieval (IR). Then, we describe QuickScorer~\cite{lucchese2015quickscorer,dato2016fast,8035185} an efficient algorithm for scoring ensemble of regression trees. Finally, we discuss the field of model compression, a branch of machine learning that aims to compress Deep Neural Networks without affecting their accuracy. Here, we focus our attention in particular on pruning techniques.

% subsection subsection_name (end)}
\subsection{Learning to Rank}
\label{subsec:ltr}
Learning to Rank (LtR) consists in applying machine learning techniques to the problem of ranking documents with respect to a query.
%such as BM25~\cite{robertson2009probabilistic} and Query Likelihood. 
RankNet~\cite{burges2005learning} leverages a  probabilistic ranking framework based on a pairwise approach to train a neural network. The difference between the predicted scores of two different documents is mapped to a probability by means of the sigmoid function. Hence, using the cross-entropy loss this probability is compared with the ground truth labels, and Stochastic Gradient Descent (SGD) is used to minimize this loss. FRank~\cite{tsai2007frank} exploits a generative additive model and substitutes the cross-entropy loss with the fidelity loss, a distance metric adopted in physics, superior to cross-entropy when applied on top of the aforementioned probabilistic framework since 1) has minimum in zero, 2) is bounded in $[0,1]$. 
%proposes a probabilistic cost function framing the ranking problem into a \emph{pairwise} approach. 
Neither RankNet nor FRank directly optimize a ranking metric (\emph{e.g.}, NDCG), and this discrepancy weakens the power of the model. Since ranking metrics are flat and discontinuous, coding them into the loss function is troublesome.
To overcome this issue, LambdaRank~\cite{burges2007learning} heuristically corrects the RankNet gradients, exploiting the rank position of the document in the overall sorting: it multiplies the RankNet gradient with a term that measures the increase in terms of NDCG when switching the terms, generating the so-called $\lambda$-gradients.
%	Multiple Additive Regression Trees (MART) have shown remarkable results for the ranking problem. 
McRank~\cite{li2008mcrank} casts the problem of ranking as MultiClass classification task, using a boosting tree algorithm to learn the class probabilities and then converting them into relevances with the expected relevance, outperforming LambdaRank.  This work also highlights that modeling the ranking problem as a classification task works better than modeling it as a regression one.  
LamdaMART~\cite{burges2010ranknet} combines the successful training methodology provided by $\lambda$-gradients with Multiple Additive Regression Trees (MART) - as McRank~\cite{li2008mcrank}, and it has been establishing as the state-of-the-art in LtR. Currently, ensembles of regression trees are the most effective solution among LtR techniques when dealing with handcrafted features. In the next section, we describe state-of-the-art approaches for efficient traversal of these trees, in order to employ them in latency-bound scenarios.


\subsection{Efficient Traversal of Tree-based Models}
\label{subsec:quickscorer}
QuicksScorer~\cite{lucchese2015quickscorer} is a state-of-the-art algorithm that allows to speedup the traversal of an ensemble of regression trees. As detailed in the previous section, ensemble of regression trees is the model exploited by several state-of-the-art learning-to-rank solutions, e.g., LambdaMART~\cite{burges2010ranknet}.
QuickScorer codes each tree of the ensemble as a bitvector of length $n$, where $n$ is the number of leaves, which is used to select the \textit{exit leaf} in the tree. Furthermore, each decision node in each tree is associated with a bitvector of the same length called \textit{mask}. If the corresponding test is evaluated to false, the bits corresponding to the unreachable leaves are set to zero. By performing the logical \texttt{AND} among all the masks, we obtain another bitvector, named \emph{leafidx}, in which the first one entry corresponds to the exit leaf. To efficiently compute the exit leaf, QuickScorer process all the nodes in a \textit{feature by feature} fashion. For each feature $f$, the associated thresholds among all the nodes in the forest are sorted in ascending order. Let us a consider a threshold $\gamma$ associated with a node $g$: when $x_f > \gamma$, 
the corresponding \emph{leafidx} is updated performing the \texttt{AND} operation with the \emph{mask} relative to $g$. Since the thresholds are sorted, as soon as $x_f \leq \gamma$, the evaluation of the current feature is interrupted, since the following instances will evaluate true as well. To further improve the efficiency of the algorithm, two variations of the original algorithm are introduced:  1) Block-Wise QuickScorer (BWQS), in which the forest is partitioned into blocks of trees fitting the L3 cache, reducing the cache-miss ration and 2) Vectorized QuickScorer (vQS)~\cite{lucchese2016exploiting}, in which scoring is vectorized using AVX2 instructions and 256-bit registers, allowing to process up to $8$ document at time. 
Lettich \emph{et al.}~\cite{8035185} propose a GPU version of QuickScorer, to exploit the massive parallelism of this computational engine. By properly managing the GPU memory hierarchy and furnishing an adequate degree of parallelism in the document scoring process, this version results up to 100x faster than the corresponding CPU version, when dealing with very large forests ($20$,$000$ trees).
 
The cost of traversing an ensemble of regression trees with QuickScorer depends on the number of false nodes, rather than on the length of the root-to-leaf paths. Since machine-learnt trees are imbalanced, the authors experimentally show that this reduces the percentage of nodes to evaluate from 80\% of classical traversal to the 30\% of QuickScorer~\cite{lucchese2015quickscorer}. Moreover, QuickScorer is implemented carefully taking into account cache and CPU issues. For example, QuickScorer structures are accessed sequentially thus favoring pre-fetching and avoiding branch mispredictions. However, when the number of leaves is larger than $64$, scoring a model with QuickScorer can be inefficient. Recently, RapidScorer tackles the problem of forest with a larger number of leaves~\cite{ye2018rapidscorer}. In fact, when $|\text{leaves}| > 64$, the logical \texttt{AND} between the bitvectors cannot be carried out in just one CPU instruction, hampering efficiency. For this reason, RapidScorer introduces a tree-size insensitive encoding, named \emph{epitome}. Moreover, it leverages a node merging strategy that evaluates just once nodes sharing the same threshold on the same feature. By doing so, RapidScorer outperforms  QuickScorer when dealing with a large number of leaves.

\subsection{Model Compression}
\label{subsec:modelcompr}
The effectiveness of Deep Neural Networks (DNNs) comes at the cost of a high computational complexity~\cite{nnstats}, hindering the deployment and the usage of DNNs, especially for resource-constrained devices. An inherent feature of DNNs is \textit{over-parameterization}, \textit{i.e.,} the redundancy of networks parameters: it has been proven that the same performance can be obtained with just a portion of the original parameters~\cite{denil2013predicting}. Model Compression (MC) is a recent research field investigating effective techniques for reducing the memory impact of DNNs, their inference time, and energy consumption without affecting their accuracy, exploiting over-parameterization. 
%TODO mancherebbe low rank decomposition
In MC techniques, we observe the presence of several lines of research: pruning~\cite{DBLP:journals/corr/HanPTD15,DBLP:journals/corr/LiKDSG16, molchanov2019pruning,DBLP:journals/corr/HanMD15,DBLP:journals/corr/GuoYC16,yu2017scalpel,vieira2017learning,he2017channel,he2018amc}, quantization~\cite{DBLP:journals/corr/LiL16,DBLP:journals/corr/ZhuHMD16, DBLP:journals/corr/RastegariORF16,hubara2017quantized,Cai_2017_CVPR,DBLP:journals/corr/ZhouYGXC17} design of efficient architectures~\cite{DBLP:journals/corr/IandolaMAHDK16,zhang2018shufflenet,howard2017mobilenets,sandler2018mobilenetv2}, knowledge distillation~\cite{bucilua2006model,ba2014deep,DBLP:journals/corr/HintonVD15}. 

%($17.7x$, AlexNet on ImageNet, ).
%Network quantization techniques reduce the number of bits necessary to represent each weight thus generating lighter models providing faster inference and lower energy consumption~\cite{sze2017efficient}. Here, data-driven methods introduce a limited loss of accuracy even when using $1$ or $2$ bits~\cite{DBLP:journals/corr/LiL16, DBLP:journals/corr/ZhuHMD16,DBLP:journals/corr/RastegariORF16,hubara2017quantized,Cai_2017_CVPR}. Moreover, the incremental learning of the optimal quantization outperforms the original model performance~\cite{DBLP:journals/corr/ZhouYGXC17}.
%An orthogonal approach is the design of new and efficient architectures by modifying existing layers~\cite{DBLP:journals/corr/IandolaMAHDK16} or creating new ones~\cite{DBLP:journals/corr/ZhangZLS17, howard2017mobilenets,sandler2018mobilenetv2,DBLP:journals/corr/abs-1807-11164}. Automatizing the design and compression of neural architectures by mean of reinforcement learning and evolution techniques have proven to be effective~\cite{tan2019efficientnet,cai2018proxylessnas,tan2019mnasnet}
%Knowledge distillation techniques~\cite{DBLP:journals/corr/HintonVD15} exploit a \textit{teacher} network at training time to improve the learning process of a shallower and faster \textit{student} network. Finally, the combination of different compression techniques has proven to be an effective solution~\cite{DBLP:journals/corr/HanMD15,polino2018model}
%Pruning techniques belong to the family of Model Compression (MC) methods. MC is a recent research field investigating effective techniques for reducing the memory impact of DNNs, their inference time, and energy consumption without affecting their accuracy. Lossless compression is allowed by the significant redundancy of parameters, i.e., over-parametrization, that has been discovered in neural networks~\cite{DBLP:journals/corr/DenilSDRF13}. 
Recently, pruning has shown to be extremely effective~\cite{DBLP:journals/corr/HanPTD15,DBLP:journals/corr/LiKDSG16,DBLP:journals/corr/HanMD15,DBLP:journals/corr/GuoYC16,luo2017thinet,huang2018data,yu2017scalpel,vieira2017learning,he2017channel,he2018amc}. Pruning techniques delete useless connections in a pre-trained model, producing sparse weight tensors that are lighter to store and allow for faster inference time. Performing a retraining after pruning avoids accuracy loss, even in the case of high compression factors~\cite{DBLP:journals/corr/GuoYC16}. 
The canonical classification of pruning techniques divide them into two families: 1) element-wise pruning, which sets to zero individual weights, generating sparse weight tensors and 2) structured pruning, which prunes entire groups of weights, \textit{i.e.,} columns, filters, or even entire layers. In the latter case, the resulting network's weights still belong to the dense domain. 
%Structured pruning for feed-forward network is applied columns-wise. Once estimated the importance of each column by mean of an heuristic (\textit{e.g.,} $L_1$-norm),  a percentage of less important columns is removed from the model. Given an original model architecture with $l$ layers $A_l = \{l_1, \dots , l_{l}\}$, we obtain a reshaped dense model $A_l^p =  \{l_1^p, \dots, l_l^p \}$. 
%We experimentally verified that it makes no difference whether the $A_l^p$ model is trained from scratch or obtained from a pruning-finetuning procedure on a pre-trained model, as happens in models for Image Classification tasks~\cite{liu2018rethinking}.   
%MC techniques can be grouped in four main lines of research: \textit{pruning}~\cite{DBLP:journals/corr/HanPTD15,DBLP:journals/corr/LiKDSG16, DBLP:journals/corr/MolchanovTKAK16, ullrich2017soft,DBLP:journals/corr/HanMD15,DBLP:journals/corr/GuoYC16,DBLP:journals/corr/YangCS16a,DBLP:journals/corr/LuoWL17,huang2018data,wen2016learning,yu2017scalpel,vieira2017learning,he2017channel,he2018amc,prakash2018repr}, \textit{quantization}~\cite{DBLP:journals/corr/LiL16,DBLP:journals/corr/ZhuHMD16, DBLP:journals/corr/RastegariORF16,hubara2017quantized,DBLP:journals/corr/ZhouYGXC17} \textit{design of efficient architectures}~\cite{DBLP:journals/corr/IandolaMAHDK16,DBLP:journals/corr/ZhangZLS17,howard2017mobilenets,sandler2018mobilenetv2}, \textit{knowledge distillation}~\cite{DBLP:journals/corr/HintonVD15,romero2014fitnets,chen2015net2net}. 
In this paper, we focus on element wise-pruning techniques. These methods employ heuristics to determine what are the relevant weights of the network. In particular, \textit{magnitude-based} heuristics work by removing low absolute-value weights and are proved to be effective~\cite{DBLP:journals/corr/HanPTD15,DBLP:journals/corr/GuoYC16}. In their na\"ive version, magnitude based approaches remove a fixed percentage of weights from the original model (\emph{level pruning}). Han \emph{et al.} show that the gradual increase of the target sparsity, interleaved with a number of steps of re-training, can improve the accuracy of the final model~\cite{DBLP:journals/corr/HanPTD15}. Furthermore, they propose a layer-wise threshold-based method to determine whether a parameter shall be kept or not. For each layer, its threshold $t_i$ is computed as as $t_i = \sigma_i * s_i$, with $\sigma_i$ the standard deviation of weights distribution and $s_i$ a sensitivity parameter to be chosen. By assuming that parameters follow a Normal distribution $ \mathcal{N}(0, \sigma^2) $, setting $s_i = 1$ would approximately prune away about the 68\% of the weights. The pruning step is followed by a number of re-training epochs on the surviving weights. The procedure can be then iterated by gradually increasing $s_i$ thus inducing higher sparsity. The Distiller Framework~\cite{nzmora2019distiller} version that we adopt, keeps this threshold fixed, relying on the fact that as the tensor is pruned, more elements are pulled towards the center of the distribution and then pruned. Pruning techniques have shown to be able to sparsify state-of-the-art neural architectures up to 90\%, thus strongly reducing their memory burden and easing the transmission and deployment on resource-constrained devices.



\section{Training by Scores Approximation}
\label{sec:cohen}
In this section, we detail the methodology proposed by Cohen \textit{et al.}~\cite{cohen2018universal} to train neural models approximating ensembles of regression trees.
Their technique can be considered as a special case of Knowledge Distillation~\cite{ba2014deep,DBLP:journals/corr/HintonVD15}.
 %Observe that learning the outputs of another model instead that directly the ground truth is locate their work close in the knowledge distillation area~\cite{ba2014deep,DBLP:journals/corr/HintonVD15}.
Knowledge distillation is a training technique in which a small \emph{student} model is trained to mimic the outputs of a large and expressive \emph{teacher} model.
In the case of Cohen \emph{et al.}, the ensemble of regression trees plays the role of the teacher, while the neural network is the student model.
%We now describe the approach by Cohen \textit{et al.}~\cite{cohen2018universal} in detail.
The core idea of their approach is to treat the tree-based model as a black box producing accurate scores. Formally, let us consider a Learning to Rank dataset $D = (X, Y)$,  $X \in \mathbb{R}^{f \times |D|}$, where $f$ is the number of extracted features per document, $|D|$ is the cardinality of the dataset, and $Y \in \mathbb{N}^{|D|}$ is the set of ground-truth relevances of a document w.r.t. a query.
Let $F: \mathbb{R}^{f} \rightarrow \mathbb{R}$ be the underlying function learned by an ensemble of regression trees during the training that maps a single document $x \in X$ into a relevance score.
If the neural model can reproduce the function $F$, it achieves the same ranking quality as the original model. The effectiveness of this approach relies on theoretical results showing that NNs can approximate continuous~\cite{hornik1991approximation} and piecewise continuous functions~\cite{llanas2008constructive}. In practice, the approximation is implemented by using the \emph{Mean Squared Error} as loss function computed between the network prediction and the ensemble prediction. Furthermore, the training procedure is enriched with a data augmentation step which enforces the approximation capabilities of the neural network. Consider the set of $f$ features in the dataset. For each feature, Cohen \emph{et al.}~\cite{cohen2018universal} build a list composed of 
the split points corresponding to that feature in the ensemble of regression trees, and, in the same list, they also put the maximum and the minimum for that feature in the training set.
This way they obtain a set of $f$ lists, where $f$ is the number of the features in the dataset. Each of these lists is then sorted, and replaced with its ordered midpoints, \emph{e.g.}, each adjacent pair $\{x_i, x_{i+1}\}$ is replaced with its midpoint, $\frac{x_i + x_{i+1}}{2}$.  At each training step, half of the training data is built by randomly sampling from this feature-wise set of lists to have a better coverage of the whole feature space. Before feeding them to the network, all the training data are normalized by subtracting the mean and by dividing by the variance ($Z$-normalization).
This approach is more proficient than directly learning the ground-truth relevance~\cite{cohen2018universal}. As detailed in Section \ref{sec:introduction}, the approximation error introduced is small but statistically significant in terms of ranking quality. In Section~\ref{sec:neuraleng}, we show how to mitigate this effect.
