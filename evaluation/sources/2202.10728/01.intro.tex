% !TEX root = paper.tex
% !TeX spellcheck = en_US

\section{Introduction}
\label{sec:introduction}

\IEEEPARstart{T}{he} estimation of relevance is a task of paramount importance in Web search. In fact, search engines provide the users with a list of relevant results answering a information need formulated as a textual query. In the last years, Learning to Rank (LtR) techniques have been successfully applied to solve this task. LtR is the field of machine learning devoted to the development of supervised techniques addressing the ranking problem. LtR techniques have been proficiently used in Web search, a scenario characterized by tight latency bounds for query processing~\cite{cambazoglu2011scalability}. For this reason, the investigation of new LtR techniques targets both effectiveness and efficiency to provide accurate solutions that can be used in modern query processors. State-of-the-art approaches in learning to rank are ensembles of regression trees. Specifically, LambdaMART~\cite{burges2010ranknet} is an effective state-of-the-art LtR algorithm that builds ensembles of regression trees by optimizing a loss function that depends on a listwise information retrieval metric, e.g., NDCG~\cite{jarvelin2002cumulated}. The counterpart of the retrieval accuracy guaranteed by tree-based models is the computational effort needed to traverse hundreds or even thousands of trees. This computational effort hinders the application of this kind of models on low-latency query processors. Furthermore, each tree in an ensemble work by testing a sequence of boolean conditions on the input. The natural translation of this structure in \textit{if-then-else} code conflicts with modern CPU architectures that heavily rely on \textit{branch prediction} and \textit{caching}. A recent line of research investigates techniques for efficient traversal of ensembles of regression trees. The state-of-the-art algorithm for traversing tree-based models is QuickScorer~\cite{lucchese2015quickscorer,dato2016fast,8035185,lucchese2016exploiting}, which implements an interleaved feature-wise traversal of the ensemble that maximizes the efficiency of branch predictor and cache of modern CPUs.

Motivated by the success of neural solutions in other fields such as Natural Language Processing and Computer Vision, several attempts have been made to bring Neural Networks (NNs) in the LtR field. Despite that, tree-based solutions still provide state-of-the-art performances on different benchmarks, especially when dealing with handcrafted features~\cite{qin2020neural}. 
Recently, Qin \emph{et al}~\cite{qin2020neural} identify the reasons for the superiority of tree-based solutions in i) the sensitiveness of neural network to input features scale and transformations, ii) the lack of expressiveness in mostly adopted neural models in LtR, iii) the limited size of available LtR datasets w.r.t. to Natural Language Processing or Computer Vision. Cohen \textit{et al.}~\cite{cohen2018universal} develop an approach that permit to overcome these limitations on standard LtR datasets by training classic multi-layer perceptrons using simple data normalization ($Z$-normalization) and by leveraging a data augumentation technique (Section \ref{sec:cohen}). 
%\cosimo{One of the inherent difficulties of ranking is that evaluation metrics are non-differentiable, hindering the usage of Stochastic Gradient Descent (SGD), the optimization algorithm used for neural network training. In general, differentiable proxies of ranking metrics are employed to train machine learning models.}
Cohen \textit{et al.}~\cite{cohen2018universal} propose to train neural networks to mimic the outputs of a pre-trained ensemble of regression trees. They do so by employing a knowledge distillation approach~\cite{ba2014deep,DBLP:journals/corr/HintonVD15} that treats the ensemble of regression trees as a black box generating accurate document scores.
Given that neural models are universal approximators~\cite{hornik1991approximation}, the network can reproduce the predictions of the ensemble of regression trees. In practice, this is done by using the Mean Square Error between the scores and the network predictions as training loss. The performance of a neural network trained by scores approximation are bounded by the performance of the tree-based model used to generate the scores: even in a perfect approximation scenario, the neural model will introduce no improvement in terms of effectiveness. In general, instead, the approximation will cause a degradation in the ranking precision. However, the reason to move to a neural document scoring engine is to exploit fast inference mechanisms available for NNs.
%\cosimo{In this direction, Cohen \textit{et al.}~\cite{cohen2018universal} compare the efficiency of a neural solution for ranking  with QuickScorer~\cite{lucchese2015quickscorer}.  }
In this direction, Cohen \textit{et al.}~\cite{cohen2018universal} compare the efficiency of a neural solution for ranking (on GPU and CPU) with QuickScorer~\cite{lucchese2015quickscorer} (on CPU).
In the original work, the authors claim that neural models are as accurate as ensembles of regression trees in terms of Mean Average Precision (MAP), and largely outperform them in terms of execution time ($\mu$s/doc). We observe that their comparison presents some weaknesses.
They compare a single-thread CPU version of QuickScorer against a multi-thread GPU version of the neural forward pass. Due to the differences between the computational engines, this does not permit to actually state which one of the two solutions is the more efficient.
 %They report the results of a CPU-based - single-thread - version of QuickScorer. We observe that comparing it with a multi-thread GPU version of the neural forward pass evaluates more the power of the computational engines than the efficiency of the algorithms.  
 Even when comparing on CPU, the comparison is done using: i) a single-threaded C++ implementation of QuickScorer for ensembles of regression trees and ii) a multi-threaded Python neural inference running with an unspecified number of threads. The use of Python APIs may also entail some latency in calling the underlying optimized matrix multiplication routine on which these frameworks usually rely.\footnote{See for example \url{https://scipy-cookbook.readthedocs.io/items/ParallelProgramming.html\#Use-parallel-primitives}} 
%\cosimo{They do not specify the number of threads used in the python version. Also, there may be a latency   }
Moreover, the two sets of experiments are conducted on different CPUs. These aspects hamper a direct comparison of the performance achieved.


In this article, we propose a solid, fair and comprehensive comparison of the efficiency of ensemble of regression trees and neural models.
We compare QuickScorer~\cite{dato2016fast} against a novel and optimized implementation of neural network inference written in C++. We perform the evaluation on the same hardware by executing the two solutions using a single thread. Moreover, both solutions exploit instruction-level parallelism (AVX2 instruction set). Since CPU and GPU are two different processing units and each of them requires specific optimization techniques, in this work we focus on providing an accurate study of the efficiency of the two approaches on CPU, while we plan to extend it to the GPU in the future. Regarding the training phase, we adopt the same neural architectures of Cohen \textit{et al.}~\cite{cohen2018universal} and we re-implement their methodology with our own code in Pytorch~\cite{NEURIPS2019_9015}. However, differently from the original work, in our experiments we train the ensemble of regression trees with the LightGBM library~\cite{NIPS2017_6907}, since it is the state-of-the-art library for learning ensemble models on ranking tasks~\cite{NIPS2017_6907,qin2020neural}.

\begin{table}[htb]
\centering
\begin{tabular}{llllr}	
		\toprule
		Model & NDCG@10   & NDCG&  MAP & \thead{ Scoring Time \\ ($\mu s$/ doc)} \\
		\midrule
		Large Forest & 0.5246\textsuperscript{$\star \dag$} & 0.7473\textsuperscript{$\star \dag$} & 0.6604\textsuperscript{$\star \dag$} & 8.2  	\\
		\cdashlinelr{1-5}
		Mid Forest & 0.5206\textsuperscript{$\dag$}&0.7454\textsuperscript{$\dag$} &  0.6582\textsuperscript{$\dag$}& 1.5 	\\
		Small Forest & 0.5181& 0.7438 &  0.6578& 0.8 \\
		\midrule
		Large Net & 0.5198\textsuperscript{$\dag$} & 0.7445\textsuperscript{$\dag$} & 0.6582\textsuperscript{$\dag$} & 24.4 \\
		Small Net & 0.5171 &0.7432 & 0.6575   & 2.2 \\
		\bottomrule
\end{tabular}
\caption{A comparison between QuickScorer and Neural Networks on the \msn dataset. 
Symbols evidence statistically significant improvement w.r.t. to Mid Forest (\textsuperscript{$\star$}), and Small Forest (\textsuperscript{$\dag$}),  according to the Fisher's randomization test,  $p < 0.05$.}
\label{tab:firsttab}
\end{table}

%\ftodo{tabella sopra. perche' large forest ha simbolo di stat sig da sola? FM}

The results of our comprehensive experimentation on the \msn dataset show that, in contrast with the results reported by Cohen \textit{et al.}~\cite{cohen2018universal}, ensembles of regression trees are both faster and more accurate than neural models. In Table \ref{tab:firsttab}, we report the Mean Average Precision (MAP), the Normalized Discounted Cumulative Gain (NDCG, with cutoff at 10 and without cutoff), and the scoring time per document.
 %Moreover, models associated with the same symbol ($\star$, \dag, $\ast$) are statistically equivalent, according to the Fisher's randomization test,  $p < 0.05$. 
%We observe that the performance of the models with the same symbol ($\star$, \dag, $\ast$) are statistically equivalent. 
Symbols evidence statistically significant improvement w.r.t. to Mid Forest\textsuperscript{$\star$}, and Small Forest\textsuperscript{$\dag$}, according to the Fisher's randomization test,  $p < 0.05$. We run different tests for each metrics, but we use shared symbols to ease the notation.
Table \ref{tab:firsttab} shows that ensemble of regression trees deliver the same performance of neural models while being largely faster, with a speedup ranging from $2.8$x (Small Net vs Small Forest) to $16.2$x (Large Net vs Mid Forest). Also, the Large Forest is the best performing model with a large margin, while being $3$x faster than the Large Net.
 %\textit{i.e.}, \textit{Large Forest} and \textit{Large Net} respectively. Moreover, when considering a given NDCG@10 value fixed by the neural network, tree-based models result consistently faster, with a speedup ranging from $2.8$x (Small Net vs Small Forest) to $16.2$x (Large Net vs Mid Forest). 
 These evidences highlight how tree-based solutions are currently faster than neural networks on CPU. We bridge the large gap between tree-based models and neural networks by proposing a novel framework to efficiently design and train effective and efficient feed-forward networks for ranking on CPU.

The novel contributions of this article are:
\begin{itemize}
\item we present a combination of state-of-the-art approaches to improve the performance of neural networks on Learning to Rank tasks. By leveraging efficiency-oriented pruning techniques and high-performance Dense and Sparse Matrix Multiplication techniques, we build neural models that outperform ensembles of regression trees. An extensive experimental evaluation on two well-established public benchmarks, \textit{i.e.}, the \msn~\cite{DBLP:journals/corr/QinL13} and the Tiscali \istella~\cite{dato2016fast} datasets, shows the effectiveness of our method. Experimental results confirm that on the \msn dataset it is possible to obtain up to $4.4$x faster scoring time with no loss of accuracy.

\item we provide a novel way to estimate the execution time of neural network forward pass, by mean of dense and sparse time predictors, respectively for Dense-Dense and Sparse-Dense Matrix Multiplication (DMM \& SDMM). To the best of our knowledge, this is the first work that dives into the technicality of matrix multiplication to precisely predict the execution time of neural models.
These predictors are derived from a broad study of the implementation of the relative operations on modern CPUs. In explaining how predictors are developed, we also provide a clear and concise explanation of these two fundamental operations with plenty of scientific applications. 

\item we develop an efficient and effective approach to design neural models, using the aforementioned time predictors, which allow to estimate the execution time of a feed-forward network \textit{a priori}, by providing the architecture - \textit{i.e.,} the number of layers and the neurons per layer - and the sparsity level of each layer. This design methodology tackles the costly problem of model architectures search~\cite{strubell2019energy,patterson2021carbon}, since it allows to train \emph{exclusively} the models respecting the latency requirements, tearing down the costs, in terms of time and energy consumption, of the experimental phase.
\end{itemize}

The rest of the paper is organized as follows: Section~\ref{sec:related} discusses the related work in the field. Section~\ref{sec:cohen} details the process of distilling ensemble of regression trees into neural networks as proposed by Cohen \emph{et al.}~\cite{cohen2018universal}. Section~\ref{sec:ModelMatMult} introduces the implementation of dense-dense matrix multiplication and sparse-dense matrix multiplication on modern CPUs, together with our time predictors. Section~\ref{sec:neuraleng} describes our novel method for designing efficient neural models for ranking. Moreover, Section~\ref{sec:experiments} presents a comprehensive experimental evaluation of our proposed technique on public data. Finally, Section~\ref{sec:conclusions} concludes the work.

