% !TEX root = paper.tex
% !TeX spellcheck = en_US

\section{Conclusions and Future Work}
\label{sec:conclusions}
In this paper, we presented an effective and efficient methodology to design neural networks for document scoring in a modern information retrieval system. The neural models we take into account are trained to approximate the scores of an ensemble of regression trees. By leveraging a combination of high-performance dense-dense, sparse-dense matrix multiplication, and element-wise pruning, the neural models can compete with the original models.
Thus, our methodology is \textit{effective}. By developing time predictors based on an accurate study of how these operations are implemented on modern processors, we are capable to precisely estimate the execution time of a given architecture by knowing the shape and the sparsity level of each layer. This allows to train only a limited number of models, the ones matching the time requirements given by the specific context. Our methodology is thus \textit{efficient}. Besides presenting our method, throughout the paper emerges a comparison between ensembles of regression trees and NNs on the document scoring task, tested on the \msn and \istella datasets. In our experiments,  neural networks are not capable of reaching the accuracy of their \textit{teacher}, hence tree-based methods are superior in top-quality retrieval scenarios. At any other level of the efficiency-effectiveness trade-off, neural models designed and trained with our approach can always outscore or at least compete with ensembles of regression trees.

As future work, we intend to apply different compression methods such as quantization or early exiting to further improve the efficiency of our neural models. Moreover, we plan to extend our comparison between neural networks and ensemble of regression trees to other computational engines, such as General-Purpose Graphic Processing Unit (GPU) or Field Programmable Gate Array (FPGA). We also aim at improving the training by distillation procedure of neural models, in order to bridge the effectiveness gap with ensembles of regression trees. 

\smallskip
\noindent \textbf{Acknowledgements}.
This paper is partially supported by the ``Algorithms, Data Structures and Combinatorics for Machine Learning'' (MIUR-PRIN 2017) and the OK-INSAID (MIUR-PON 2018, grant agreement ARS01\_00917) projects.

 %we plan to extend our comparison between neural networks and ensemble of regression trees to other computational engines, such as General-Purpose Graphic Processing Unit (GPU) or Field Programmable Gate Array (FPGA). We also intend to apply different compression methods such as quantization or early exiting to Furthermore, we intend to apply our approach to other domains in which neural models have proven to be successful, such as Computer Vision (CV) or Natural Language Processing.

%\fnote{direi troppo generici i future work. starei pi√π sul dettagliato. FM}

%\fnote{io il pezzo sotto non lo metterei. non e' il main result dell'articolo e non e' una motivazione concreta per usare l'uno o l'altro. se sei google, che ci vuole a implementare uno scoring efficiente di alberi con AVX512?}

%However, a neural network-based scoring systems presents a technological advantage: the efficiency of the forward-pass of a neural network strictly depends on matrix multiplication. As mentioned earlier in this paper, several high performance libraries~\cite{van2015blis,xianyi2012openblas} furnish highly optimized versions of this routine, on which we can rely when developing our scoring system. On the other hand, at the moment do not exist libraries for fast traversal of ensemble of trees. %and the QuickScorer code is not publicy available. 
%Developing a tree-based scoring system entails to re-implement an efficient scoring algorithm for regression forests, which should adapt to different architectures and should be maintained and updated to deal with the continuous improvements in the industry of CPUs.   
%For example, QuickScorer does not employ AVX512 vectorized instructions. Updating and maintaining it fully harvest the feature of  modern CPUs is a really challenging task, which requires a considerable effort in terms of development and testing time. 





