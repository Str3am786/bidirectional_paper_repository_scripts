% Generated by IEEEtranS.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtranS.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{abadi2016tensorflow}
M.~Abadi, P.~Barham, J.~Chen, Z.~Chen, A.~Davis, J.~Dean, M.~Devin,
  S.~Ghemawat, G.~Irving, M.~Isard \emph{et~al.}, ``Tensor{F}low: A system for
  large-scale machine learning,'' in \emph{USENIX Symposium on Operating
  Systems Design and Implementation (OSDI)}, 2016.

\bibitem{nnstats}
S.~Albanie, ``Memory consumption and {FLOP} count estimates for convnets,''
  \url{https://github.com/albanie/convnet-burden}.

\bibitem{ba2014deep}
J.~Ba and R.~Caruana, ``Do deep nets really need to be deep?'' in
  \emph{Advances in Neural Information Processing Systems (NIPS)}, 2014.

\bibitem{bergstra2013making}
J.~Bergstra, D.~Yamins, and D.~Cox, ``Making a science of model search:
  Hyperparameter optimization in hundreds of dimensions for vision
  architectures,'' in \emph{International {C}onference on {M}achine {L}earning
  (ICML)}, 2013.

\bibitem{breuer2019petaflop}
A.~Breuer, Y.~Cui, and A.~Heinecke, ``Petaflop seismic simulations in the
  public cloud,'' in \emph{International Conference on High Performance
  Computing (HiPC)}, 2019.

\bibitem{bucilua2006model}
C.~Buciluǎ, R.~Caruana, and A.~Niculescu-Mizil, ``Model compression,'' in
  \emph{Proceedings of the ACM International Conference on Knowledge Discovery
  and Data Mining (SIGKDD)}, 2006.

\bibitem{burges2005learning}
C.~Burges, T.~Shaked, E.~Renshaw, A.~Lazier, M.~Deeds, N.~Hamilton, and
  G.~Hullender, ``Learning to rank using gradient descent,'' in
  \emph{Proceedings of the International Conference on Machine learning
  (ICML)}, 2005.

\bibitem{burges2007learning}
C.~J. Burges, R.~Ragno, and Q.~V. Le, ``Learning to rank with nonsmooth cost
  functions,'' in \emph{Advances in Neural Information Processing Systems
  (NIPS)}, 2007.

\bibitem{burges2010ranknet}
C.~J. Burges, ``From {R}ank{N}et to {L}ambda{R}ank to {L}ambda{M}art: An
  overview,'' \emph{Learning}, 2010.

\bibitem{Cai_2017_CVPR}
Z.~Cai, X.~He, J.~Sun, and N.~Vasconcelos, ``Deep learning with low precision
  by half-wave gaussian quantization,'' in \emph{The IEEE Conference on
  Computer Vision and Pattern Recognition (CVPR)}, 2017.

\bibitem{cambazoglu2011scalability}
B.~B. Cambazoglu and R.~Baeza-Yates, ``Scalability challenges in web search
  engines,'' in \emph{Advanced topics in information retrieval}.\hskip 1em plus
  0.5em minus 0.4em\relax Springer, 2011.

\bibitem{cohen2018universal}
D.~Cohen, J.~Foley, H.~Zamani, J.~Allan, and W.~B. Croft, ``Universal
  approximation functions for fast learning to rank: Replacing expensive
  regression forests with simple feed-forward networks,'' in \emph{The ACM
  International Conference on Research \& Development in Information Retrieval
  (SIGIR)}, 2018.

\bibitem{dato2016fast}
D.~Dato, C.~Lucchese, F.~M. Nardini, S.~Orlando, R.~Perego, N.~Tonellotto, and
  R.~Venturini, ``Fast ranking with additive ensembles of oblivious and
  non-oblivious regression trees,'' \emph{ACM Transactions on Information
  Systems (TOIS)}, 2016.

\bibitem{denil2013predicting}
M.~Denil, B.~Shakibi, L.~Dinh, M.~Ranzato, and N.~De~Freitas, ``Predicting
  parameters in deep learning,'' in \emph{Advances in Neural Information
  Processing Systems (NIPS)}, 2013.

\bibitem{Goodfellow-et-al-2016}
I.~Goodfellow, Y.~Bengio, and A.~Courville, \emph{Deep Learning}, 2016.

\bibitem{goto2008anatomy}
K.~Goto and R.~A. v.~d. Geijn, ``Anatomy of high-performance matrix
  multiplication,'' \emph{ACM Transactions on Mathematical Software (TOMS)},
  2008.

\bibitem{DBLP:journals/corr/GuoYC16}
Y.~Guo, A.~Yao, and Y.~Chen, ``Dynamic network surgery for efficient dnns,'' in
  \emph{Advances in Neural Information Processing Systems (NIPS)}, 2016.

\bibitem{DBLP:journals/corr/HanMD15}
S.~Han, H.~Mao, and W.~J. Dally, ``Deep {C}ompression: Compressing deep neural
  network with pruning, trained quantization and huffman coding,'' in
  \emph{Proceedings of International Conference on Learning Representations
  (ICLR)}, 2016.

\bibitem{DBLP:journals/corr/HanPTD15}
S.~Han, J.~Pool, J.~Tran, and W.~Dally, ``Learning both weights and connections
  for efficient neural network,'' in \emph{Advances in Neural Information
  Processing Systems (NIPS)}, 2015.

\bibitem{he2018amc}
Y.~He, J.~Lin, Z.~Liu, H.~Wang, L.-J. Li, and S.~Han, ``{AMC}: Automl for model
  compression and acceleration on mobile devices,'' in \emph{Proceedings of the
  European Conference on Computer Vision (ECCV)}, 2018.

\bibitem{he2017channel}
Y.~He, X.~Zhang, and J.~Sun, ``Channel pruning for accelerating very deep
  neural networks,'' in \emph{Proceedings of the IEEE International Conference
  on Computer Vision (ICCV)}, 2017.

\bibitem{heinecke2016libxsmm}
A.~Heinecke, G.~Henry, M.~Hutchinson, and H.~Pabst, ``{LIBXSMM}: accelerating
  small matrix multiplications by runtime code generation,'' in
  \emph{Proceedings of the International Conference for High Performance
  Computing, Networking, Storage and Analysis}, 2016.

\bibitem{DBLP:journals/corr/HintonVD15}
G.~E. Hinton, O.~Vinyals, and J.~Dean, ``Distilling the knowledge in a neural
  network,'' \emph{CoRR}, vol. abs/1503.02531, 2015.

\bibitem{hornik1991approximation}
K.~Hornik, ``Approximation capabilities of multilayer feedforward networks,''
  \emph{Neural networks}, 1991.

\bibitem{howard2017mobilenets}
A.~G. Howard, M.~Zhu, B.~Chen, D.~Kalenichenko, W.~Wang, T.~Weyand,
  M.~Andreetto, and H.~Adam, ``Mobile{N}ets: Efficient convolutional neural
  networks for mobile vision applications,'' \emph{arXiv preprint
  arXiv:1704.04861}, 2017.

\bibitem{huang2016blislab}
J.~Huang and R.~A. Van~de Geijn, ``{BLIS}lab: A sandbox for optimizing
  {GEMM},'' \emph{arXiv preprint arXiv:1609.00076}, 2016.

\bibitem{huang2018data}
Z.~Huang and N.~Wang, ``Data-driven sparse structure selection for deep neural
  networks,'' in \emph{Proceedings of the European Conference on Computer
  Vision (ECCV)}, 2018.

\bibitem{hubara2017quantized}
I.~Hubara, M.~Courbariaux, D.~Soudry, R.~El-Yaniv, and Y.~Bengio, ``Quantized
  neural networks: Training neural networks with low precision weights and
  activations,'' \emph{The Journal of Machine Learning Research}, 2017.

\bibitem{DBLP:journals/corr/IandolaMAHDK16}
F.~N. Iandola, M.~W. Moskewicz, K.~Ashraf, S.~Han, W.~J. Dally, and K.~Keutzer,
  ``Squeeze{N}et: {A}lex{N}et-level accuracy with 50x fewer parameters and
  {\textless}1{MB} model size,'' vol. abs/1602.07360, 2016.

\bibitem{jarvelin2002cumulated}
K.~J{\"a}rvelin and J.~Kek{\"a}l{\"a}inen, ``Cumulated gain-based evaluation of
  {IR} techniques,'' \emph{ACM Transactions on Information Systems (TOIS)},
  2002.

\bibitem{NIPS2017_6907}
G.~Ke, Q.~Meng, T.~Finley, T.~Wang, W.~Chen, W.~Ma, Q.~Ye, and T.-Y. Liu,
  ``Light{GBM}: A highly efficient gradient boosting decision tree,'' in
  \emph{Advances in Neural Information Processing Systems (NIPS)}, 2017.

\bibitem{kim2011fast}
J.~Kim and H.~Park, ``Fast nonnegative matrix factorization: An active-set-like
  method and comparisons,'' \emph{SIAM Journal on Scientific Computing}, 2011.

\bibitem{kingma2014adam}
D.~P. Kingma and J.~Ba, ``Adam: A method for stochastic optimization,'' in
  \emph{Proceedings of the International Conference on Learning Representations
  (ICLR)}, 2015.

\bibitem{lawson1979basic}
C.~L. Lawson, R.~J. Hanson, D.~R. Kincaid, and F.~T. Krogh, ``Basic linear
  algebra subprograms for {F}ortran usage,'' \emph{ACM Transactions on
  Mathematical Software (TOMS)}, 1979.

\bibitem{8035185}
F.~{Lettich}, C.~{Lucchese}, F.~M. {Nardini}, S.~{Orlando}, R.~{Perego},
  N.~{Tonellotto}, and R.~{Venturini}, ``Multicore/manycore parallel traversal
  of large forests of regression trees,'' in \emph{International Conference on
  High Performance Computing Simulation (HPCS)}, 2017.

\bibitem{DBLP:journals/corr/LiL16}
F.~Li and B.~Liu, ``Ternary weight networks,'' \emph{CoRR}, vol.
  abs/1605.04711, 2016.

\bibitem{DBLP:journals/corr/LiKDSG16}
H.~Li, A.~Kadav, I.~Durdanovic, H.~Samet, and H.~P. Graf, ``Pruning filters for
  efficient convnets,'' \emph{CoRR}, vol. abs/1608.08710, 2016.

\bibitem{li2008mcrank}
P.~Li, Q.~Wu, and C.~J. Burges, ``Mc{R}ank: Learning to rank using multiple
  classification and gradient boosting,'' in \emph{Advances in Neural
  Information Processing Systems (NIPS)}, 2008.

\bibitem{llanas2008constructive}
B.~Llanas, S.~Lantar{\'o}n, and F.~J. S{\'a}inz, ``Constructive approximation
  of discontinuous functions by neural networks,'' \emph{Neural Processing
  Letters}, 2008.

\bibitem{low2016analytical}
T.~M. Low, F.~D. Igual, T.~M. Smith, and E.~S. Quintana-Orti, ``Analytical
  modeling is enough for high-performance {BLIS},'' \emph{ACM Transactions on
  Mathematical Software (TOMS)}, 2016.

\bibitem{lucchese2015quickscorer}
C.~Lucchese, F.~M. Nardini, S.~Orlando, R.~Perego, N.~Tonellotto, and
  R.~Venturini, ``Quick{S}corer: A fast algorithm to rank documents with
  additive ensembles of regression trees,'' in \emph{Proceedings of ACM
  International Conference on Research and Development in Information Retrieval
  (SIGIR)}, 2015.

\bibitem{lucchese2016exploiting}
C.~Lucchese, F.~M. Nardini~\vspace{0mm}, S.~Orlando, R.~Perego, N.~Tonellotto,
  and R.~Venturini, ``Exploiting {CPU} {SIMD} extensions to speed-up document
  scoring with tree ensembles,'' in \emph{Proceedings of the ACM International
  Conference on Research and Development in Information Retrieval (SIGIR)},
  2016.

\bibitem{luo2017thinet}
J.-H. Luo, J.~Wu, and W.~Lin, ``Thi{N}et: A filter level pruning method for
  deep neural network compression,'' in \emph{Proceedings of the IEEE
  International Conference on Computer Vision (ICCV)}, 2017.

\bibitem{molchanov2019pruning}
P.~Molchanov, S.~Tyree, T.~Karras, T.~Aila, and J.~Kautz, ``Pruning
  convolutional neural networks for resource efficient inference,'' in
  \emph{Proceedings of the International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem{NEURIPS2019_9015}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga, A.~Desmaison, A.~Kopf, E.~Yang, Z.~DeVito,
  M.~Raison, A.~Tejani, S.~Chilamkurthy, B.~Steiner, L.~Fang, J.~Bai, and
  S.~Chintala, ``Py{T}orch: An imperative style, high-performance deep learning
  library,'' in \emph{Advances in Neural Information Processing Systems
  (NIPS)}, 2019.

\bibitem{patterson2021carbon}
D.~Patterson, J.~Gonzalez, Q.~Le, C.~Liang, L.-M. Munguia, D.~Rothchild, D.~So,
  M.~Texier, and J.~Dean, ``Carbon emissions and large neural network
  training,'' \emph{arXiv e-prints}, 2021.

\bibitem{DBLP:journals/corr/QinL13}
T.~Qin and T.~Liu, ``Introducing {LETOR} 4.0 datasets,'' \emph{CoRR}, vol.
  abs/1306.2597, 2013.

\bibitem{qin2020neural}
Z.~Qin, L.~Yan, H.~Zhuang, Y.~Tay, R.~K. Pasumarthi, X.~Wang, M.~Bendersky, and
  M.~Najork, ``Are neural rankers still outperformed by gradient boosted
  decision trees?'' in \emph{Proceedings of International Conference on
  Learning Representations}, 2020.

\bibitem{DBLP:journals/corr/RastegariORF16}
M.~Rastegari, V.~Ordonez, J.~Redmon, and A.~Farhadi, ``{XNOR}-{N}et: Imagenet
  classification using binary convolutional neural networks,'' in
  \emph{Proceedings of the European Conference con Computer Vision (ECCV)},
  2016.

\bibitem{sandler2018mobilenetv2}
M.~Sandler, A.~Howard, M.~Zhu, A.~Zhmoginov, and L.-C. Chen, ``Mobile{N}et{V}2:
  Inverted residuals and linear bottlenecks,'' in \emph{Proceedings of the IEEE
  Conference on Computer Vision and Pattern Recognition (CVPR)}, 2018.

\bibitem{NIPS2010_4099}
K.~Scheinberg, S.~Ma, and D.~Goldfarb, ``Sparse inverse covariance selection
  via alternating linearization methods,'' in \emph{Advances in Neural
  Information Processing Systems (NIPS)}, 2010.

\bibitem{strubell2019energy}
E.~Strubell, A.~Ganesh, and A.~McCallum, ``Energy and policy considerations for
  deep learning in {NLP},'' in \emph{Proceedings of the Annual Meeting of the
  Association for Computational Linguistics}, 2019.

\bibitem{tiskin2001all}
A.~Tiskin, ``All-pairs shortest paths computation in the {BSP} model,'' in
  \emph{International Colloquium on Automata, Languages, and Programming},
  2001.

\bibitem{tsai2007frank}
M.-F. Tsai, T.-Y. Liu, T.~Qin, H.-H. Chen, and W.-Y. Ma, ``{FR}ank: a ranking
  method with fidelity loss,'' in \emph{Proceedings of the ACM International
  Conference on Research and Development in Information Retrieval (SIGIR)},
  2007.

\bibitem{van2015blis}
F.~G. Van~Zee and R.~A. Van De~Geijn, ``{BLIS}: A framework for rapidly
  instantiating {BLAS} functionality,'' \emph{ACM Transactions on Mathematical
  Software (TOMS)}, 2015.

\bibitem{vieira2017learning}
T.~Vieira and J.~Eisner, ``Learning to prune: Exploring the frontier of fast
  and accurate parsing,'' \emph{Transactions of the Association for
  Computational Linguistics (ACL)}, 2017.

\bibitem{wang2014intel}
E.~Wang, Q.~Zhang, B.~Shen, G.~Zhang, X.~Lu, Q.~Wu, and Y.~Wang, ``Intel math
  kernel library,'' in \emph{High-Performance Computing on the
  Intel{\textregistered} Xeon Phi™}, 2014.

\bibitem{xianyi2012openblas}
Z.~Xianyi, W.~Qian, and Z.~Chothia, ``Open{BLAS},'' 2012.

\bibitem{ye2018rapidscorer}
T.~Ye, H.~Zhou, W.~Y. Zou, B.~Gao, and R.~Zhang, ``Rapid{S}corer: fast tree
  ensemble evaluation by maximizing compactness in data level
  parallelization,'' in \emph{Proceedings of the ACM International Conference
  on Knowledge Discovery \& Data Mining (SIGKDD)}, 2018.

\bibitem{yu2017scalpel}
J.~Yu, A.~Lukefahr, D.~Palframan, G.~Dasika, R.~Das, and S.~Mahlke, ``Scalpel:
  Customizing dnn pruning to the underlying hardware parallelism,'' in
  \emph{ACM SIGARCH Computer Architecture News}, 2017.

\bibitem{zhang2018shufflenet}
X.~Zhang, X.~Zhou, M.~Lin, and J.~Sun, ``Shuffle{N}et: An extremely efficient
  convolutional neural network for mobile devices,'' in \emph{Proceedings of
  the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 2018.

\bibitem{DBLP:journals/corr/ZhouYGXC17}
A.~Zhou, A.~Yao, Y.~Guo, L.~Xu, and Y.~Chen, ``Incremental network
  quantization: Towards lossless cnns with low-precision weights,''
  \emph{CoRR}, vol. abs/1702.03044, 2017.

\bibitem{DBLP:journals/corr/ZhuHMD16}
C.~Zhu, S.~Han, H.~Mao, and W.~J. Dally, ``Trained ternary quantization,''
  \emph{CoRR}, vol. abs/1612.01064, 2016.

\bibitem{nzmora2019distiller}
N.~Zmora, G.~Jacob, L.~Zlotnik, B.~Elharar, and G.~Novik, ``Neural network
  distiller: A python package for dnn compression research,'' 2019.

\end{thebibliography}
