\section{Related Work}
Post-correction for OCR is well-studied for high-resource languages. Early approaches include lexical methods and weighted finite-state methods (see \citet{schulz-kuhn-2017-multi} for an overview). Recent work has primarily focused on using neural sequence-to-sequence models. \citet{hamalainen-hengchen-2019-paft} use a BiLSTM encoder-decoder with attention for historical English post-correction. Similar to our base model, \citet{dong-smith-2018-multi} use a multi-source model to combine the first pass OCR from duplicate documents in English. 

There has been little work on lower-resourced languages. \citet{kolak-resnik-2005-ocr} present a probabilistic edit distance based post-correction model applied to Cebuano and Igbo, and \citet{krishna-etal-2018-upcycle} show improvements on Romanized Sanksrit OCR by adding a copy mechanism to a neural sequence-to-sequence model.

Multi-source encoder-decoder models have been used for various tasks including machine translation~\cite{zoph-knight-2016-multi,libovicky-helcl-2017-attention} and morphological inflection~\cite{kann-etal-2017-neural,anastasopoulos-neubig-2019-pushing}. Perhaps most relevant to our work is the multi-source model presented by \citet{anastasopoulos+chiang:interspeech2018}, which uses high-resource translations to improve speech transcription of lower-resourced languages.

Finally, \citet{bustamante-etal-2020-data} construct corpora for four endangered languages from text-based PDFs using rule-based heuristics. Data creation from such unstructured text files is an important research direction, complementing our method of extracting data from scanned images.