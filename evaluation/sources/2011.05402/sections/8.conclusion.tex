\section{Conclusion}
This work presents a first step towards extracting textual data in endangered languages from scanned images of paper books. We create a benchmark dataset with transcribed images in three endangered languages: Ainu, Griko, and Yakkha. We propose an OCR post-correction method that facilitates learning from small amounts of data, which results in a 34\% average relative error reduction in both the character and word recognition rates.

As future work, we plan to investigate the effect of using other available data for the three languages (for example, word lists collected by documentary linguists or the additional Griko folk tales collected by~\citet{anastasopoulos-etal-2018-part}). 

Additionally, it would be valuable to examine whether our method can improve the OCR on high-resource languages, which typically have much better recognition rates in the first pass transcription than the endangered languages in our dataset.

Further, we note our use of the Google Vision OCR system to obtain the first pass OCR for our experiments, primarily because it provides script-specific models as opposed to other general-purpose OCR systems that rely on language-specific models (as discussed in \autoref{sec:analysis}). Future work that focuses on overcoming the challenges of applying language-specific models to endangered language texts would be needed to confirm our method's applicability to post-correcting the first pass transcriptions from different OCR systems.

Lastly, given the annotation effort involved, this paper explores only a small fraction of the endangered language data available in linguistic and general-purpose archives.
Future work will focus on large-scale digitization of scanned documents, aiming to expand our OCR benchmark on as many endangered languages as possible, in the hope of both easing linguistic documentation and preservation efforts and collecting enough data for NLP system development in under-represented languages.
