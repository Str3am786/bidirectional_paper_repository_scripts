\section{Problem Setting}
\label{sec:setting}
In this section, we first define the task of OCR post-correction and introduce how we incorporate translations into the correction model. Next, we discuss the sources from which we obtain scanned documents containing endangered language texts.

\subsection{Formulation}
\label{sec:formulation}
\paragraph{Optical Character Recognition} OCR tools are trained to find the best transcription corresponding to the text in an image. The system typically consists of a recognition model that produces candidate text sequences conditioned on the input image and a language model that determines the probability of these sequences in the target language. We use a general-purpose OCR system (detailed in \autoref{sec:analysis}) to produce a \emph{first pass transcription} of the endangered language text in the image. Let this be a sequence of characters $\boldsymbol{x} = [x_1, \dots, x_N]$.

\paragraph{OCR post-correction} The goal of post-correction is to reduce recognition errors in the first pass transcription --- often caused by low quality scanning, physical deterioration of the paper book, or diverse layouts and typefaces~\cite{dong-smith-2018-multi}. The focus of our work is on using post-correction to counterbalance the lack of OCR training data in the target endangered languages. The correction model takes $\boldsymbol{x}$ as input and produces the \emph{final transcription} of the endangered language document, a sequence of characters $\boldsymbol{y} = [y_1, \dots , y_K]$. 
$$\boldsymbol{y} = \argmax_{\boldsymbol{y'}} p_\text{corr}(\boldsymbol{y'}|\boldsymbol{x})$$

\noindent
\textbf{Incorporating translations}\quad We use information from high-resource translations of the endangered language text. These translations are commonly found within the same paper book or linguistic archive (e.g., \autoref{fig:dataset_example}). We use an existing OCR system to obtain a transcription of the scanned translation, a sequence of characters $\boldsymbol{t} = [t_1, \dots, t_M]$. This is used to condition the model:
$$\boldsymbol{y} = \argmax_{\boldsymbol{y'}} p_\text{corr}(\boldsymbol{y'}|\boldsymbol{x}, \boldsymbol{t})$$

\subsection{Endangered Language Documents}
We explore online archives to determine how many scanned documents in endangered languages exist as potential sources for data extraction (as of this writing, October 2020).

The Internet Archive,\footnote{\url{https://archive.org/}} a general-purpose archive of web content, has scanned books labeled with the language of their content. We find 11,674 books labeled with languages classified as \ba\ba endangered'' by UNESCO.
Additionally, we find that endangered language linguistic archives contain thousands of documents in PDF format --- the Archive of the Indigenous Languages of Latin America (AILLA)\footnote{\url{https://ailla.utexas.org}} contains $\approx$10,000 such documents and the Endangered Languages Archive (ELAR)\footnote{\url{https://elar.soas.ac.uk/}} has $\approx$7,000. 

\medskip
\noindent
\textbf{How common are translations?} As described in the introduction, endangered language documents often contain a translation into another (usually high-resource) language. While it is difficult to estimate the number of documents with translations, multilingual documents represent the majority in the archives we examined; AILLA contains 4,383 PDFs with bilingual text and 1,246 PDFs with trilingual text, while ELAR contains $\approx$5,000 multilingual documents. The structure of translations in these documents is varied, from dictionaries and interlinear glosses to scanned multilingual books.
