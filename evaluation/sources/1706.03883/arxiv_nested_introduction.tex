In numerous applications in engineering and sciences, data are often organized in
a multilevel structure. For instance, a typical structural view of text data in machine learning
is to have words grouped into documents, documents are grouped into 
corpora. A prominent strand of modeling and algorithmic works in the past couple decades
has been to discover latent multilevel structures from these hierarchically structured data. 
For specific clustering tasks, one may be interested in simultaneously partitioning the data in each 
group (to obtain local clusters) and partitioning a collection of data groups (to obtain global clusters). 
Another concrete example is the problem of clustering images (i.e., global clusters) where each image 
contains partions of multiple annotated regions (i.e., local clusters) \citep{Oliva-2001}. 
While hierachical clustering techniques may be employed to find a tree-structed clustering 
given a collection of data points, they are not applicable to discovering the nested structure of multilevel data.
Bayesian hierarchical models provide a powerful approach, exemplified by influential works such as
\cite{Blei-etal-03,Pritchard-etal-00,Teh-etal-06}. More specific to the simultaneous and multilevel
clustering problem, we mention the paper of \cite{Rodriguez-etal-08}. In this interesting work,
a Bayesian nonparametric model, namely the nested Dirichlet process (NDP) model,
was introduced that enables
the inference of clustering of a collection of probability distributions from which different 
groups of data are drawn. With suitable extensions, this modeling framework
has been further developed for simultaneous multilevel clustering, see for instance, 
\citep{Wulsin-2016,Vu-2014,Viet-2016}. 
%The main drawback of these modeling approaches is the 
%high computational cost involved in the posterior inference for latent clustering structures. 
%This prevents some of these methods from being deployed in very large-scale applications.

The focus of this paper is on the multilevel clustering problem motivated in the aforementioned
modeling works, but we shall take a purely optimization approach. 
We aim to formulate optimization problems that enable the discovery of multilevel clustering 
structures hidden in grouped data. Our technical approach is
inspired by the role of optimal transport distances in hierarchical modeling and clustering problems.
The optimal transport distances, also known as Wasserstein distances \citep{Villani-03},
have been shown to be the natural distance metric for the convergence theory of latent
mixing measures arising in both mixture models \citep{Nguyen-13} and hierarchical models \citep{Nguyen-2016}.
They are also intimately connected to the problem of clustering --- this relationship goes
back at least to the work of \citep{Pollard-1982}, where it is pointed out that the well-known 
K-means clustering algorithm can be directly linked to the quantization problem --- the problem
of determining an optimal finite discrete probability measure that minimizes its 
second-order Wasserstein distance from the empirical distribution of given data \citep{Graf-2000}.
%Under that viewpoint, by representing each group in a multilevel clustering problem by an unknown 
%local discrete measure (local clustering) with finite number of atoms, we perform another K-
%means problem or equivalently quantization problem over these local measures (global 
%clustering) to obtain a global discrete measure over the space of measures of measures. 

If one is to perform simultaneous K-means clustering for hierarchically grouped data, both at the global
level (among groups), and local level (within each group), then this can be achieved by a joint optimization
problem defined with suitable notions of Wasserstein distances inserted into the objective
function. In particular, multilevel clustering requires the optimization in the space of
probability measures defined in \emph{different} levels of abstraction, including the space of
measures of measures on the space of grouped data.
Our goal, therefore, is to formulate this optimization precisely, to develop algorithms
for solving the optimization problem efficiently, and to make sense of the obtained solutions
in terms of statistical consistency. 

The algorithms that we propose address directly a multilevel clustering problem 
formulated from a purely optimization viewpoint, but they may also be taken as a fast approximation to
the inference of latent mixing measures that arise in the nested Dirichlet process of \citep{Rodriguez-etal-08}.
From a statistical viewpoint, we shall establish a consistency theory for our multilevel clustering
problem in the manner achieved for K-means clustering \citep{Pollard-1982}. From a computational viewpoint,
quite interestingly, we will be able to explicate and exploit the connection betwen our optimization
and that of finding the Wasserstein barycenter \citep{Carlier-2011}, an interesting
computational problem that have also attracted much recent interests, e.g., \citep{Cuturi-2014}.

In summary, the main contributions offered in this work include (i) 
a new optimization formulation to the multilevel clustering problem using Wasserstein
distances defined on different levels of the hierarchical data structure; (ii) 
fast algorithms by exploiting the connection of our formulation to the Wasserstein 
barycenter problem; (iii) consistency theorems established for proposed
estimates under very mild condition of data's distributions; 
(iv) several flexibile alternatives by introducing constraints that encourage the borrowing
of strength among local and global clusters, and (v) finally, demonstration of 
efficiency and flexibility of our approach in a number of simulated and real data sets.  

The paper is organized as follows. Section \ref{Section:prelim} provides preliminary 
background on Wasserstein distance, Wasserstein barycenter, and the connection between 
K-means clustering and the quantization problem. Section 
\ref{Section:multilevel_Wasserstein} presents
several optimization formulations of the multilevel clusering problem, and the algorithms
for solving them. Section
\ref{Section:consistency_multilevel_Kmeans} establishes consistency results of the estimators
introduced in Section \ref{Section:consistency_multilevel_Kmeans}. Section 
\ref{Section:data_analysis} presents careful simulation studies with both synthetic and real 
data. Finally, we conclude the paper with a discussion in Section \ref{Section:discussion}.
Additional technical details, including all proofs, are given in the Supplement.
