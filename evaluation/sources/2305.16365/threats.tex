\section{Threats to the validity and Limitations}\label{sec_threats_to_the_validity} 

In this section, we discuss the threats to the validity and limitations of our studies.

\subsection{\textbf{Threats to Validity -- quantitative study}}

\textit{\textbf{Construct Validity.}} The threats to construct validity are concerned with the extent to which the
operational measures in the study really represent what researchers intended to measure. We define the delivery time of a PR as the time between when a PR is merged and the moment at which a PR is delivered through a release. However, the way we link PRs to releases may not match the actual number of delivered PRs per release. For instance, if a version control system of a project has the following release tags \textit{v1.0, v2.0, no-ver, and v3.0}, we
remove the \textit{no-ver} tag. If there are PRs associated with the
\textit{no-ver release}, such PRs will be associated with release
\textit{v3.0}. However, only 5.36\% (\nicefrac{403}{7519}) of our studied
releases are affected by this bias.

\noindent\textit{\textbf{Internal Validity.}} Internal threats are concerned with the
ability to draw conclusions from the relationship between the dependent
variable (delivery time of merged PRs) and independent variables (e.g.,
release commits and queue rank). 
Regarding our models, we acknowledge that our independent variables
are not exhaustive. Although our models achieve sound $R^2$ values, other variables may
be used to improve performance (e.g., a \textit{boolean} indicating whether a PR
is associated with an issue report and another \textit{boolean} that verifies whether a PR was submitted by a core developer or an external contributor). Nevertheless, our set of independent variables
should be approached as a starting set that can be easily computed rather than a final solution.

At the time the quantitative data was extracted from GitHub, our projects had a median of 5.1 years of life (2 without using \textsc{TravisCI} and 3.1 using \textsc{TravisCI}). As we extracted data from the entire lifetime of the projects, we collected more PRs in the \textit{after-\textsc{TravisCI}} time period. However, the Mann-Whitney-Wilcoxon (MWW) test and Cliff's delta effect size, which were the statistical methods used to perform the comparisons, are suitable for groups of different sizes \citep{mann_whitney_1947}. 
Furthermore, we are aware that factors not related to the adoption of \textsc{TravisCI} may have impacted the delivery time of merged PRs as well (i.e., project maturity and team size). We argue that this concern was alleviated by the conceptual replication of our study conducted by \cite{guo2019studying}. First, they replicated our study using the same subject projects and methodology. Then, they addressed the same research question of our study, by using the Regression Discontinuity Design (RDD), which allows verifying whether there is a trend of PR delivery times over time and whether this trend changes significantly when \textsc{TravisCI} is introduced. Finally, they introduced a control group of comparable projects that never used \textsc{TravisCI}. They found that the results of our quantitative study largely hold in their replication study.

\noindent\textit{\textbf{External Validity.}} External threats are concerned with the extent to which we can generalize our results \citep{Perry:2000:ESS:336512.336586}. 
In this work, we analyzed 162,653 PRs of 87 popular open-source projects from GitHub. All projects adopt \textsc{TravisCI} as their CI service. 
We acknowledge that we cannot generalize our results to any other projects with similar or different settings (e.g., private software projects). Nevertheless, in order to achieve more generalizable results, replications of our study in different settings are required. 
For replication purposes, we publicize our datasets and results to the interested researcher.\footnote{\url{https://prdeliverydelay.github.io/\#datasets}}  

\subsection{\textbf{Limitation of our qualitative study}}

The main limitation of our qualitative study is the thematic analysis of the survey responses. The first author coded all responses to the 13 open-ended questions of the survey, and the second author coded a sub-set of 10\% of the responses to each question, which was selected randomly. Although the systematic analytical process was used to analyze the data, the credibility of the findings is enhanced if more than one researcher completely analyzes the data. Nevertheless, we used the Cohen's Kappa test to verify the agreement level of the two authors when coding the answers. Given that we achieved an almost perfect level of agreement between them (median Kappa's value of $0.84$), we believe that the coding process has been consistent. 

While we achieved theoretical saturation concerning the responses to the research questions of our qualitative study, we are aware that the self-selection of our participants may have biased our results. From the subset of 3,105 individuals, we received 450 responses to the survey (14.5\% response rate). Therefore, the contributors that did not respond to our survey invitation may have different views on the questions, which could lead us to different results.
Additionally, the participants that responded to our survey may have been affected by social desirability bias, which is the tendency to answer questions in a way that will be seen as advantageous by others (i.e., participants responding according to what they think the ``correct'' answer should be, making themselves and their software development look better than it actually is).

In our qualitative study, we investigated the perception of CI practitioners regarding the influence of CI on the delivery time of PRs, and its potential influence on the review and release processes of software projects. The participants of our qualitative study are contributors from 73 out of the 87 GitHub projects studied in our quantitative analysis. 
However, we cannot measure the degree to which the studied projects use CI. Hence, selecting participants of projects that use a CI service, but not necessarily CI as a whole practice, may bias our analysis. This is because we may have received responses from participants that did not fully witness the benefits of CI when it is fully implemented.  
Nevertheless, 73.2\% of the participants reported that they used CI in 60--100\% of their projects, which suggests that most of our participants have a varied experience with CI. Another issue is that we did not distinguish our participants by core and external contributors. 
Had we considered the responses from different types of contributors separately, different conclusions could have been drawn. On the other hand, we were able to collect a diverse set of participants, i.e., while 77.8\% of the participants have web development as one of their main activities, 52\% of participants consider code review as one of their main activities, and 30.9\% are involved in project management (see 
Figure \ref{fig:main_development_activities}), which provides us with insightful feedback.