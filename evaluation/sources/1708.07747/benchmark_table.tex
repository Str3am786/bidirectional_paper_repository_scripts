\begin{longtable}{llrr}
\caption{Benchmark on \fmnist (Fashion) and MNIST.} \label{tbl:benchmark} \\
\toprule
& & \multicolumn{2}{c}{Test Accuracy} \\
\cmidrule{3-4}
\multicolumn{1}{l}{Classifier} &
\multicolumn{1}{l}{Parameter} &
\multicolumn{1}{c}{Fashion} &
\multicolumn{1}{c}{MNIST} \\ 
\midrule
\endfirsthead

\multicolumn{4}{c}%
{{\tablename\ \thetable{} -- continued from previous page}} \\
\toprule
& & \multicolumn{2}{c}{Test Accuracy} \\
\cmidrule{3-4}
\multicolumn{1}{l}{Classifier} &
\multicolumn{1}{l}{Parameter} &
\multicolumn{1}{c}{Fashion} &
\multicolumn{1}{c}{MNIST} \\ 
\midrule
\endhead

\midrule \multicolumn{4}{r}{{Continued on next page}} \\ \bottomrule
\endfoot

\bottomrule
\endlastfoot
DecisionTreeClassifier & \tiny{criterion=\texttt{entropy} max\_depth=$10$ splitter=\texttt{best}} &$0.798$ & $0.873$\\
& \tiny{criterion=\texttt{entropy} max\_depth=$10$ splitter=\texttt{random}} &$0.792$ & $0.861$\\
& \tiny{criterion=\texttt{entropy} max\_depth=$50$ splitter=\texttt{best}} &$0.789$ & $0.886$\\
& \tiny{criterion=\texttt{entropy} max\_depth=$100$ splitter=\texttt{best}} &$0.789$ & $0.886$\\
& \tiny{criterion=\texttt{gini} max\_depth=$10$ splitter=\texttt{best}} &$0.788$ & $0.866$\\
& \tiny{criterion=\texttt{entropy} max\_depth=$50$ splitter=\texttt{random}} &$0.787$ & $0.883$\\
& \tiny{criterion=\texttt{entropy} max\_depth=$100$ splitter=\texttt{random}} &$0.787$ & $0.881$\\
& \tiny{criterion=\texttt{gini} max\_depth=$100$ splitter=\texttt{best}} &$0.785$ & $0.879$\\
& \tiny{criterion=\texttt{gini} max\_depth=$50$ splitter=\texttt{best}} &$0.783$ & $0.877$\\
& \tiny{criterion=\texttt{gini} max\_depth=$10$ splitter=\texttt{random}} &$0.783$ & $0.853$\\
& \tiny{criterion=\texttt{gini} max\_depth=$50$ splitter=\texttt{random}} &$0.779$ & $0.873$\\
& \tiny{criterion=\texttt{gini} max\_depth=$100$ splitter=\texttt{random}} &$0.777$ & $0.875$\\
\midrule
ExtraTreeClassifier & \tiny{criterion=\texttt{gini} max\_depth=$10$ splitter=\texttt{best}} &$0.775$ & $0.806$\\
& \tiny{criterion=\texttt{entropy} max\_depth=$100$ splitter=\texttt{best}} &$0.775$ & $0.847$\\
& \tiny{criterion=\texttt{entropy} max\_depth=$10$ splitter=\texttt{best}} &$0.772$ & $0.810$\\
& \tiny{criterion=\texttt{entropy} max\_depth=$50$ splitter=\texttt{best}} &$0.772$ & $0.847$\\
& \tiny{criterion=\texttt{gini} max\_depth=$100$ splitter=\texttt{best}} &$0.769$ & $0.843$\\
& \tiny{criterion=\texttt{gini} max\_depth=$50$ splitter=\texttt{best}} &$0.768$ & $0.845$\\
& \tiny{criterion=\texttt{entropy} max\_depth=$50$ splitter=\texttt{random}} &$0.752$ & $0.826$\\
& \tiny{criterion=\texttt{entropy} max\_depth=$100$ splitter=\texttt{random}} &$0.752$ & $0.828$\\
& \tiny{criterion=\texttt{gini} max\_depth=$50$ splitter=\texttt{random}} &$0.748$ & $0.824$\\
& \tiny{criterion=\texttt{gini} max\_depth=$100$ splitter=\texttt{random}} &$0.745$ & $0.820$\\
& \tiny{criterion=\texttt{gini} max\_depth=$10$ splitter=\texttt{random}} &$0.739$ & $0.737$\\
& \tiny{criterion=\texttt{entropy} max\_depth=$10$ splitter=\texttt{random}} &$0.737$ & $0.745$\\
\midrule
GaussianNB & \tiny{priors=\texttt{[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]}} &$0.511$ & $0.524$\\
\midrule
GradientBoostingClassifier & \tiny{n\_estimators=$100$ loss=\texttt{deviance} max\_depth=$10$} &$0.880$ & $0.969$\\
& \tiny{n\_estimators=$50$ loss=\texttt{deviance} max\_depth=$10$} &$0.872$ & $0.964$\\
& \tiny{n\_estimators=$100$ loss=\texttt{deviance} max\_depth=$3$} &$0.862$ & $0.949$\\
& \tiny{n\_estimators=$10$ loss=\texttt{deviance} max\_depth=$10$} &$0.849$ & $0.933$\\
& \tiny{n\_estimators=$50$ loss=\texttt{deviance} max\_depth=$3$} &$0.840$ & $0.926$\\
& \tiny{n\_estimators=$10$ loss=\texttt{deviance} max\_depth=$50$} &$0.795$ & $0.888$\\
& \tiny{n\_estimators=$10$ loss=\texttt{deviance} max\_depth=$3$} &$0.782$ & $0.846$\\
\midrule
KNeighborsClassifier & \tiny{weights=\texttt{distance} n\_neighbors=$5$ p=$1$} &$0.854$ & $0.959$\\
& \tiny{weights=\texttt{distance} n\_neighbors=$9$ p=$1$} &$0.854$ & $0.955$\\
& \tiny{weights=\texttt{uniform} n\_neighbors=$9$ p=$1$} &$0.853$ & $0.955$\\
& \tiny{weights=\texttt{uniform} n\_neighbors=$5$ p=$1$} &$0.852$ & $0.957$\\
& \tiny{weights=\texttt{distance} n\_neighbors=$5$ p=$2$} &$0.852$ & $0.945$\\
& \tiny{weights=\texttt{distance} n\_neighbors=$9$ p=$2$} &$0.849$ & $0.944$\\
& \tiny{weights=\texttt{uniform} n\_neighbors=$5$ p=$2$} &$0.849$ & $0.944$\\
& \tiny{weights=\texttt{uniform} n\_neighbors=$9$ p=$2$} &$0.847$ & $0.943$\\
& \tiny{weights=\texttt{distance} n\_neighbors=$1$ p=$2$} &$0.839$ & $0.943$\\
& \tiny{weights=\texttt{uniform} n\_neighbors=$1$ p=$2$} &$0.839$ & $0.943$\\
& \tiny{weights=\texttt{uniform} n\_neighbors=$1$ p=$1$} &$0.838$ & $0.955$\\
& \tiny{weights=\texttt{distance} n\_neighbors=$1$ p=$1$} &$0.838$ & $0.955$\\
\midrule
LinearSVC & \tiny{loss=\texttt{hinge} C=$1$ multi\_class=\texttt{ovr} penalty=\texttt{l2}} &$0.836$ & $0.917$\\
& \tiny{loss=\texttt{hinge} C=$1$ multi\_class=\texttt{crammer\_singer} penalty=\texttt{l2}} &$0.835$ & $0.919$\\
& \tiny{loss=\texttt{squared\_hinge} C=$1$ multi\_class=\texttt{crammer\_singer} penalty=\texttt{l2}} &$0.834$ & $0.919$\\
& \tiny{loss=\texttt{squared\_hinge} C=$1$ multi\_class=\texttt{crammer\_singer} penalty=\texttt{l1}} &$0.833$ & $0.919$\\
& \tiny{loss=\texttt{hinge} C=$1$ multi\_class=\texttt{crammer\_singer} penalty=\texttt{l1}} &$0.833$ & $0.919$\\
& \tiny{loss=\texttt{squared\_hinge} C=$1$ multi\_class=\texttt{ovr} penalty=\texttt{l2}} &$0.820$ & $0.912$\\
& \tiny{loss=\texttt{squared\_hinge} C=$10$ multi\_class=\texttt{ovr} penalty=\texttt{l2}} &$0.779$ & $0.885$\\
& \tiny{loss=\texttt{squared\_hinge} C=$100$ multi\_class=\texttt{ovr} penalty=\texttt{l2}} &$0.776$ & $0.873$\\
& \tiny{loss=\texttt{hinge} C=$10$ multi\_class=\texttt{ovr} penalty=\texttt{l2}} &$0.764$ & $0.879$\\
& \tiny{loss=\texttt{hinge} C=$100$ multi\_class=\texttt{ovr} penalty=\texttt{l2}} &$0.758$ & $0.872$\\
& \tiny{loss=\texttt{hinge} C=$10$ multi\_class=\texttt{crammer\_singer} penalty=\texttt{l1}} &$0.751$ & $0.783$\\
& \tiny{loss=\texttt{hinge} C=$10$ multi\_class=\texttt{crammer\_singer} penalty=\texttt{l2}} &$0.749$ & $0.816$\\
& \tiny{loss=\texttt{squared\_hinge} C=$10$ multi\_class=\texttt{crammer\_singer} penalty=\texttt{l2}} &$0.748$ & $0.829$\\
& \tiny{loss=\texttt{squared\_hinge} C=$10$ multi\_class=\texttt{crammer\_singer} penalty=\texttt{l1}} &$0.736$ & $0.829$\\
& \tiny{loss=\texttt{hinge} C=$100$ multi\_class=\texttt{crammer\_singer} penalty=\texttt{l1}} &$0.516$ & $0.759$\\
& \tiny{loss=\texttt{hinge} C=$100$ multi\_class=\texttt{crammer\_singer} penalty=\texttt{l2}} &$0.496$ & $0.753$\\
& \tiny{loss=\texttt{squared\_hinge} C=$100$ multi\_class=\texttt{crammer\_singer} penalty=\texttt{l1}} &$0.492$ & $0.746$\\
& \tiny{loss=\texttt{squared\_hinge} C=$100$ multi\_class=\texttt{crammer\_singer} penalty=\texttt{l2}} &$0.484$ & $0.737$\\
\midrule
LogisticRegression & \tiny{C=$1$ multi\_class=\texttt{ovr} penalty=\texttt{l1}} &$0.842$ & $0.917$\\
& \tiny{C=$1$ multi\_class=\texttt{ovr} penalty=\texttt{l2}} &$0.841$ & $0.917$\\
& \tiny{C=$10$ multi\_class=\texttt{ovr} penalty=\texttt{l2}} &$0.839$ & $0.916$\\
& \tiny{C=$10$ multi\_class=\texttt{ovr} penalty=\texttt{l1}} &$0.839$ & $0.909$\\
& \tiny{C=$100$ multi\_class=\texttt{ovr} penalty=\texttt{l2}} &$0.836$ & $0.916$\\
\midrule
MLPClassifier & \tiny{activation=\texttt{relu} hidden\_layer\_sizes=\texttt{[100]}} &$0.871$ & $0.972$\\
& \tiny{activation=\texttt{relu} hidden\_layer\_sizes=\texttt{[100, 10]}} &$0.870$ & $0.972$\\
& \tiny{activation=\texttt{tanh} hidden\_layer\_sizes=\texttt{[100]}} &$0.868$ & $0.962$\\
& \tiny{activation=\texttt{tanh} hidden\_layer\_sizes=\texttt{[100, 10]}} &$0.863$ & $0.957$\\
& \tiny{activation=\texttt{relu} hidden\_layer\_sizes=\texttt{[10, 10]}} &$0.850$ & $0.936$\\
& \tiny{activation=\texttt{relu} hidden\_layer\_sizes=\texttt{[10]}} &$0.848$ & $0.933$\\
& \tiny{activation=\texttt{tanh} hidden\_layer\_sizes=\texttt{[10, 10]}} &$0.841$ & $0.921$\\
& \tiny{activation=\texttt{tanh} hidden\_layer\_sizes=\texttt{[10]}} &$0.840$ & $0.921$\\
\midrule
PassiveAggressiveClassifier & \tiny{C=$1$} &$0.776$ & $0.877$\\
& \tiny{C=$100$} &$0.775$ & $0.875$\\
& \tiny{C=$10$} &$0.773$ & $0.880$\\
\midrule
Perceptron & \tiny{penalty=\texttt{l1}} &$0.782$ & $0.887$\\
& \tiny{penalty=\texttt{l2}} &$0.754$ & $0.845$\\
& \tiny{penalty=\texttt{elasticnet}} &$0.726$ & $0.845$\\
\midrule
RandomForestClassifier & \tiny{n\_estimators=$100$ criterion=\texttt{entropy} max\_depth=$100$} &$0.873$ & $0.970$\\
& \tiny{n\_estimators=$100$ criterion=\texttt{gini} max\_depth=$100$} &$0.872$ & $0.970$\\
& \tiny{n\_estimators=$50$ criterion=\texttt{entropy} max\_depth=$100$} &$0.872$ & $0.968$\\
& \tiny{n\_estimators=$100$ criterion=\texttt{entropy} max\_depth=$50$} &$0.872$ & $0.969$\\
& \tiny{n\_estimators=$50$ criterion=\texttt{entropy} max\_depth=$50$} &$0.871$ & $0.967$\\
& \tiny{n\_estimators=$100$ criterion=\texttt{gini} max\_depth=$50$} &$0.871$ & $0.971$\\
& \tiny{n\_estimators=$50$ criterion=\texttt{gini} max\_depth=$50$} &$0.870$ & $0.968$\\
& \tiny{n\_estimators=$50$ criterion=\texttt{gini} max\_depth=$100$} &$0.869$ & $0.967$\\
& \tiny{n\_estimators=$10$ criterion=\texttt{entropy} max\_depth=$50$} &$0.853$ & $0.949$\\
& \tiny{n\_estimators=$10$ criterion=\texttt{entropy} max\_depth=$100$} &$0.852$ & $0.949$\\
& \tiny{n\_estimators=$10$ criterion=\texttt{gini} max\_depth=$50$} &$0.848$ & $0.948$\\
& \tiny{n\_estimators=$10$ criterion=\texttt{gini} max\_depth=$100$} &$0.847$ & $0.948$\\
& \tiny{n\_estimators=$50$ criterion=\texttt{entropy} max\_depth=$10$} &$0.838$ & $0.947$\\
& \tiny{n\_estimators=$100$ criterion=\texttt{entropy} max\_depth=$10$} &$0.838$ & $0.950$\\
& \tiny{n\_estimators=$100$ criterion=\texttt{gini} max\_depth=$10$} &$0.835$ & $0.949$\\
& \tiny{n\_estimators=$50$ criterion=\texttt{gini} max\_depth=$10$} &$0.834$ & $0.945$\\
& \tiny{n\_estimators=$10$ criterion=\texttt{entropy} max\_depth=$10$} &$0.828$ & $0.933$\\
& \tiny{n\_estimators=$10$ criterion=\texttt{gini} max\_depth=$10$} &$0.825$ & $0.930$\\
\midrule
SGDClassifier & \tiny{loss=\texttt{hinge} penalty=\texttt{l2}} &$0.819$ & $0.914$\\
& \tiny{loss=\texttt{perceptron} penalty=\texttt{l1}} &$0.818$ & $0.912$\\
& \tiny{loss=\texttt{modified\_huber} penalty=\texttt{l1}} &$0.817$ & $0.910$\\
& \tiny{loss=\texttt{modified\_huber} penalty=\texttt{l2}} &$0.816$ & $0.913$\\
& \tiny{loss=\texttt{log} penalty=\texttt{elasticnet}} &$0.816$ & $0.912$\\
& \tiny{loss=\texttt{hinge} penalty=\texttt{elasticnet}} &$0.816$ & $0.913$\\
& \tiny{loss=\texttt{squared\_hinge} penalty=\texttt{elasticnet}} &$0.815$ & $0.914$\\
& \tiny{loss=\texttt{hinge} penalty=\texttt{l1}} &$0.815$ & $0.911$\\
& \tiny{loss=\texttt{log} penalty=\texttt{l1}} &$0.815$ & $0.910$\\
& \tiny{loss=\texttt{perceptron} penalty=\texttt{l2}} &$0.814$ & $0.913$\\
& \tiny{loss=\texttt{perceptron} penalty=\texttt{elasticnet}} &$0.814$ & $0.912$\\
& \tiny{loss=\texttt{squared\_hinge} penalty=\texttt{l2}} &$0.814$ & $0.912$\\
& \tiny{loss=\texttt{modified\_huber} penalty=\texttt{elasticnet}} &$0.813$ & $0.914$\\
& \tiny{loss=\texttt{log} penalty=\texttt{l2}} &$0.813$ & $0.913$\\
& \tiny{loss=\texttt{squared\_hinge} penalty=\texttt{l1}} &$0.813$ & $0.911$\\
\midrule
SVC & \tiny{C=$10$ kernel=\texttt{rbf}} &$0.897$ & $0.973$\\
& \tiny{C=$10$ kernel=\texttt{poly}} &$0.891$ & $0.976$\\
& \tiny{C=$100$ kernel=\texttt{poly}} &$0.890$ & $0.978$\\
& \tiny{C=$100$ kernel=\texttt{rbf}} &$0.890$ & $0.972$\\
& \tiny{C=$1$ kernel=\texttt{rbf}} &$0.879$ & $0.966$\\
& \tiny{C=$1$ kernel=\texttt{poly}} &$0.873$ & $0.957$\\
& \tiny{C=$1$ kernel=\texttt{linear}} &$0.839$ & $0.929$\\
& \tiny{C=$10$ kernel=\texttt{linear}} &$0.829$ & $0.927$\\
& \tiny{C=$100$ kernel=\texttt{linear}} &$0.827$ & $0.926$\\
& \tiny{C=$1$ kernel=\texttt{sigmoid}} &$0.678$ & $0.898$\\
& \tiny{C=$10$ kernel=\texttt{sigmoid}} &$0.671$ & $0.873$\\
& \tiny{C=$100$ kernel=\texttt{sigmoid}} &$0.664$ & $0.868$\\
\end{longtable}
