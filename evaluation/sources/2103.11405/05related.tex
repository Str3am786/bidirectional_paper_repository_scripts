\section{Related Work}
\paragraph{Non-autoregressive Machine Translation.}
\citet{nat} first develop a non-autoregressive Transformer~(NAT) for machine translation, which produces the outputs in parallel, and the inference speed is thus significantly boosted. 
Due to the missing of dependencies among the target outputs, the translation quality is largely sacrificed.

A line of work proposes to mitigate such performance degradation by enhancing the decoder inputs.  
\citet{iter_nat} propose a method of iterative refinement based on the previous outputs. 
\citet{enat} enhance decoder input by introducing the phrase table in statistical machine translation and embedding transformation. 
There are also some work focuses on improving the decoder inputs' supervision, including imitation learning from autoregressive models~\citep{imitate_nat} or regularizing the hidden state with backward reconstruction error~\citep{nat_reg}. 

Another work proposes modeling the dependencies among target outputs, which is explicitly missed in the vanilla NAT models. 
\citet{glat,cmlm} propose to model the target-side dependencies with a masked language model, modeling the directed dependencies between the observed target and the unobserved words. 
Different from their work, we model the target-side dependencies in the latent space, which follows the latent variable Transformer fashion.

\paragraph{Latent Variable Transformer.} 
More close to our work is the latent variable Transformer, which takes the latent variable as inputs to modeling the target-side information. 
\citet{lv_nar} combine continuous latent variables and deterministic inference procedure to find the target sequence that maximizes the lower bound to the log-probability. 
\citet{flowseq} propose to use generative flows to the model complex prior distribution. 
\citet{lt} propose to autoregressively decode a shorter latent sequence encoded from the target sentence, then simultaneously generate the sentence from the latent sequence. 
\citet{pnat} model the target position of decode input as a latent variable and introduce a heuristic search algorithm to guide the position learning. 
\citet{syn_st} first autoregressively predict a chunked parse tree and then simultaneously generate the target tokens from the predicted syntax.

% such as~\citet{lt} and~\citet{vqvae} utilize discrete latent variables for making decoding more parallelizable. 
% \baoy{Since our work builds upon the latent Transformer, we mainly survey the related work to the latent-variable based models in NAT. Such as the continuous latent variables or discrete latent variables.}

% \paragraph{Continuous Latent Variables in the Non-Autoregressive Transformer}
% \begin{itemize}
%     \item LV-NAR
%     \item Flowseq
%     \item or more recent works
% \end{itemize}

% \textbf{Discreted Latent Variables in the Non-Autoregressive Transformer}
% \begin{itemize}
%     \item LT, VQ-VAE
%     \item PNAT
%     \item Syn-NAT
% \end{itemize}