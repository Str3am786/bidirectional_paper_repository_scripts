\section{Conclusion}
We propose~\method, which implicitly models the categorical codes of the target language, narrowing the performance gap between the non-autoregressive decoding and autoregressive decoding.
Specifically, \method builds upon the latent Transformer and models the target-side categorical information with vector quantization and conditional random fields~(CRF) model. 
We further employ a gated neural network to form the decoder inputs. Equipped with the scheduled sampling, \method works more robust.
As a result, the \method achieves a significant improvement and moves closer to the performance of the Transformer on machine translation.