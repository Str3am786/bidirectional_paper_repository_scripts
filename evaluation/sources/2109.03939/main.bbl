\begin{thebibliography}{55}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Bai et~al.(2019)Bai, Kolter, and Koltun}]{bai2019deep}
Shaojie Bai, J~Zico Kolter, and Vladlen Koltun. 2019.
\newblock Deep equilibrium models.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:690--701.

\bibitem[{Baum(1988)}]{Baum:1988jc}
Eric~B Baum. 1988.
\newblock On the capabilities of multilayer perceptrons.
\newblock \emph{Journal of complexity}, 4(3):193--215.

\bibitem[{Bengio et~al.(2013)Bengio, L{\'e}onard, and
  Courville}]{bengio2013estimating}
Yoshua Bengio, Nicholas L{\'e}onard, and Aaron Courville. 2013.
\newblock Estimating or propagating gradients through stochastic neurons for
  conditional computation.
\newblock \emph{arXiv preprint arXiv:1308.3432}.

\bibitem[{Bojar et~al.(2014)Bojar, Buck, Federmann, Haddow, Koehn, Leveling,
  Monz, Pecina, Post, Saint-Amand, Soricut, Specia, and
  Tamchyna}]{bojar:2014-findings}
Ond{\v{r}}ej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp
  Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve
  Saint-Amand, Radu Soricut, Lucia Specia, and Ale{\v{s}} Tamchyna. 2014.
\newblock Findings of the 2014 workshop on statistical machine translation.
\newblock In \emph{Proceedings of the Ninth Workshop on Statistical Machine
  Translation}, Baltimore, Maryland, USA. Association for Computational
  Linguistics.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell et~al.}]{brown2020language}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al. 2020.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}.

\bibitem[{Cettolo et~al.(2015)Cettolo, Niehues, St{\"u}ker, Bentivogli, and
  Federico}]{Cettolo:2015ReportOT}
M.~Cettolo, J.~Niehues, S.~St{\"u}ker, L.~Bentivogli, and Marcello Federico.
  2015.
\newblock Report on the 11 th iwslt evaluation campaign , iwslt 2014.
\newblock In \emph{Proceedings of IWSLT}.

\bibitem[{Chen et~al.(2020)Chen, Frankle, Chang, Liu, Zhang, Wang, and
  Carbin}]{chen2020lottery}
Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang
  Wang, and Michael Carbin. 2020.
\newblock The lottery ticket hypothesis for pre-trained bert networks.
\newblock \emph{arXiv preprint arXiv:2007.12223}.

\bibitem[{Choromanski et~al.(2020)Choromanski, Likhosherstov, Dohan, Song,
  Davis, Sarlos, Belanger, Colwell, and Weller}]{Choromanski:2020performer}
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Jared
  Davis, Tamas Sarlos, David Belanger, Lucy Colwell, and Adrian Weller. 2020.
\newblock Masked language modeling for proteins via linearly scalable
  long-context transformers.
\newblock \emph{arXiv preprint arXiv:2006.03555}.

\bibitem[{Conneau et~al.(2017)Conneau, Kiela, Schwenk, Barrault, and
  Bordes}]{Conneau:2017infersent}
Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes.
  2017.
\newblock Supervised learning of universal sentence representations from
  natural language inference data.
\newblock \emph{arXiv preprint arXiv:1705.02364}.

\bibitem[{Dehghani et~al.(2018)Dehghani, Gouws, Vinyals, Uszkoreit, and
  Kaiser}]{dehghani2018universal}
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz
  Kaiser. 2018.
\newblock Universal transformers.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Devlin et~al.(2018)Devlin, Chang, Lee, and
  Toutanova}]{Devlin:2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}.

\bibitem[{Edunov et~al.(2018)Edunov, Ott, Auli, Grangier, and
  Ranzato}]{edunov:2018classical}
Sergey Edunov, Myle Ott, Michael Auli, David Grangier, and Marc{'}Aurelio
  Ranzato. 2018.
\newblock Classical structured prediction losses for sequence to sequence
  learning.
\newblock In \emph{Proceedings of the 2018 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long Papers)}, New Orleans, Louisiana. Association
  for Computational Linguistics.

\bibitem[{Fan et~al.(2019)Fan, Grave, and Joulin}]{fan2019reducing}
Angela Fan, Edouard Grave, and Armand Joulin. 2019.
\newblock Reducing transformer depth on demand with structured dropout.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Frankle and Carbin(2018)}]{frankle2018lottery}
Jonathan Frankle and Michael Carbin. 2018.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock \emph{arXiv preprint arXiv:1803.03635}.

\bibitem[{Frankle et~al.(2020)Frankle, Dziugaite, Roy, and
  Carbin}]{frankle2020linear}
Jonathan Frankle, Gintare~Karolina Dziugaite, Daniel Roy, and Michael Carbin.
  2020.
\newblock Linear mode connectivity and the lottery ticket hypothesis.
\newblock In \emph{International Conference on Machine Learning}, pages
  3259--3269. PMLR.

\bibitem[{Gaier and Ha(2019)}]{gaier2019weight}
Adam Gaier and David Ha. 2019.
\newblock Weight agnostic neural networks.
\newblock \emph{arXiv preprint arXiv:1906.04358}.

\bibitem[{Gallicchio and Micheli(2017)}]{Gallicchio:2017echo}
Claudio Gallicchio and Alessio Micheli. 2017.
\newblock Echo state property of deep reservoir computing networks.
\newblock \emph{Cognitive Computation}, 9(3):337--350.

\bibitem[{Gamba et~al.(1961)Gamba, Gamberini, Palmieri, and
  Sanna}]{Gamba:1961papa}
A.~Gamba, L.~Gamberini, G.~Palmieri, and R.~Sanna. 1961.
\newblock Further experiments with papa.
\newblock \emph{Il Nuovo Cimento (1955-1965)}, 20(2):112--115.

\bibitem[{Gan et~al.(2021)Gan, Chen, Li, Chen, Cheng, Wang, and
  Liu}]{gan2021playing}
Zhe Gan, Yen-Chun Chen, Linjie Li, Tianlong Chen, Yu~Cheng, Shuohang Wang, and
  Jingjing Liu. 2021.
\newblock Playing lottery tickets with vision and language.
\newblock \emph{arXiv preprint arXiv:2104.11832}.

\bibitem[{Iyer et~al.(2017)Iyer, Dandekar, and Csernai}]{iyer2017qqp}
Shankar Iyer, Nikhil Dandekar, and Kornl Csernai. 2017.
\newblock First quora dataset release: Question pairs, 2017.
\newblock \emph{URL https://data. quora.
  com/First-Quora-Dataset-Release-Question-Pairs}.

\bibitem[{Jaeger(2003)}]{Jaeger:2003echostate}
Herbert Jaeger. 2003.
\newblock Adaptive nonlinear system identification with echo state networks.
\newblock In \emph{Advances in neural information processing systems}.

\bibitem[{Jiao et~al.(2020)Jiao, Yin, Shang, Jiang, Chen, Li, Wang, and
  Liu}]{jiao2020tinybert}
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang
  Wang, and Qun Liu. 2020.
\newblock Tinybert: Distilling bert for natural language understanding.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: Findings}, pages 4163--4174.

\bibitem[{Lan et~al.(2020)Lan, Chen, Goodman, Gimpel, Sharma, and
  Soricut}]{lan2020albert}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
  Radu Soricut. 2020.
\newblock Albert: A lite bert for self-supervised learning of language
  representations.

\bibitem[{Li et~al.(2020)Li, Wallace, Shen, Lin, Keutzer, Klein, and
  Gonzalez}]{li2020train}
Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and
  Joey Gonzalez. 2020.
\newblock Train big, then compress: Rethinking model size for efficient
  training and inference of transformers.
\newblock In \emph{International Conference on Machine Learning}. PMLR.

\bibitem[{Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov}]{Liu:2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}.

\bibitem[{Luko{\v{s}}evi{\v{c}}ius and
  Jaeger(2009)}]{Lukovsevivcius:2009reservoir}
Mantas Luko{\v{s}}evi{\v{c}}ius and Herbert Jaeger. 2009.
\newblock Reservoir computing approaches to recurrent neural network training.
\newblock \emph{Computer Science Review}, 3(3).

\bibitem[{Maass et~al.(2002)Maass, Natschl{\"a}ger, and
  Markram}]{Maass:2002lsm}
Wolfgang Maass, Thomas Natschl{\"a}ger, and Henry Markram. 2002.
\newblock Real-time computing without stable states: A new framework for neural
  computation based on perturbations.
\newblock \emph{Neural computation}, 14(11):2531--2560.

\bibitem[{Michel et~al.(2019)Michel, Levy, and Neubig}]{michel2019sixteen}
Paul Michel, Omer Levy, and Graham Neubig. 2019.
\newblock Are sixteen heads really better than one?
\newblock \emph{Advances in Neural Information Processing Systems},
  32:14014--14024.

\bibitem[{Mikolov et~al.(2013)Mikolov, Chen, Corrado, and
  Dean}]{mikolov2013efficient}
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013.
\newblock Efficient estimation of word representations in vector space.
\newblock \emph{arXiv preprint arXiv:1301.3781}.

\bibitem[{Ott et~al.(2018)Ott, Edunov, Grangier, and Auli}]{Ott:2018scaling}
Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. 2018.
\newblock Scaling neural machine translation.
\newblock \emph{arXiv preprint arXiv:1806.00187}.

\bibitem[{Pao et~al.(1994)Pao, Park, and Sobajic}]{Pao:1994nc}
Yoh-Han Pao, Gwang-Hoon Park, and Dejan~J Sobajic. 1994.
\newblock Learning and generalization characteristics of the random vector
  functional-link net.
\newblock \emph{Neurocomputing}, 6(2):163--180.

\bibitem[{Peng et~al.(2021)Peng, Pappas, Yogatama, Schwartz, Smith, and
  Kong}]{peng2021random}
Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and
  Lingpeng Kong. 2021.
\newblock Random feature attention.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Pennington et~al.(2014)Pennington, Socher, and
  Manning}]{pennington-etal-2014-glove}
Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014.
\newblock \href {https://doi.org/10.3115/v1/D14-1162} {{G}lo{V}e: Global
  vectors for word representation}.
\newblock In \emph{Proceedings of the 2014 Conference on Empirical Methods in
  Natural Language Processing ({EMNLP})}, pages 1532--1543, Doha, Qatar.
  Association for Computational Linguistics.

\bibitem[{Pilault et~al.(2020)Pilault, Park, and Pal}]{Pilault:2020impressive}
Jonathan Pilault, Jaehong Park, and Christopher Pal. 2020.
\newblock On the impressive performance of randomly weighted encoders in
  summarization tasks.
\newblock \emph{arXiv preprint arXiv:2002.09084}.

\bibitem[{Prasanna et~al.(2020)Prasanna, Rogers, and
  Rumshisky}]{prasanna2020bert}
Sai Prasanna, Anna Rogers, and Anna Rumshisky. 2020.
\newblock When bert plays the lottery, all tickets are winning.
\newblock \emph{arXiv preprint arXiv:2005.00561}.

\bibitem[{Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu}]{raffel2019exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu. 2019.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{arXiv preprint arXiv:1910.10683}.

\bibitem[{Rahimi and Recht(2008)}]{Rahimi:2008random}
Ali Rahimi and Benjamin Recht. 2008.
\newblock Random features for large-scale kernel machines.
\newblock In \emph{Advances in neural information processing systems}, pages
  1177--1184.

\bibitem[{Rahimi and Recht(2009)}]{Rahimi:2009kitchen}
Ali Rahimi and Benjamin Recht. 2009.
\newblock Weighted sums of random kitchen sinks: Replacing minimization with
  randomization in learning.
\newblock In \emph{Advances in neural information processing systems}, pages
  1313--1320.

\bibitem[{Ramanujan et~al.(2020)Ramanujan, Wortsman, Kembhavi, Farhadi, and
  Rastegari}]{Ramanujan:2020hidden}
Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, and
  Mohammad Rastegari. 2020.
\newblock What's hidden in a randomly weighted neural network?
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 11893--11902.

\bibitem[{Renda et~al.(2020)Renda, Frankle, and Carbin}]{renda2020comparing}
Alex Renda, Jonathan Frankle, and Michael Carbin. 2020.
\newblock Comparing rewinding and fine-tuning in neural network pruning.
\newblock \emph{arXiv preprint arXiv:2003.02389}.

\bibitem[{Sanh et~al.(2020)Sanh, Wolf, and Rush}]{sanh2020movement}
Victor Sanh, Thomas Wolf, and Alexander Rush. 2020.
\newblock Movement pruning: Adaptive sparsity by fine-tuning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33.

\bibitem[{Scardapane and Wang(2017)}]{Scardapane:2017randomness}
Simone Scardapane and Dianhui Wang. 2017.
\newblock Randomness in neural networks: an overview.
\newblock \emph{Wiley Interdisciplinary Reviews: Data Mining and Knowledge
  Discovery}, 7(2):e1200.

\bibitem[{Schmidt et~al.(1992)Schmidt, Kraaijveld, and Duin}]{Schmidt:1992pr}
Wouter~F Schmidt, Martin~A Kraaijveld, and Robert~PW Duin. 1992.
\newblock Feedforward neural networks with random weights.
\newblock In \emph{Proceedings of the 11th International Conference on Pattern
  Recognition, 1992. Vol. II. Conference B: Pattern Recognition Methodology and
  Systems}, pages 1--4.

\bibitem[{Shen et~al.(2021)Shen, Baevski, Morcos, Keutzer, Auli, and
  Kiela}]{shen2020reservoir}
Sheng Shen, Alexei Baevski, Ari~S Morcos, Kurt Keutzer, Michael Auli, and Douwe
  Kiela. 2021.
\newblock Reservoir transformers.
\newblock In \emph{ACL}.

\bibitem[{Shen et~al.(2020)Shen, Dong, Ye, Ma, Yao, Gholami, Mahoney, and
  Keutzer}]{shen2020q}
Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,
  Michael~W Mahoney, and Kurt Keutzer. 2020.
\newblock Q-bert: Hessian based ultra low precision quantization of bert.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 8815--8821.

\bibitem[{Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and
  Catanzaro}]{shoeybi2019megatron}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
  and Bryan Catanzaro. 2019.
\newblock {Megatron-LM}: Training multi-billion parameter language models using
  gpu model parallelism.
\newblock \emph{arXiv preprint arXiv:1909.08053}.

\bibitem[{Sun et~al.(2020)Sun, Yu, Song, Liu, Yang, and
  Zhou}]{sun2020mobilebert}
Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.
  2020.
\newblock Mobilebert: a compact task-agnostic bert for resource-limited
  devices.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 2158--2170.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
  Gomez, Kaiser, and Polosukhin}]{Vaswani:2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin. 2017.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pages
  5998--6008.

\bibitem[{Voita and Titov(2020)}]{voita2020information}
Elena Voita and Ivan Titov. 2020.
\newblock Information-theoretic probing with minimum description length.
\newblock \emph{arXiv preprint arXiv:2003.12298}.

\bibitem[{Wieting and Kiela(2019)}]{Wieting:2019notraining}
John Wieting and Douwe Kiela. 2019.
\newblock No training required: Exploring random encoders for sentence
  classification.
\newblock \emph{arXiv preprint arXiv:1901.10444}.

\bibitem[{Williams et~al.(2017)Williams, Nangia, and Bowman}]{Williams2017mnli}
Adina Williams, Nikita Nangia, and Samuel~R Bowman. 2017.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock \emph{arXiv preprint arXiv:1704.05426}.

\bibitem[{Wortsman et~al.(2020)Wortsman, Ramanujan, Liu, Kembhavi, Rastegari,
  Yosinski, and Farhadi}]{wortsman2020supermasks}
Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu, Aniruddha Kembhavi, Mohammad
  Rastegari, Jason Yosinski, and Ali Farhadi. 2020.
\newblock Supermasks in superposition for continual learning.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  6.

\bibitem[{Yao et~al.(2021)Yao, Ma, Shen, Keutzer, and
  Mahoney}]{yao2021mlpruning}
Zhewei Yao, Linjian Ma, Sheng Shen, Kurt Keutzer, and Michael~W Mahoney. 2021.
\newblock Mlpruning: A multilevel structured pruning framework for
  transformer-based models.
\newblock \emph{arXiv preprint arXiv:2105.14636}.

\bibitem[{Yu et~al.(2019)Yu, Edunov, Tian, and Morcos}]{yu2019playing}
Haonan Yu, Sergey Edunov, Yuandong Tian, and Ari~S Morcos. 2019.
\newblock Playing the lottery with rewards and multiple languages: lottery
  tickets in rl and nlp.
\newblock \emph{arXiv preprint arXiv:1906.02768}.

\bibitem[{Zhou et~al.(2019)Zhou, Lan, Liu, and
  Yosinski}]{Zhou:2019deconstructing}
Hattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosinski. 2019.
\newblock Deconstructing lottery tickets: Zeros, signs, and the supermask.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3597--3607.

\end{thebibliography}
