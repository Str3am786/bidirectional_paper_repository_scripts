



@article{prasanna2020bert,
  title={When bert plays the lottery, all tickets are winning},
  author={Prasanna, Sai and Rogers, Anna and Rumshisky, Anna},
  journal={arXiv preprint arXiv:2005.00561},
  year={2020}
}

@article{voita2020information,
  title={Information-theoretic probing with minimum description length},
  author={Voita, Elena and Titov, Ivan},
  journal={arXiv preprint arXiv:2003.12298},
  year={2020}
}

@article{gaier2019weight,
  title={Weight agnostic neural networks},
  author={Gaier, Adam and Ha, David},
  journal={arXiv preprint arXiv:1906.04358},
  year={2019}
}

@article{frankle2018lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:1803.03635},
  year={2018}
}

@article{chen2020lottery,
  title={The lottery ticket hypothesis for pre-trained bert networks},
  author={Chen, Tianlong and Frankle, Jonathan and Chang, Shiyu and Liu, Sijia and Zhang, Yang and Wang, Zhangyang and Carbin, Michael},
  journal={arXiv preprint arXiv:2007.12223},
  year={2020}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{shoeybi2019megatron,
  title={{Megatron-LM}: Training multi-billion parameter language models using gpu model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@article{raffel2019exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={arXiv preprint arXiv:1910.10683},
  year={2019}
}

@inproceedings{frankle2020linear,
  title={Linear mode connectivity and the lottery ticket hypothesis},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel and Carbin, Michael},
  booktitle={International Conference on Machine Learning},
  pages={3259--3269},
  year={2020},
  organization={PMLR}
}

@article{yu2019playing,
  title={Playing the lottery with rewards and multiple languages: lottery tickets in rl and nlp},
  author={Yu, Haonan and Edunov, Sergey and Tian, Yuandong and Morcos, Ari S},
  journal={arXiv preprint arXiv:1906.02768},
  year={2019}
}

@article{renda2020comparing,
  title={Comparing rewinding and fine-tuning in neural network pruning},
  author={Renda, Alex and Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:2003.02389},
  year={2020}
}

@inproceedings{Schrauwen:2007lsmhardware,
author = {Schrauwen, Benjamin and D'Haene, Michiel and Verstraeten, David and Campenhout, Jan},
year = {2007},
month = {09},
pages = {1097 - 1102},
title = {Compact hardware for real-time speech recognition using a Liquid State Machine},
journal = {IEEE International Conference on Neural Networks - Conference Proceedings},
doi = {10.1109/IJCNN.2007.4371111}
}

@inproceedings{jawahar-etal-2019-bert,
    title = "What Does {BERT} Learn about the Structure of Language?",
    author = "Jawahar, Ganesh  and
      Sagot, Beno{\^\i}t  and
      Seddah, Djam{\'e}",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    year = "2019",
}

@misc{frankle2020training,
    title={Training BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs},
    author={Jonathan Frankle and David J. Schwab and Ari S. Morcos},
    year={2020},
    eprint={2003.00152},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{ainslie-etal-2020-etc,
    title = "{ETC}: Encoding Long and Structured Inputs in Transformers",
    author = "Ainslie, Joshua  and
      Ontanon, Santiago  and
      Alberti, Chris  and
      Cvicek, Vaclav  and
      Fisher, Zachary  and
      Pham, Philip  and
      Ravula, Anirudh  and
      Sanghai, Sumit  and
      Wang, Qifan  and
      Yang, Li",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    year = "2020",
}

@inproceedings{
peng2021random,
title={Random Feature Attention},
author={Hao Peng and Nikolaos Pappas and Dani Yogatama and Roy Schwartz and Noah Smith and Lingpeng Kong},
booktitle={International Conference on Learning Representations},
year={2021},
}

@article{Hicke:2013semiconductor,
author = {Hicke, Konstantin and Escalona-Moran, Miguel and Brunner, Daniel and Soriano, Miguel and Fischer, Ingo and Mirasso, Claudio},
year = {2013},
month = {07},
pages = {1501610-1501610},
title = {Information Processing Using Transient Dynamics of Semiconductor Lasers Subject to Delayed Feedback},
volume = {19},
journal = {Selected Topics in Quantum Electronics, IEEE Journal of},
doi = {10.1109/JSTQE.2013.2241738}
}

@book{Hadaeghi:2017hardware,
  title={Unconventional Information Processing Systems, Novel Hardware: A Tour D'Horizon},
  author={Hadaeghi, Fatemeh and He, Xu and Jaeger, Herbert},
  year={2017}
}

@article{Tanaka:2019review,
title = "Recent advances in physical reservoir computing: A review",
journal = "Neural Networks",
volume = "115",
pages = "100 - 123",
year = "2019",
author = "Gouhei Tanaka and Toshiyuki Yamane and Jean Benoit Héroux and Ryosho Nakane and Naoki Kanazawa and Seiji Takeda and Hidetoshi Numata and Daiju Nakano and Akira Hirose"
}

@article{Garg:2020echostatenmt,
  title={Echo State Neural Machine Translation},
  author={Garg, Ankush and Cao, Yuan and Ge, Qi},
  journal={arXiv preprint arXiv:2002.11847},
  year={2020}
}

@article{Enguehard:2019neurallanguagepriors,
  title={Neural Language Priors},
  author={Enguehard, Joseph and Busbridge, Dan and Zhelezniak, Vitalii and Hammerla, Nils},
  journal={arXiv preprint arXiv:1910.03492},
  year={2019}
}

@article{Pilault:2020impressive,
  title={On the impressive performance of randomly weighted encoders in summarization tasks},
  author={Pilault, Jonathan and Park, Jaehong and Pal, Christopher},
  journal={arXiv preprint arXiv:2002.09084},
  year={2020}
}

@article{Wieting:2019notraining,
  title={No training required: Exploring random encoders for sentence classification},
  author={Wieting, John and Kiela, Douwe},
  journal={arXiv preprint arXiv:1901.10444},
  year={2019}
}
@article{bengio2013estimating,
  title={Estimating or propagating gradients through stochastic neurons for conditional computation},
  author={Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron},
  journal={arXiv preprint arXiv:1308.3432},
  year={2013}
}
@inproceedings{Ramanujan:2020hidden,
  title={What's Hidden in a Randomly Weighted Neural Network?},
  author={Ramanujan, Vivek and Wortsman, Mitchell and Kembhavi, Aniruddha and Farhadi, Ali and Rastegari, Mohammad},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11893--11902},
  year={2020}
}

@article{Frankle:2018lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:1803.03635},
  year={2018}
}

@inproceedings{Zhou:2019deconstructing,
  title={Deconstructing lottery tickets: Zeros, signs, and the supermask},
  author={Zhou, Hattie and Lan, Janice and Liu, Rosanne and Yosinski, Jason},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3597--3607},
  year={2019}
}

@article{Frankle:2020batchnorm,
  title={Training BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs},
  author={Frankle, Jonathan and Schwab, David J and Morcos, Ari S},
  journal={arXiv preprint arXiv:2003.00152},
  year={2020}
}

@inproceedings{Rahimi:2008random,
  title={Random features for large-scale kernel machines},
  author={Rahimi, Ali and Recht, Benjamin},
  booktitle={Advances in neural information processing systems},
  pages={1177--1184},
  year={2008}
}

@inproceedings{Rahimi:2009kitchen,
  title={Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning},
  author={Rahimi, Ali and Recht, Benjamin},
  booktitle={Advances in neural information processing systems},
  pages={1313--1320},
  year={2009}
}

@article{Maass:2002lsm,
  title={Real-time computing without stable states: A new framework for neural computation based on perturbations},
  author={Maass, Wolfgang and Natschl{\"a}ger, Thomas and Markram, Henry},
  journal={Neural computation},
  volume={14},
  number={11},
  pages={2531--2560},
  year={2002},
  publisher={MIT Press}
}

@inproceedings{Jaeger:2003echostate,
  title={Adaptive nonlinear system identification with echo state networks},
  author={Jaeger, Herbert},
  booktitle={Advances in neural information processing systems},
  year={2003}
}

@article{Lukovsevivcius:2009reservoir,
  title={Reservoir computing approaches to recurrent neural network training},
  author={Luko{\v{s}}evi{\v{c}}ius, Mantas and Jaeger, Herbert},
  journal={Computer Science Review},
  volume={3},
  number={3},
  year={2009},
  publisher={Elsevier}
}

@article{Rogers2020:primer,
  title={A primer in bertology: What we know about how bert works},
  author={Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  journal={arXiv preprint arXiv:2002.12327},
  year={2020}
}

@article{Tenney:2019bert,
  title={BERT rediscovers the classical NLP pipeline},
  author={Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
  journal={arXiv preprint arXiv:1905.05950},
  year={2019}
}

@article{Elbayad:2019depth,
  title={Depth-adaptive transformer},
  author={Elbayad, Maha and Gu, Jiatao and Grave, Edouard and Auli, Michael},
  journal={arXiv preprint arXiv:1910.10073},
  year={2019}
}

@article{Fan:2019reducing,
  title={Reducing transformer depth on demand with structured dropout},
  author={Fan, Angela and Grave, Edouard and Joulin, Armand},
  journal={arXiv preprint arXiv:1909.11556},
  year={2019}
}


@article{Bachlechner:2020rezero,
  title={Rezero is all you need: Fast convergence at large depth},
  author={Bachlechner, Thomas and Majumder, Bodhisattwa Prasad and Mao, Huanru Henry and Cottrell, Garrison W and McAuley, Julian},
  journal={arXiv preprint arXiv:2003.04887},
  year={2020}
}

@article{Li:2020traincompress,
  title={Train large, then compress: Rethinking model size for efficient training and inference of transformers},
  author={Li, Zhuohan and Wallace, Eric and Shen, Sheng and Lin, Kevin and Keutzer, Kurt and Klein, Dan and Gonzalez, Joseph E},
  journal={arXiv preprint arXiv:2002.11794},
  year={2020}
}

@article{Wang:2020linformer,
  title={Linformer: Self-Attention with Linear Complexity},
  author={Wang, Sinong and Li, Belinda and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@article{Kitaev:2020reformer,
  title={Reformer: The efficient transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal={arXiv preprint arXiv:2001.04451},
  year={2020}
}

@article{Beltagy:2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{Katharopoulos:2020linearattn,
  title={Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  journal={arXiv preprint arXiv:2006.16236},
  year={2020}
}

@article{Kasai:2020deepshallow,
  title={Deep encoder, shallow decoder: Reevaluating the speed-quality tradeoff in machine translation},
  author={Kasai, Jungo and Pappas, Nikolaos and Peng, Hao and Cross, James and Smith, Noah A},
  journal={arXiv preprint arXiv:2006.10369},
  year={2020}
}

@article{Brock:2017freezeout,
  title={Freezeout: Accelerate training by progressively freezing layers},
  author={Brock, Andrew and Lim, Theodore and Ritchie, James M and Weston, Nick},
  journal={arXiv preprint arXiv:1706.04983},
  year={2017}
}

@article{Scardapane:2017randomness,
  title={Randomness in neural networks: an overview},
  author={Scardapane, Simone and Wang, Dianhui},
  journal={Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  volume={7},
  number={2},
  pages={e1200},
  year={2017},
  publisher={Wiley Online Library}
}

@inproceedings{Daniely:2016randominit,
  title={Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity},
  author={Daniely, Amit and Frostig, Roy and Singer, Yoram},
  booktitle={Advances In Neural Information Processing Systems},
  pages={2253--2261},
  year={2016}
}

@article{Giryes:2016metriclearning,
  title={Deep neural networks with random gaussian weights: A universal classification strategy?},
  author={Giryes, Raja and Sapiro, Guillermo and Bronstein, Alex M},
  journal={IEEE Transactions on Signal Processing},
  volume={64},
  number={13},
  pages={3444--3457},
  year={2016},
  publisher={IEEE}
}

@inproceedings{Ulyanov:2018deepimageprior,
  title={Deep image prior},
  author={Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={9446--9454},
  year={2018}
}

@article{Strubell:2019energy,
  title={Energy and policy considerations for deep learning in NLP},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  journal={arXiv preprint arXiv:1906.02243},
  year={2019}
}

@inproceedings{Vaswani:2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@inproceedings{Li:2018overparameterized,
  title={Learning overparameterized neural networks via stochastic gradient descent on structured data},
  author={Li, Yuanzhi and Liang, Yingyu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8157--8166},
  year={2018}
}

@article{Kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@inproceedings{Du2019gradient,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International Conference on Machine Learning},
  pages={1675--1685},
  year={2019}
}

@inproceedings{Rosenfeld:2019intriguing,
  title={Intriguing properties of randomly weighted networks: Generalizing while learning next to nothing},
  author={Rosenfeld, Amir and Tsotsos, John K},
  booktitle={2019 16th Conference on Computer and Robot Vision (CRV)},
  pages={9--16},
  year={2019},
  organization={IEEE}
}

@inproceedings{Pons2019randomly,
  title={Randomly weighted CNNs for (music) audio classification},
  author={Pons, Jordi and Serra, Xavier},
  booktitle={ICASSP 2019-2019 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={336--340},
  year={2019},
  organization={IEEE}
}

@article{Zhang:2019alllayersequal,
  title={Are all layers created equal?},
  author={Zhang, Chiyuan and Bengio, Samy and Singer, Yoram},
  journal={arXiv preprint arXiv:1902.01996},
  year={2019}
}

@incollection{Gallicchio:2020survey,
  title={Deep Randomized Neural Networks},
  author={Gallicchio, Claudio and Scardapane, Simone},
  booktitle={Recent Trends in Learning From Data},
  pages={43--68},
  year={2020},
  publisher={Springer}
}

@article{Gallicchio:2017echo,
  title={Echo state property of deep reservoir computing networks},
  author={Gallicchio, Claudio and Micheli, Alessio},
  journal={Cognitive Computation},
  volume={9},
  number={3},
  pages={337--350},
  year={2017},
  publisher={Springer}
}

@article{Devlin:2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{Radford:2019gpt2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year=2018
}

@article{Brown:2020gpt3,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{Ott:2018scaling,
  title={Scaling neural machine translation},
  author={Ott, Myle and Edunov, Sergey and Grangier, David and Auli, Michael},
  journal={arXiv preprint arXiv:1806.00187},
  year={2018}
}

@misc{Mahoney:2011text8,
  title={Large text compression benchmark},
  author={Mahoney, Matt},
  year=2011
}

@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@inproceedings{pennington-etal-2014-glove,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}

@article{iyer2017qqp,
  title={First Quora dataset release: Question pairs, 2017},
  author={Iyer, Shankar and Dandekar, Nikhil and Csernai, Kornl},
  journal={URL https://data. quora. com/First-Quora-Dataset-Release-Question-Pairs},
  year={2017}
}

@inproceedings{Glorot:2010init,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010}
}

@article{michel2019sixteen,
  title={Are Sixteen Heads Really Better than One?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={14014--14024},
  year={2019}
}

@inproceedings{fan2019reducing,
  title={Reducing Transformer Depth on Demand with Structured Dropout},
  author={Fan, Angela and Grave, Edouard and Joulin, Armand},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{yao2021mlpruning,
  title={MLPruning: A Multilevel Structured Pruning Framework for Transformer-based Models},
  author={Yao, Zhewei and Ma, Linjian and Shen, Sheng and Keutzer, Kurt and Mahoney, Michael W},
  journal={arXiv preprint arXiv:2105.14636},
  year={2021}
}

@article{bai2019deep,
  title={Deep Equilibrium Models},
  author={Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={690--701},
  year={2019}
}

@inproceedings{dehghani2018universal,
  title={Universal Transformers},
  author={Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, Lukasz},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{sanh2020movement,
  title={Movement Pruning: Adaptive Sparsity by Fine-Tuning},
  author={Sanh, Victor and Wolf, Thomas and Rush, Alexander},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@inproceedings{shen2020q,
  title={Q-bert: Hessian based ultra low precision quantization of bert},
  author={Shen, Sheng and Dong, Zhen and Ye, Jiayu and Ma, Linjian and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={05},
  pages={8815--8821},
  year={2020}
}

@article{lan2020albert,
  title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  year={2020}
}

@inproceedings{sun2020mobilebert,
  title={MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices},
  author={Sun, Zhiqing and Yu, Hongkun and Song, Xiaodan and Liu, Renjie and Yang, Yiming and Zhou, Denny},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={2158--2170},
  year={2020}
}

@inproceedings{jiao2020tinybert,
  title={TinyBERT: Distilling BERT for Natural Language Understanding},
  author={Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings},
  pages={4163--4174},
  year={2020}
}

@article{gan2021playing,
  title={Playing lottery tickets with vision and language},
  author={Gan, Zhe and Chen, Yen-Chun and Li, Linjie and Chen, Tianlong and Cheng, Yu and Wang, Shuohang and Liu, Jingjing},
  journal={arXiv preprint arXiv:2104.11832},
  year={2021}
}

@article{wortsman2020supermasks,
  title={Supermasks in superposition for continual learning},
  author={Wortsman, Mitchell and Ramanujan, Vivek and Liu, Rosanne and Kembhavi, Aniruddha and Rastegari, Mohammad and Yosinski, Jason and Farhadi, Ali},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={6},
  year={2020}
}

@inproceedings{li2020train,
  title={Train big, then compress: Rethinking model size for efficient training and inference of transformers},
  author={Li, Zhuohan and Wallace, Eric and Shen, Sheng and Lin, Kevin and Keutzer, Kurt and Klein, Dan and Gonzalez, Joey},
  booktitle={International Conference on Machine Learning},
  year={2020},
  organization={PMLR}
}

@inproceedings{shen2020reservoir,
  title={Reservoir Transformers},
  author={Shen, Sheng and Baevski, Alexei and Morcos, Ari S and Keutzer, Kurt and Auli, Michael and Kiela, Douwe},
  booktitle={ACL},
  year={2021}
}

@inproceedings{Ott:2019fairseq,
  title = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
  author = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},
  booktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},
  year = {2019},
}

@article{Cover:1965theorem,
  title={Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition},
  author={Cover, Thomas M},
  journal={IEEE transactions on electronic computers},
  number={3},
  pages={326--334},
  year={1965},
  publisher={IEEE}
}

@article{Johnson:1984extensions,
  title={Extensions of Lipschitz mappings into a Hilbert space},
  author={Johnson, William B and Lindenstrauss, Joram},
  journal={Contemporary mathematics},
  volume={26},
  number={189-206},
  pages={1},
  year={1984}
}

@inproceedings{Sahlgren:2005randomindexing,
  title={An introduction to random indexing},
  author={Sahlgren, Magnus},
  booktitle={Methods and applications of semantic indexing workshop at the 7th international conference on terminology and knowledge engineering},
  year={2005}
}

@article{Liu:2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{Jaderberg:2017synthetic,
  title={Decoupled neural interfaces using synthetic gradients},
  author={Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Silver, David and Kavukcuoglu, Koray},
  booktitle={International Conference on Machine Learning},
  pages={1627--1635},
  year={2017},
  organization={PMLR}
}

@article{Neftci:2017randombackprop,
  title={Event-driven random back-propagation: Enabling neuromorphic deep learning machines},
  author={Neftci, Emre O and Augustine, Charles and Paul, Somnath and Detorakis, Georgios},
  journal={Frontiers in neuroscience},
  volume={11},
  pages={324},
  year={2017},
  publisher={Frontiers}
}

@article{Czarnecki:2017dni,
  title={Understanding synthetic gradients and decoupled neural interfaces},
  author={Czarnecki, Wojciech Marian and {\'S}wirszcz, Grzegorz and Jaderberg, Max and Osindero, Simon and Vinyals, Oriol and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1703.00522},
  year={2017}
}

@article{Tay:2020transformersurvey,
  title={Efficient Transformers: A Survey},
  author={Yi Tay and Mostafa Dehghani and Dara Bahri and Donald Metzler},
  journal={arXiv preprint arXiv:2009.06732},
  year={2020}
}

@article{Schwartz:2019greenai,
  title={Green ai},
  author={Schwartz, Roy and Dodge, Jesse and Smith, Noah A and Etzioni, Oren},
  journal={arXiv preprint arXiv:1907.10597},
  year={2019}
}

@article{Oktay:2020randomizedautodiff,
  title={Randomized Automatic Differentiation},
  author={Oktay, Deniz and McGreivy, Nick and Aduol, Joshua and Beatson, Alex and Adams, Ryan P},
  journal={arXiv preprint arXiv:2007.10412},
  year={2020}
}

@article{Conneau:2017infersent,
  title={Supervised learning of universal sentence representations from natural language inference data},
  author={Conneau, Alexis and Kiela, Douwe and Schwenk, Holger and Barrault, Loic and Bordes, Antoine},
  journal={arXiv preprint arXiv:1705.02364},
  year={2017}
}

@inproceedings{bojar:2014-findings,
    title = "Findings of the 2014 Workshop on Statistical Machine Translation",
    author = "Bojar, Ond{\v{r}}ej  and
      Buck, Christian  and
      Federmann, Christian  and
      Haddow, Barry  and
      Koehn, Philipp  and
      Leveling, Johannes  and
      Monz, Christof  and
      Pecina, Pavel  and
      Post, Matt  and
      Saint-Amand, Herve  and
      Soricut, Radu  and
      Specia, Lucia  and
      Tamchyna, Ale{\v{s}}",
    booktitle = "Proceedings of the Ninth Workshop on Statistical Machine Translation",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland, USA",
    publisher = "Association for Computational Linguistics",
}

@inproceedings{Cettolo:2015ReportOT,
  title={Report on the 11 th IWSLT Evaluation Campaign , IWSLT 2014},
  author={M. Cettolo and J. Niehues and S. St{\"u}ker and L. Bentivogli and Marcello Federico},
  year={2015},
  booktitle={Proceedings of IWSLT},
}

@inproceedings{LLC:2009long,
  title={Large text compression benchmark},
  author={LLC, MultiMedia},
  year={2009},
}


@inproceedings{dai:2019-transformer,
    title = "Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context",
    author = "Dai, Zihang  and
      Yang, Zhilin  and
      Yang, Yiming  and
      Carbonell, Jaime  and
      Le, Quoc  and
      Salakhutdinov, Ruslan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
}

@inproceedings{edunov:2018classical,
    title = "Classical Structured Prediction Losses for Sequence to Sequence Learning",
    author = "Edunov, Sergey  and
      Ott, Myle  and
      Auli, Michael  and
      Grangier, David  and
      Ranzato, Marc{'}Aurelio",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
}

@inproceedings{Zhang:2018language,
  title={Language modeling teaches you more than translation does: Lessons learned through auxiliary syntactic task analysis},
  author={Zhang, Kelly and Bowman, Samuel},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  year={2018}
}

@article{Zaheer2020big,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={arXiv preprint arXiv:2007.14062},
  year={2020}
}

@article{Choromanski:2020performer,
  title={Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Davis, Jared and Sarlos, Tamas and Belanger, David and Colwell, Lucy and Weller, Adrian},
  journal={arXiv preprint arXiv:2006.03555},
  year={2020}
}

@article{Tay:2020synthesizer,
  title={Synthesizer: Rethinking Self-Attention in Transformer Models},
  author={Tay, Yi and Bahri, Dara and Metzler, Donald and Juan, Da-Cheng and Zhao, Zhe and Zheng, Che},
  journal={arXiv preprint arXiv:2005.00743},
  year={2020}
}

@article{Block:1962perceptron,
  title={The perceptron: A model for brain functioning. i},
  author={Block, Hans-Dieter},
  journal={Reviews of Modern Physics},
  volume={34},
  number={1},
  pages={123},
  year={1962},
  publisher={APS}
}

@book{Minsky:2017book,
  title={Perceptrons: An introduction to computational geometry},
  author={Minsky, Marvin and Papert, Seymour A},
  year={2017},
  publisher={MIT press}
}


@article{Gamba:1961papa,
  title={Further experiments with PAPA},
  author = {A. Gamba and L. Gamberini and G. Palmieri and R. Sanna},
  journal = {Il Nuovo Cimento (1955-1965)},
  year = 1961,
  volume = 20,
  number = 2,
  pages={112--115}
}

@article{Borsellino:1961papa,
  title={An outline of a mathematical theory of PAPA},
  author={Borsellino, A and Gamba, A},
  journal={Il Nuovo Cimento (1955-1965)},
  volume={20},
  number={2},
  pages={221--231},
  year={1961}
}

@article{Huang:2006nc,
  title={Extreme learning machine: theory and applications},
  author={Huang, Guang-Bin and Zhu, Qin-Yu and Siew, Chee-Kheong},
  journal={Neurocomputing},
  volume={70},
  number={1-3},
  pages={489--501},
  year={2006},
  publisher={Elsevier}
}

@inproceedings{Schmidt:1992pr,
  title={Feedforward neural networks with random weights},
  author={Schmidt, Wouter F and Kraaijveld, Martin A and Duin, Robert PW},
  booktitle={Proceedings of the 11th International Conference on Pattern Recognition, 1992. Vol. II. Conference B: Pattern Recognition Methodology and Systems},
  pages={1--4},
  year=1992
}


@article{Baum:1988jc,
  title={On the capabilities of multilayer perceptrons},
  author={Baum, Eric B},
  journal={Journal of complexity},
  volume={4},
  number={3},
  pages={193--215},
  year={1988},
  publisher={Academic Press}
}

@article{Pao:1994nc,
  title={Learning and generalization characteristics of the random vector functional-link net},
  author={Pao, Yoh-Han and Park, Gwang-Hoon and Sobajic, Dejan J},
  journal={Neurocomputing},
  volume={6},
  number={2},
  pages={163--180},
  year={1994},
  publisher={Elsevier}
}

@article{jim1996analysis,
  title={An analysis of noise in recurrent neural networks: convergence and generalization},
  author={Jim, Kam-Chuen and Giles, C Lee and Horne, Bill G},
  journal={IEEE Transactions on neural networks},
  volume={7},
  number={6},
  pages={1424--1438},
  year={1996},
  publisher={IEEE}
}

@inproceedings{Jim1995effects,
  title={Effects of noise on convergence and generalization in recurrent networks},
  author={Jim, Kam and Horne, Bill G and Giles, C Lee},
  booktitle={Advances in neural information processing systems},
  pages={649--656},
  year={1995}
}

@article{Cho:2014gru,
  title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1406.1078},
  year={2014}
}

@inproceedings{Noh:2017regularizing,
  title={Regularizing deep neural networks by noise: Its interpretation and optimization},
  author={Noh, Hyeonwoo and You, Tackgeun and Mun, Jonghwan and Han, Bohyung},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5109--5118},
  year={2017}
}

@article{Lecun:1998cnn,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@article{Bradley:1997roc,
  title={The use of the area under the ROC curve in the evaluation of machine learning algorithms},
  author={Bradley, Andrew P},
  journal={Pattern recognition},
  volume={30},
  number={7},
  pages={1145--1159},
  year={1997}
}

@inproceedings{Gulcehre:2016noisy,
  title={Noisy activation functions},
  author={Gulcehre, Caglar and Moczulski, Marcin and Denil, Misha and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={3059--3068},
  year={2016}
}

@inproceedings{Socher:2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1631--1642},
  year={2013}
}

@article{Williams2017mnli,
  title={A broad-coverage challenge corpus for sentence understanding through inference},
  author={Williams, Adina and Nangia, Nikita and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1704.05426},
  year={2017}
}

@article{Kim2014convolutional,
  title={Convolutional neural networks for sentence classification},
  author={Kim, Yoon},
  journal={arXiv preprint arXiv:1408.5882},
  year={2014}
}

@article{Baevski2019vqwav2vec,
  title={vq-wav2vec: Self-supervised learning of discrete speech representations},
  author={Baevski, Alexei and Schneider, Steffen and Auli, Michael},
  journal={arXiv preprint arXiv:1910.05453},
  year={2019}
}

@article{Saxe:2013orthogonal,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}

@article{Carion:2020detr,
  title={End-to-End Object Detection with Transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  journal={arXiv preprint arXiv:2005.12872},
  year={2020}
}

@article{Wu2019pay,
  title={Pay less attention with lightweight and dynamic convolutions},
  author={Wu, Felix and Fan, Angela and Baevski, Alexei and Dauphin, Yann N and Auli, Michael},
  journal={arXiv preprint arXiv:1901.10430},
  year={2019}
}

@article{Press2019improving,
  title={Improving Transformer Models by Reordering their Sublayers},
  author={Press, Ofir and Smith, Noah A and Levy, Omer},
  journal={arXiv preprint arXiv:1911.03864},
  year={2019}
}

@article{Williams1992reinforce,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}


@article{vinyals2019grandmaster,
	title = {Grandmaster level in {StarCraft} {II} using multi-agent reinforcement learning},
	volume = {575},
    number = {7782},
	journal = {Nature},
	author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Michaël and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, Rémi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and Wünsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
	month = nov,
	year = {2019},
	pages = {350--354}
}
