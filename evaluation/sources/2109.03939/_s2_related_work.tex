% \vspace{-3mm}
\section{Related Work}
\vspace{-1mm}
\noindent\textbf{Lottery Tickets Hypothesis.}
\citet{frankle2018lottery} found that NNs for computer vision contain subnetworks that can be effectively trained from scratch when reset to their initialization.
Subsequent works~\citep{Zhou:2019deconstructing,Ramanujan:2020hidden,wortsman2020supermasks} demonstrated that so-called winning tickets can achieve performance without training, where the mask for finding the subnetwork at initialization is called ``supermask.'' 
In NLP, previous works find that matching subnetworks exist early in training with Transformers~\citep{yu2019playing}, LSTMs~\citep{renda2020comparing}, and fully-weighted per-trained BERT~\citep{chen2020lottery,prasanna2020bert} or Vison-and-Language model~\citep{gan2021playing}, but not at \textit{initialization}. 

\noindent\textbf{Random Feature.} 
In the early days of neural networks, fixed random layers~\citep{Baum:1988jc,Schmidt:1992pr,Pao:1994nc} have been studied in reservoir computing~\citep{Maass:2002lsm,Jaeger:2003echostate,Lukovsevivcius:2009reservoir}, ``random kitchen sink'' kernel machines~\citep{Rahimi:2008random,Rahimi:2009kitchen}, and so on. 
Recently, random features have also been extensively explored for modern neural networks in deep reservoir computing networks \citep{Scardapane:2017randomness,Gallicchio:2017echo,shen2020reservoir}, random kernel feature~\citep{peng2021random,Choromanski:2020performer}, and  applications in text classification~\citep{Conneau:2017infersent,Wieting:2019notraining}, summarization \citep{Pilault:2020impressive} and probing \citep{voita2020information}. 

\noindent\textbf{Compressing Transformer.} A wide range of neural network compression techniques have been applied to Transformers. 
This includes pruning  \citep{fan2019reducing,michel2019sixteen,sanh2020movement,yao2021mlpruning} where parts of the model weights are dropped, parameter-sharing \citep{lan2020albert,dehghani2018universal,bai2019deep} where the same
parameters are used in different parts of a model, quantization \citep{shen2020q,li2020train}
where the weights of the Transformer model are represented with fewer bits, and distilliation \citep{sun2020mobilebert,jiao2020tinybert} where a compact student model is trained to mimic a larger teacher model. 
To find the proposed subnetwork at initialization, we develop our method in the spirit of parameter sharing and pruning. 