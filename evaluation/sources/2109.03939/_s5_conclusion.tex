\section{Conclusions}
\label{sec:conclusions}
In this paper, we validate the existence of effective subnetworks in a one-layer randomly weighted Transformer on translation tasks. 
Hidden within a one-layer randomly weighted Transformer$_\text{wide/wider}$ with fixed pre-trained embedding layers, we find there exist subnetworks that are smaller than, but can competitively match, the performance of a trained Transformer$_\text{small/base}$ on IWSLT14/WMT14. 



% \section*{Ethical Considerations}
% One caveat of the proposed method is that data collected from public available resources that may contain biases, toxic contents, and other ethical issues. 
% This problem is common to AI models and we stress that de-biasing and a detailed examination are needed before deploying the system.
\section*{Acknowledgements}
We thank anonymous reviewers for their comments and suggestions. SS and KK were supported by grants from Samsung, Facebook, and the Berkeley Deep Drive Consortium. We would like to acknowledge DARPA, IARPA, NSF, and ONR for providing partial support of this work.