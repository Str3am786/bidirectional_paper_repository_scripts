\section{Setup}
\label{sec:background}
In this section, we define and elaborate the common setup and notations for long-context QA.

Assume that we are given a question $q_i$ and a context $\mathcal{S}_i{=}\langle {s_1},...,{s_M}\rangle$ consisting of $M$ sentences ($s_j$ can be also a document/paragraph/passage, depending on the dataset). From $\mathcal{S}_i$, the task is to identify the correct answer $a_i$ and a set of $N$ evidence spans $\mathcal{S}^+_i{=}\{s_{i_1},..., s_{i_N}\}$ where $i_j$ are indices of the sentences that are the supporting evidence for answering the question $q_i$. %

As common in the input setup of long-context transformer models \cite{longformer,zaheer2020big,caciularu-etal-2021-cdlm-cross}, which are the current state-of-the-art models for solving long-context QA tasks, 
the question and context sentences are concatenated in a single long sequence with special sentence tokens specifying sentence boundaries. 
Then the input is passed to the long-context transformer, which is trained to jointly identify the evidence sentences and extract or generate the answer.

Concretely, for each example, we prepare the following concatenated input sequence:
\begin{equation*}
[\texttt{$\langle$s$\rangle$},q_i,\texttt{$\langle$/s$\rangle$},s_{1},\texttt{$\langle$/s$\rangle$},s_{2},...,\texttt{$\langle$/s$\rangle$},s_{M}]
\label{eq:input}
\end{equation*}
where ``,'' is the string concatenation operation, $q_i$ and $s_j$ are sequences of tokens corresponding to the question and the $j$th sentence in the input context, and \texttt{$\langle$s$\rangle$} and \texttt{$\langle$/s$\rangle$} are special tokens representing the question and a context sentence, respectively (See Fig.~\ref{fig:model} for an example). %

Then, a QA loss, which we denote by $\mathcal{L}_{QA}$ is applied over the contextualized representation of each sentence token, and is optimized using supervision.  $\mathcal{L}_{QA}$ depends on the dataset and can take the form of a multi-task objective, representing multiple tasks in the context of QA~\cite{dasigi-etal-2021-dataset} (particularly evidence extraction and answer generation). 



