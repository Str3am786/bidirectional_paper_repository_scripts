\begin{thebibliography}{18}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Barkan et~al.(2020)Barkan, Caciularu, and
  Dagan}]{barkan-etal-2020-within}
Oren Barkan, Avi Caciularu, and Ido Dagan. 2020.
\newblock \href {https://doi.org/10.18653/v1/2020.emnlp-main.284}
  {Within-between lexical relation classification}.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 3521--3527, Online. Association
  for Computational Linguistics.

\bibitem[{Beltagy et~al.(2020)Beltagy, Peters, and Cohan}]{longformer}
Iz~Beltagy, Matthew~E Peters, and Arman Cohan. 2020.
\newblock Longformer: The long-document transformer.
\newblock \emph{arXiv preprint arXiv:2004.05150}.

\bibitem[{Caciularu et~al.(2021)Caciularu, Cohan, Beltagy, Peters, Cattan, and
  Dagan}]{caciularu-etal-2021-cdlm-cross}
Avi Caciularu, Arman Cohan, Iz~Beltagy, Matthew Peters, Arie Cattan, and Ido
  Dagan. 2021.
\newblock \href {https://aclanthology.org/2021.findings-emnlp.225} {{CDLM}:
  Cross-document language modeling}.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2021}, pages 2648--2662, Punta Cana, Dominican Republic. Association
  for Computational Linguistics.

\bibitem[{Chen et~al.(2016)Chen, Xu, Zhang, and Guestrin}]{gradckpt}
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016.
\newblock Training deep nets with sublinear memory cost.
\newblock \emph{arXiv preprint}, abs/1604.06174.

\bibitem[{Chen et~al.(2020)Chen, Kornblith, Norouzi, and
  Hinton}]{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{International Conference on Machine Learning (ICML)}.

\bibitem[{Chen and He(2021)}]{chen2021exploring}
Xinlei Chen and Kaiming He. 2021.
\newblock Exploring simple siamese representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 15750--15758.

\bibitem[{Choi et~al.(2017)Choi, Hewlett, Uszkoreit, Polosukhin, Lacoste, and
  Berant}]{choi-etal-2017-coarse}
Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre
  Lacoste, and Jonathan Berant. 2017.
\newblock \href {https://doi.org/10.18653/v1/P17-1020} {Coarse-to-fine question
  answering for long documents}.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 209--220,
  Vancouver, Canada. Association for Computational Linguistics.

\bibitem[{Dasigi et~al.(2021)Dasigi, Lo, Beltagy, Cohan, Smith, and
  Gardner}]{dasigi-etal-2021-dataset}
Pradeep Dasigi, Kyle Lo, Iz~Beltagy, Arman Cohan, Noah~A. Smith, and Matt
  Gardner. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.naacl-main.365} {A dataset of
  information-seeking questions and answers anchored in research papers}.
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 4599--4610, Online. Association for Computational
  Linguistics.

\bibitem[{Dror et~al.(2018)Dror, Baumer, Shlomov, and
  Reichart}]{dror-etal-2018-hitchhikers}
Rotem Dror, Gili Baumer, Segev Shlomov, and Roi Reichart. 2018.
\newblock \href {https://doi.org/10.18653/v1/P18-1128} {The hitchhiker{'}s
  guide to testing statistical significance in natural language processing}.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 1383--1392,
  Melbourne, Australia. Association for Computational Linguistics.

\bibitem[{Gao et~al.(2021)Gao, Yao, and Chen}]{gao-etal-2021-simcse}
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
\newblock \href {https://aclanthology.org/2021.emnlp-main.552} {{S}im{CSE}:
  Simple contrastive learning of sentence embeddings}.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 6894--6910, Online and Punta Cana,
  Dominican Republic. Association for Computational Linguistics.

\bibitem[{Gunel et~al.(2021)Gunel, Du, Conneau, and
  Stoyanov}]{gunel2021supervised}
Beliz Gunel, Jingfei Du, Alexis Conneau, and Veselin Stoyanov. 2021.
\newblock Supervised contrastive learning for pre-trained language model
  fine-tuning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}.

\bibitem[{Howard and Ruder(2018)}]{howard2018universal}
Jeremy Howard and Sebastian Ruder. 2018.
\newblock \href {https://doi.org/10.18653/v1/P18-1031} {Universal language
  model fine-tuning for text classification}.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 328--339,
  Melbourne, Australia. Association for Computational Linguistics.

\bibitem[{Iter et~al.(2020)Iter, Guu, Lansing, and
  Jurafsky}]{iter-etal-2020-pretraining}
Dan Iter, Kelvin Guu, Larry Lansing, and Dan Jurafsky. 2020.
\newblock \href {https://doi.org/10.18653/v1/2020.acl-main.439} {Pretraining
  with contrastive sentence objectives improves discourse performance of
  language models}.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 4859--4870, Online. Association for
  Computational Linguistics.

\bibitem[{Kingma and Ba(2014)}]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba. 2014.
\newblock \href {https://arxiv.org/abs/1412.6980} {Adam: A method for
  stochastic optimization}.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}.

\bibitem[{Oord et~al.(2018)Oord, Li, and Vinyals}]{oord2018representation}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals. 2018.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{arXiv preprint arXiv:1807.03748}.

\bibitem[{Pang et~al.(2021)Pang, Parrish, Joshi, Nangia, Phang, Chen,
  Padmakumar, Ma, Thompson, He et~al.}]{quality2021}
Richard~Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang,
  Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He~He, et~al.
  2021.
\newblock Quality: Question answering with long input texts, yes!
\newblock \emph{arXiv preprint arXiv:2112.08608}.

\bibitem[{Yang et~al.(2018)Yang, Qi, Zhang, Bengio, Cohen, Salakhutdinov, and
  Manning}]{yang-etal-2018-hotpotqa}
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan
  Salakhutdinov, and Christopher~D. Manning. 2018.
\newblock \href {https://doi.org/10.18653/v1/D18-1259} {{H}otpot{QA}: A dataset
  for diverse, explainable multi-hop question answering}.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pages 2369--2380, Brussels, Belgium.
  Association for Computational Linguistics.

\bibitem[{Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti,
  Ontanon, Pham, Ravula, Wang, Yang, and Ahmed}]{zaheer2020big}
Manzil Zaheer, Guru Guruganesh, Kumar~Avinava Dubey, Joshua Ainslie, Chris
  Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li~Yang,
  and Amr Ahmed. 2020.
\newblock {Big Bird}: Transformers for longer sequences.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}.

\end{thebibliography}
