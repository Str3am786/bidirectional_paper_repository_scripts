\section{Measurements}
Using our pipeline for identifying violating comments and bootstrap sampling of comments, we estimated the dependent variables of interest. In this section, we present our specific measures that we derived from our method. 


\subsection{Estimating the Prevalence of Violating Comments}

\subsubsection{Overall prevalence} 
Our core dependent variable is the percentage of unmoderated comments that are \rnr{macro norm} violations. For each bootstrap sample, we calculated the percentage of violating comments. We then calculated our estimate as the median result across the 1,000 bootstrap samples, and calculated the 95\% confidence interval across those samples. 

\subsubsection{Prevalence per subreddit} 
Do some subreddits contain a higher rate of macro norm violating comments than the others, and if so, why? To answer this question, we estimated the percentage of violating comments for each of the subreddits across each of the bootstrap samples, and again calculated the median and 95\% confidence interval per subreddit. Given that the $T_{2020}$ sample, as mentioned previously, is a smaller scale replication of the $T_{2016}$ sample, we found that there is not enough data to draw statistically meaningful results for each of the subreddits for $T_{2020}$. Therefore, we focused our analysis in this particular subsection on the $T_{2016}$ sample. 

We tested the factors that were associated with a subreddit having a higher count of violations. We started by taking the sampling approach as we did above to estimate the count of violating comments for each of the study subreddits. We hypothesized that two relevant predictive variables might influence the rate of violating comments: 1) the topic of a given subreddit and 2) the ratio of the number of moderators to the number of comments. Our focus on the first variable was motivated by our observation that some communities might be more prone to hosting macro norm violations that are topically relevant (e.g., politically inflammatory comments for political subreddits or pornography for NSFW subreddits), while our focus on the second variable was informed by prior work suggesting that moderators in subreddit communities are overloaded and unable to review every comment posted~\cite{Chandrasekharan2018internet, 18_Gilbert}.

To test these hypotheses, we first collected the relevant data for these variables. To get the respective topics for our study subreddits, we matched the study subreddits to categories on r/ListOfSubreddits’ thematically organized list of subreddits.\footnote{\url{https://www.reddit.com/r/ListOfSubreddits/wiki/listofsubreddits}} The list contains multiple layers of thematic hierarchy. From this, we picked the second-highest thematic layer (e.g. r/AskReddit was categorized as “Discussion,” and r/Pokemon as “Entertainment”) for our subreddits as the top layer was too broad to be meaningful. This resulted in eleven mutually exclusive topic categories for our 97 study subreddits.  We then manually collected the number of moderators who were present in October 2016 (the middle of the 11-month period when $\mathcal{M}$ was collected) for each of the subreddits by using the Wayback Machine to access an archived version of each subreddit's homepage on October 1st, 2016, or the earliest subsequent date when the page was crawled. We then calculated the ratio of moderators to comments for each of the subreddits. Finally, we retrieved the number of all comments (including those that were removed by the users or moderators) that were posted on each of the 97 study subreddits from $T_{2016}$ -- we used this information as the offset to our Poisson model described below. To address the non-normal distribution of the number of comments and moderator-to-comment ratio in our regression, we log transformed these variables.

We then employed a Poisson regression \cite{63_poisson} to model the rate of macro norm violating comments in a given subreddit using the three aforementioned predictive variables as our independent variables. \rnr{Poisson regression was an appropriate choice because it is designed to model rate data. We followed a common practice of setting an offset---in our case, the size of the subreddit in terms of the number of comments--- to the Poisson regression model to make it appropriate for rate data \cite{94_Anderson, 95_Gardner}.} For better interpretability of our model, we treated the ``general content'' topic category as the baseline of the indicator (dummy) variable for topic. Finally, to interpret our results, we exponentiated the variable coefficients in our model to calculate the incidence rate ratios. For instance, as we will cover in the next section, \rnr{the number of moderators to comments ratio in a subreddit is a significant predictor of the rate of violating comments in a subreddit (p<0.001) where for every unit increase in the log-transformed number of comments in a subreddit, the rate of violating comments is expected to increase by a factor of 2.07 (=$e^{0.728}$).}


\subsection{Determining the Characteristics of Violating Comments}

\subsubsection{Prevalence by violation category} 
So far, we defined a comment to be violating if it violates one of the macro norms of Reddit. But here, we describe our analysis that takes a closer look at the variance in the prevalence of different macro norm violations and the rate at which they occur. In order to better understand the prevalence of each of the macro norm violations among the unmoderated comments, \rnr{we measure the percentage of sampled violations that fall into each of the violation categories}. Specifically, we take the average number of norm violations per category that across the study subreddits in each bootstrap iteration, and as before, report the median and confidence interval across the 1,000 bootstrap iterations.

Are all categories of macro norm violations removed by the moderators at the same rate? To answer this question, we \rnr{take the following steps to} estimate the number of all comments (moderated and those still online) that violate each specific one of the macro norms. To estimate the number of violations for each of the macro norms within the moderated comments, we require a complete set of all moderated comments from our study subreddits, which we gathered earlier as $\mathcal{M}$ for $T_{2016}$. To extend $\mathcal{M}$ to $T_{2020}$, we reimplemented the original high-frequency scraping process used in prior work to generate $\mathcal{M}$ for $T_{2016}$~\cite{Chandrasekharan2018internet}. We then randomly sampled from $\mathcal{M}$ (N=776 comments for the longer $T_{2016}$ period; N=188 for the shorter $T_{2020}$ period). We then followed the same human annotation process from Section~\ref{sec:annotation} to annotate each moderated comment with any macro norms that it violated. Using this, we calculated the estimated number of comments that violate each of the macro norms within the moderated comments dataset. We combined this with the estimated number of unmoderated violations for each macro norm to estimate the overall number of violations for each of the macro norms considering both moderated and unmoderated comments combined. With this number of violations, we then proceeded to calculate the rate at which each of the macro norms are moderated.

\subsubsection{Comparing the rate of engagement} 
Reddit comments accrue engagement over time in the form of upvotes, downvotes, and replies. The “score” of a comment is determined by subtracting the number of downvotes from the number of upvotes the comment gained over time, and “replies” are comments that respond to the given comment in the thread. Score is a useful dependent variable to track because it is a strong determinant of how high in the thread the comment sits.\footnote {On Reddit, users can choose one of the three options -- top, hot, and best -- for determining what comments should come at the top. Top is simply whichever comment has the highest score, hot is the log-transformed absolute value of the score with extra weights for the age of the content, and best is the number of upvotes divided by the number of downvotes. See \url{https://www.reddit.com/r/explainlikeimfive/comments/1u0q4s/eli5_difference_between_best_hot_and_top_on_reddit/}} The number of top-level replies can serve as a proxy for measuring whether the given comment generated, or ended, a discussion.\footnote{The Perspective model from Jigsaw for measuring toxicity used human annotations to train where a comment was asked to be annotated if it were “rude, disrespectful, or unreasonable… that is likely to make you leave a discussion” \cite{60_Perspective}.}
We explored whether the violating comments result in significantly higher or lower scores, and whether they receive more or fewer “top-level replies” (replies that directly respond to the given comment in the thread of replies). To investigate this, we compared the scores and the number of top-level replies that the unmoderated violating comments got to the global average number of online comments the study subreddits in our $T_{2016}$ and $T_{2020}$ samples.


\subsubsection{Comparing language usage} 
Does the way in which a macro norm is violated impact its likelihood of remaining on the site or being moderated? We compared the writing style of moderated comments vs. unmoderated violating comments on two dimensions: readability and emotionality. For measuring readability of a comment, we used the Flesch–Kincaid readability test~\cite{64-kinkaid} to retrieve the comment’s readability score, where a higher score indicates that the comment is easier to read. For measuring emotionality of a comment, we used Evaluative Lexicon 2.0~\cite{65_Rocklage, 66_Rocklage}, an imputation-based dictionary that assigns an emotionality score to text where higher score means that the comment is more likely to trigger an emotional reaction rather than a cognitive response that reflects a person’s beliefs about the topic of discussion \cite{66_Rocklage}.
