\section{Introduction}
How widespread is anti-social behavior online? Surveys suggest that over four in ten U.S. adults have experienced harassment online~\cite{pew_harassment_results}, including the vast majority of women~\cite{vitak_harassment}, and that online political hostility is omnipresent~\cite{bor_hostility}. Over 3\% of posts on popular news websites are flagged by users for rule violations~\cite{cheng2017anyone}, and 96\% of minutes watched on a popular video streaming platform included at least one action by moderators to remove a piece of violating content~\cite{twitch_transparency_report}. Anti-social behaviors such as inflammatory comments \cite{1_Chandrasekharan}, hate speech~\cite{2_Donovan}, trolling~\cite{82_Claire, 83_Kayany}, and other ``undesirable'' comments~\cite{4_Chancellor, 5_Cheng, 6_Sood, 7_Chandrasekharan} not only hamper discussions in online communities~\cite{88_Kraut}, but also cause serious harm~\cite{84_Yavuz, 85_Wiener} and seed even more anti-social behavior~\cite{cheng2017anyone}. As platforms such as Reddit, Facebook, and Twitter have grown in size and consequence, concerns over the prevalence of anti-social behavior on these platforms have mounted~\cite{71_Laub}. Platforms responded by deploying thousands of paid moderators~\cite{8_Gillespie, 9_Roberts}, an array of algorithms~\cite{28_Gorwa, 11_Hosseini}, and tools for community self-moderation~\cite{seering2020reconsidering, chandrasekharan2019crossmod}. Researchers have likewise sought to help reduce the prevalence of these behaviors, for example by creating tools for governance~\cite{13_Fan,zhang2020policykit} and automatic detection of anti-social behaviors~\cite{14_Butler, 15_Xu}.

Given these harms, and the effort to combat them, it is critical to understand how much of the content in online communities remains anti-social. A large empirical foundation in social psychology demonstrates that if these behaviors are visible and widespread, it will encourage others to engage in anti-social behaviors as well~\cite{5_Cheng, cheng2017anyone}. Benchmarking progress, or regression, requires an honest accounting of the situation. Platforms themselves have begun measuring the prevalence of specific behaviors such as hate speech and harassment~\cite{58_Culliford, 77_YoutubeTeam, 78_Vincent}.

Despite this need, empirically measuring the prevalence of anti-social behavior remains difficult. AI tools remain too error-prone to be fully relied upon~\cite{10_Binns, 11_Hosseini}, and manual labeling via random sampling is labor-intensive to perform at scale. In addition, defining which behaviors cross the line remains contentious, with different online communities applying different definitions~\cite{71_Laub} and platforms each establishing different standards for content and enforcement~\cite{71_Laub, 79_Klonick}. Therefore, it is no surprise that empirical studies that measure the prevalence of anti-social behavior are few and far between, with platform-published performance metrics often tucked under the platforms’ “transparency” or compliance reports using vaguely defined categories~\cite{58_Culliford}. 

In this paper, we develop a method to measure the proportion of these behaviors in online communities, and apply that method to measure the prevalence of anti-social behaviors on a major platform. Our focus is on Reddit, one of the most popular websites in the United States, and one where interaction is spread across thousands of independent communities (subreddits). While norms vary across subreddit, we adapt a set of macro Reddit moderation norms, such as removing misogyny and hate speech, from prior work~\cite{Chandrasekharan2018internet} that identified a set of these macro-level norms for user behavior and moderation that are shared by a strong majority of the largest subreddits. We verify that these norms are enforced by moderators across essentially all of the most popular subreddits, and then set out to sample comments on these popular subreddits to identify how many unmoderated comments violate each of these norms. We also seek to measure how many of comments were removed by existing moderation tools and processes.

More precisely, we present a fully realized human-AI pipeline that identifies these macro-norm violating comments at scale, and a bootstrap estimation procedure for estimating overall prevalence rates based on the identified violating comments. We use a publicly available dataset of removed comments from the 100 most popular subreddits  \cite{Chandrasekharan2018internet}, and a matching dataset of comments that were not removed, to train classifiers that can identify macro norm violations with recall at $0.99$. To validate our classifiers' decisions and complement high recall with high precision, we employ crowd workers to annotate a sample of the comments that the classifiers flagged, confirming whether these comments indeed violate at least one of the eight macro norms previously identified. We then model this process via a statistical bootstrap to estimate confidence intervals for the proportion of violating comments that remain on popular subreddits. We repeat this analysis twice, once on a dataset from May 2016--March 2017 and once from the last three weeks of December 2020.

Using our approach, we find that that 6.39\% (95\% confidence interval [5.49\%, 7.39\%]) of all comments on the platform in 2016--2017 are macro norm violations, as are 4.44\% (95\% CI [2.61\%, 6.39\%]) of comments on the platform in December 2020. So, even relatively recently, roughly one in twenty comments is a violation of norms against behaviors such as misogyny, threatening violence, and hate speech. Overall, moderation only catches a small percentage of violating comments, removing 4.86\% of these violations in 2016--2017, and 10.54\% of the violations in 2020. Some categories of violation were more likely to slip through the cracks: politically inflammatory comments and misogyny/vulgarity were the least likely to be moderated, while pornography and bigotry (particularly in 2020) were among the most aggressively moderated. Subreddits about hobbies and occupations had the fewest violating comments remaining online, controlling for the number of comments submitted and the ratio of comments per moderator.

Our results highlight the prevalence of content that violates established macro norms: they imply that over 16 million macro norm violating comments from the May 2016--March 2017 period, and over a half a million macro norm violating comments from the last three weeks of December 2020, remain unmoderated. (Though not directly comparable, it raised an alarm in the content moderation community when Facebook's transparency report suggested that just 0.1\% of its content views are categorized as hate speech~\cite{58_Culliford}, which translates to millions of affected users). 
It is not surprising that nearly half of Americans and the vast majority of women experience harassment, when 4.44\% of comments that remain online are norm-violating: it suggests that a person would encounter anti-social behavior after reading 23 random comments on Reddit on average. Chances are high (96\%) that the comment would either be a personal attack or misogyny/vulgarity.

These violations are often less flagrant than highly publicized issues such as misinformation campaigns or hate speech, but they can aggregate into an unwelcoming atmosphere.
We offer a reflection of how our results might help platform designers and community members make decisions about their processes. 
Finally, we point to the further need for empirical measurements and auditing of our content moderation strategies, and provide a working model pipeline for identifying norm-violating behavior online. Our code is available open-source at \url{https://github.com/StanfordHCI/ContentModAudit_CodeRelease}, so other researchers can apply it to measure moderation outcomes in any Reddit community.

\medskip

\textsc{Content Warning: } {\it Please be advised that some of the example comments in this paper contain offensive language.}  


