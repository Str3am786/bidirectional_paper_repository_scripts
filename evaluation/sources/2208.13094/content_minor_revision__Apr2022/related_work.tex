\section{Related Work}
In this section, we cover prior work in characterizing and identifing anti-social behavior in online communities. Despite the continued effort to identify harmful content online, the existing approaches face significant methodological challenges. This presents a fertile ground in which empirical results highlighting what today's content moderation fails to capture could provide value in guiding the future effort in content moderation.

\subsection{Anti-social behavior in online communities}
Anti-social behavior has been documented for essentially as long as online communities have been documented~\cite{Dibbell1993rape}. A review of the causes of, and responses to, such behavior, is the focus of~\citeauthor{kiesler2012regulating}~\cite{kiesler2012regulating}. One form of anti-social behavior is trolling, which is typically defined as intentional disruption of the community. Trolling is sometimes attributed to the online disinhibition effect~\cite{suler2004online}, where we engage in behavior online that we might not in face-to-face interaction. There exists a set of individuals who are dispositionally oriented toward troll behavior~\cite{buckels2014trolls}, and engage in anti-social behavior because they are redirecting internal feelings~\cite{varjas2010high} or do it for fun~\cite{shachaf2010beyond}. While such individuals are equally uncivil both online and online, they have more reach and visibility online~\cite{bor_petersen_2021}.

Another form of anti-social behavior is non-premediated, often referred to as flaming~\cite{83_Kayany,Kiesler1984social}. Evidence suggests that many online users can be tipped into engaging in flaming through a combination of mood and environmental signals~\cite{cheng2017anyone}. Human observers can predict with reasonable accuracy whether a discussion thread is going to end with hostility~\cite{zhang2018conversations}, but we are poor at estimating how others will react to our own comments~\cite{chang2020don}.

Facebook answered the increasing pressure related to hate speech on its platform with a brief transparency report suggesting, for the first time, that roughly 1 in 1,000 content views on its platform include what the platform considers to be hate speech \cite{58_Culliford}. Similarly, Reddit's Safety Team announced the impact of a certain form of moderation like banning of a toxic community on curbing toxic content \cite{78_Vincent} while YouTube provided a summary of the type of content the platform is trying to remove \cite{77_YoutubeTeam}.

Measuring the prevalence of these behaviors remains an open challenge. One thread of prior work has used survey methods to understand the prevalance of anti-social behaviors online (e.g.,~\cite{pew_harassment_results,vitak_harassment}). Another thread has used log data to observe when content is flagged or removed (e.g.,~\cite{cheng2017anyone,twitch_transparency_report}). The present research extends these efforts through directly sampling and coding online behaviors as violations. Such an approach allows us to avoid sampling and reporting biases, though it makes tradeoffs to do so. For example, not all violations are created equal, and survey methods will be better at capturing the impact of seeing a violation.


\subsection{Moderation}
Content moderation is a common response to anti-social behaviors. Content moderation strategies can be broadly categorized into human moderation and automated moderation. Where online forums and discussion boards were previously managed in large part by human moderators who would carefully sift through the content posted and remove those that go against a given community's standard \cite{27_Lampe}, online social media companies today are increasingly relying on automated tools to scale up the moderation effort \cite{28_Gorwa, 29_Roberts}. But despite the ongoing efforts to improve these strategies, both human moderation and automated moderation face challenges.

\subsubsection{Human Content Moderators}
Online content moderation strategies that rely on human moderators to control anti-social content involve dedicated moderators to manually inspect and act on such content. Although human moderators can often make more nuanced decisions than automated moderators by taking into consideration the context \cite{28_Gorwa, 30_MSB, 31_Lessig, alkhatib2019street} and ``behind-the-scenes'' norms of a local community that are less explicit \cite{Chandrasekharan2018internet}, they face significant challenges in ensuring that all content is reviewed in modern-day social media platforms where participants number in hundreds of millions to even billions \cite{32_Preece, 33_Williams}. Therefore, human moderators often take a reactive instead of a proactive approach in which the content that users share is first posted online and then reviewed once they have been flagged by others as anti-social \cite{9_Roberts, 34_Gillespie}. This results in anti-social content remaining visible to the users within the online community until it is reviewed by a moderator, or in the worst case, remain unaddressed indefinitely \cite{34_Gillespie}. 

% In recent years, large social media companies such as Facebook, Twitter, and YouTube have hired external human moderators to scale up such moderation processes \cite{35_Seering}. However, such partnering has drawn critiques not only because these external human moderators are continuously exposed to highly violent and inappropriate content for low wages \cite{chandrasekharan2019crossmod}, but also because they are dispersed globally and might not be familiar with social norms or culturally sensitive racist, homophobic, and misogynistic tropes within the country where the content originates from \cite{36_Stecklow, 37_Chen, 38_Wray}.
 
\subsubsection{Automated Content Moderators}
To moderate the large volume of content that is shared on social media platforms, \rnr{there has been an increasing effort to create tools that can automatically identify and act on anti-social behavior \cite{39_Bickert, 40_Google, 41_Twitter, 92_Chandrasekharan}}. \rnr{Such tools range from simple word or source-ban lists that filter content based on block-listed words, URLs, and source IP addresses \cite{42_HN}, to more sophisticated AI tools that leverage machine learning and natural language processing techniques \cite{chandrasekharan2019crossmod, 93_Chancellor}}. \rnr{Particularly within the context of self-governed online communities, recent academic research has tried to both better understand what the online communities value and consider to be norm violating content that should be removed \cite{Chandrasekharan2018internet, 35_Seering}, and build tools and datasets that would help these communities implement automated ways for identifying and flagging the norm violating content~\cite{1_Chandrasekharan, chandrasekharan2019crossmod, 93_Chancellor}.}

And concurrent with the research that showed automated tools could be useful for efficiently capturing textual cyberbullying \cite{43_Dinakar, 44_Xu}, vulgarity and other anti-social behaviors online \cite{4_Chancellor, 1_Chandrasekharan, 5_Cheng}, large social media platforms started to transition their moderation strategy to be more reliant on automated tools \cite{35_Seering, 45_Geiger}. For example, following a major public controversy in which Facebook reportedly became the medium for hate speech that incited genocide against the Muslim population in Myanmar, the social media platform improved its Burmese hate speech classifier to increase automated takedowns of content by 39\% \cite{46_GIFCT}. Meanwhile, YouTube reports that 93\% of the videos on its platform that are removed for violent extremism are flagged by machine-learning algorithms \cite{40_Google}, while Reddit’s subcommunities now routinely employ AutoModerator to automatically filter out comments violating the community standards \cite{47_reddit}.

However, the automated tools for fighting anti-social content online have received criticism for their limitations. One of the most commonly cited limitations includes automated moderation tools’ inability to take into consideration the context in which an online conversation is taking place \cite{30_MSB, 31_Lessig, 48_Pater}; \rnr{a comment may be construed as acceptable or even encouraged in one community while being completely unacceptable in another \cite{seering2020reconsidering}}. At the same time, many of the modern automated moderation tools are considered to be too brittle, often suffering from high false-positive rates and failing to understand the finer nuances or irony in user-generated content \cite{49_Sood, 50_Brody, 51_Chancellor, 52_li}. As a case in point, Jigsaw---a subsidiary of Google---published a Perspective API that aimed to provide a state of the art machine learning model with high test accuracy for quantifying ``toxicity'' of textual content. The API often failed to match observers’ expectations and could easily be tricked (e.g., the single-term comment ‘Arabs’ was determined to be 63\% toxic, while ‘I love fuhrer’ was only 3\% toxic \cite{53_Sinders}). Additionally, some worry that these automated tools would exhibit discriminatory bias like other machine learning models, thereby making moderation decisions that unfairly treat people of a certain gender, race, ethnicity, or age and cause more harm than good \cite{54_Diaz, 55_Blodgett, 56_Zehlike, 57_Barocas}. As such, there is a growing consensus that automated moderation tools should be used in conjunction with human moderators, for example by triaging potentially anti-social behavior for human moderators to review \cite{28_Gorwa, chandrasekharan2019crossmod}. 

Given this, rigorous empirical study exploring the prevalence of antisocial behavior, and the impact of content moderation, can help moderators and designers plan their strategies more effectively. Therefore, in this work, we aim to operationally define, quantify, and characterize anti-social behavior that populates some of the most popular communities on Reddit. 