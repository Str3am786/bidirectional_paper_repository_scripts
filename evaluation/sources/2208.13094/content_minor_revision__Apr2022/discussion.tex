\section{Discussion}
The contribution we make in this paper is twofold. First, we present a large-scale analysis that investigates the prevalence and characteristics of macro norm violating comments that are left unmoderated on Reddit. In doing so, we provide the social computing literature with empirical results that can help aid policies, moderation, and design interventions. These results illustrate how roughly one in every 20 comments on Reddit violates the platform's own expectations of what should not be present, and shines light on the the most prevalent types of violations. Second, we demonstrate a materialized human-AI-bootstrap pipeline for identifying violating comments. We use classifiers trained on a dataset of previously moderated comments on the 97 most popular subreddit communities on Reddit to triage potentially violating comments at scale, employ trained human annotators to validate a sample of the classifiers’ decisions, and calculate the final estimation and our confidence using the bootstrap resampling technique.

In the remainder of this section, we discuss what our findings may mean for social media and for efforts to rein in anti-social behavior online; we try to interpret what the rate of violating comments in the 4\% to 6\% range means in reality, and scope out what can be inferred from our results. Finally, we end with our work’s limitations and potential avenues for future research. 


\subsection{Is This Result Good, Bad, or Neither?}
Comments that violated the macro norms were prevalent on the study subreddits during both time periods. As noted in the introduction, even Facebook's transparency report of 0.1\% of its content being categorized as hate speech~\cite{58_Culliford, 91_Barrett} raised concerns as it translates to affecting millions of users. Given this, 4\% to 6\% of norm violating comments likely is not a small amount. Also, the overall rate of moderation, though has increased over the years perhaps in some part due to the growing awareness of the negative impacts these content can have, remains low with one in twenty of the norm violating comments in 2016, and one in ten in 2020, being removed by the moderators. These rates are comparable to the 3--5\% rate of hate speech moderation reported in Facebook’s internal documents that were made public in recent whistleblower exposures \cite{facebook_scandal, facebook_scandal_2}. This provides an important context for both our findings and those in Facebook’s internal documents; that these two findings covering very different contexts and likely, using diverging methods of measurement (it is unclear how Facebook measured their moderation rate), are roughly converging may illustrate a broader point about the challenges of today’s large-scale content moderation.

\mrev{It is also worth highlighting that for many communities we studied in this work, our results on the prevalence of norm violating comments likely represents the lower bound for two reasons. First, we are explicitly not measuring the prevalence of the comments that do not violate the macro norms, yet still violate community-specific local, micro norms. For example, subreddits for minority groups often have strong prohibitions on attacks, and topic-based groups such as r/science remove all comments except those focusing on the science itself. Second, our approach for identifying violating comments was based on reviewing the comment in isolation and therefore did not include norm-violating comments that were violating only in a specific conversational context.} These factors further highlight that despite the growing effort to moderate violating content online, content moderation still remains challenging with much work to be done. 

\subsubsection{\mrev{What Does Not Get Moderated?}}
Our findings also suggest that there are certain categories of norm violations and community specific characteristics that make it more or less likely for violating comments to be moderated on Reddit. For instance, comments that contain pornographic materials were moderated almost at the rate of nearly 50\% in 2020. Of course, not all violations are created equal: some categories are more harmful than others, and violations may be more or less harmful based on who they're directed to and in what context. Our analysis cannot measure this; it is more effective at identifying the overall volume of microaggressions and micro-violations. However, by combining our approach with survey analyses, it might be possible to weigh the impact of each violation to produce a better overall sense of which groups are harmed, when, and how. We also note that higher rates of moderation for some categories may be due to platform incentives (e.g., will Reddit threaten the subcommunity if they are not removed), as well as the ease of automatic moderation. Pornography, for example, may be more readily identifiable via URL by regex-based AutoModerators on Reddit. 


\subsection{\mrev{What Should the Moderation Community Aim For?}} 
\mrev{The estimation we present here may seem like cause for concern---many macro norm violating comments in these online communities remain unmoderated. The important question then is, where do we go from here? What is a reasonable goal the moderation community should strive for? We believe that it would be a mistake to take our results presented here to mean that the moderation community is failing or needs to put in even more work going forward. Particularly in the context of Reddit, it is important to note that communities are largely self-governed and the moderation effort falls singularly on unpaid, and often overworked, volunteers~\cite{Kiene2019, Gilbert2020}, so the amount of available labor might be fundamentally limited in such communities and simply asking for more labor from the moderators would likely be ineffective, if not inappropriate.} 

\mrev{Given this, the more constructive message we can derive from our findings would be that we need to better support the moderators so that whatever labor they are able to put in can yield the largest possible benefit to the community.} For instance, we could envision ways to provide better moderation tools that can more effectively flag content that needs to be reviewed by human moderators, both for its harmful content and for how exposed it might be (e.g., appearing at the top of a community’s feed). It also might be more tractable to train classifiers to identify the suspected type of violation in the moderators' AutoMod queues, so that moderators can more effectively triage. \mrev{Finally, it would also be critical to support the volunteer moderators in their work environment where they can be emotionally and psychologically strained~\cite{Matias2016, 35_Seering}, perhaps by constructing a protocol for common negative situations as suggested in prior work~\cite{Wohn2019}.} 


\subsection{\mrev{Local Norms May Conflict With Macro Norms}}
\mrev{One caveat that is worth highlighting is how content from individual subreddits may not necessarily subscribe to all of the macro norms. That is, a macro norm violating comment might be acceptable in certain communities. For example, NSFW subreddits might be more lenient towards pornographic materials while political subreddits might be more lenient towards certain types of inflammatory political comments, which explains why topics on politics and NSFW are significant predictors of higher rates of violating comments. This is a tradeoff one has to make to measure the amount of violating content with a wide-lens across an entire platform. And although we believe that the eight categories of macro norms we used in our analysis give us a more nuanced understanding of what is going on in these communities than catch-all metrics (e.g., toxicity), we note that any work that aims to take such an approach needs to be cautious so as to not unfairly flag minority communities as being more violating~ \cite{Oliva2020} just because they deviate from certain macro norms.}

\mrev{In the scope of our work, we do find that even when excluding communities in topic categories such as NSFW and politics, the rate of violating comments remains high. This suggests that although some of the comments marked by our method as macro norm violating may be accepted by moderators within certain subreddits, this does not account for the majority of such comments. However, the estimations presented here should still be interpreted primarily as an attempt to understand the aggregate trend of norm violations across a large number of communities rather than to single out any one community as particularly problematic. This tension between the local norms and the macro norms should continue to be discussed in the future literature, importantly by directly communicating with the moderators and the members of their communities to better understand and reconcile the differences in between these two classes of norms.} 


\subsection{Limitations and Future Work}
Our analysis is a wide-lens, macro-scale approach. However, such an analysis makes inevitable tradeoffs compared to deeper qualitative and inductive, or even more focused measurement, studies. Firstly, we focused on some of the most popular subreddits in 2016--2017. When studying the 2020 period, we stayed with these subreddits as we were interested in a longitudinal study of how these subreddits developed over time, but future work could investigate how 2020's most popular subreddits perform. It is possible that the reduced violation rate in 2020 could be attributed to subreddits losing popularity and trolls migrating elsewhere to the newer popular subreddits. We also restricted our analyses to the top 100 most popular subreddits, which is consistent with prior work but prevents us from measuring prevalence in smaller communities. An open question remains as to whether these smaller communities better regulate violating comments, or do less because they are not in the spotlight. 

In addition, our work contains methodological limitations that future work could expand upon. For instance, we focused on estimating the prevalence of violating comments. But one could also approach this problem by measuring the content view of such comments. That is, how often are violating comments seen by the users or are they mostly ignored and placed at the edge of the comment section? Here, we only indirectly touched on this topic by showing the significant difference in engagement between the moderated and unmoderated violating comments as engagement often dictates where a given comment is placed within the comment section. Also, we operationalized violations as what moderators would remove, but there may be more normative definitions that would produce different estimates. Future work should continue to rigorously define and operationalize violating comments. One important venue of such future work would work directly with the moderators and their communities to consider what the members of that particular community would consider to be a violation of their standards and choose to remove. Additionally, our classifiers could introduce bias or stale outputs in part because the training data is a few years old. We believe this is largely accounted for as the classifiers maintained high recall and their outputs were reviewed by human annotators. But developing better performing algorithms for identifying violating comments remains an important but also a challenging task. 

Finally, it is worth noting that a limitation to our estimation approach, which centers around human annotators, is the human cost. In many cases, employing annotators for a large number of hours may not be possible, particularly for an academic researcher, due to the limited resources available. However, we believe that it is important for non-industry academics to audit and inspect the state of social computing platforms. In our study, we overcome some of these practical challenges by combining human annotations with bootstrap resampling but we hope future work will continue to build on this precedence and refine techniques for large scale measurement studies.
