
\documentclass[final,5p]{elsarticle}





\usepackage{amssymb}
\usepackage{amsthm}


\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{mathdots}
\usepackage{dsfont}

\usepackage{tikz}
\usetikzlibrary{external}
\tikzexternalize[prefix=tikz_external/]
\usepackage{pgfplots}
\usepgfplotslibrary{groupplots}
\usepgfplotslibrary{colorbrewer}
\usepgfplotslibrary{fillbetween}
\usetikzlibrary{colorbrewer}

\usepackage{tabularx}

\graphicspath{{fig/}}

\usepackage{xcolor}
\newcommand{\todo}[1]{{\color{red}TODO:~#1}}

\usepackage[hidelinks]{hyperref}

\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}
\newcommand{\secref}[1]{Section~\ref{#1}}
\LetLtxMacro{\originaleqref}{\eqref}
\renewcommand{\eqref}{Eq.~\originaleqref}

\usepackage{booktabs} %

\input{nomenclature}


\journal{}

\begin{document}

\begin{frontmatter}



\title{Deep Reinforcement Learning for Turbulence Modeling in Large Eddy Simulations}

\author[label1]{Marius Kurz\corref{cor1}}
\ead{marius.kurz@iag.uni-stuttgart.de}

\author[label2]{Philipp Offenh\"auser}
\ead{philipp.offenhaeuser@hpe.com}

\author[label1]{Andrea Beck}
\ead{beck@iag.uni-stuttgart.de}

\address[label1]{Institute of Aerodynamics and Gas Dynamics, University of Stuttgart, Pfaffenwaldring 21, 70569 Stuttgart, Germany}
\address[label2]{Hewlett Packard Enterprise (HPE), Herrenberger Straße 140, 71034  Böblingen, Germany}

\cortext[cor1]{Corresponding author}


\begin{abstract}

  Over the last years, supervised learning (SL) has established itself as the state-of-the-art for data-driven turbulence modeling.
  In the SL paradigm, models are trained based on a dataset, which is typically computed a priori from a high-fidelity solution by applying the respective filter function, which separates the resolved and the unresolved flow scales.
  For implicitly filtered large eddy simulation (LES), this approach is infeasible, since here, the employed discretization itself acts as an implicit filter function.
  As a consequence, the exact filter form is generally not known and thus, the corresponding closure terms cannot be computed even if the full solution is available.
  The reinforcement learning (RL) paradigm can be used to avoid this inconsistency by training not on a previously obtained training dataset, but instead by interacting directly with the dynamical LES environment itself.
  This allows to incorporate the potentially complex implicit LES filter into the training process by design.
  In this work, we apply a reinforcement learning framework to find an optimal eddy-viscosity for implicitly filtered large eddy simulations of forced homogeneous isotropic turbulence.
  For this, we formulate the task of turbulence modeling as an RL task with a policy network based on convolutional neural networks that adapts the eddy-viscosity in LES dynamically in space and time based on the local flow state only.
  We demonstrate that the trained models can provide long-term stable simulations and that they outperform established analytical models in terms of accuracy.
  In addition, the models generalize well to other resolutions and discretizations.
  We thus demonstrate that RL can provide a framework for consistent, accurate and stable turbulence modeling especially for implicitly filtered LES.
\end{abstract}



\begin{keyword}
  Turbulence Modeling \sep Deep Reinforcement Learning \sep Large Eddy Simulation
\end{keyword}

\end{frontmatter}


\input{introduction}
\input{rl}
\input{turbulence}
\input{results}
\input{conclusion}

\section*{Acknowledgment} %
The research presented in this paper was funded by Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy - EXC 2075 - 390740016.
The authors gratefully acknowledge the support and the computing time on "Hawk" provided by the HLRS through the project "hpcdg" and the support by the Stuttgart Center for Simulation Science (SimTech).
\appendix

\input{appendix}


\bibliographystyle{elsarticle-num}
\bibliography{bibliography}

\end{document}
