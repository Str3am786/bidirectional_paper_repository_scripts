
\documentclass{article} % For LaTeX2e
%\usepackage{iclr_preprint,times}
\usepackage{iclr2022_conference,times}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{url}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsthm}

\def\x{\bm x}
\def\w{\bm w}
\def\W{\bm W}
\def\d{\partial}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}

\newcommand{\cep}[1]{\textcolor{purple}{[Cengiz: #1]}}
\newcommand{\bb}[1]{\textcolor{red}{[Blake: #1]}}
\newcommand{\aba}[1]{\textcolor{blue}{[Alex: #1]}}


\def\x{\mathbf x}
\def\w{\mathbf w}
\def\v{\mathbf v}
\def\bSigma{\mathbf \Sigma}
\def\A{\mathbf A}
\def\D{\mathcal D}
\def\u{\mathbf u}
\def\I{\mathbf I}
\def\bOne{\bm 1}
\def\G{\mathbf G}
\def\C{\mathbf C}
\def\g{\mathbf g}
\def\h{\mathbf h}
\def\K{\mathbf K}
\def\e{\bm e}
\def\bkappa{\bm\kappa}

\title{Learning Curves for Stochastic Gradient Descent on Structured Features}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Blake Bordelon \& Cengiz Pehlevan
\\
John A. Paulson School of Engineering and Applied Sciences\\
Center for Brain Science\\
Harvard University\\
Cambridge, MA 02138, USA \\
\texttt{\{blake\_bordelon,cpehlevan\}@g.harvard.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy 
% Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
 The generalization performance of a machine learning algorithm such as a neural network depends in an intricate way on the structure of the data distribution. To analyze the influence of data structure on test loss dynamics, we study an exactly solveable model of stochastic gradient descent (SGD) which predicts test loss when training on features with arbitrary covariance structure. We solve the theory exactly for both Gaussian features and arbitrary features and we show that the simpler Gaussian model accurately predicts test loss of nonlinear random-feature models and deep neural networks trained with SGD on real datasets such as MNIST and CIFAR-10. We show that the optimal batch size at a fixed compute budget is typically small and depends on the feature correlation structure, demonstrating the computational benefits of SGD with small batch sizes. Lastly, we extend our theory to the more usual setting of stochastic gradient descent on a fixed subsampled training set, showing that both training and test error can be accurately predicted in our framework on real data.
\end{abstract}



\section{Introduction}

%\bb{Goal: introduce the problem, state why it is important, list our contributions. Move all discussion to the related works and discussion section at the end. }

Understanding the dynamics of SGD on realistic learning problems is fundamental to learning theory. Due to the challenge of modeling the structure of realistic data, theoretical studies of generalization often attempt to derive data-agnostic generalization bounds or study the typical performance of the algorithm on high-dimensional, factorized data distributions \citep{engel_van_den_broeck_2001}. %The first set of theories derive bounds based on the complexity or capacity of the function class and often struggle to explain the success of modern learning systems which generalize well on real data but are sufficiently powerful to fit random noise \citep{mohri_book, Zhang2017UnderstandingDL}. Rather than exploring data-agnostic bounds, it is often useful to analyze how algorithms generalize typically or on average over a stipulated data distribution \citep{engel_van_den_broeck_2001}. In this style of analysis, the data distribution is usually assumed to be highly symmetric, stipulating that input data follows a factorized probability distribution across input variables \citep{Advani_2013}. For example, \textit{spherical cow} models treat data vectors as drawn from the isotropic Gaussian distribution or uniformly from the sphere while Boolean hypercube models treat data as random binary vectors. 
Realistic datasets, however, often lie on low dimensional structures embedded in high dimensional ambient spaces \citep{pope2021the}. For example, MNIST and CIFAR-10 lie on surfaces with intrinsic dimension of $\sim 14$ and $\sim 35$ respectively \citep{Spigler_2020}. To understand the average-case performance of SGD in more realistic learning problems and its dependence on data, model and hyperparameters, incorporating structural information about the learning problem is necessary.

%Models which study such simplified data distributions have been employed in several classic and recent studies exploring the capacity of supervised learning algorithms and associative memory \citep{Gardner1988OptimalSP, Hopfield2554}, overfitting peaks and phase transitions in learning \citep{Hertz_1989, nakkiran2019data, dAscoli_triple, advani_ganguli_convex, mei2020generalization, hu2021universality}, and neural network training dynamics \citep{ADVANI_SAXE_Somp}.

%Incorporating data manifold structure into models of generalization has provided more accurate assessments of classifier capacity \citep{chung_manifold, chung_2}, nonlinear function approximation \citep{Spigler_2020, gerace, bordelon2020spectrum, Canatar2020SpectralBA, loureiro2021capturing, jacot2020kernel, bahri2021explaining}, linear network dynamics \citep{Saxe_McClelland_Ganguli} and two-layer neural network test error \citep{goldt_hidden_manifold,yoshida_okada} on realistic learning problems such as MNIST or CIFAR-10 \citep{lecun-mnisthandwrittendigit-2010,cifar_cite}. The analysis of two layer networks revealed the importance of modeling the intrinsically low-dimensional latent structure of the data when analyzing learning dynamics. The authors of that study propose a \textit{hidden manifold} model of the data where labels are generated by a teacher network which receives the low dimensional latent variables as input \citep{goldt_hidden_manifold}. 

%Of significant practical interest to machine learning theory is the dynamics of the test loss during stochastic gradient descent, which quantifies the expected error rate of the model throughout optimization. Several works have provided asymptotic guarantees for the convergence rate of SGD in general settings \citep{Polyak1992AccelerationOS, Ruppert,shapiro_stoch,robbins_monro, chung, duchi_ruan, yu2020analysis, anastasiu, gurbuzbalaban2020heavy}, obtaining worst case bounds in terms of general assumptions on the structure of the gradient and Hessian of the loss.
%, often invoking decaying step sizes and Polyak-Ruppert averaging of the iterates. Tight asymptotic loss scalings have been obtained for SGD on high dimensional least squares, though only the exponents of the power-law scalings were exactly computed from the feature covariance \citep{berthier2020tight,dieuleveut,pillaudvivien2018statistical, fischer_sobolev}. A recent study showed that such power law scalings in the expected test loss occur under broad conditions on the second and fourth moment structure of high dimensional features \citep{varre2021iterate}. Alternatively, SGD has been studied in the typical case in several works in the spirit of statistical physics, providing exact average test loss expressions for very simple data distributions. These include studies of single layer \citep{heskes, Werfel,Biehl_1994,Mace1998StatisticalMA} and two-layer \citep{saad_solla,lecun_solla_eigenspectrum_cov,goldt_first} neural networks as well as shallow Gaussian mixture classification \citep{mignacco2020dynamical}. 

In this paper, we calculate the average case performance of SGD on models of the form $f(\x) = \w \cdot \bm \psi(\x)$ for nonlinear feature map $\bm\psi$ trained with MSE loss. We express test loss dynamics in terms of the induced second and fourth moments of $\bm\psi$. Under a regularity condition on the fourth moments, we show that the test error can be accurately predicted in terms of second moments alone. We demonstrate the accuracy of our theory on random feature models and wide neural networks trained on MNIST and CIFAR-10 and accurately predict test loss scalings on these datasets. We explore in detail the effect of minibatch size, $m$, on learning dynamics. By varying $m$, we can interpolate our theory between single sample SGD ($m=1$) and gradient descent on the population loss ($m\to\infty$). To explore the computational advantages SGD compared to standard full batch gradient descent we analyze the loss achieved at a fixed compute budget $C = t m$ for different minibatch size $m$ and number of steps $t$, trading off the number of parameter update steps for denoising through averaging. We show that generally, the optimal batch size is small, with the precise optimum dependent on the learning rate and structure of the features. Overall, our theory shows how learning rate, minibatch size and data structure interact with the structure of the learning problem to determine generalization dynamics. It provides a predictive account of training dynamics in wide neural networks.


\subsection{Our Contributions}
The novel contributions of this work are described below.
\begin{itemize}
    \item We calculate the exact expected test error for SGD on MSE loss for arbitrary feature structure in terms of second and fourth moments as we discuss in Section \ref{th_general_fourth}. We show how structured gradient noise induced by sampling alters the loss curve compared to vanilla GD.
    \item For Gaussian features (or those with regular fourth moments), we compute the test error in Section \ref{th_gauss}. This theory is shown to be accurate in experiments with random feature models and wide networks in the kernel regime trained on MNIST and CIFAR-10. %\textcolor{red}{We extend this theory beyond fixed learning rate SGD to momentum, showing possible improvements over standard SGD.}
    \item We show that for fixed compute/sample budgets and structured features with power law spectral decays, optimal batch sizes are small. We study how optimal batch size depends on the structure of the feature correlations and learning rate.
    \item We extend our exact theory to study multi-pass SGD on a fixed finite training set. Both test and training error can be accurately predicted for random feature models on MNIST.
\end{itemize}


\section{Related Work}

%\cep{Add these left-over citations from intro as much as possible} \citep{shapiro_stoch,robbins_monro, chung, duchi_ruan, yu2020analysis, anastasiu, gurbuzbalaban2020heavy}\citep{fischer_sobolev} \citep{heskes,Biehl_1994,Mace1998StatisticalMA} \citep{saad_solla,lecun_solla_eigenspectrum_cov,goldt_first} \citep{mignacco2020dynamical} \citep{mel2021theory}

The analysis of stochastic gradient descent has a long history dating back to seminal works of \citet{Polyak1992AccelerationOS} and \citet{Ruppert}, who analyzed time-averaged iterates in a noisy problem. Many more works have examined a similar setting, identifying how averaged and accelerated versions of SGD perform asymptotically when the target function is noisy (not a deterministic function of the input) \citep{flammarion2015averaging, flammarion_noisy_1_over_n, shapiro_stoch,robbins_monro, chung, duchi_ruan, yu2020analysis, anastasiu, gurbuzbalaban2020heavy}. 

Recent studies have also analyzed the asymptotics of noise-free MSE problems with arbitrary feature structure to see what stochasticity arises from sampling. Prior works have found exponential loss curves problems as an upper bound \citep{jain2018accelerating} or as typical case behavior for SGD on unstructured data \citep{Werfel}. A series of more recent works have considered the over-parameterized (possibly infinite dimension) setting for SGD, deriving power law test loss curves emerge with exponents which are better than the $O(t^{-1})$ rates which arise in the noisy problem \citep{berthier2020tight, pillaudvivien2018statistical, dieuleveut, varre2021iterate, dieuleveut_nonparametric, ying_nonparametric, fischer_sobolev, zou2021benign}.  These works provide bounds of the form $O(t^{-\beta})$ for exponents $\beta$ which depend on the task and feature distribution.
%Our work improves upon these prior results by examining the effect of minibatch size, test train splits, and providing exact expressions for both Gaussian and arbitrary features. 

Several works have analyzed average case online learning in shallow and two-layer neural networks. Classical works often analyzed unstructured data \citep{heskes,Biehl_1994,Mace1998StatisticalMA, saad_solla,lecun_solla_eigenspectrum_cov,goldt_first}, but recently the hidden manifold model enabled characterization of learning dynamics in continuous time when trained on structured data, providing an equivalence with a Gaussian covariates model \citep{goldt_hidden_manifold, goldt2021gaussian}. In the continuous time limit considered in these works, SGD converges to gradient flow on the population loss, where fluctuations due to sampling disappear and order parameters obey deterministic dynamics. Other recent works, however, have provided dynamical mean field frameworks which allow for fluctuations due to random sampling of data during a continuous time limit of SGD, though only on simple generative data models \citep{mignacco2020dynamical, mignacco2021stochasticity}. 

Studies of fully trained linear (in trainable parameters) models also reveal striking dependence on data and feature structure. Analysis for models trained on MSE \citep{bartlett2020benign, tsigler2020benign, bordelon2020spectrum, Canatar2020SpectralBA}, hinge loss \citep{chatterji2021finite, cao2021risk, cao2019generalization} and general convex loss functions \citep{loureiro2021capturing} have now been performed, demonstrating the importance of data structure for offline generalization.

%Studying the simple setting of least squares regression on isotropic Gaussian features, \citet{Werfel} computed average case learning curves for SGD with minibatch size of one. Their results are non-asympotic and exact, though the assumptions on the features are highly restrictive. We generalize their method so that it can describe structured features and arbitrary batch sizes. 

Other works have studied the computational advantages of SGD at different batch sizes $m$. \citet{ma_batch_size} study the tradeoff between taking many steps of SGD at small $m$ and taking a small number of steps at large $m$. After a critical $m$, they observe a saturation effect where increasing $m$ provides diminishing returns. \citet{zhang2019algorithmic} explore how this critical batch size depends on SGD and momentum hyperparameters in a noisy quadratic model. Since they stipulate constant gradient noise induced by sampling, their analysis results in steady state error rather than convergence at late times, which may not reflect the true noise structure induced by sampling.   %Our results improve upon this initial study since we provide an exact analysis of SGD with varying batch size at fixed compute budget and show how the optimal batch sizes depend on the feature covariance. 





\section{Problem Definition and Setup}

We study stochastic gradient descent on a linear model with parameters $\w$ and feature map $\bm\psi(\x) \in \mathbb{R}^N$ (with $N$ possibly infinite). Some interesting examples of linear models are random feature models, where $\bm\psi(\x) = \phi(\bm G \x)$ for random matrix $\bm G$ and point-wise nonlinearity $\phi$ \citep{Rahimi_Recht, mei2020generalization}. Another interesting linearized setting is wide neural networks with neural tangent kernel (NTK) parameterization \citep{jacot2020kernel, Lee_2020}. Here the features are parameter gradients of the neural network function $\bm\psi(\x) = \nabla_{\bm\theta} f(\x,\bm\theta)|_{\bm\theta_0}$ at initialization. We will study both of these special cases in experiments. 

We optimize the set of parameters $\w$ by SGD to minimize a population loss of the form 
\begin{equation}
    L(\w) = \left< \left( \w \cdot \bm\psi(\x) - y(\x) \right)^2 \right>_{\x \sim p(\x)},
\end{equation}
where $\x$ are input data vectors associated with a probability distribution $p(\x)$, $\bm\psi(\x)$ is a nonlinear feature map and $y(\x)$ is a target function which we can evaluate on training samples. We assume that the target function is square integrable $\left< y(\x)^2 \right>_{\x} < \infty$ over $p(\x)$. Our aim is to elucidate how this population loss evolves during stochastic gradient descent on $\w$.  We derive a formula in terms of the eigendecomposition of the feature correlation matrix and the target function
\begin{equation}
    \bSigma = \left< \bm\psi(\x) \bm\psi(\x)^\top \right>_{\x} =  \sum_{k=1}^N \lambda_k \u_k \u_k^\top \ , \quad y(\x) = \sum_{k} v_k \u_k^\top \bm\psi(\x) + y_{\bot}(\x),
\end{equation}
%
where $\left< y_\perp(\x) \bm\psi(\x) \right> = 0$. We justify this decomposition of $y(\x)$ in the Appendix \ref{target_decompose} using an eigendecomposition and show that it is general for target functions and features with finite variance. 



%\subsection{Random Feature Models}
%Our theory can be used to study the popular random feature models on realistic data by constructing a feature map as $\bm\psi(\x) = \phi(\G \x)$ with input data $\x \sim p(\x)$, and projection matrix $\bm G \in \mathbb{R}^{N \times D}$ (usually taken to be a random matrix with Gaussian entries), and nonlinear activation function $\phi$. The random feature model is thus a linear model with covariance structure
%\begin{equation}
%    \bSigma = \left< \phi(\h) \phi(\h)^\top \right>_{\h \sim \mathcal{N}\left(\bm 0, \G \mathbf{\Sigma}_\x \G^\top \right)} \ , \ \mathbf{\Sigma}_\x = \left< \x \x^\top \right>_\x.
%\end{equation}
%By diagonalizing $\bSigma$ we can find eigenvalues $\bm\lambda$ and eigenvectors $\u_k$. These quantities, along with information about the target function, will be inputs into our theory, allowing us to predict learning curves during SGD. 

%\subsection{Kernel Methods and Linearized Neural Networks}

%Wide neural networks behave as linear functions of their parameters around the initialization and nonlinear functions of the input data \citep{Lee_2020}. To study such linearized networks with parameters $\bm\theta$ and initial parameters $\bm\theta_0$ in the framework of our theory, we interpret $\w = \bm\theta-\bm\theta_0$ as the displacement in the weights from initialization. This allows construction of a nonlinear feature map of the form $\bm\psi(\x) = \nabla_{\bm\theta} f(\x,\bm\theta)|_{\bm\theta_0}$. In this setting it suffices to understand the correlation structure
%\begin{equation}
%    \bSigma = \left< \nabla_{\bm\theta} f(\x,\bm\theta) \nabla_{\bm\theta} f(\x,\bm\theta)^\top \right>_{\x \sim p(\x)},
%\end{equation}
%which is simply the Fisher information matrix \citep{Pennington}.



%Let $\bm\psi \in \mathbb{R}^N$ (with $N$ possibly infinite) be feature vectors with correlation structure $\bSigma = \left< \bm\psi \bm\psi^\top \right>$.
During learning, parameters $\w$ are updated to estimate a target function $y$ which, as discussed above, can generally be expressed as a linear combination of features $y = \w^* \cdot \bm\psi + y_{\perp}$. %This decomposition can be performed through a straightforward eigenfunction analysis (See Appendix \ref{target_decompose}).
At each time step $t$, the weights are updated by taking a stochastic gradient step on a fresh mini-batch of $m$ examples 
\begin{equation}
    \w_{t+1} = \w_t - \frac{\eta}{m} \sum_{\mu = 1}^m \bm\psi_{t,\mu} \left( \w_t \cdot \bm\psi_{t,\mu} - y_{t,\mu} \right)
,\end{equation}
where each of the vectors $\bm\psi_{t,\mu}$ are sampled independently and $y_{t,\mu} = \w^* \cdot \bm\psi_{t,\mu}$. The learning rate $\eta$ controls the gradient descent step size while the batch size $m$ gives a empirical estimate of the gradient at timestep $t$. At each timestep, the test-loss, or generalization error, has the form
\begin{equation}
    L_t = \left< \left( \w_t \cdot \bm\psi(\x) - \w^* \cdot \bm\psi(\x) - y_{\perp}(\x) \right)^2 \right>_{\bm \x} = (\w_t-\w^*)^\top \bSigma (\w_t-\w^*) + \left< y_{\perp}(\x)^2 \right>,
\end{equation}
which quantifies exactly the test error of the vector $\w_t$. Note, however, that $L_t$ is a random variable since $\w_t$ depends on the precise history of sampled feature vectors $\mathcal D_t = \{ \bm\psi_{t,\mu} \}$. Our theory, which generalizes the recursive method of \citep{Werfel} allows us to compute the \textit{expected} test loss by averaging over all possible sequences to obtain $\left< L_t\right>_{\mathcal D_t}$. Our calculated learning curves are not limited to the one-pass setting, but rather can accommodate sampling minibatches from a finite training set with replacement and testing on a separate test set which we address in Section \ref{sec:test_train_split}.
%Using a similar technique, we also provide a calculation of the variance $\left< L_t^2 \right> - \left< L_t \right>^2$ which quantifies the fluctuations in the learning curve due to stochastic sampling of features.

In summary, we will develop a theory that predicts the expected test loss $\left< L_t \right>_{\mathcal D_t}$ averaged over training sample sequences $\mathcal D_t$ in terms of the quantities $\{ \lambda_k , v_k, \left< y_{\perp}(\x)^2 \right>_\x \}$. This will reveal how the structure in the data and the learning problem influence test error dynamics during SGD. This theory is a quite general analysis of linear models on square loss, analyzing the performance of linearized models on arbitrary data distributions, feature maps $\bm\psi$, and target functions $y(\x)$. 

\section{Analytic Formulae for Learning Curves}

\subsection{Learnable and Noise Free Problems}\label{sec:learnable_noise_free}

Before studying the general case, we first analyze the setting where the target function is \textit{learnable}, meaning that there exist weights $\w^*$ such that $y(\x) = \w^* \cdot \bm\psi(\x)$. For many cases of interest, this is a reasonable assumption, especially when applying our theory to real datasets by fitting an atomic measure on $P$ points $\frac{1}{P} \sum_{\mu}\delta(\x-\x^\mu)$. We will further assume that the induced feature distribution is Gaussian so that all moments of $\bm\psi$ can be written in terms of the covariance $\bSigma$. We will remove these assumptions in later sections. 

\begin{theorem}\label{th_gauss}
Suppose the features $\bm\psi$ follow a Gaussian distribution $\bm\psi \sim \mathcal N(0, \bSigma)$ and the target function is learnable in these features $y = \w^* \cdot \bm\psi$. After $t$ steps of SGD with minibatch size $m$ and learning rate $\eta$, the expected (over possible sample sequences $\D_t$) test loss $\left< L_t \right>_{\mathcal D_t}$ has the form
\begin{equation}
    \left< L_t \right>_{\D_t} = \bm\lambda^\top \mathbf A^t \mathbf{v}^2 \ , \ \A =  \left( \I - \eta \ \text{diag}(\bm\lambda) \right)^2 + \frac{\eta^2}{m} \ \text{diag}\left( \bm\lambda^2 \right) + \frac{\eta^2}{m} \bm\lambda \bm\lambda^\top
\end{equation}
%
where $\bm\lambda$ is a vector containing the eigenvalues of $\bSigma$ and $\mathbf{v}^2$ is a vector containing elements $(\mathbf{v}^2)_k = v_k^2 = ( \u_k \cdot \w^*)^2$ for eigenvectors $\u_k$ of $\bSigma$. The function $\text{diag}(\cdot)$ constructs a diagonal matrix with the argument vector placed along the diagonal.  
\end{theorem}
\begin{proof}
See Appendix \ref{gauss_derivation} for the full derivation. We will provide a brief sketch of the proof here. The strategy of the proof relies on the fact that $\left< L_t \right> = \text{Tr} \ \bm\Sigma \ \C_t$ where $\C_t = \left< \left( \w_t - \w^* \right)(\w_t-\w^*)^\top \right>_{\mathcal D_t}$. We derive the following recursion relation for this error matrix
\begin{align}
    \C_{t+1} = (\I - \eta \bm\Sigma) \C_t (\I -\eta \bm\Sigma) + \frac{\eta^2}{m}\left[ \bm\Sigma \C_t \bm\Sigma + \bm\Sigma \text{Tr}\left( \bm\Sigma  \C_t \right) \right]
\end{align}
The loss only depends on $c_{k,t} = \u_k^\top \C_t \u_k$. Solving the recurrence, $\bm c_{t} = \A^t \mathbf{v}^2$ and using $\left< L_t \right> = \sum_k \lambda_k \u_k^\top \C_t \u_k = \sum_k c_{k,t} \lambda_k = \bm\lambda^\top \A^t \mathbf{v}^2$, we obtain the desired result.
\end{proof}

Below we provide some immediate interpretations of this result.
\begin{itemize}[leftmargin=*]
    \item The matrix $\A$ contains two components; a matrix $\left( \I - \eta \ \text{diag}(\bm\lambda) \right)^2$ which represents the time-evolution of the loss under \textit{average gradient updates}. The remaining matrix $\frac{\eta^2}{m} \left( \text{diag}(\bm\lambda^2) + \bm\lambda \bm\lambda^\top \right)$ arises due to fluctuations in the gradients, a consequence of the stochastic sampling process. 
    \item The test loss obtained when training directly on the population loss can be obtained by taking the minibatch size $m \to \infty$. In this case, $\A \to (\I - \eta \  \text{diag}(\bm\lambda))^2$ and one obtains the population loss $L_t^{pop} = \sum_k v_k^2 \lambda_k (1-\eta\lambda_k)^{2t}$. This population loss can also be obtained by considering small learning rates, i.e. the $\eta \to 0$ limit, where $\A = (\I - \eta \ \text{diag}(\bm\lambda))^2 + O(\eta^2)$. 
    \item For general $\bm\lambda$ and $\eta^2 / m > 0$, $\A$ is non-diagonal, indicating that the components $\{\u_1,...,\u_k\}$ are not learned independently as $t$ increases like for $L^{pop}_t$, but rather interact during learning due to non-trivial coupling across eigenmodes at large $\eta^2/m$. This is unlike offline theory for learning in feature spaces (kernel regression), \citep{bordelon2020spectrum, Canatar2020SpectralBA}, this observation of mixing across covariance eigenspaces agrees with a recent analysis of SGD, which introduced recursively defined ``mixing terms" that couple each mode's evolution \citep{varre2021iterate}. 
    %\item The maximum learning rate that guarantees convergence is not the usual $2/\lambda_1$ bound for the population loss gradient descent dynamics, but is a smaller value $\eta$ for which guarantees that all eigenvalues of $\A$ are less than unity \cp{is this obvious?}. 
    \item Though increasing $m$ always improves generalization at fixed time $t$ (proof given in Appendix \ref{minibatch_deriv}), learning with a fixed compute budget (number of gradient evaluations) $C = t m$, can favor smaller batch sizes. We provide an example of this in the next sections and Figure \ref{fig:isotropic} (d)-(f).
    \item The lower bound $\left< L_t\right> \geq \bm\lambda^\top \bm v^2 \left[ (1-\eta)^2 + \frac{\eta^2}{m} |\bm\lambda|^2 \right]^t$ can be used to find necessary stability conditions on $m,\eta$. This bound implies that $\left<L_t\right>$ will diverge if $m < \frac{\eta }{2-\eta} |\bm\lambda|^2$. The learning rate must be sufficiently small and the batch size sufficiently large to guarantee convergence. This stability condition depends on the features through $|\bm\lambda|^2 = \sum_k \lambda_k^2$. One can derive heuristic optimal batch sizes and optimal learning rates through this lower bound. See Figure \ref{fig:optimal_batch_size} and Appendix \ref{stability_proof}.
\end{itemize}

%We not only can compute the average test loss at time $t$, but also its variance $ \text{Var}[L_t] = \left< L_t^2 \right> - \left< L_t \right>^2$.
%\begin{theorem}\label{var_loss_th}
%Assuming Gaussian features $\bm\psi \sim \mathcal{N}(0,\bSigma)$ and a learnable target function $y$, the variance of the loss $L_t$ at time $t$ is
%\begin{equation}
%    \text{Var}[L_t] = {\bm\lambda^2}^\top \left[ \A^t \mathbf{v}^2 \right]^2 + \sum_{i \neq j} v_i^2 v_j^2 \lambda_i \lambda_j \left( 1- \eta (\lambda_i + \lambda_j) + \frac{\eta^2 (m+1)}{m} \lambda_i \lambda_j \right)^{2t},
%\end{equation}
%where $\left[ \cdot \right]^2$ is element wise square and $\A$ is defined in the same way as the expected loss formula. 
%\end{theorem}

%\begin{proof}A proof is provided in Appendix %\ref{variance_proof}.
%\end{proof}

\subsubsection{Special Case 1: Unstructured Isotropic Features}

This special case was previously analyzed by  \citet{Werfel} which takes $\bSigma = \I \in \mathbb{R}^{N \times N}$ and $m = 1$. We extend their result for arbitrary $m$, giving the following learning curve
\begin{equation}\label{eq:unstruct}
    \left< L_t \right>_{\D_t}
    = \left( \left( 1 -  \eta\right)^2 + \frac{1+N}{m}\eta^2  \right)^t ||\w^*||^2  \ , \quad \left< L_t^* \right>_{\mathcal D_t} = \left(1-\frac{m}{m+N+1} \right)^t ||\w^*||^2,
\end{equation}
where the second expression has optimal $\eta$. First, we note the strong dependence on the ambient dimension $N$: as $N \gg m$, learning happens at a rate $ \left< L_t \right> \sim e^{-t m/N}$. Increasing the minibatch size $m$ improves the exponential rate by reducing the gradient noise variance. Second, we note that this feature model has the same rate of convergence for every learnable target function $y$. At small $m$, the convergence at any learning rate $\eta$ is much slower than the convergence of the $m\to\infty$ limit, $L_{pop} = (1-\eta)^{2t} ||\w^*||^2$ which does not suffer from a dimensionality dependence due to gradient noise. Lastly, for a fixed compute budget $C=tm$, the optimal batch size is $m^* = 1$; see Figure \ref{fig:isotropic} (d). This can be shown by differentiating $\left< L_{C/m} \right>$ with respect to $m$ (see Appendix \ref{fixed_compute_minibatch}). 
In Figure \ref{fig:isotropic} (a) we show  theoretical and simulated learning curves for this model for varying values of $N$ at the optimal learning rate and in Figure \ref{fig:isotropic} (d), we show the loss as a function of minibatch size for a fixed compute budget $C = t m = 100$. While fixed $C$ represents fixed sample complexity, we stress that it may not represent wall-clock run time when data parallelism is available  \citep{shallue2018measuring}. 
%\begin{itemize}[leftmargin=*]
%    \item Strong dimension dependence: as $N \gg m$, we see that, learning happens at a rate $ \left< L_t \right> \sim e^{-t m/N}$. This small exponent is due to the necessity of scaling $\eta$ inversely with the dimension $N$ since the term coming from gradient variance in \eqref{eq:unstruct} scales like $N/m$. Increasing the minibatch size $m$ improves the exponential rate by reducing the gradient noise variance. In the large batch limit $m \gg N$, the optimal loss scales as $\left< L_t \right> \sim \left( \frac{N}{m} \right)^t$. We also note that this feature model has the same rate of convergence for every learnable target function $y$.
%    \item At small $m$, the convergence at any learning rate $\eta$ is much slower than the convergence of the $m\to\infty$ limit, $L_{pop} = (1-\eta)^{2t} ||\w^*||^2$ which does not suffer from a dimensionality dependence due to gradient noise. 
%    \item For a fixed compute budget $C=tm$, the optimal batch size is $m^* = 1$; see Figure \ref{fig:isotropic} (d). This can be shown by differentiating $\left< L_{C/m} \right>$ with respect to $m$ (see Appendix \ref{fixed_compute_minibatch})
%\end{itemize}


\begin{figure}[t]
    \centering
    \subfigure[Isotropic Features]{\includegraphics[width=0.32\linewidth]{figures/isotropic_vary_N.pdf}}
    \subfigure[Power Law Features]{\includegraphics[width=0.32\linewidth]{figures/power_law_vary_N.pdf}}
    \subfigure[MNIST Random ReLU Features]{\includegraphics[width=0.32\linewidth]{figures/test_err_mnist_random_power_law_RF.pdf}}
    \subfigure[Fixed Compute Isotropic ]{\includegraphics[width=0.32\linewidth]{figures/isotropic_optimal_m.pdf}}
    \subfigure[Fixed Compute Power Law]{\includegraphics[width=0.32\linewidth]{figures/optimal_m_powerlaw_synthetic.pdf}}
    \subfigure[Fixed Compute ReLU MNIST ]{\includegraphics[width=0.32\linewidth]{figures/optimal_m_powerlaw.pdf}}
    \caption{ Isotropic features generated as $\bm\psi \sim \mathcal{N}(0,\I)$ have qualitatively different learning curves than power-law features observed in real data. Black dashed lines are theory. (a) Online learning with $N$-dimensional isotropic features gives a test loss which scales like $L_t \sim e^{-t/N}$ for \textit{any target function}, indicating that learning requires $t \sim N$ steps of SGD, using the optimal learning rates $\eta^* = \frac{m}{N+m+1}$. (b) Power-law features $\bm\psi \sim \mathcal{N}(0,\bm\Lambda)$ with $\Lambda_{kl} = \delta_{k,l} k^{-2}$ have non-extensive give a \textit{power-law scaling} $L_t \sim t^{-\beta}$ with exponent $\beta = O_N(1)$. (c) Learning to discrimninate MNIST 8's and 9's with $N=4000$ dimensional random ReLU features \citep{Rahimi_Recht}, generates a power law scaling at large $t$, which is both quantitatively and qualitatively different than the scaling predicted by isotropic features $e^{-t/N}$. (d)-(f) The loss at a fixed compute budget $C = tm = 100$ for (d) isotropic features, (e) power law features and (f) MNIST ReLU random features with simulations (dots average and standard deviation for $30$ runs).  Intermediate batch sizes are preferable on real data. }
    \label{fig:isotropic}
\end{figure}

\subsubsection{Special Case 2: Power Laws and Effective Dimensionality}\label{powerlaw_subsec}

\begin{figure}[t]
    \centering
    \subfigure[Power Law Exponents $b$]{\includegraphics[width=0.32\linewidth]{figures/loss_vs_m_b.pdf}}
    \subfigure[Optimal Batchsize vs $\bm\lambda$]{\includegraphics[width=0.32\linewidth]{figures/optimal_batch_vs_b.pdf}}
    %\subfigure[Color varies $\sigma$; $C=500$]{\includegraphics[width=0.32\linewidth]{figures/loss_vs_m_sigma.pdf}}
    \subfigure[Hyper-parameter Dependence]{\includegraphics[width=0.32\linewidth]{figures/true_theory_heatmap.pdf}}
    \caption{Optimal batch size depends on feature structure and noise level. (a) For power law features $\lambda_k \sim k^{-b}$, $\lambda_k v_k^2 \sim k^{-a}$, the $m$ dependence of the loss $L_{C/m}$ depends strongly on the feature exponent $b$. Each color is a different $b$ value, evenly spaced in $[0.6,2.5]$ with $a=2.5, C=500$. Solid lines show exact theory while dashed lines show the error predicted by approximating the mode coupling term $\frac{\eta^2}{m} \bm\lambda\bm\lambda^\top$ with decoupled $\frac{\eta^2}{m} \text{diag}(\bm\lambda^2)$. Mode coupling is thus necessary to accurately predict optimal $m$. (b) The optimal $m$ scales proportionally with $|\bm\lambda|^2 \approx \frac{1}{2b-1}$. We plot the lower bound $m_{min}$ (black), the heuristic optimum  ($m$ which optimizes a lower bound for $L$, green) and $\frac{2 \eta}{2-\eta}|\bm\lambda|^2$ (red). (c) The loss at fixed compute $C=150$, $a=2$, $b=0.85$, optimal batchsize $m$ for each $\eta$ shown in dashed black. For sufficiently small $\eta$, the optimal batchsize is $m=1$. For large $\eta$, it is better to trade off update steps for denoised gradients resulting in $m^* > 1$.  }
    \label{fig:optimal_batch_size}
\end{figure}

Realistic datasets such as natural images or audio tend to exhibit nontrivial correlation structure, which often results in power-law spectra when the data is projected into a feature space, such as a randomly intialized neural network \citep{Spigler_2020,Canatar2020SpectralBA, bahri2021explaining}. In the $\frac{\eta^2}{m} \ll 1$ limit, if the feature spectra and task specra follow power laws, $\lambda_k \sim k^{-b}$ and $\lambda_k v_k^2 \sim k^{-a}$ with $a, b > 1$, then Theorem \ref{th_gauss} implies that generalization error also falls with a power law: $\left< L_t \right> \sim C t^{- \beta }, \quad \beta = \frac{a-1}{b}$ where $C$ is a constant. See Appendix \ref{Power_law_appendix} for a derivation with saddle point integration. %For a fixed feature map, some target functions $y(\x) = \w^* \cdot \bm\psi(\x)$ will be easier to learn than others indicating a strong inductive bias.  
Notably, these predicted exponents we recovered as a special case of our theory agree with prior work on SGD with power law spectra, which give exponents in terms of the feature correlation structure \citep{berthier2020tight, dieuleveut, velikanov2021universal, varre2021iterate}.   Further, our power law scaling appears to accurately match the qualitative behavior of wide neural networks trained on realistic data \citep{hestness2017deep,bahri2021explaining}, which we study in Section \ref{sec:NN}.

We show an example of such a power law scaling with synthetic features in Figure \ref{fig:isotropic} (b). Since the total variance approaches a finite value as $N\to\infty$, the learning curves are relatively insensitive to $N$, and are rather sensitive to the eigenspectrum through terms like $|\bm\lambda|^2$ and $\bm 1^\top \bm\lambda$, etc. In Figure \ref{fig:isotropic} (c), we see that the scaling of the loss is more similar to the power law setting than the isotropic features setting in a random features model of MNIST, agreeing excellently with our theory. For this model, we find that there can exist optimal batch sizes when the compute budget $C = tm$ is fixed (Figure \ref{fig:isotropic} (e) and (f)). 
In  Appendix \ref{app:heuristic_optimal_hyperparams}, we heuristically argue that the optimal batch size for power law features should scale as, $m^* \approx \frac{1}{(2b-1)}$. Figure \ref{fig:optimal_batch_size} tests this result.  %Again, an  optimal batch size exists when the compute budget $C = tm$ is fixed (Figure \ref{fig:isotropic} (f)).  

We provide further evidence of the existence of power law structure on realistic data in Figure \ref{fig:feature_spectra_learning_curve} (a)-(c), where we provide spectra and test loss learning curves for MNIST and CIFAR-10 on ReLU random features. The eigenvalues $\lambda_k \sim k^{-b}$ and the task power tail sums $\sum_{n=k}^\infty \lambda_n v_n^2 \sim k^{-a+1}$ both follow power laws, generating power law test loss curves. These learning curves are contrasted with isotropically distributed data in $\mathbb{R}^{784}$ passed through the same ReLU random feature model and we see that structured data distributions allow much faster learning than the unstructured data. Our theory is predictive across variations in learning rate, batch size and noise (Figure \ref{fig:feature_spectra_learning_curve}).


%As we discussed in the previous section, the contribution of the error from each eigenmode decouple in the $\frac{\eta^2}{m} \to 0$ limit. In this limit, each eigendirection $\u_k$ is learned with a different timeconstant $\left< L_t \right> \sim \sum_k \left( \w^* \cdot \u_k \right)^2 \lambda_k \exp(- 2\eta\lambda_k t)$.  Thus, the coefficient along direction $\u_k$ is learned with a time-constant $\tau_k = \frac{1}{2\eta\lambda_k}$. Noting that $\lambda_k (\w^* \cdot \u_k)^2$ is the variance of $y(\x)$ along the $k$-th eigenfunction, it follows that tasks  $y$ which have most of their variance in the top eigenspace will be learned rapidly since their variance is estimated using feature space directions with small time-constants. Thus, feature maps which give better alignment to the task (larger $a$) will have better generalization. For $t \approx \tau_{k^*}$, the error can be crudely approximated as a tail sum of remaining variance in the target function $\left< L_{\tau_{k^*}} \right> \sim \sum_{k > k^*} v_k^2 \lambda_k$. This motivates use of tail sums to quantify feature and task alignment. 





%Since $t$ is not only the number of training steps, but also is proportional to the number of samples from the distribution, we can compare this power law scaling to what is obtained in the offline equivalent of our model, known as kernel regression. In this setting the scaling exponents for ridgeless kernel interpolation is $\beta = a-1$, while the reg ularized kernel ridge regression exponent gives $\beta = \frac{a-1}{b}$ \citep{bordelon2020spectrum,Spigler_2020,bahri2021explaining}. The equivalence of the SGD training exponent and the regularized kernel ridge regression exponent suggests that SGD has an implicit regularizing influence \cp{not sure if I understand this}.


\begin{figure}[t]
    \centering
    \subfigure[Feature Spectra]{\includegraphics[width=0.32\linewidth]{figures/spectra_relu_mnist_vs_iso.pdf}}
    \subfigure[Task Power Tail Sum ]{\includegraphics[width=0.32\linewidth]{figures/task_power_relu_mnist_vs_iso.pdf}}
    \subfigure[Learning Curves $m=5$]{\includegraphics[width=0.32\linewidth]{figures/structured_data.pdf}}
    %\subfigure[MNIST Feature Spectra]{\includegraphics[width=0.32\linewidth]{figures/spectra_relu_tanh_mnist_random_features.pdf}}
    %\subfigure[Task Power Tail Sum]{\includegraphics[width=0.32\linewidth]{figures/task_power_relu_tanh_mnist_random_features.pdf}}
    %\subfigure[Learning Curves $m=5$]{\includegraphics[width=0.32\linewidth]{figures/test_err_relu_tanh_mnist_random_features_m_5.pdf}}
    %\subfigure[MNIST vs Isotropic Features]{}
    \subfigure[Vary $\eta$ (20 trials) ]{\includegraphics[width=0.32\linewidth]{figures/mnist_random_feature_vary_lr_avg.pdf}}
    \subfigure[Vary $m$ (20 Trials)]{\includegraphics[width=0.32\linewidth]{figures/mnist_random_feature_vary_batch_avg.pdf}}
    \subfigure[Vary $\sigma$ (20 Trials)]{\includegraphics[width=0.32\linewidth]{figures/mnist_random_feature_vary_noise_avg.pdf}}
    
    %\subfigure[Explicit Feature Dimension $N$]{ \includegraphics[width=0.32\linewidth]{figures/spectra_relu_mnist_vary_N.pdf}}
    %\subfigure[Total Feature Variance vs $N$]{\includegraphics[width=0.32\linewidth]{figures/spectra_sums_relu_mnist_vary_N.pdf}}
    %\subfigure[Participation Ratio]{\includegraphics[width=0.32\linewidth]{figures/spectra_participation_relu_mnist_vary_N.pdf}}
    \caption{Structure in the data distribution, nonlinearity, batchsize and learning rate all influence learning curves. (a) ReLU random feature embedding in $N=4000$ dimensions of MNIST and CIFAR images have very different eigenvalue scalings than spherically isotropic vectors in $784$ dimensions. (b) The task power spectrum decays much faster for MNIST than for random isotropic vectors. (c) Learning curves reveal the data-structure dependence of test error dynamics. Dashed lines are theory curves derived from equation. 
    %(d) The spectra of the random feature map for nonlinearities $\phi(\cdot) \in \{ \tanh(\cdot), \max(0,\cdot) \}$. The ReLU features have higher dimensionality than the Tanh features (slower decay in $\lambda_k$). (e) The tail sums of projection values reveal that the top $k$ eigenfunctions explain a greater fraction of variance in the target function for the ReLU random features compared to Tanh random features. (f) Experimental (solid) and theory (dashed) learning curves $L_t$ for the two models. As expected from the task and feature spectra, the ReLU model obtains a better rate at large $t$. 
    (d) Increasing the learning rate increases the initial speed of learning but induces large fluctuations in the loss and can be worse at large $t$. Experiment curves averaged over $20$ random trajectories of SGD. (e) Increasing the batch size alters both the average test loss $L_t$ and the variance. (f)  Noise in the target values during training produces an asymptotic error $L_{\infty}$ which persists even as $t \to \infty$. }
    \label{fig:feature_spectra_learning_curve}
\end{figure}


\subsection{Arbitrary Induced Feature Distributions: The General Solution}\label{th_general_fourth}

The result in the previous section was proven exactly for Gaussian vectors (see Appendix \ref{gauss_derivation}). For arbitrary distributions, we obtain a slightly more involved result (see Appendix \ref{non_gauss}).

\begin{theorem}\label{arbitary_features}
Let $\bm\psi(\x) \in \mathbb{R}^N$ be an arbitrary feature map with covariance matrix $\bSigma = \sum_k \lambda_k \u_k \u_k^\top$. After diagonalizing the features $\phi_k(\x) = \u_k^\top \bm\psi(\x)$, introduce the fourth moment tensor $\kappa_{ijkl}^4 = \left< \phi_i \phi_j \phi_k \phi_l \right>$. The expected loss is exactly $\left< L_t \right> = \sum_k \lambda_k c_k( \bm\lambda, \bm \kappa, \v, t)$. 
\end{theorem}
We provide an exact formula for $c_k$ in the Appendix \ref{non_gauss} 
We see that the test loss dynamics depends \textit{only} on the second and fourth moments of the features through quantities $\lambda_k$ and $\kappa_{ijk\ell}$ respectively. We recover the Gaussian result as a special case when $\kappa_{ijkl}$ is a simple weighted sum of these three products of Kronecker tensors $\kappa_{ijkl}^{Gauss} = \lambda_i \lambda_j \delta_{ik}\delta_{jl} + \lambda_i \lambda_k \delta_{ij}\delta_{kl} + \lambda_{i} \lambda_j \delta_{il}\delta_{jk}$. As an alternative to the above closed form expression for $\left< L_t \right>$, a recursive formula which tracks $N$ mixing coefficients has also been used to analyze the test loss dynamics for arbitrary distributions \citep{varre2021iterate}. 

 Next we show that a regularity condition, similar to those assumed in other recent works \citep{jain2018accelerating,berthier2020tight, varre2021iterate}, on the fourth moment structure of the features allows derivation of an upper bound which is qualitatively similar to the Gaussian theory. 
\begin{theorem}\label{th_regularity}
If the fourth moments satisfy $\left< \bm\psi \bm\psi^\top \bm G \bm\psi \bm\psi^\top  \right> \preceq (\alpha+1) \bm\Sigma \bm G \bm \Sigma + \alpha \bm\Sigma \text{Tr} \bm\Sigma \bm G$ for any positive-semidefinite $\bm G$, then
\begin{align}
    L_t \leq \bm\lambda^\top \A^t \mathbf{v}^2 \ , \ \A = \left( \I - \eta  \ \text{diag}(\bm\lambda) \right)^2 + \frac{\alpha \eta^2}{m} \left[ \text{diag}(\bm\lambda^2) +  \bm\lambda\bm\lambda^\top \right].
\end{align}
\end{theorem}
We provide this proof in Appendix \ref{app_proof_regularity}. We note that the assumed bound on the fourth moments is tight for Gaussian features with $\alpha=1$, recovering our previous theory. Thus, if this condition on the fourth moments is satisfied, then the loss for the non-Gaussian features is upper bounded by the Gaussian test loss theory with the batch size effectively altered $\tilde{m} = m / \alpha$. 

The question remains whether the Gaussian approximation will provide an accurate model on \textit{realistic data}. We do not provide a proof of this conjecture, but verify its accuracy in empirical experiments on MNIST and CIFAR-10 as shown in Figure \ref{fig:feature_spectra_learning_curve}. In Appendix Figure \ref{fig:nonlin_batch_noise}, we show that the fourth moment matrix for a ReLU random feature model and its projection along the eigenbasis of the feature covariance is accurately approximated by the equivalent Gaussian model. %We provide some additional experimental evidence that $\left< \bm\psi \bm\psi^\top \bm G \bm\psi \bm\psi^\top \right>$ is close to the Gaussian expectation for the MNIST and CIFAR-10 random feature models in \ref{app_proof_regularity}.

\subsection{Unlearnable or Noise Corrupted Problems}

In general, the target function $y(\x)$ may depend on features which cannot be expressed as linear combinations of features $\bm\psi(\x)$, $y(\x) = \w^* \cdot \bm\psi(\x) + y_\perp(\x)$. Let $\left< y_\perp(\x)^2 \right>_\x=\sigma^2$. Note that $y_{\perp}$ need not be deterministic, but can also be a stochastic process which is uncorrelated with $\bm\psi(\x)$.

\begin{theorem}\label{th_unlearnable}
For a target function with unlearnable variance $\left< y_{\perp}^2 \right> = \sigma^2$ trained on Gaussian $\bm\psi$, the expected test loss has the form
\begin{equation}
    \left< L_t \right> - \sigma^2 = \bm\lambda^\top \A^t \mathbf{v}^2 + \frac{1}{m} \eta^2\sigma^2 \bm\lambda^\top (\I - \A)^{-1} (\I - \A^t) \bm\lambda
\end{equation}
which has an asymptotic, irreducible error $\left< L_\infty \right> = \sigma^2 + \frac{1}{m} \eta^2 \sigma^2 \bm\lambda^\top (\I - \A)^{-1} \bm\lambda$ as $t \to \infty$.
\end{theorem}
See Appendix \ref{app_proof_unlearnable} for the proof. The convergence to the asymptotic error takes the form $\left< L_t - L_\infty\right> = \bm\lambda^\top \A^t \left( \mathbf{v}^2 - \frac{1}{m} \eta^2 \sigma^2 (\I - \A)^{-1} \bm\lambda \right)$. We note that this quantity is not necessarily monotonic in $t$ and can exhibit local maxima for sufficiently large $\sigma^2$, as in Figure \ref{fig:feature_spectra_learning_curve} (f).  %This is reminiscent of the sample-wise double descent phenomenon in offline learning curves \citep{mei2020generalization,nakkiran2019data, Canatar2020SpectralBA, dAscoli_triple}, yet the peaking behavior in this model is limited to linear combinations of decaying exponentials $\left< L_t - L_{\infty}\right> = \sum_{k} c_k \alpha_k^t$ (where $\alpha_k$ are the eigenvalues of $\A$) rather than divergences of the form $|t-t^*|^{-1}$, as in the offline double descent model.

%\cp{Decaying Learning Rate?}

\subsection{Test/Train Splits}\label{sec:test_train_split}
Rather than interpreting our theory as a description of the average test loss during SGD in a one-pass setting, where data points are sampled from the a distribution at each step of SGD, our theory can be suitably modified to accommodate multiple random passes over a finite training set. To accomplish this, one must first recognize that the training and test distributions are different. 
\begin{theorem}\label{thm_test_train_split}
Let $\hat{p}(\x) = \frac{1}{M} \sum_\mu \delta(\x-\x^\mu)$ be the empirical distribution on the $M$ training data points and let $\bm{\hat\Sigma} = \left< \bm\psi(\x) \bm\psi(\x)^\top \right>_{\x\sim\hat{p}(\x)} = \sum_k \hat{\lambda}_k \u_k \u_k^\top $ be the feature correlation matrix on this training set. Let $p(\x)$ be the test distribution $\bm\Sigma$ its corresponding feature correlation. Then we have
\begin{align}
   \left< L_{\text{train}, t} \right> &= \text{Tr}\left[  \hat{\bm \Sigma} \C_t \right] \ , \quad \left< L_{\text{test}, t} \right> = \text{Tr}\left[ \bm\Sigma \C_t \right] \nonumber 
   \\
   \C_{t+1} &= (\I - \eta \bm{\hat\Sigma}) \C_t (\I - \eta \bm{\hat \Sigma} ) + \frac{\eta^2}{m}\left[\left< \bm\psi(\x) \bm\psi(\x)^\top \C_t \bm\psi(\x) \bm\psi(\x)^\top \right>_{\x\sim \hat{p}(\x)} - \bm{\hat\Sigma} \C_t \bm{\hat\Sigma} \right]
   %\\
   %\u_k^\top \C_{t} \u_k &= \left[ \A^t \bm v^2 \right]_k \ , \quad \u_k^\top \C_{t} \u_\ell = \left(1-\eta\hat{\lambda}_k-\eta \hat{\lambda}_\ell + \eta^2 \left(1 + \frac{1}{m} \right)\hat{\lambda}_k \hat{\lambda}_\ell \right)^t v_k v_\ell
   %\C_{t+1} &= \C_t - \eta\bm{\hat \Sigma} \C_t - \eta \C_t \bm{\hat \Sigma} + \eta^2 \left(1-\frac{1}{m} \right) \bm{\hat\Sigma} \C_t \bm{\hat\Sigma} + \frac{\eta^2}{m} \left< \bm\psi \bm\psi^\top \C_t \bm\psi \bm\psi^\top \right>.
\end{align}
%where $\bm A = \left( (\bm I - \text{diag}(\bm{\hat\lambda}) )^2 + \frac{\eta^2}{m} \left[\text{diag}(\bm{\hat\lambda}^2) + \bm{\hat \lambda} \bm{\hat \lambda}^\top \right] \right)$.
\end{theorem}
We provide the proof of this theorem in Appendix \ref{app:test_train_split}. The interpretation of this result is that it provides the expected training and test loss if, at each step of SGD, $m$ points from the training set $\{\x^1,...,\x^M\}$ are sampled uniformly with replacement and used to calculate a stochastic gradient. Note that while $\bm\Sigma$ can be full rank, the rank of $\bm{\hat\Sigma}$ has rank upper bounded by $M$, the number of training samples. The recurrence for $\C_t$ can again be more easily solved under a Gaussian approximation which we employ in Figure \ref{fig:test_train_split}. Since learning will only occur along the $M$ dimensional subspace spanned by the data, the test error will have an irreducible component at large time, as evidenced in Figure \ref{fig:test_train_split}. While the training errors continue to go to zero, the test errors saturate at a $M$-dependent final loss. This result can also allow one to predict errors on other test distributions. 
\begin{figure}[t]
    \centering
    \subfigure[MNIST Training Error]{\includegraphics[width=0.32\linewidth]{figures/train_split_vary_M.pdf}}
    \subfigure[MNIST Test Error]{\includegraphics[width=0.32\linewidth]{figures/test_split_vary_M.pdf}}
    \caption{Training and test errors of a model trained on a training set of size $M$ can be computed with the $\C_t$ matrix. Dashed black lines are theory. (a) The training error for MNIST random feature model approaches zero asymptotically. (b) The test error saturates to a quantity dependent on $M$. }
    \label{fig:test_train_split}
\end{figure}

\section{Comparing Neural Network Feature Maps}\label{sec:NN}

We can utilize our theory to compare how wide neural networks of different depths generalize when trained with SGD on a real dataset. With a certain parameterization, large width NNs are approximately linear in their parameters \citep{Lee_2020}. To predict test loss dynamics with our theory, it therefore suffices to characterize the geometry of the gradient features $\bm\psi(\x) = \nabla_{\bm\theta} f(\x,\bm\theta)$. In Figure \ref{fig:NTK_fig}, we show the Neural Tangent Kernel (NTK) eigenspectra and task-power spectra for fully connected neural networks of varying depth, calculated with the Neural Tangents API \citep{neuraltangents2020}. We compute the kernel on a subset of $10,000$ randomly sampled MNIST images and estimate the power law exponents for the kernel and task spectra $\lambda_k$ and $v_k^2$. Across architectures, the task spectra $v_k^2$ are highly similar, but that the kernel eigenvalues $\lambda_k$ decay more slowly for deeper models, corresponding to a smaller exponent $b$. As a consequence, deeper neural network models train more quickly during stochastic gradient descent as we show in Figure \ref{fig:NTK_fig} (c). After fitting power laws to the spectra $\lambda_k \sim k^{-b}$ and the task power $v_k^2 \sim k^{-a}$, we compared the true test loss dynamics (color) for a width-500 neural network model with the predicted power-law scalings $\beta = \frac{a-1}{b}$ from the fit exponents $a,b$. The predicted scalings from NTK regression accurately describe trained width-500 networks. On CIFAR-10, we compare the scalings of the CNN model and a standard MLP and find that the CNN obtains a better exponent due to its faster decaying tail sum $\sum_{n=k}^{\infty} \lambda_n v_n^2$. We stress that the exponents $\beta$ were estimated from our one-pass theory, but were utilized experiments on a finite training set. This approximate and convenient version of our theory is quite accurate across these varying models, in line with recent conjectures about early training dynamics \citep{nakkiran2021the}.

\begin{figure}[t]
    \centering
    %\includegraphics[width=0.32\linewidth]{figures/cifar_conv_vs_MLP_spectra_scaling_exponents.pdf}
    %\includegraphics[width=0.32\linewidth]{figures/cifar_conv_vs_MLP_task_scaling_exponents_tailsum.pdf}
    %\includegraphics[width=0.32\linewidth]{figures/wide_conv_vs_mlp_cifar (1).pdf}
    \subfigure[MNIST NTK Spectra]{\includegraphics[width=0.32\linewidth]{figures/mnist_spectra_scaling_exponents_depth_no_fit.pdf}}
    \subfigure[MNIST Task Spectra]{\includegraphics[width=0.32\linewidth]{figures/mnist_task_scaling_exponents_depth_tailsum_no_fit.pdf}}
    \subfigure[Test Loss Scaling Laws]{\includegraphics[width=0.32\linewidth]{figures/mlp_depth_scalings_2.pdf}}
    \subfigure[CIFAR-10 NTK Spectra]{\includegraphics[width=0.32\linewidth]{figures/cifar_conv_vs_MLP_spectra_scaling_exponents.pdf}}
    \subfigure[CIFAR-10 Task Spectra]{\includegraphics[width=0.32\linewidth]{figures/cifar_conv_vs_MLP_task_scaling_exponents_tailsum_better_title.pdf}}
    \subfigure[Test Loss Scalings]{\includegraphics[width=0.32\linewidth]{figures/wide_conv_vs_mlp_cifar_evenlonger.pdf}}
    \caption{ReLU neural networks of depth $D$ and width $500$ are trained with SGD on full MNIST. (a)-(b) Feature and spectra are estimated by diagonalizing the infinite width NTK matrix on the training data. We fit a simple power law to each of the curves $\lambda_k \sim k^{-b}$ and $v_k^2 \sim k^{-a}$. (c) Experimental test loss during SGD (color) compared to theoretical power-law scalings $t^{-\frac{a-1}{b}}$ (dashed black). Deeper networks train faster due to their slower decay in their feature eigenspectra $\lambda_k$, though they have similar task spectra. (d)-(f) The spectra and test loss for convolutional and fully connected networks on CIFAR-10. The CNN obtains a better convergence exponent due to its faster decaying task spectra. The predicted test loss scalings (dashed black) match experiments (color). }
    \label{fig:NTK_fig}
\end{figure}


%Though our model successfully incorporates the structure of the data into a prediction of the test loss dynamics, it is limited in that it applies to linearized machine learning models, where one learns a linear combinations of nonlinear static features. Thus, our theory's application to artificial neural networks is limited to random feature models, where only the last layer is trained, or deep networks in the lazy learning regime, where the network acts as a structured and static feature map \citep{Chizat}. In finite width neural networks, understanding the test loss dynamics during SGD will require coping with non-convexity of the objective and the time evolution of the gradient features. Adaptive learning rate schedules would also be a fruitful extension of the present work, closing the gap between theory and the optimizers used in practice. We hope that our work can inspire future studies on the structure of the data distribution and its interaction with network architecture in the nonlinear feature-learning regime.


\section{Conclusion}\label{discussion}

Studying a simple model of SGD, we were able to uncover how the feature geometry governs the dynamics of the test loss. We derived average learning curves $\left< L_t \right>$ for both Gaussian and general features and showed conditions under which the Gaussian approximation is accurate. The proposed model allowed us to explore the role of the data distribution and neural network architecture on the learning curves, and choice of hyperparameters on realistic learning problems. While our theory accurately describes networks in the lazy training regime, average case learning curves in the feature learning regime would be interesting future extension. Further extensions of this work could be used to calculate the expected loss throughout curriculum learning where the data distribution evolves over time as well as alternative optimization strategies such as SGD with momentum. 


\section*{Reproducibility Statement}

The code to reproduce the experimental components of this paper can be found here \url{https://github.com/Pehlevan-Group/sgd_structured_features}, which contains jupyter notebook files which we ran in Google Colab. More details about the experiments can be found in Appendix \ref{expt_details}. Generally, detailed derivations of our theoretical results are provided in the Appendix. 

\section*{Acknowledgements}
We thank the Harvard Data Science Initiative and Harvard Dean’s Competitive Fund for Promising Scholarship for their support. We also thank Jacob Zavatone-Veth for useful discussions and comments on this manuscript.

\bibliographystyle{iclr2022_conference}
\bibliography{bib}


\pagebreak

\appendix

\counterwithin{figure}{section}

\section{Decomposition of the Features and Target Function}\label{target_decompose}
Let $y(\x)$ be a square integrable target function with $\left< y(\x)^2 \right> < \infty$. Define the following integral operator $T_K$ for kernel $K(\x,\x') = \bm\psi(\x) \cdot \bm\psi(\x')$:
\begin{align}
    T_K[\phi](\x') = \int p(\x) K(\x,\x') \phi(\x) d\x
\end{align}
We are interested in eigenfunctions of this operator, function $\phi_k$ for which $T_K[\phi_k] = \lambda_k \phi_k$. For kernels with finite trace $\int K(\x,\x) p(\x) d\x < \infty$, Mercer's theorem \citep{RasmussenWilliams} guarantees the existence of a set of orthonormal eigenfunctions. Since $\bm\psi(\x)$ spans an $N$ dimensional function space, only $N$ of the kernel eigenfunctions will have non-zero eigenvalue. Since the basis of kernel eigenfunctions (including the zero eigenvalue functions) is complete over the space of square integrable functions. After ordering the eigenvalues $\lambda_1 > \lambda_2 > ... > \lambda_N$ with $\lambda_{N+\ell} = 0$, we obtain the expansion 
\begin{align}
    y(\x) = \sum_k \left< y(\x) \phi_k(\x) \right>_{\x} \phi_k(\x) = \sum_{k\leq N} v_k \phi_k(\x)  + y_{\perp}(\x) \ , \ y_{\perp}(\x) = \sum_{k > N} \left< y(\x) \phi_k(\x) \right> \phi_k(\x)
\end{align}
Further, we can decompose the feature map in this basis $\bm\psi(\x) = \sum_{k=1}^N \sqrt{\lambda_k} \bm u_k \phi_k(\x)$. We recognize through these decompositions the coefficients $v_k$ can be computed uniquely as $v_k = \lambda_k^{-1/2}\bm u_k^\top \left< \bm\psi(\x) y(\x) \right>$. This provides a recipe for determining the necessary spectral quantities for our theory. We see that the feature map's decomposition above reveals that $\lambda_k$ are also the eigenvalues of the feature correlation matrix $\bm\Sigma$ since
\begin{align}
    \bm\Sigma = \left< \bm\psi(\x) \bm\psi(\x)^\top \right> = \sum_{k\ell} \sqrt{\lambda_k \lambda_{\ell}} \u_k \u_{\ell}^\top \left< \phi_k(\x) \phi_{\ell}(\x) \right> = \sum_{k} \lambda_k \u_k \u_k^\top. 
\end{align}


\subsection{Finite Sample Spaces}
When we discuss experiments on MNIST or CIFAR, we use this technology for an atomic data distribution $p(\x) = \frac{1}{M} \sum_{\mu=1}^M \delta(\x-\x')$. Plugging this into the integral operator gives 
\begin{align}
    T_K[\phi](\x) = \frac{1}{M} \sum_{\mu} K(\x,\x^\mu) \phi(\x^\mu)
\end{align}
We see that, restricting the domain to the set of points $\{ \x_1,...,\x_M \}$, this amounnts to solving a matrix eigenvalue problem $\frac{1}{M} \bm K \bm\phi_k = \lambda_k \bm\phi_k$ where $\bm K \in \mathbb{R}^{M \times M}$ is the kernel gram matrix with entries $K_{\mu\nu} = K(\x^\mu,\x^\nu)$ and $\bm\phi_k$ has entries $\phi_{k,\mu} = \phi_k(\x^\mu)$.

\section{Proof of Theorem \ref{th_gauss}}\label{gauss_derivation}

Let $\bm\Delta_t = \w_t - \w^*$ represent the difference between the current and optimal weights and define the correlation matrix for this difference
\begin{equation}
    \C_t = \left< \bm\Delta_t \bm\Delta_t^\top \right>_{\mathcal D_{t-1}}.
\end{equation}

Using stochastic gradient descent, $\w_{t+1} = \w_t - \eta \g_t$ with gradient vector $\g_t = \frac{1}{m} \sum_{i=1}^m \bm\psi_i \bm\psi_i^\top \bm\Delta_t$, the $\C_t$ matrix satisfies the recursion
\begin{align}
    \C_{t+1} = \left< ( \bm\Delta_t - \eta \g_t ) (\bm \Delta_t - \eta \g_t )^\top  \right>_{\mathcal D_t} = \C_t - \eta \left<\g_t \bm\Delta_t^\top \right> - \eta \left< \bm\Delta_t \g_t^\top  \right> + \eta^2 \left< \g_t \g_t^\top \right>.
\end{align}
%
First, note that since $\psi_i$ are all independently sampled at timestep $t$, we can break up the average into the fresh batch of $m$ samples and an average over $\mathcal{D}_{t-1}$
%
\begin{equation}
    \left< \g_t \bm\Delta_t \right>_{\mathcal D_t} = \frac{1}{m} \sum_{i=1}^m \left< \bm\psi_i \bm\psi_i^\top \right>_{\bm\psi_i} \left< \bm\Delta \bm\Delta_t^\top \right>_{\mathcal D_{t-1}} = \bSigma \C_t.
\end{equation}
%
The last term requires computation of fourth moments
\begin{align}
    \left< \g_t \g_t^\top \right> &= \frac{1}{m^2} \sum_{i,j} \left< \bm\psi_i \bm\psi_i^\top \left< \bm\Delta_t \bm\Delta_t^\top \right>_{\mathcal D_{t-1}} \bm\psi_j \bm\psi_j^\top  \right>_{\bm\psi_i,\bm\psi_j} 
    \\
    &= \frac{1}{m^2} \sum_{i,j} \left< \bm\psi_i \bm\psi_i^\top \C_t \bm\psi_j \bm\psi_j^\top  \right>_{\bm\psi_i,\bm\psi_j} .
\end{align}

First, consider the case where $i=j$. Letting $\bm \psi = \bm\psi_i$, we need to compute terms of the form
\begin{equation}
    \sum_{k,\ell} C_{k,\ell} \left< \psi_j \psi_k \psi_\ell \psi_n \right>.
\end{equation}
%
For Gaussian random vectors, we resort to Wick-Isserlis theorem for the fourth moment
\begin{equation}
    \left< \psi_j \psi_k \psi_l \psi_n \right> = \left< \psi_j \psi_k \right>\left< \psi_\ell \psi_n \right> +  \left< \psi_j \psi_\ell \right>\left< \psi_k \psi_n \right> +  \left< \psi_j \psi_n \right>\left< \psi_\ell \psi_k \right>
\end{equation}
giving
\begin{equation}
    \left< \g_t \g_t^\top \right> = \frac{m+1}{m} \bSigma \C_t \bSigma + \frac{1}{m} \bSigma \  \text{Tr}\left(  \bSigma \C_t \right).
\end{equation}
%
This correlation structure for $\g_t$ implies that its covariance has the form
\begin{equation}
    \left< \text{Cov}_{\bm\psi}(\g_t) \right>_{\mathcal D_t} = \frac{1}{m} \bSigma \C_t \bSigma + \frac{1}{m} \bSigma \text{Tr}(\bSigma \C_t).
\end{equation}
%
Using the formula for $\left< \g_t \g_t^\top \right>$, we arrive at the following recursion relation for $\C_t$
\begin{equation}
    \C_{t+1} = \C_t - \eta \C_t \bSigma - \eta\bSigma \C_t + \eta^2 \frac{m + 1}{m} \bSigma \C_t \bSigma + \frac{1}{m} \eta^2 \bSigma \ \text{Tr}\left( \bSigma \C_t \right).
\end{equation}

Since we are ultimately interested in the generalization error $\left< L_t \right> = \left< \bm\Delta_t^\top \bm\Sigma \bm\Delta_t \right>= \text{Tr} \bSigma \C_t = \sum_{k} \lambda_k \u_k^\top \C_t \u_k$, it suffices to track the evolution of $c_{t,k} = \u_k^\top \C_t \u_k$
\begin{equation}
    c_{t+1,k} = \left(1-2 \eta \lambda_k + \eta^2 \frac{m + 1}{m} \lambda_k^2 \right) c_{t,k} + \frac{1}{m}\eta^2 \lambda_k \sum_{j } \lambda_j c_{t,j}.
\end{equation}
%
Vectorizing this equation for $\mathbf{c}$ generates the following solution
\begin{equation}
    \mathbf{c}_t = \A^t \mathbf{c}_0 \ , \ \A = \I - 2\eta \ \text{diag}(\bm\lambda) + \frac{m+1}{m} \eta^2 \ \text{diag}(\bm\lambda^2) + \frac{\eta^2}{m} \bm\lambda \bm\lambda^\top.
\end{equation}
%
The coefficient $c_{0,k} = v_k^2 = \left( \u_k^\top \w^* \right)^2$. To get the generalization error, we merely compute $\left< L_t \right> =  \bm\lambda^\top \mathbf{a}_t = \bm\lambda^\top \A^t \mathbf{v}^2$ as desired.

\section{Proof of Stability Conditions}\label{stability_proof}

We will first establish the following lower bound on the loss, where without loss of generality we assumed the maximum correlation is $1$:
\begin{align}
    L_t \geq \bm \lambda^\top \mathbf{v}^2 \left[ (1-\eta)^2 + \frac{\eta^2}{m} |\bm\lambda|^2 \right]^t .
\end{align}
We will then use this lower bound to provide necessary conditions on the learning rate and batch size for stability of the loss evolution. First, note that the following inequality holds elementwise

\begin{align}
    \A \bm\lambda = \left[ (\bm I - \eta \ \text{diag}\left( \bm \lambda) \right)^2 + \frac{\eta^2}{m} \text{diag}(\bm\lambda^2) + \frac{\eta^2}{m} \bm\lambda \right] \bm\lambda \geq \left[ (1-\eta)^2 + \frac{\eta^2}{m} |\bm\lambda|^2 \right] \bm\lambda
\end{align}
%
Repeating this inequality $t$ times gives $\bm A^t \bm\lambda \geq \left[ (1-\eta)^2 + \frac{\eta^2}{m}|\bm\lambda|^2 \right]^t \bm\lambda$. Using the fact that $L_t = \bm\lambda^\top \A^t \mathbf{v}^2$ gives the desired inequality. Note that this inequality is very close to the true result for isotropic features $\bm\lambda=\bm 1$ which gives $L_t \propto \left[ (1-\eta)^2 + \frac{\eta^2}{m} (|\bm\lambda|^2+1) \right]^t$. For anisotropic features with small learning rate, this bound becomes less tight.
%
For the loss to converge to zero at large time, the quantity in brackets must necessarily be less than one. This implies the following necessary condition on the batchsize and learning rate
\begin{align}
    \eta < \frac{2 m}{m + |\bm\lambda|^2}  \iff m > m_{min} = \frac{\eta |\bm\lambda|^2}{2-\eta}
\end{align}
where $m_{min}$ is the minimal batch size for learning rate $\eta$ and feature covariance eigenvalues $\bm\lambda$.

\subsection{Heuristic Batch Size and Learning Rate}\label{app:heuristic_optimal_hyperparams}

We can derive heuristic optimal choices of the learning rate and batch size hyperparameters $\eta, m$ at a fixed compute budget which optimize the lower bound derived above. 
\subsubsection{Fixed Learning Rate}
We will first consider optimizing only the batch size at a fixed learning rate $\eta$ before discussing the optimal $m$ when $\eta$ is chosen optimally.  The loss at a fixed compute budget $C =tm$ is lower bounded by
\begin{align}
    L_{C/m} \geq \bm\lambda^\top \bm v^2 \left[ (1-\eta)^2 + \frac{\eta^2}{m} |\bm\lambda|^2   \right]^{C/m}
\end{align}
For the purposes of optimization, we introduce $x = 1/m$ and consider optimizing
\begin{align}
    f(x) = x \ln\left[ A + B x \right] \ , \ A = (1-\eta)^2 , B = \eta^2 |\bm\lambda|^2
\end{align}
The first order optimality condition $f'(x) = 0$ implies that $\ln(A+Bx) + \frac{Bx}{A+Bx} = 0$. Letting $z = A + Bx$, this is equivalent to $z \ln z + z - A = 0$. This equation has solutions for all valid $A \in (0,1)$ giving solutions $z \in(e^{-1}, 1)$. Letting $z(\eta)$, represent the solution to $z + z\ln z - (1-\eta)^2 = 0$, the optimal batchsize has the form
\begin{align}
 m^*(\eta) = \frac{B}{z(A) -A} = \frac{\eta^2 |\bm\lambda|^2}{z(\eta) - (1-\eta)^2}
\end{align}
We can gain intuition for this result by considering the limit of $\eta \to 0$ and $\eta \to 1$. First, in the $\eta \to 0$ limit, we find that $z \sim \frac{A+1}{2}$ so $m^* \sim \frac{2 \eta |\bm\lambda|^2}{2-\eta} = 2 m_{min}$, making contact with the stability bound derived in Appendix Section \ref{stability_proof}. Thus for small learning rates, this heuristic optimum suggests doubling the minimal stable batchsize for optimal convergence. At large learning rates, $\eta \sim 1$ with $A \sim 0$, we find $z \sim e^{-1}$ so $m^*(\eta) \sim e \eta^2 |\bm\lambda|^2$. Thus for small $\eta$, we expect $m^*$ to scale linearly with $\eta$ while for large $\eta$, we expect a scaling of the form $\eta^2$. In either case, the optimal batchsize scales with feature eigenvalues through the sum of the squares $|\bm\lambda|^2 = \sum_k \lambda_k^2$.
%We can get an intuition for this optimality criterion by noticing that $x \approx A$ gives a decent approximation to the solution. Thus we expect optimal batch size to scale roughly as which is roughly $2/\eta$ the size of the stability bound $m_{min}$ when $\eta$ is small. 

\subsubsection{Heuristic Optimal Learning Rate and Batch Size}

We will now examine what happens when one first optimizes loss bounmd with respect to the the learning rate at any value of $m$ and then subsequently optimizes over the batch size $m$. We can easily find the $\eta$ which minimizes the lower bound. 
\begin{align}
    \frac{\partial}{\partial \eta} \left[ (1-\eta)^2 + \frac{\eta^2 |\bm\lambda|^2}{m} \right] = 0 \implies \eta^* = \frac{m}{m+|\bm\lambda|^2}
\end{align}
Note that this heuristic optimal learning rate is very close to the true optimum in the isotropic data setting $\eta_{true} = \frac{m}{m+N+1} \approx \frac{m}{m+N}$. Plugging this back into the loss bound, we find that at fixed compute $C = tm$, the loss scales like
\begin{align}
   L_{C/m} \geq \bm\lambda^\top \bm v^2 \left[ \frac{|\bm\lambda|^2}{m + |\bm\lambda|^2} \right]^{C/m}
\end{align}
With the optimal choice of learning rate, the loss at fixed compute monotonically increases with batch size, giving an optimal batchsize of $m = 1$. This shows that if the learning rate is chosen optimally then small batch sizes give the best performance per unit of computation. This corresponds to the hyperparameter choices $(\eta, m) = \left( \frac{1}{1+|\bm\lambda|^2}, 1 \right)$.


\section{Increasing $m$ Reduces the Loss at fixed $t$}\label{minibatch_deriv}

We will show that for a fixed number of steps $t$, increasing the minibatch size $m$ can only decrease the expected error. To do this, we will simply show that the derivative of the expected loss with respect to $m$,
\begin{equation}
    \frac{\partial \left<L_t\right>}{\partial m} =  \bm\lambda^\top \frac{\partial \A^t}{\partial m} \mathbf{v}^2,
\end{equation}
is always non-positive. The derivative of the $t$-th power of $\A$ can be identified with the chain rule
\begin{equation}
    \frac{\partial \A^t}{\partial m} = \frac{\partial \A}{\partial m} \A^{t-1} + \A \frac{\partial \A}{\partial m} \A^{t-2} + \A^2 \frac{\partial \A}{\partial m} \A^{t-3} + ... + \A^{t-1}  \frac{\partial \A}{\partial m}.
\end{equation}
%
Note that the matrix
\begin{equation}
    \frac{\partial \A}{\partial m} = - \frac{\eta^2}{m^2}\left[ \text{diag}(\bm\lambda^2) + \bm\lambda\bm\lambda^\top \right]
\end{equation}
has all non-positive entries. Thus we find that
\begin{equation}
    \frac{\partial \left< L_t \right>}{\partial m} = \sum_{n=0}^{t-1} \bm\lambda^\top \A^n  \frac{\partial \A}{\partial m} \A^{t-n-1} \mathbf{v}^2.
\end{equation}
%
Note that since all entries in $\mathbf{v}_k^2$ and $\A^{t-n-1}$ are non-negative, the vector $\bm{z}_n = \A^{t-n-1} \mathbf{v}^2$ has non-negative entries. By the same argument, the vector $\bm{q}_n = \A^{n} \bm\lambda$ is also non-negative in each entry. Therefore, each of the terms in $\frac{\partial \left< L_t \right>}{\partial m}$ above must be non-positive 
\begin{equation}
    \frac{\partial \left< L_t \right>}{\partial m} = \sum_{n=0}^{t-1} \bm z_n^\top \frac{\partial \A}{\partial m} \bm q_n = - \frac{\eta^2}{m} \sum_{n} \sum_{k,\ell} z_{n,k} \left[ \delta_{k,\ell} \lambda_k^2 + \lambda_k \lambda_\ell  \right] q_{n,\ell} \leq 0.
\end{equation}
%
Thus we find $\frac{\partial \left< L_t \right>}{\partial m} \leq 0$, implying that optimal $\left< L_t\right>$ is always obtained (possibly non-uniquely) at $m \to \infty$.

\section{Increasing $m$ Increases the Loss at Fixed $C = tm$ on Isotropic Features}\label{fixed_compute_minibatch}

Unlike the previous section, which considered fixed $t$ and varying $m$, in this section we consider fixing the total number of samples (or gradient evaluations) which we call the compute budget $C=tm$. For a fixed compute budget $C = tm$, and unstructured $N$ dimensional Gaussian features and optimal learning rate $\eta^* = \frac{m}{m+N+1}$, we have
\begin{equation}
   \left<  L_{C/m} \right> = \left( \frac{N+1}{m+N+1} \right)^{C/m} ||\w^*||^2.
\end{equation}
Taking a derivative with respect to the batch size we get
\begin{align}
   \frac{\partial}{\partial m} \log  \left< L_{C/m} \right> &= \frac{\partial }{\partial m} \frac{C}{m} \log\left( \frac{N+1}{m+N+1} \right) \nonumber
   \\
   &= \frac{C}{m^2} \log\left( \frac{m+N+1}{N+1} \right) +  \frac{C}{m (m+N+1)} > 0.
\end{align} 
This exercise demonstrates that, for the isotropic features, smaller batch-sizes are preferred at a fixed compute budget $C$. 
%
This result does not hold for arbitrary spectra $\lambda_k$. In the general case, optimal minibatch sizes can exist as we show in Figure \ref{fig:isotropic} (e)-(f) for power law and MNIST spectra.


%\section{Large Batch Approximate Normality of Gradient Updates}

%The distribution of gradient updates $\bm g_t = \frac{1}{m} \sum_{\mu=1}^m \bm\psi_\mu \bm\psi_\mu^\top \bm\Delta_t$ conditioned on the previous weight discrepancy $\bm\Delta_t$ is
%\begin{align}
%    p(\bm g |\bm\Delta_t) &= \int \prod_{\mu} d\bm\psi_\mu p(\bm\psi_\mu) \ \delta\left(\bm g - \frac{1}{m} \sum_{\mu=1}^m \bm\psi_\mu \bm\psi_\mu^\top \bm\Delta_t \right)
%    \\
%    &= \frac{1}{(2\pi)^{N}} \int d\bm q \exp\left( i \bm q \cdot \bm g \right) \left[ \left< \exp\left( - \frac{i}{m}  \bm\psi^\top \bm  q \bm\Delta_t^\top \bm\psi \right) \right>_{\bm \psi} \right]^m
%    \\
%    &= \frac{m}{(2\pi)^{N}} \int d \bm q \exp\left( i m \bm q \cdot \bm g - \frac{m}{2} \log\left(1 +  2i \bm\Delta_t^\top \bm\Sigma \bm q \right) \right) 
%\end{align}

%\textcolor{red}{Saddle point integral assuming large $m$, recovers gaussian distribution for $\bm g_t$}

%Differentiation with respect to $i\bm q$.
%\begin{align}
%    \bm g = \frac{\bm\Sigma \bm q}{1 + 2 \Delta \bm\Sigma \bm{\bm q} } 
%\end{align}

%First note $\bm q = C \bm\Sigma^{-1} \bm g$ we need
%\begin{align}
%    1 = \frac{C}{1+ 2 C \bm\Delta^\top \bm g }
%\end{align}



%\subsection{Proof of Theorem \ref{var_loss_th}}\label{variance_proof}

%Under the assumption of Gaussian features, the discrepancy $\bm\Delta_t = \w_t - \w^*$ is the sum of Gaussian random variables and is therefore Gaussian. By again appealing to Wick-Isserlis theorem, the second moment of the loss can be shown to have the form
%\begin{equation}
%    \left< L_t^2 \right>_{\mathcal D_t} = \left< \bm\Delta^\top \bSigma \bm\Delta \bm\Delta^\top \bSigma \bm\Delta \right>_{\mathcal D_t} = \left< L_t \right>_{\mathcal D_t}^2 + 2 \ \text{Tr}\left( \C_t \bSigma \C_t \bSigma \right).
%\end{equation}
%%
%Decomposing $\C_t = \sum_{ij} c_{ij}(t) \u_i \u_j^\top$ in the appropriate basis, we find
%\begin{align}
%    \text{Var}[ L_t ] &= 2 \sum_{ij} c_{ij}(t)^2 \lambda_i \lambda_j \ , \ c_{ij}(t+1) \\
%    &= \left(1- \eta (\lambda_i + \lambda_j) + \eta^2 \frac{m+1}{m} \lambda_i \lambda_j \right) c_{ij}(t) + \delta_{ij} \lambda_i \sum_k \lambda_k c_{kk}(t).
%\end{align}
%
%The diagonal elements can be solved for as $\mathbf{c}_t= \A^t \mathbf{v}^2$ while the off diagonal elements all decouple and satisfy
%\begin{equation}
%    c_{ij}(t) = v_i v_j \left(1- \eta (\lambda_i + \lambda_j) + \eta^2 \frac{m+1}{m} \lambda_i \lambda_j \right)^t.
%\end{equation}
%
%Thus the total variance takes the form
%\begin{equation}
%    \text{Var}[L_t] = 2 {\bm\lambda^2}^\top [\A^t \mathbf{v}^2]^2 + 2\sum_{i \neq j} v_i^2 v_j^2 \lambda_i \lambda_j \left(1- \eta (\lambda_i + \lambda_j) + \eta^2 \frac{m+1}{m} \lambda_i \lambda_j \right)^{2t}.
%\end{equation}

%\section{Average Case Loss Dynamics for SGD with Momentum}

%Instead of implementing the usual SGD update $\w_{t+1} = \w_t - \eta \g_t$ for gradient estimate $\g_t$, in this section, we consider the momentum scheme
%\begin{align}
%    \bm p_{t} = \gamma \bm p_{t-1} + \bm g_t \ , \ \w_{t+1} = \w_t - \eta \bm p_t
%\end{align}
%where $\gamma \in (0,1)$ is a hyper-parameter which controls the recency bias of the gradient estimator. This update can also be interpreted as first computing an exponential moving average $\bm p_t$ of past gradients $\bm p_t = \sum_{s \leq t} \gamma^{t-s} \g_s$ and then using this estimate to update the parameters. As before, it suffices to determine the matrix $\C_t = \left< (\w_t-\w^*) (\w_t - \w^*) \right>_{\mathcal D_t}$ and quantities $c_{t, k} = \u_k^\top \C_t \u_k$ which can be used to compute the test loss $\left< L_t \right> = \sum_k c_{t,k} \lambda_k$. Since we no longer have only $\bm\Delta_t = \w_t - \w^*$ but also $\m_t$, we need to introduce two new matrices 
%\begin{align}
%    \bm R_t = \left< \bm p_t \bm \Delta_t^\top \right>_{\mathcal D_t}
%    \\
%    \bm Z_t = \left< \bm p_t \bm p_t^\top \right>_{\mathcal D_t}
%\end{align}
%Collectively these matrices form a closed linear dynamical system
%\begin{align}
%    \C_{t+1} &= \C_t - \eta \bm R_t - \eta \bm R_t^\top + \eta^2 \bm Z_t \nonumber
%    \\
%    \bm R_{t+1} &= \bSigma \C_{t+1} + \gamma \bm R_t - \eta \gamma \bm Z_t \nonumber
%    \\
%    \bm Z_{t+1} &= \frac{m-1}{m} \bSigma \C_{t+1} \bSigma + \frac{1}{m} \bSigma \text{Tr}[\C_{t+1}\bSigma] +  \gamma \bSigma[ \bm R_t - \eta \bm Z_t ]^\top + \gamma [\bm R_t - \eta \bm Z_t] \bm \Sigma + \gamma^2 \bm Z_t
%\end{align}

%We can easily replace $\bm C_{t+1}$ in the last two expressions with its formula in terms of matrices $\{\C_t,\bm R_t, \bm Z_t \}$. We now introduce the following quantities $c_{t,k} = \u_k^\top \C_t \u_k$, $r_{t,k} = \u_k^\top \bm R_t \u_k$ and $z_{t,k} = \u_k^\top \bm Z_t \u_k$, which also form a closed linear dynamical system. We can vectorize these quantities and write their dynamical system
%\begin{align}
%    \begin{bmatrix}
%    \bm c_{t+1} \\
%    \bm r_{t+1} \\
%    \bm z_{t+1}
%    \end{bmatrix} = \begin{bmatrix}
%                    \bm I & -2\eta \bm I & \eta^2 \bm I \\
%                    \text{diag}(\bm\lambda)  & - \eta \ \text{diag}(\bm\lambda) + \gamma \bm I & \eta^2 \text{diag}(\bm\lambda) - \eta \gamma \bm I \\
%                    \bm D_1 & \bm D_2 & \bm D_3
%    \end{bmatrix}
%    \begin{bmatrix}
%    \bm c_{t} \\
%    \bm r_{t} \\
%    \bm z_{t}
%    \end{bmatrix}
%\end{align}
%where the matrices $\bm D_j$ have the following forms
%\begin{align}
%    \bm D_1 &= \frac{m-1}{m} \text{diag}(\bm\lambda^2) + \frac{1}{m} \bm\lambda \bm\lambda^\top  \nonumber
%    \\
%    \bm D_2 &= -2\eta \bm D_1 + 2\gamma \text{diag}(\bm\lambda) \nonumber
%    \\
%    \bm D_3 &= \eta^2 \bm D_1 - \eta\gamma \text{diag}(\bm\lambda) + \gamma^2 \bm I
%\end{align}
%Solving these dynamics for $\bm c_t$, we can then compute generalization through $\left< L_t \right> = \bm\lambda^\top \bm c_{t}$. The $m \to \infty$ limit again decouples the errors across eigenmodes by eliminating the non-diagonal components in $\bm D_1$. In this setting, we find that each mode $k$ evolves according to
%\begin{align}
%    \begin{bmatrix}
%    c_{t+1,k}\\
%    r_{t+1,k}\\
%    z_{t+1,k}
%    \end{bmatrix} = \begin{bmatrix} 
%                    1 & -2\eta & \eta^2 \\
%                    \lambda_k & -\eta\lambda_k +\gamma & \eta^2\lambda_k - \eta \gamma \\
%                    \lambda_k^2 & -2\eta \lambda_k^2 + 2\gamma \lambda_k & \eta^2 \lambda_k^2 - \eta\gamma \lambda_k + \gamma^2 
%                    \end{bmatrix}
%                    \begin{bmatrix}
%                        c_{t,k}\\
%                        r_{t,k} \\
%                        z_{t,k}
%                    \end{bmatrix}
%\end{align}

\section{Proof of Theorem \ref{arbitary_features}}\label{non_gauss}

Let $\text{Vec}(\cdot)$ denote a flattening of an $N\times N$ matrix into a vector of length $N^2$ and let $\text{Mat}(\cdot)$ represent a flattening of a 4D tensor into a $N^2 \times N^2$ two-dimensional matrix. We will show that the expected loss (over $\mathcal D_t$) is 
\begin{align}
    \left< L_t \right> &= \sum_{k} \lambda_k c_{t,kk}  \ , \ \mathbf{c}_{t}=\left( \G + \frac{\eta^2}{m} \text{Mat}(\bm\kappa) \right)^t \text{Vec}(\mathbf{v}\mathbf{v}^\top) \in \mathbb{R}^{N^2} 
\end{align}
where $\left[ \mathbf{G} \right]_{ij, kl} = \delta_{ik}\delta_{jl} \left(1 - \eta(\lambda_i + \lambda_j) + \frac{\eta^2(m-1)}{m} \lambda_i \lambda_j \right)$ and $[\mathbf{v}]_k = \u_k \cdot \w^*. $

We rotate all of the feature vectors into the eigenbasis of the covariance, generating diagonalized features $\phi_k = \u_k^\top \bm\psi$ and introduce the following fourth moment tensor
\begin{equation}
    \kappa_{ijkl} = \left< \phi_i \phi_j \phi_k \phi_\ell \right>.
\end{equation}
%
We redefine $\C_t$ in the appropriate (rotated) basis by projecting onto the eigenvectors of the covariance
\begin{equation}
    \C_t = \mathbf{U}^\top \left< \bm\Delta_t \bm\Delta_t^\top \right> \mathbf{U},
\end{equation}
%
where $\mathbf{U} = [\u_1,\u_2,...,\u_N]$. With this definition, $\C$'s dynamics take the form
\begin{equation}
    \C_{t+1} = \C_t - \mathbf{\Lambda} \C_t - \C_t \mathbf{\Lambda} + \frac{\eta^2 (m-1)}{m} \mathbf{\Lambda} \C_t \mathbf{\Lambda} + \left< \bm\phi \bm\phi^\top \C_t \bm\phi \bm\phi^\top \right>.
\end{equation}
%
The elements of the matrix can be expressed with the fourth moment tensor
\begin{equation}
    \sum_{k\ell} \left< \phi_i \phi_{j} \phi_k \phi_{\ell} \right> C_{k\ell} = \sum_{k\ell} \kappa_{ijkl} C_{k,\ell}.
\end{equation}
%
We thus generate the following dynamics for $C_{ij}^t$ 
%
\begin{equation}
    C_{ij}^{t+1} = \left (1-\eta(\lambda_i + \lambda_j) +  \frac{\eta^2(m-1)}{m} \lambda_i \lambda_j \right) C_{ij}^t +\frac{\eta^2}{m} \sum_{kl} \kappa_{ijkl} C_{kl}^t.
\end{equation}
%
Let $\mathbf{c}_t = \text{Vec}(\C_t)$, then we have
\begin{equation}
    \mathbf{c}_{t+1} = \left( \G_0 + \frac{\eta^2}{m } \text{Mat}(\bm\kappa) \right) \mathbf{c}_t \ , \ [\G_0]_{ij,k\ell} = \delta_{ik}\delta_{j\ell} \left[ 1 - \eta (\lambda_i+\lambda_j) + \frac{\eta^2(m-1)}{m} \lambda_i\lambda_j \right].
\end{equation}
%
Solving these dynamics for $\mathbf{c}$, recognizing that $\mathbf{c}_0 = \text{Vec}(\mathbf{v}\mathbf{v}^\top )$, and computing $\left< L_t \right> = \text{Tr} \bSigma \C_t = \sum_{k} C_{kk} \lambda_k$ gives the desired result. 


\subsection{Proof of Theorem \ref{th_regularity}}\label{app_proof_regularity}

Suppose that the features satisfy the regularity condition 
\begin{align}
    \left< \bm \psi \bm\psi^\top \bm G \bm\psi \bm\psi^\top \right> \preceq (\alpha+1)\bm\Sigma \bm G \bm\Sigma + \alpha \bm\Sigma \text{Tr}\left(\bm\Sigma \bm G \right)
\end{align}
%
Recalling the recursion relation for $\bm C_t$
\begin{align}
    \C_{t+1} &= \C_t - \eta \bm\Sigma\C_t - \eta \C_t \bm\Sigma + \eta^2 \frac{m^2-m}{m^2} \bm\Sigma \C_t \bm\Sigma + \frac{\eta^2}{m} \left< \bm\psi \bm\psi^\top \C_t \bm\psi \bm\psi^\top \right>  \nonumber  
    \\
    &\preceq \bm C_t - \eta \bm\Sigma\C_t - \eta \C_t \bm\Sigma + \eta^2 \frac{m^2-m}{m^2} \bm\Sigma \C_t \bm\Sigma + \frac{\eta^2}{m} \left[ (\alpha+1) \bm\Sigma \C_t \bm\Sigma + \alpha \bm\Sigma \text{Tr}\C_t \bm\Sigma \right]  
    \\
    &= (\I - \eta\bm\Sigma) \C_t (\I - \eta \bm\Sigma) + \frac{\alpha \eta^2}{m} \left[ \bm\Sigma \C_t \bm\Sigma + \bm\Sigma \text{Tr}\bm\Sigma\C_t \right]
\end{align}
Defining that $c_{k,t} = \bm u_k^\top \C_t \bm u_k$, we note $\bm c_{t+1} \leq \left( (\bm I - \eta \ \text{diag}(\bm\lambda))^2 + \frac{\eta^2 \alpha}{m}\left[ \text{diag}(\bm\lambda^2) + \bm\lambda\bm\lambda^\top \right] \right) \bm c_t$. Using the fact that $L_t = \bm c_t^\top \bm\lambda$, we find
\begin{align}
    L_t \leq \bm\lambda^\top \left( (\bm I - \eta \ \text{diag}(\bm\lambda))^2 + \frac{\eta^2 \alpha}{m}\left[ \text{diag}(\bm\lambda^2) + \bm\lambda\bm\lambda^\top \right] \right)^t \bm v^2
\end{align}
which proves the desired bound.




\begin{figure}[ht]
    \centering
    \subfigure[Non-Gaussian Effects on MNIST]{\includegraphics[width=0.495\linewidth]{figures/non_gauss_diff_effect_mnist.pdf}}
     \subfigure[Non-Gaussian Effects on CIFAR ]{\includegraphics[width=0.495\linewidth]{figures/non_gauss_diff_effect_cifar.pdf}}
    %\subfigure[MNIST Feature Spectra]{\includegraphics[width=0.32\linewidth]{figures/spectra_relu_tanh_mnist_random_features.pdf}}
    %\subfigure[MNIST Task Spectra]{\includegraphics[width=0.32\linewidth]{figures/task_power_relu_tanh_mnist_random_features.pdf}}
    %\subfigure[Learning Curves $m=5$]{\includegraphics[width=0.32\linewidth]{figures/test_err_relu_tanh_mnist_random_features.pdf}}
    %\subfigure[MNIST vs Isotropic Features]{}
    %\subfigure[Varying Learning Rate]{\includegraphics[width=0.32\linewidth]{figures/mnist_random_feature_vary_lr.pdf}}
    %\subfigure[Varying Batch Size]{\includegraphics[width=0.32\linewidth]{figures/mnist_random_feature_vary_batch.pdf}}
    %\subfigure[Vary Noise (Averaged 10 Trials)]{\includegraphics[width=0.32\linewidth]{figures/mnist_random_feature_vary_noise.pdf}}
    \caption{Non-Gaussian effects are small on random feature models. (a)-(b) The first 20-dimensions of the summed fourth moment matrix $\kappa^4_{ij} = \u_i^\top \left< \bm\psi \bm\psi^\top \bm\psi \bm\psi^\top  \right> \u_j$ are plotted for the Gaussian approximation and the empirical fourth moment. Differences between the Gaussian approximation and true fourth moment matrices on this example are visible, but are only on the order of $\sim 5\%$ of the size of the entries in $\kappa_4$. }
    \label{fig:nonlin_batch_noise}
\end{figure}

%We provide further evidence of non-Gaussian effects on realistic data and random feature models in Figure \ref{fig:gauss_equiv_model}. We compare SGD training on the real feature distribution $p(\psi)$ with its Gaussian approximation with matching second moment structure $\mathcal{N}(0,\left< \bm\psi \bm\psi^\top \right>)$. The Gaussian theory does provide a better match for Gaussian features, especially on noisy problems.  
%\begin{figure}
%    \centering
%    \includegraphics[width=0.32\linewidth]{figures/mnist_random_feature_vary_lr_avg_gauss_eq.pdf}
%    \includegraphics[width=0.32\linewidth]{figures/mnist_random_feature_vary_batch_avg_gauss_eq.pdf}
%    \includegraphics[width=0.32\linewidth]{figures/mnist_random_feature_vary_noise_gauss_eq.pdf}
%    \includegraphics[width=0.32\linewidth]{figures/mnist_random_feature_vary_lr_avg_title.pdf}
%    \includegraphics[width=0.32\linewidth]{figures/mnist_random_feature_vary_batch_avg_title.pdf}
%    \includegraphics[width=0.32\linewidth]{figures/mnist_random_feature_vary_noise_avg_title.pdf}
%    \caption{A comparison of learning curves on ReLU random features on MNIST and a Gaussian feature model with matching covariance structure. The Gaussian equivalent data exhibits heavier fluctuations than what is observed in practice. The Gaussian theory's average loss (black) also provides a slight overestimate of the average loss in the noisy problem on real data. }
%    \label{fig:gauss_equiv_model}
%\end{figure}


\section{Power Law Scalings in Small Learning Rate Limit}\label{Power_law_appendix}

By either taking a small learning rate $\eta$ or a large batch size, the test loss dynamics reduce to the test loss obtained from gradient descent on the population loss. In this section, we consider the small learning rate limit $\eta \to 0$, where the average test loss follows 
\begin{equation}
    \left< L_t \right> \sim \sum_{k=1}^\infty \lambda_k v_k^2 (1-\eta\lambda_k)^{2t}.
\end{equation}
%
Under the assumption that the eigenvalue and target function power spectra both follow power laws $\lambda_k \sim k^{-b}$ and $v_k^2 \lambda_k \sim k^{-a}$, the loss can be approximated by an integral over all modes $k$
\begin{align}
    \left< L_t \right> = \sum_{k} k^{-a} (1-\eta k^{-b})^{2t} \sim \int_1^\infty \exp\left(  2\eta \ln(1- \eta k^{-b}) t - a \ln k \right) dk 
    \\
    \sim \int_1^\infty \exp\left( - 2\eta  \eta k^{-b} t - a \ln k \right) dk  \ , \ \eta \to 0
\end{align}
%
We identify the function $f(k) = 2\eta k^{-b} + \frac{1}{t} \ln k$ and proceed with Laplace's method \citep{bender}. This consists of Taylor expanding $f(k)$ around its minimum to second order and computing a Gaussian integral
\begin{equation}
    \int \exp(- t f(k) ) dk \sim \int \exp\left(-t f(k^*) - \frac{t}{2} f''(k^*) (k-k^*)^2 \right) \sim \exp(-t f(k^*) ) \sqrt{\frac{2\pi}{t f''(k^*)}}.
\end{equation}
%
We must identify the $k^*$ which minimizes $f(k)$. The interpretation of this value is that it indexes the mode which dominates the error at a large time $t$. The first order condition gives 
\begin{equation}
    f'(k) = -2b\eta k^{-b-1} + \frac{a}{t k} = 0 \implies k^* = \left(2 b \eta t / a  \right)^{1/b}. 
\end{equation}
%
The second derivative has the form
\begin{equation}
    f''(k^*) = 2 b^2 \eta k^{-b-2} - \frac{1}{tk^2} |_{k^*} = 2 b^2 \eta \left( 2 b \eta t / a \right)^{-(b+2)/b} - \frac{1}{t} \left( 2b \eta t / a \right)^{-2/b} \sim t^{-1 - 2/b}.
\end{equation}
%
Thus we are left with a scaling of the form
\begin{equation}
    \left< L_t  \right> \sim \exp(-a/b\ln t) t^{1/b} \sim t^{-\frac{a-1}{b}}.
\end{equation}


\section{Proof of Theorem \ref{th_unlearnable}}\label{app_proof_unlearnable}

Let $\left< y_\perp^2 \right> = \sigma^2$ and $\left< y_\perp \right> = 0$, $\left< y_{\perp} \bm\psi \right> = \bm 0$. The gradient descent updates take the following form $\bm\Delta_{t+1} = \bm\Delta_t - \eta \g_t$ with
\begin{equation}
    \g_t = \frac{1}{m} \sum_{i=1}^m \bm\psi_i \left[ \bm\psi_i^\top \bm\Delta_t + y_{\perp,i} \right].
\end{equation}
%
Again, defining $\C_t = \left< \bm\Delta_t \bm\Delta_t^\top \right>$ we perform the average over each of the $\bm\psi_i$ vectors to obtain the following recursion relation
\begin{align}
    \C_{t+1} &= \left< \bm\Delta_t \bm\Delta_t^\top \right> - \eta \left< \bm\Delta_t \g_t^\top \right> -\eta \left<  \g_t  \bm\Delta_t^\top \right> + \eta^2 \left< \g_t \g_t^\top \right> \nonumber
    \\
    &=\C_t - \eta \bm\Sigma \C_t - \eta \C_t \bm\Sigma + \frac{m+1}{m} \bm\Sigma \C_t \bSigma + \frac{1}{m} \bSigma \text{Tr}\left( \C_t \bSigma \right) + \eta^2 \sigma^2 \bSigma.
\end{align}
%
Again, analyzing $c_{t,k} = \u_k^\top \C_t \u_k$ we find
\begin{equation}
    c_{t+1,k} = \left(1-2\eta\lambda_k + \eta^2 \frac{m+1}{m} \lambda_k^2 \right) c_{t,k} + \frac{1}{m} \sum_\ell \lambda_{\ell} c_{t,\ell} + \eta^2 \sigma^2 \lambda_k.
\end{equation}
%
The vector $\bm c_t$ follows the linear evolution
\begin{equation}
    \bm c_{t+1} = \A \bm c_t + \eta^2 \sigma^2 \bm\lambda.
\end{equation}

Let $\bm b = \eta^2 \sigma^2 \bm\lambda$. Writing out the first few steps, we identify a pattern
\begin{align}
    \bm c_{1} &= \A \bm c_0 + \bm b \nonumber
    \\
    \bm c_{2} &= \A \bm c_1 + \bm b = \A^2 \bm c_0 + \A \bm b + \bm b \nonumber
    \\
    \bm c_{3} &= \A \bm c_2 + \bm b = \A^3 \bm c_0 + \A^2 \bm b + \A \bm b + \bm b \nonumber
    \\
    ... \nonumber
    \\
    \bm c_{t} &= \A^t \bm c_0 + \left( \sum_{n=0}^{t-1} \A^n \right) \bm b. 
\end{align}
%
The geometric sum $\left( \sum_{n=0}^{t-1} \A^n \right)$ can be computed exactly under the assumption that $(\I - \A)$ is invertible which holds provided all of $\A$'s eigenvalues are less than unity, which necessarily holds provided the system is stable. The geometric sum yields
\begin{equation}
    \left( \sum_{n=0}^{t-1} \A^n \right) = (\I - \A)^{-1} \left( \I - \A^t \right).
\end{equation}

Recalling the definition of $\bm b = \sigma^2 \eta^2 \bm \lambda$ and the definition of the average loss $\left< L_t \right> = \bm\lambda^\top \bm c_t$, we have
\begin{equation}
    \left< L_t \right> = \sigma^2 +  \bm\lambda^\top \A^t \bm c_0 + \eta^2\sigma^2 \bm\lambda^\top (\I - \A)^{-1} \left( \I - \A^t \right) \bm\lambda .
\end{equation}
Recognizing $\bm c_0 = \mathbf{v}^2$ gives the desired result.

\section{Proof of theorem \ref{thm_test_train_split}}\label{app:test_train_split}

We will now prove that if the training $\hat{p}(\x)$ and test distributions $p(\x)$ are different and have feature correlation matrices $\bm{\hat\Sigma}$ and $\bm\Sigma$ respectively, then the average training and test losses have the form
\begin{align}
    L_{\text{train}} = \text{Tr}\left[ \bm{\hat \Sigma} \C_t \right] \ L_{\text{test}} = \text{Tr}\left[ \bm{ \Sigma} \C_t \right] .
\end{align}
As before, we will assume that there exist weights $\w^*$ which satisfy $y = \w^* \cdot \bm\psi$. We start by noticing that the update rule for gradient descent 
\begin{align}
    \w_t = \w_t - \eta \g_t \ , \ \g_t = \frac{1}{m} \sum_{\mu=1}^m \bm \psi_{t,\mu} \bm \psi_{t,\mu}^\top \left[ \w_t - \w^* \right]
\end{align}
generates the following dynamics for the weight discrepancy correlation $\C_t= \left<(\w_t-\w^*) (\w_t -\w^*)^\top \right>_{\mathcal D_t}$.
\begin{align}
    \C_{t+1} &= \C_t - \eta\bm{\hat \Sigma} \C_t - \eta \C_t \bm{\hat \Sigma} + \eta^2 \left(1-\frac{1}{m} \right) \bm{\hat\Sigma} \C_t \bm{\hat\Sigma} + \frac{\eta^2}{m} \left< \bm\psi \bm\psi^\top \C_t \bm\psi \bm\psi^\top \right>
\end{align}
This formula can be obtained through the simple averaging procedure shown in \ref{gauss_derivation}. Under the Gaussian approximation, we can obtain a simplification
\begin{align}
    \C_{t+1} &= (\I - \eta \bm\Sigma)\C_t(\I-\eta \bm\Sigma) + \frac{\eta^2}{m}\left[ \bm\Sigma\C_t \bm\Sigma + \bm\Sigma\text{Tr}\bm\Sigma \C_t \right]
\end{align}
We can solve for the evolution of the diagonal and off-diagonal entries in this matrix giving

\begin{align}
    \u_k^\top \C_{t} \u_k &= \left[ \A^t \bm v^2 \right]_k \ , \ \u_k^\top \C_{t} \u_\ell = \left(1-\eta\hat{\lambda}_k-\eta \hat{\lambda}_\ell + \eta^2 \left(1 + \frac{1}{m} \right)\hat{\lambda}_k \hat{\lambda}_\ell \right)^t v_k v_\ell
\end{align}

To calculate the training and test error, we have
\begin{align}
    \left< L_{\text{test}} \right>= \left< ( \bm\psi(\x) \cdot \w_t - \bm\psi(\x) \cdot \bm w^* )^2 \right>_{\x \sim p(\x), \mathcal D_t } = \left< (\w_t- \w^*) \bm\Sigma \left( \w_t - \w^* \right) \right> = \text{Tr} \bm\Sigma \C_t. \nonumber
    \\
    \left< L_{\text{train}} \right> = \left< ( \bm\psi(\x) \cdot \w_t - \bm\psi(\x) \cdot \bm w^* )^2 \right>_{\x \sim \hat{p}(\x), \mathcal D_t } = \left< (\w_t- \w^*) \bm{\hat\Sigma} \left( \w_t - \w^* \right) \right> = \text{Tr} \bm{\hat \Sigma} \C_t.
\end{align}

Note that in the training error formula, since $\bm{\hat \Sigma}$ has eigenvectors $\u_k$ only the diagonal terms $\u_k^\top \C_t \u_k$ enter into the formula for $L_{train}$, but off-diagonal components $\u_k^\top \C_t \u_{\ell}$ do enter into the formula for $L_{test}$


\section{Experimental Details}\label{expt_details}

For Figures \ref{fig:feature_spectra_learning_curve}, we use the last two classes of MNIST and CIFAR-10. We encode the target values as binary $y\in \{+1,-1\}$. For Figure \ref{fig:NTK_fig}, we use $6000$ random training points drawn from entire MNIST and CIFAR-10 datasets to calculate the spectrum of the Fisher information matrix. We train with SGD on these training data, using one-hot label vectors for each training example and plot the error on the test set. We train our models on a Google Colab GPU and include code to reproduce all experimental results in the supplementary materials. To match our theory, we use fixed learning rate SGD. Both evaluation of the infinite width kernels and training were performed with the Neural Tangents API \citep{neuraltangents2020}.


\end{document}