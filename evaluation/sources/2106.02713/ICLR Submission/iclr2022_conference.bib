@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@inproceedings{mel2021theory,
  title={A theory of high dimensional regression with arbitrary correlations between input features and target functions: sample complexity, multiple descent curves and a hierarchy of phase transitions},
  author={Mel, Gabriel and Ganguli, Surya},
  booktitle={International Conference on Machine Learning},
  pages={7578--7587},
  year={2021},
  organization={PMLR}
}
@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{bahri2021explaining,
  title={Explaining Neural Scaling Laws},
  author={Bahri, Yasaman and Dyer, Ethan and Kaplan, Jared and Lee, Jaehoon and Sharma, Utkarsh},
  journal={arXiv preprint arXiv:2102.06701},
  year={2021}
}

@article{gurbuzbalaban2020heavy,
  title={The heavy-tail phenomenon in sgd},
  author={Gurbuzbalaban, Mert and Simsekli, Umut and Zhu, Lingjiong},
  journal={arXiv preprint arXiv:2006.04740},
  year={2020}
}

@article{Lee_2020,
   title={Wide neural networks of any depth evolve as linear models under gradient descent},
   volume={2020},
   ISSN={1742-5468},
   url={http://dx.doi.org/10.1088/1742-5468/abc62b},
   DOI={10.1088/1742-5468/abc62b},
   number={12},
   journal={Journal of Statistical Mechanics: Theory and Experiment},
   publisher={IOP Publishing},
   author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel S and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
   year={2020},
   month={Dec},
   pages={124002}
}

@article{dieuleveut_nonparametric,
author = {Aymeric Dieuleveut and Francis Bach},
title = {{Nonparametric stochastic approximation with large step-sizes}},
volume = {44},
journal = {The Annals of Statistics},
number = {4},
publisher = {Institute of Mathematical Statistics},
pages = {1363 -- 1399},
keywords = {‎reproducing kernel Hilbert ‎space, stochastic approximation},
year = {2016},
doi = {10.1214/15-AOS1391},
URL = {https://doi.org/10.1214/15-AOS1391}
}

@misc{tarrès2013online,
      title={Online Learning as Stochastic Approximation of Regularization Paths}, 
      author={Pierre Tarrès and Yuan Yao},
      year={2013},
      eprint={1103.5538},
      archivePrefix={arXiv},
      primaryClass={math.PR}
}

@inproceedings{ma_batch_size,
  author={Siyuan Ma and Raef Bassily and Mikhail Belkin},
  title={The Power of Interpolation: Understanding the Effectiveness of SGD in Modern Over-parametrized Learning},
  year={2018},
  cdate={1514764800000},
  pages={3331-3340},
  url={http://proceedings.mlr.press/v80/ma18a.html},
  booktitle={ICML},
  crossref={conf/icml/2018}
}


@article{ying_nonparametric,
author = {Ying, Yiming and Pontil, Massimiliano},
title = {Online Gradient Descent Learning Algorithms},
year = {2008},
issue_date = {October   2008},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {8},
number = {5},
issn = {1615-3375},
abstract = {This paper considers the least-square online gradient descent algorithm in a reproducing
kernel Hilbert space (RKHS) without an explicit regularization term. We present a
novel capacity independent approach to derive error bounds and convergence results
for this algorithm. The essential element in our analysis is the interplay between
the generalization error and a weighted cumulative error which we define in the paper.
We show that, although the algorithm does not involve an explicit RKHS regularization
term, choosing the step sizes appropriately can yield competitive error rates with
those in the literature.},
journal = {Found. Comput. Math.},
month = oct,
pages = {561–596},
numpages = {36}
}

@inproceedings{Jain2018AcceleratingSG,
  title={Accelerating Stochastic Gradient Descent for Least Squares Regression},
  author={Prateek Jain and S. Kakade and Rahul Kidambi and Praneeth Netrapalli and Aaron Sidford},
  booktitle={COLT},
  year={2018}
}

@article{Duchi2016AsymptoticOI,
  title={Asymptotic optimality in stochastic optimization},
  author={John C. Duchi and Feng Ruan},
  journal={arXiv: Statistics Theory},
  year={2016}
}

@misc{zhang2019algorithmic,
      title={Which Algorithmic Choices Matter at Which Batch Sizes? Insights From a Noisy Quadratic Model}, 
      author={Guodong Zhang and Lala Li and Zachary Nado and James Martens and Sushant Sachdeva and George E. Dahl and Christopher J. Shallue and Roger Grosse},
      year={2019},
      eprint={1907.04164},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@InProceedings{flammarion_noisy_1_over_n,
  title = 	 {Stochastic Composite Least-Squares Regression with Convergence Rate $O(1/n)$},
  author = 	 {Flammarion, Nicolas and Bach, Francis},
  booktitle = 	 {Proceedings of the 2017 Conference on Learning Theory},
  pages = 	 {831--875},
  year = 	 {2017},
  editor = 	 {Kale, Satyen and Shamir, Ohad},
  volume = 	 {65},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {07--10 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v65/flammarion17a/flammarion17a.pdf},
  url = 	 {https://proceedings.mlr.press/v65/flammarion17a.html},
  abstract = 	 {We consider the minimization of composite objective functions composed of the expectation of quadratic functions and an arbitrary convex function. We study the stochastic dual averaging algorithm with a constant step-size, showing that it leads to a convergence rate of O(1/n) without strong convexity assumptions. This thus extends earlier results on least-squares regression with the Euclidean geometry to (a) all convex regularizers and constraints, and (b) all geometries represented by a Bregman divergence. This is achieved by a new proof technique that relates stochastic and deterministic recursions}
}


@misc{flammarion2015averaging,
      title={From Averaging to Acceleration, There is Only a Step-size}, 
      author={Nicolas Flammarion and Francis Bach},
      year={2015},
      eprint={1504.01577},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{varre2021iterate,
      title={Last iterate convergence of SGD for Least-Squares in the Interpolation regime}, 
      author={Aditya Varre and Loucas Pillaud-Vivien and Nicolas Flammarion},
      year={2021},
      eprint={2102.03183},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Zhang2017UnderstandingDL,
  title={Understanding deep learning requires rethinking generalization},
  author={C. Zhang and S. Bengio and Moritz Hardt and B. Recht and Oriol Vinyals},
  journal={ArXiv},
  year={2017},
  volume={abs/1611.03530}
}
@article{Spigler_2020,
   title={Asymptotic learning curves of kernel methods: empirical data versus teacher–student paradigm},
   volume={2020},
   ISSN={1742-5468},
   url={http://dx.doi.org/10.1088/1742-5468/abc61d},
   DOI={10.1088/1742-5468/abc61d},
   number={12},
   journal={Journal of Statistical Mechanics: Theory and Experiment},
   publisher={IOP Publishing},
   author={Spigler, Stefano and Geiger, Mario and Wyart, Matthieu},
   year={2020},
   month={Dec},
   pages={124001}
}
@misc{domingos2020model,
      title={Every Model Learned by Gradient Descent Is Approximately a Kernel Machine}, 
      author={Pedro Domingos},
      year={2020},
      eprint={2012.00152},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{lecun-mnisthandwrittendigit-2010,
  added-at = {2010-06-28T21:16:30.000+0200},
  author = {LeCun, Yann and Cortes, Corinna},
  biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
  groups = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash = {21b9d0558bd66279df9452562df6e6f3},
  intrahash = {935bad99fa1f65e03c25b315aa3c1032},
  keywords = {MSc _checked character_recognition mnist network neural},
  lastchecked = {2016-01-14 14:24:11},
  timestamp = {2016-07-12T19:25:30.000+0200},
  title = {{MNIST} handwritten digit database},
  url = {http://yann.lecun.com/exdb/mnist/},
  username = {mhwombat},
  year = 2010
}

@misc{mignacco2020dynamical,
      title={Dynamical mean-field theory for stochastic gradient descent in Gaussian mixture classification}, 
      author={Francesca Mignacco and Florent Krzakala and Pierfrancesco Urbani and Lenka Zdeborová},
      year={2020},
      eprint={2006.06098},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{yoshida_okada,
 author = {Yoshida, Yuki and Okada, Masato},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Data-Dependence of Plateau Phenomenon in Learning with Neural Network --- Statistical Mechanical Analysis},
 url = {https://proceedings.neurips.cc/paper/2019/file/287e03db1d99e0ec2edb90d079e142f3-Paper.pdf},
 volume = {32},
 year = {2019}
}



@article{saad_solla,
author = {Saad, David and Solla, Sara},
year = {1999},
month = {04},
pages = {},
title = {Dynamics of On-Line Gradient Descent Learning for Multilayer Neural Networks}
}

@inproceedings{goldt_first,
 author = {Goldt, Sebastian and Advani, Madhu and Saxe, Andrew M and Krzakala, Florent and Zdeborov\'{a}, Lenka},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup},
 url = {https://proceedings.neurips.cc/paper/2019/file/cab070d53bd0d200746fb852a922064a-Paper.pdf},
 volume = {32},
 year = {2019}
}



@article{lecun_solla_eigenspectrum_cov,
  title = {Eigenvalues of covariance matrices: Application to neural-network learning},
  author = {Cun, Yann Le and Kanter, Ido and Solla, Sara A.},
  journal = {Phys. Rev. Lett.},
  volume = {66},
  issue = {18},
  pages = {2396--2399},
  numpages = {0},
  year = {1991},
  month = {May},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.66.2396},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.66.2396}
}


@article{cifar_cite,
title= {CIFAR-10 (Canadian Institute for Advanced Research)},
journal= {},
author= {Alex Krizhevsky and Vinod Nair and Geoffrey Hinton},
year= {},
url= {http://www.cs.toronto.edu/~kriz/cifar.html},
abstract= {The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. 

The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. },
keywords= {Dataset},
terms= {}
}

@misc{canatar2021spectral,
      title={Spectral Bias and Task-Model Alignment Explain Generalization in Kernel Regression and Infinitely Wide Neural Networks}, 
      author={Abdulkadir Canatar and Blake Bordelon and Cengiz Pehlevan},
      year={2021},
      eprint={2006.13198},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{advani_ganguli_convex,
  title = {Statistical Mechanics of Optimal Convex Inference in High Dimensions},
  author = {Advani, Madhu and Ganguli, Surya},
  journal = {Phys. Rev. X},
  volume = {6},
  issue = {3},
  pages = {031034},
  numpages = {16},
  year = {2016},
  month = {Aug},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevX.6.031034},
  url = {https://link.aps.org/doi/10.1103/PhysRevX.6.031034}
}
@misc{mei2020generalization,
      title={The generalization error of random features regression: Precise asymptotics and double descent curve}, 
      author={Song Mei and Andrea Montanari},
      year={2020},
      eprint={1908.05355},
      archivePrefix={arXiv},
      primaryClass={math.ST}
}

@article{Advani_2013,
   title={Statistical mechanics of complex neural systems and high dimensional data},
   volume={2013},
   ISSN={1742-5468},
   url={http://dx.doi.org/10.1088/1742-5468/2013/03/P03014},
   DOI={10.1088/1742-5468/2013/03/p03014},
   number={03},
   journal={Journal of Statistical Mechanics: Theory and Experiment},
   publisher={IOP Publishing},
   author={Advani, Madhu and Lahiri, Subhaneil and Ganguli, Surya},
   year={2013},
   month={Mar},
   pages={P03014}
}

@inproceedings{Werfel,
 author = {Werfel, Justin and Xie, Xiaohui and Seung, H.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 pages = {},
 publisher = {MIT Press},
 title = {Learning Curves for Stochastic Gradient Descent in Linear Feedforward Networks},
 url = {https://proceedings.neurips.cc/paper/2003/file/f8b932c70d0b2e6bf071729a4fa68dfc-Paper.pdf},
 volume = {16},
 year = {2004}
}

@misc{velikanov2021universal,
      title={Universal scaling laws in the gradient descent training of neural networks}, 
      author={Maksim Velikanov and Dmitry Yarotsky},
      year={2021},
      eprint={2105.00507},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{Saxe_McClelland_Ganguli,
  author    = {Andrew M. Saxe and
               James L. McClelland and
               Surya Ganguli},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Exact solutions to the nonlinear dynamics of learning in deep linear
               neural networks},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014,
               Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  year      = {2014},
  url       = {http://arxiv.org/abs/1312.6120},
  timestamp = {Thu, 04 Apr 2019 13:20:07 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SaxeMG13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Canatar2020SpectralBA,
  title={Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks},
  author={Abdulkadir Canatar and B. Bordelon and C. Pehlevan},
  journal={Nature Communications},
  year={2020},
  volume={12},
  pages={1-12}
}

@article{Polyak1992AccelerationOS,
  title={Acceleration of stochastic approximation by averaging},
  author={B. Polyak and A. Juditsky},
  journal={Siam Journal on Control and Optimization},
  year={1992},
  volume={30},
  pages={838-855}
}

@article{chung,
author = {K. L. Chung},
title = {{On a Stochastic Approximation Method}},
volume = {25},
journal = {The Annals of Mathematical Statistics},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {463 -- 483},
year = {1954},
doi = {10.1214/aoms/1177728716},
URL = {https://doi.org/10.1214/aoms/1177728716}
}


@article{robbins_monro,
author = {Herbert Robbins and Sutton Monro},
title = {{A Stochastic Approximation Method}},
volume = {22},
journal = {The Annals of Mathematical Statistics},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {400 -- 407},
year = {1951},
doi = {10.1214/aoms/1177729586},
URL = {https://doi.org/10.1214/aoms/1177729586}
}


@article{shapiro_stoch,
author = {Alexander Shapiro},
title = {{Asymptotic Properties of Statistical Estimators in Stochastic Programming}},
volume = {17},
journal = {The Annals of Statistics},
number = {2},
publisher = {Institute of Mathematical Statistics},
pages = {841 -- 858},
keywords = {$M$-estimators, asymptotic normality, cone approximation, inequality constraints, Lagrange multipliers, optimality conditions, stochastic programming},
year = {1989},
doi = {10.1214/aos/1176347146},
URL = {https://doi.org/10.1214/aos/1176347146}
}


@article{Ruppert,
author = {Ruppert, David},
year = {1988},
month = {02},
pages = {},
title = {Efficient Estimations from a Slowly Convergent Robbins-Monro Process}
}

@article{Hertz_1989,
	doi = {10.1088/0305-4470/22/12/016},
	url = {https://doi.org/10.1088/0305-4470/22/12/016},
	year = 1989,
	month = {jun},
	publisher = {{IOP} Publishing},
	volume = {22},
	number = {12},
	pages = {2133--2150},
	author = {J A Hertz and A Krogh and G I Thorbergsson},
	title = {Phase transitions in simple learning},
	journal = {Journal of Physics A: Mathematical and General},
	abstract = {The authors investigate learning in the simplest type of a layered neural network, the one layer perceptron. The learning process is treated as a statistical dynamical problem. Quantities one is interested in include the relaxation time (the learning time) and the capacity and how they depend on noise and constraints on the weights. The relaxation time is calculated as a function of the noise level and the number p of associations to be learned. They consider three different cases for input patterns that are random and uncorrelated. In the first, where the connection weights are constrained to satisfy N-1 Sigma i omega i2=S2, there is a critical value of p(<N) separating regimes of perfect and imperfect learning at zero noise. In contrast, the second model, unconstrained learning, exhibits a different kind of transition at p=N, and noise plays no role. In the third model, where the constraint is imposed only on the thermal fluctuations, there is a line of phase transitions terminating at p=N and zero noise. They have also considered learning with correlated input patterns. The most important difference is the emergence of a second relaxation time, which the authors interpret as the time it takes to learn a prototype of the patterns.}
}

@article {Hopfield2554,
	author = {Hopfield, J J},
	title = {Neural networks and physical systems with emergent collective computational abilities},
	volume = {79},
	number = {8},
	pages = {2554--2558},
	year = {1982},
	doi = {10.1073/pnas.79.8.2554},
	publisher = {National Academy of Sciences},
	abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/79/8/2554},
	eprint = {https://www.pnas.org/content/79/8/2554.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}


@article{Gardner1988OptimalSP,
  title={Optimal storage properties of neural network models},
  author={E. Gardner and B. Derrida},
  journal={Journal of Physics A},
  year={1988},
  volume={21},
  pages={271-284}
}
@inproceedings{dAscoli_triple,
 author = {d\textquotesingle Ascoli, St\'{e}phane and Sagun, Levent and Biroli, Giulio},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {3058--3069},
 publisher = {Curran Associates, Inc.},
 title = {Triple descent and the two kinds of overfitting: where \&amp; why do they appear?},
 url = {https://proceedings.neurips.cc/paper/2020/file/1fd09c5f59a8ff35d499c0ee25a1d47e-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{hestness2017deep,
      title={Deep Learning Scaling is Predictable, Empirically}, 
      author={Joel Hestness and Sharan Narang and Newsha Ardalani and Gregory Diamos and Heewoo Jun and Hassan Kianinejad and Md. Mostofa Ali Patwary and Yang Yang and Yanqi Zhou},
      year={2017},
      eprint={1712.00409},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{Rahimi_Recht,
 author = {Rahimi, Ali and Recht, Benjamin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Random Features for Large-Scale Kernel Machines},
 url = {https://proceedings.neurips.cc/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf},
 volume = {20},
 year = {2008}
}



@inproceedings{Pennington,
 author = {Pennington, Jeffrey and Worah, Pratik},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {The Spectrum of the Fisher Information Matrix of a Single-Hidden-Layer Neural Network},
 url = {https://proceedings.neurips.cc/paper/2018/file/18bb68e2b38e4a8ce7cf4f6b2625768c-Paper.pdf},
 volume = {31},
 year = {2018}
}



@inproceedings{Chizat,
 author = {Chizat, L\'{e}na\"{\i}c and Oyallon, Edouard and Bach, Francis},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {On Lazy Training in Differentiable Programming},
 url = {https://proceedings.neurips.cc/paper/2019/file/ae614c557843b1df326cb29c57225459-Paper.pdf},
 volume = {32},
 year = {2019}
}




@misc{jacot2020kernel,
      title={Kernel Alignment Risk Estimator: Risk Prediction from Training Data}, 
      author={Arthur Jacot and Berfin Şimşek and Francesco Spadaro and Clément Hongler and Franck Gabriel},
      year={2020},
      eprint={2006.09796},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{nakkiran2019data,
      title={More Data Can Hurt for Linear Regression: Sample-wise Double Descent}, 
      author={Preetum Nakkiran},
      year={2019},
      eprint={1912.07242},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{ADVANI_SAXE_Somp,
title = {High-dimensional dynamics of generalization error in neural networks},
journal = {Neural Networks},
volume = {132},
pages = {428-446},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.08.022},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020303117},
author = {Madhu S. Advani and Andrew M. Saxe and Haim Sompolinsky},
keywords = {Neural networks, Generalization error, Random matrix theory},
abstract = {We perform an analysis of the average generalization dynamics of large neural networks trained using gradient descent. We study the practically-relevant “high-dimensional” regime where the number of free parameters in the network is on the order of or even larger than the number of examples in the dataset. Using random matrix theory and exact solutions in linear models, we derive the generalization error and training error dynamics of learning and analyze how they depend on the dimensionality of data and signal to noise ratio of the learning problem. We find that the dynamics of gradient descent learning naturally protect against overtraining and overfitting in large networks. Overtraining is worst at intermediate network sizes, when the effective number of free parameters equals the number of samples, and thus can be reduced by making a network smaller or larger. Additionally, in the high-dimensional regime, low generalization error requires starting with small initial weights. We then turn to non-linear neural networks, and show that making networks very large does not harm their generalization performance. On the contrary, it can in fact reduce overtraining, even without early stopping or regularization of any sort. We identify two novel phenomena underlying this behavior in overcomplete models: first, there is a frozen subspace of the weights in which no learning occurs under gradient descent; and second, the statistical properties of the high-dimensional regime yield better-conditioned input correlations which protect against overtraining. We demonstrate that standard application of theories such as Rademacher complexity are inaccurate in predicting the generalization performance of deep neural networks, and derive an alternative bound which incorporates the frozen subspace and conditioning effects and qualitatively matches the behavior observed in simulation.}
}



@book{engel_van_den_broeck_2001, 
title={Statistical Mechanics of Learning}, DOI={10.1017/CBO9781139164542}, publisher={Cambridge University Press}, author={Engel, A. and Van den Broeck, C.}, year={2001}}

@misc{hu2021universality,
      title={Universality Laws for High-Dimensional Learning with Random Features}, 
      author={Hong Hu and Yue M. Lu},
      year={2021},
      eprint={2009.07669},
      archivePrefix={arXiv},
      primaryClass={cs.IT}
}


@article{goldt_hidden_manifold,
  title = {Modeling the Influence of Data Structure on Learning in Neural Networks: The Hidden Manifold Model},
  author = {Goldt, Sebastian and M\'ezard, Marc and Krzakala, Florent and Zdeborov\'a, Lenka},
  journal = {Phys. Rev. X},
  volume = {10},
  issue = {4},
  pages = {041044},
  numpages = {32},
  year = {2020},
  month = {Dec},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevX.10.041044},
  url = {https://link.aps.org/doi/10.1103/PhysRevX.10.041044}
}

@misc{pillaudvivien2018statistical,
      title={Statistical Optimality of Stochastic Gradient Descent on Hard Learning Problems through Multiple Passes}, 
      author={Loucas Pillaud-Vivien and Alessandro Rudi and Francis Bach},
      year={2018},
      eprint={1805.10074},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{fischer_sobolev,
  author  = {Simon Fischer and Ingo Steinwart},
  title   = {Sobolev Norm Learning Rates for Regularized Least-Squares Algorithms},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {205},
  pages   = {1-38},
  url     = {http://jmlr.org/papers/v21/19-734.html}
}

@article{dieuleveut,
author = {Dieuleveut, Aymeric and Flammarion, Nicolas and Bach, Francis},
year = {2016},
month = {02},
pages = {},
title = {Harder, Better, Faster, Stronger Convergence Rates for Least-Squares Regression},
volume = {18},
journal = {Journal of Machine Learning Research}
}

@misc{berthier2020tight,
      title={Tight Nonparametric Convergence Rates for Stochastic Gradient Descent under the Noiseless Linear Model}, 
      author={Raphaël Berthier and Francis Bach and Pierre Gaillard},
      year={2020},
      eprint={2006.08212},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{loureiro2021capturing,title={Capturing the learning curves of generic features maps for realistic data sets with a teacher-student model}, 
      author={Bruno Loureiro and Cédric Gerbelot and Hugo Cui and Sebastian Goldt and Florent Krzakala and Marc Mézard and Lenka Zdeborová},
      year={2021},
      eprint={2102.08127},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@article{Biehl_1994,
	doi = {10.1209/0295-5075/28/7/012},
	url = {https://doi.org/10.1209/0295-5075/28/7/012},
	year = 1994,
	month = {dec},
	publisher = {{IOP} Publishing},
	volume = {28},
	number = {7},
	pages = {525--530},
	author = {M Biehl and P Riegler},
	title = {On-Line Learning with a Perceptron},
	journal = {Europhysics Letters ({EPL})},
	abstract = {We study on-line learning of a linearly separable rule with a simple perceptron. Training utilizes a sequence of uncorrelated, randomly drawn N-dimensional input examples. In the thermodynamic limit the generalization error after training such examples with P can be calculated exactly. For the standard perceptron algorithm it decrease like (N/P)1/3 for large P/N, in contrast to the faster (N/P)1/2-behaviour of the so-called Hebbian learning. Furthermore, we show that a specific parameter-free on-line scheme, the AdaTron algorithm, gives an asymptotic (N/P)-decay of the generalization error. This coincides (up to a constant factor) with the bound for any training process based on random examples, including off-line learning. Simulations confirm our results.}
}

@article{heskes,
  title = {Learning processes in neural networks},
  author = {Heskes, Tom M. and Kappen, Bert},
  journal = {Phys. Rev. A},
  volume = {44},
  issue = {4},
  pages = {2718--2726},
  numpages = {0},
  year = {1991},
  month = {Aug},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevA.44.2718},
  url = {https://link.aps.org/doi/10.1103/PhysRevA.44.2718}
}
@article{Mace1998StatisticalMA,
  title={Statistical mechanical analysis of the dynamics of learning in perceptrons},
  author={C. Mace and A. Coolen},
  journal={Statistics and Computing},
  year={1998},
  volume={8},
  pages={55-88}
}

@InProceedings{anastasiu, title = {Normal Approximation for Stochastic Gradient Descent via Non-Asymptotic Rates of Martingale CLT}, author = {Anastasiou, Andreas and Balasubramanian, Krishnakumar and Erdogdu, Murat A.}, booktitle = {Proceedings of the Thirty-Second Conference on Learning Theory}, pages = {115--137}, year = {2019}, editor = {Alina Beygelzimer and Daniel Hsu}, volume = {99}, series = {Proceedings of Machine Learning Research}, address = {Phoenix, USA}, month = {25--28 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v99/anastasiou19a/anastasiou19a.pdf}, url = {http://proceedings.mlr.press/v99/anastasiou19a.html}, abstract = {We provide non-asymptotic convergence rates of the Polyak-Ruppert averaged stochastic gradient descent (SGD) to a normal random vector for a class of twice-differentiable test functions. A crucial intermediate step is proving a non-asymptotic martingale central limit theorem (CLT), i.e., establishing the rates of convergence of a multivariate martingale difference sequence to a normal random vector, which might be of independent interest. We obtain the explicit rates for the multivariate martingale CLT using a combination of Stein?s method and Lindeberg?s argument, which is then used in conjunction with a non-asymptotic analysis of averaged SGD proposed in [PJ92]. Our results have potentially interesting consequences for computing confidence intervals for parameter estimation with SGD and constructing hypothesis tests with SGD that are valid in a non-asymptotic sense} }

@misc{yu2020analysis,
      title={An Analysis of Constant Step Size SGD in the Non-convex Regime: Asymptotic Normality and Bias}, 
      author={Lu Yu and Krishnakumar Balasubramanian and Stanislav Volgushev and Murat A. Erdogdu},
      year={2020},
      eprint={2006.07904},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@InProceedings{gerace, title = {Generalisation error in learning with random features and the hidden manifold model}, author = {Gerace, Federica and Loureiro, Bruno and Krzakala, Florent and Mezard, Marc and Zdeborova, Lenka}, booktitle = {Proceedings of the 37th International Conference on Machine Learning}, pages = {3452--3462}, year = {2020}, editor = {Hal Daumé III and Aarti Singh}, volume = {119}, series = {Proceedings of Machine Learning Research}, month = {13--18 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v119/gerace20a/gerace20a.pdf}, url = { http://proceedings.mlr.press/v119/gerace20a.html }, abstract = {We study generalised linear regression and classification for a synthetically generated dataset encompassing different problems of interest, such as learning with random features, neural networks in the lazy training regime, and the hidden manifold model. We consider the high-dimensional regime and using the replica method from statistical physics, we provide a closed-form expression for the asymptotic generalisation performance in these problems, valid in both the under- and over-parametrised regimes and for a broad choice of generalised linear model loss functions. In particular, we show how to obtain analytically the so-called double descent behaviour for logistic regression with a peak at the interpolation threshold, we illustrate the superiority of orthogonal against random Gaussian projections in learning with random features, and discuss the role played by correlations in the data generated by the hidden manifold model. Beyond the interest in these particular problems, the theoretical formalism introduced in this manuscript provides a path to further extensions to more complex tasks.} }


@article{duchi_ruan,
author = {John C. Duchi and Feng Ruan},
title = {{Asymptotic optimality in stochastic optimization}},
volume = {49},
journal = {The Annals of Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {21 -- 48},
abstract = {We study local complexity measures for stochastic convex optimization problems, providing a local minimax theory analogous to that of Hájek and Le Cam for classical statistical problems. We give complementary optimality results, developing fully online methods that adaptively achieve optimal convergence guarantees. Our results provide function-specific lower bounds and convergence results that make precise a correspondence between statistical difficulty and the geometric notion of tilt-stability from optimization. As part of this development, we show how variants of Nesterov’s dual averaging—a stochastic gradient-based procedure—guarantee finite time identification of constraints in optimization problems, while stochastic gradient procedures fail. Additionally, we highlight a gap between problems with linear and nonlinear constraints: standard stochastic-gradient-based procedures are suboptimal even for the simplest nonlinear constraints, necessitating the development of asymptotically optimal Riemannian stochastic gradient methods.},
keywords = {convex analysis, Local asymptotic minimax theory, manifold identification, stochastic gradients},
year = {2021},
doi = {10.1214/19-AOS1831},
URL = {https://doi.org/10.1214/19-AOS1831}
}


@article{chung_2,
    author = {Chung, SueYeon and Cohen, Uri and Sompolinsky, Haim and Lee, Daniel D.},
    title = "{Learning Data Manifolds with a Cutting Plane Method}",
    journal = {Neural Computation},
    volume = {30},
    number = {10},
    pages = {2593-2615},
    year = {2018},
    month = {10},
    abstract = "{We consider the problem of classifying data manifolds where each manifold represents invariances that are parameterized by continuous degrees of freedom. Conventional data augmentation methods rely on sampling large numbers of training examples from these manifolds. Instead, we propose an iterative algorithm, MCP, based on a cutting plane approach that efficiently solves a quadratic semi-infinite programming problem to find the maximum margin solution. We provide a proof of convergence as well as a polynomial bound on the number of iterations required for a desired tolerance in the objective function. The efficiency and performance of MCP are demonstrated in high-dimensional simulations and on image manifolds generated from the ImageNet data set. Our results indicate that MCP is able to rapidly learn good classifiers and shows superior generalization performance compared with conventional maximum margin methods using data augmentation methods.}",
    issn = {0899-7667},
    doi = {10.1162/neco_a_01119},
    url = {https://doi.org/10.1162/neco\_a\_01119},
    eprint = {https://direct.mit.edu/neco/article-pdf/30/10/2593/1046637/neco\_a\_01119.pdf},
}





@article{chung_manifold,
  title = {Classification and Geometry of General Perceptual Manifolds},
  author = {Chung, SueYeon and Lee, Daniel D. and Sompolinsky, Haim},
  journal = {Phys. Rev. X},
  volume = {8},
  issue = {3},
  pages = {031003},
  numpages = {26},
  year = {2018},
  month = {Jul},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevX.8.031003},
  url = {https://link.aps.org/doi/10.1103/PhysRevX.8.031003}
}


@book{mohri_book,
author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
title = {Foundations of Machine Learning},
year = {2012},
isbn = {026201825X},
publisher = {The MIT Press},
abstract = {This graduate-level textbook introduces fundamental concepts and methods in machine learning. It describes several important modern algorithms, provides the theoretical underpinnings of these algorithms, and illustrates key aspects for their application. The authors aim to present novel theoretical tools and concepts while giving concise proofs even for relatively advanced topics. Foundations of Machine Learning fills the need for a general textbook that also offers theoretical details and an emphasis on proofs. Certain topics that are often treated with insufficient attention are discussed in more detail here; for example, entire chapters are devoted to regression, multi-class classification, and ranking. The first three chapters lay the theoretical foundation for what follows, but each remaining chapter is mostly self-contained. The appendix offers a concise probability review, a short introduction to convex optimization, tools for concentration bounds, and several basic properties of matrices and norms used in the book. The book is intended for graduate students and researchers in machine learning, statistics, and related areas; it can be used either as a textbook or as a reference text for a research seminar. }
}

@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@book{bender,
author = {Bender, Carl and Orszag, Steven},
year = {1999},
month = {01},
pages = {},
title = {Advanced Mathematical Methods for Scientists and Engineers: Asymptotic Methods and Perturbation Theory},
volume = {1},
isbn = {978-1-4419-3187-0},
doi = {10.1007/978-1-4757-3069-2}
}


@inproceedings{
pope2021the,
title={The Intrinsic Dimension of Images and Its Impact on Learning},
author={Phil Pope and Chen Zhu and Ahmed Abdelkader and Micah Goldblum and Tom Goldstein},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=XJk19XzGq2J}
}

@book{Lax,
title = {Functional Analysis},
author = {Lax, Peter},
publisher={Wiley},
year = {1966}
}



@book{RasmussenWilliams,
author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
title = {Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)},
year = {2005},
publisher = {The MIT Press}
}

@misc{jacot2020neural,
      title={Neural Tangent Kernel: Convergence and Generalization in Neural Networks}, 
      author={Arthur Jacot and Franck Gabriel and Clément Hongler},
      year={2020},
      eprint={1806.07572},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{neuraltangents2020,
    title={Neural Tangents: Fast and Easy Infinite Neural Networks in Python},
    author={Roman Novak and Lechao Xiao and Jiri Hron and Jaehoon Lee and Alexander A. Alemi and Jascha Sohl-Dickstein and Samuel S. Schoenholz},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://github.com/google/neural-tangents}
}

@inproceedings{bordelon2020spectrum,
  title={Spectrum dependent learning curves in kernel regression and wide neural networks},
  author={Bordelon, Blake and Canatar, Abdulkadir and Pehlevan, Cengiz},
  booktitle={International Conference on Machine Learning},
  pages={1024--1034},
  year={2020},
  organization={PMLR}
}

@article{dyer2019asymptotics,
  title={Asymptotics of wide networks from feynman diagrams},
  author={Dyer, Ethan and Gur-Ari, Guy},
  journal={arXiv preprint arXiv:1909.11304},
  year={2019}
}

@article{arora2019exact,
  title={On exact computation with an infinitely wide neural net},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Ruslan and Wang, Ruosong},
  journal={arXiv preprint arXiv:1904.11955},
  year={2019}
}