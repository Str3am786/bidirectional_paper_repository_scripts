\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anastasiou et~al.(2019)Anastasiou, Balasubramanian, and
  Erdogdu]{anastasiu}
Andreas Anastasiou, Krishnakumar Balasubramanian, and Murat~A. Erdogdu.
\newblock Normal approximation for stochastic gradient descent via
  non-asymptotic rates of martingale clt.
\newblock In Alina Beygelzimer and Daniel Hsu (eds.), \emph{Proceedings of the
  Thirty-Second Conference on Learning Theory}, volume~99 of \emph{Proceedings
  of Machine Learning Research}, pp.\  115--137, Phoenix, USA, 25--28 Jun 2019.
  PMLR.
\newblock URL \url{http://proceedings.mlr.press/v99/anastasiou19a.html}.

\bibitem[Bahri et~al.(2021)Bahri, Dyer, Kaplan, Lee, and
  Sharma]{bahri2021explaining}
Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma.
\newblock Explaining neural scaling laws.
\newblock \emph{arXiv preprint arXiv:2102.06701}, 2021.

\bibitem[Bartlett et~al.(2020)Bartlett, Long, Lugosi, and
  Tsigler]{bartlett2020benign}
Peter~L Bartlett, Philip~M Long, G{\'a}bor Lugosi, and Alexander Tsigler.
\newblock Benign overfitting in linear regression.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (48):\penalty0 30063--30070, 2020.

\bibitem[Bender \& Orszag(1999)Bender and Orszag]{bender}
Carl Bender and Steven Orszag.
\newblock \emph{Advanced Mathematical Methods for Scientists and Engineers:
  Asymptotic Methods and Perturbation Theory}, volume~1.
\newblock 01 1999.
\newblock ISBN 978-1-4419-3187-0.
\newblock \doi{10.1007/978-1-4757-3069-2}.

\bibitem[Berthier et~al.(2020)Berthier, Bach, and Gaillard]{berthier2020tight}
Raphaël Berthier, Francis Bach, and Pierre Gaillard.
\newblock Tight nonparametric convergence rates for stochastic gradient descent
  under the noiseless linear model, 2020.

\bibitem[Biehl \& Riegler(1994)Biehl and Riegler]{Biehl_1994}
M~Biehl and P~Riegler.
\newblock On-line learning with a perceptron.
\newblock \emph{Europhysics Letters ({EPL})}, 28\penalty0 (7):\penalty0
  525--530, dec 1994.
\newblock \doi{10.1209/0295-5075/28/7/012}.
\newblock URL \url{https://doi.org/10.1209/0295-5075/28/7/012}.

\bibitem[Bordelon et~al.(2020)Bordelon, Canatar, and
  Pehlevan]{bordelon2020spectrum}
Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan.
\newblock Spectrum dependent learning curves in kernel regression and wide
  neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1024--1034. PMLR, 2020.

\bibitem[Canatar et~al.(2020)Canatar, Bordelon, and
  Pehlevan]{Canatar2020SpectralBA}
Abdulkadir Canatar, B.~Bordelon, and C.~Pehlevan.
\newblock Spectral bias and task-model alignment explain generalization in
  kernel regression and infinitely wide neural networks.
\newblock \emph{Nature Communications}, 12:\penalty0 1--12, 2020.

\bibitem[Cao \& Gu(2019)Cao and Gu]{cao2019generalization}
Yuan Cao and Quanquan Gu.
\newblock Generalization bounds of stochastic gradient descent for wide and
  deep neural networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 10836--10846, 2019.

\bibitem[Cao et~al.(2021)Cao, Gu, and Belkin]{cao2021risk}
Yuan Cao, Quanquan Gu, and Misha Belkin.
\newblock Risk bounds for over-parameterized maximum margin classification on
  sub-gaussian mixtures.
\newblock In \emph{Thirty-Fifth Conference on Neural Information Processing
  Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=ChWy1anEuow}.

\bibitem[Chatterji \& Long(2021)Chatterji and Long]{chatterji2021finite}
Niladri~S Chatterji and Philip~M Long.
\newblock Finite-sample analysis of interpolating linear classifiers in the
  overparameterized regime.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (129):\penalty0 1--30, 2021.

\bibitem[Chung(1954)]{chung}
K.~L. Chung.
\newblock {On a Stochastic Approximation Method}.
\newblock \emph{The Annals of Mathematical Statistics}, 25\penalty0
  (3):\penalty0 463 -- 483, 1954.
\newblock \doi{10.1214/aoms/1177728716}.
\newblock URL \url{https://doi.org/10.1214/aoms/1177728716}.

\bibitem[Dieuleveut \& Bach(2016)Dieuleveut and Bach]{dieuleveut_nonparametric}
Aymeric Dieuleveut and Francis Bach.
\newblock {Nonparametric stochastic approximation with large step-sizes}.
\newblock \emph{The Annals of Statistics}, 44\penalty0 (4):\penalty0 1363 --
  1399, 2016.
\newblock \doi{10.1214/15-AOS1391}.
\newblock URL \url{https://doi.org/10.1214/15-AOS1391}.

\bibitem[Dieuleveut et~al.(2016)Dieuleveut, Flammarion, and Bach]{dieuleveut}
Aymeric Dieuleveut, Nicolas Flammarion, and Francis Bach.
\newblock Harder, better, faster, stronger convergence rates for least-squares
  regression.
\newblock \emph{Journal of Machine Learning Research}, 18, 02 2016.

\bibitem[Duchi \& Ruan(2021)Duchi and Ruan]{duchi_ruan}
John~C. Duchi and Feng Ruan.
\newblock {Asymptotic optimality in stochastic optimization}.
\newblock \emph{The Annals of Statistics}, 49\penalty0 (1):\penalty0 21 -- 48,
  2021.
\newblock \doi{10.1214/19-AOS1831}.
\newblock URL \url{https://doi.org/10.1214/19-AOS1831}.

\bibitem[Engel \& Van~den Broeck(2001)Engel and Van~den
  Broeck]{engel_van_den_broeck_2001}
A.~Engel and C.~Van~den Broeck.
\newblock \emph{Statistical Mechanics of Learning}.
\newblock Cambridge University Press, 2001.
\newblock \doi{10.1017/CBO9781139164542}.

\bibitem[Fischer \& Steinwart(2020)Fischer and Steinwart]{fischer_sobolev}
Simon Fischer and Ingo Steinwart.
\newblock Sobolev norm learning rates for regularized least-squares algorithms.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (205):\penalty0 1--38, 2020.
\newblock URL \url{http://jmlr.org/papers/v21/19-734.html}.

\bibitem[Flammarion \& Bach(2015)Flammarion and Bach]{flammarion2015averaging}
Nicolas Flammarion and Francis Bach.
\newblock From averaging to acceleration, there is only a step-size.
\newblock In \emph{Conference on Learning Theory}, pp.\  658--695. PMLR, 2015.

\bibitem[Flammarion \& Bach(2017)Flammarion and
  Bach]{flammarion_noisy_1_over_n}
Nicolas Flammarion and Francis Bach.
\newblock Stochastic composite least-squares regression with convergence rate
  $o(1/n)$.
\newblock In Satyen Kale and Ohad Shamir (eds.), \emph{Proceedings of the 2017
  Conference on Learning Theory}, volume~65 of \emph{Proceedings of Machine
  Learning Research}, pp.\  831--875. PMLR, 07--10 Jul 2017.
\newblock URL \url{https://proceedings.mlr.press/v65/flammarion17a.html}.

\bibitem[Goldt et~al.(2019)Goldt, Advani, Saxe, Krzakala, and
  Zdeborov\'{a}]{goldt_first}
Sebastian Goldt, Madhu Advani, Andrew~M Saxe, Florent Krzakala, and Lenka
  Zdeborov\'{a}.
\newblock Dynamics of stochastic gradient descent for two-layer neural networks
  in the teacher-student setup.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/cab070d53bd0d200746fb852a922064a-Paper.pdf}.

\bibitem[Goldt et~al.(2020)Goldt, M\'ezard, Krzakala, and
  Zdeborov\'a]{goldt_hidden_manifold}
Sebastian Goldt, Marc M\'ezard, Florent Krzakala, and Lenka Zdeborov\'a.
\newblock Modeling the influence of data structure on learning in neural
  networks: The hidden manifold model.
\newblock \emph{Phys. Rev. X}, 10:\penalty0 041044, Dec 2020.
\newblock \doi{10.1103/PhysRevX.10.041044}.
\newblock URL \url{https://link.aps.org/doi/10.1103/PhysRevX.10.041044}.

\bibitem[Goldt et~al.(2021)Goldt, Loureiro, Reeves, Krzakala, M{\'e}zard, and
  Zdeborov{\'a}]{goldt2021gaussian}
Sebastian Goldt, Bruno Loureiro, Galen Reeves, Florent Krzakala, Marc
  M{\'e}zard, and Lenka Zdeborov{\'a}.
\newblock The gaussian equivalence of generative models for learning with
  shallow neural networks.
\newblock \emph{Proceedings of Machine Learning Research}, 145:\penalty0 1--46,
  2021.

\bibitem[Gurbuzbalaban et~al.(2021)Gurbuzbalaban, Simsekli, and
  Zhu]{gurbuzbalaban2020heavy}
Mert Gurbuzbalaban, Umut Simsekli, and Lingjiong Zhu.
\newblock The heavy-tail phenomenon in sgd.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3964--3975. PMLR, 2021.

\bibitem[Heskes \& Kappen(1991)Heskes and Kappen]{heskes}
Tom~M. Heskes and Bert Kappen.
\newblock Learning processes in neural networks.
\newblock \emph{Phys. Rev. A}, 44:\penalty0 2718--2726, Aug 1991.
\newblock \doi{10.1103/PhysRevA.44.2718}.
\newblock URL \url{https://link.aps.org/doi/10.1103/PhysRevA.44.2718}.

\bibitem[Hestness et~al.(2017)Hestness, Narang, Ardalani, Diamos, Jun,
  Kianinejad, Patwary, Yang, and Zhou]{hestness2017deep}
Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun,
  Hassan Kianinejad, Md. Mostofa~Ali Patwary, Yang Yang, and Yanqi Zhou.
\newblock Deep learning scaling is predictable, empirically, 2017.

\bibitem[Jacot et~al.(2020)Jacot, Simsek, Spadaro, Hongler, and
  Gabriel]{jacot2020kernel}
Arthur Jacot, Berfin Simsek, Francesco Spadaro, Clément Hongler, and Franck
  Gabriel.
\newblock Kernel alignment risk estimator: Risk prediction from training data.
\newblock In \emph{NeurIPS}, 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/hash/b367e525a7e574817c19ad24b7b35607-Abstract.html}.

\bibitem[Jain et~al.(2018)Jain, Kakade, Kidambi, Netrapalli, and
  Sidford]{jain2018accelerating}
Prateek Jain, Sham~M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron
  Sidford.
\newblock Accelerating stochastic gradient descent for least squares
  regression.
\newblock In \emph{Conference On Learning Theory}, pp.\  545--604. PMLR, 2018.

\bibitem[LeCun et~al.(1991)LeCun, Kanter, and
  Solla]{lecun_solla_eigenspectrum_cov}
Yann LeCun, Ido Kanter, and Sara~A. Solla.
\newblock Eigenvalues of covariance matrices: Application to neural-network
  learning.
\newblock \emph{Phys. Rev. Lett.}, 66:\penalty0 2396--2399, May 1991.
\newblock \doi{10.1103/PhysRevLett.66.2396}.
\newblock URL \url{https://link.aps.org/doi/10.1103/PhysRevLett.66.2396}.

\bibitem[Lee et~al.(2020)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein,
  and Pennington]{Lee_2020}
Jaehoon Lee, Lechao Xiao, Samuel~S Schoenholz, Yasaman Bahri, Roman Novak,
  Jascha Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2020\penalty0 (12):\penalty0 124002, Dec 2020.
\newblock ISSN 1742-5468.
\newblock \doi{10.1088/1742-5468/abc62b}.
\newblock URL \url{http://dx.doi.org/10.1088/1742-5468/abc62b}.

\bibitem[Loureiro et~al.(2021)Loureiro, Gerbelot, Cui, Goldt, Krzakala,
  Mézard, and Zdeborová]{loureiro2021capturing}
Bruno Loureiro, Cédric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala,
  Marc Mézard, and Lenka Zdeborová.
\newblock Capturing the learning curves of generic features maps for realistic
  data sets with a teacher-student model, 2021.

\bibitem[Ma et~al.(2018)Ma, Bassily, and Belkin]{ma_batch_size}
Siyuan Ma, Raef Bassily, and Mikhail Belkin.
\newblock The power of interpolation: Understanding the effectiveness of sgd in
  modern over-parametrized learning.
\newblock In \emph{ICML}, pp.\  3331--3340, 2018.
\newblock URL \url{http://proceedings.mlr.press/v80/ma18a.html}.

\bibitem[Mace \& Coolen(1998)Mace and Coolen]{Mace1998StatisticalMA}
C.~Mace and A.~Coolen.
\newblock Statistical mechanical analysis of the dynamics of learning in
  perceptrons.
\newblock \emph{Statistics and Computing}, 8:\penalty0 55--88, 1998.

\bibitem[Mei \& Montanari(2020)Mei and Montanari]{mei2020generalization}
Song Mei and Andrea Montanari.
\newblock The generalization error of random features regression: Precise
  asymptotics and double descent curve, 2020.

\bibitem[Mignacco et~al.(2020)Mignacco, Krzakala, Urbani, and
  Zdeborová]{mignacco2020dynamical}
Francesca Mignacco, Florent Krzakala, Pierfrancesco Urbani, and Lenka
  Zdeborová.
\newblock Dynamical mean-field theory for stochastic gradient descent in
  gaussian mixture classification, 2020.

\bibitem[Mignacco et~al.(2021)Mignacco, Urbani, and
  Zdeborová]{mignacco2021stochasticity}
Francesca Mignacco, Pierfrancesco Urbani, and Lenka Zdeborová.
\newblock Stochasticity helps to navigate rough landscapes: comparing
  gradient-descent-based algorithms in the phase retrieval problem, 2021.

\bibitem[Nakkiran et~al.(2021)Nakkiran, Neyshabur, and Sedghi]{nakkiran2021the}
Preetum Nakkiran, Behnam Neyshabur, and Hanie Sedghi.
\newblock The deep bootstrap framework: Good online learners are good offline
  generalizers.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=guetrIHLFGI}.

\bibitem[Novak et~al.(2020)Novak, Xiao, Hron, Lee, Alemi, Sohl-Dickstein, and
  Schoenholz]{neuraltangents2020}
Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander~A. Alemi, Jascha
  Sohl-Dickstein, and Samuel~S. Schoenholz.
\newblock Neural tangents: Fast and easy infinite neural networks in python.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://github.com/google/neural-tangents}.

\bibitem[Pillaud-Vivien et~al.(2018)Pillaud-Vivien, Rudi, and
  Bach]{pillaudvivien2018statistical}
Loucas Pillaud-Vivien, Alessandro Rudi, and Francis Bach.
\newblock Statistical optimality of stochastic gradient descent on hard
  learning problems through multiple passes, 2018.

\bibitem[Polyak \& Juditsky(1992)Polyak and Juditsky]{Polyak1992AccelerationOS}
B.~Polyak and A.~Juditsky.
\newblock Acceleration of stochastic approximation by averaging.
\newblock \emph{Siam Journal on Control and Optimization}, 30:\penalty0
  838--855, 1992.

\bibitem[Pope et~al.(2021)Pope, Zhu, Abdelkader, Goldblum, and
  Goldstein]{pope2021the}
Phil Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom Goldstein.
\newblock The intrinsic dimension of images and its impact on learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=XJk19XzGq2J}.

\bibitem[Rahimi \& Recht(2008)Rahimi and Recht]{Rahimi_Recht}
Ali Rahimi and Benjamin Recht.
\newblock Random features for large-scale kernel machines.
\newblock In J.~Platt, D.~Koller, Y.~Singer, and S.~Roweis (eds.),
  \emph{Advances in Neural Information Processing Systems}, volume~20. Curran
  Associates, Inc., 2008.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf}.

\bibitem[Rasmussen \& Williams(2005)Rasmussen and Williams]{RasmussenWilliams}
Carl~Edward Rasmussen and Christopher K.~I. Williams.
\newblock \emph{Gaussian Processes for Machine Learning (Adaptive Computation
  and Machine Learning)}.
\newblock The MIT Press, 2005.

\bibitem[Robbins \& Monro(1951)Robbins and Monro]{robbins_monro}
Herbert Robbins and Sutton Monro.
\newblock {A Stochastic Approximation Method}.
\newblock \emph{The Annals of Mathematical Statistics}, 22\penalty0
  (3):\penalty0 400 -- 407, 1951.
\newblock \doi{10.1214/aoms/1177729586}.
\newblock URL \url{https://doi.org/10.1214/aoms/1177729586}.

\bibitem[Ruppert(1988)]{Ruppert}
David Ruppert.
\newblock Efficient estimations from a slowly convergent robbins-monro process.
\newblock 02 1988.

\bibitem[Saad \& Solla(1999)Saad and Solla]{saad_solla}
David Saad and Sara Solla.
\newblock Dynamics of on-line gradient descent learning for multilayer neural
  networks.
\newblock 04 1999.

\bibitem[Shallue et~al.(2018)Shallue, Lee, Antognini, Sohl-Dickstein, Frostig,
  and Dahl]{shallue2018measuring}
Christopher~J Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein,
  Roy Frostig, and George~E Dahl.
\newblock Measuring the effects of data parallelism on neural network training.
\newblock \emph{arXiv preprint arXiv:1811.03600}, 2018.

\bibitem[Shapiro(1989)]{shapiro_stoch}
Alexander Shapiro.
\newblock {Asymptotic Properties of Statistical Estimators in Stochastic
  Programming}.
\newblock \emph{The Annals of Statistics}, 17\penalty0 (2):\penalty0 841 --
  858, 1989.
\newblock \doi{10.1214/aos/1176347146}.
\newblock URL \url{https://doi.org/10.1214/aos/1176347146}.

\bibitem[Spigler et~al.(2020)Spigler, Geiger, and Wyart]{Spigler_2020}
Stefano Spigler, Mario Geiger, and Matthieu Wyart.
\newblock Asymptotic learning curves of kernel methods: empirical data versus
  teacher–student paradigm.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2020\penalty0 (12):\penalty0 124001, Dec 2020.
\newblock ISSN 1742-5468.
\newblock \doi{10.1088/1742-5468/abc61d}.
\newblock URL \url{http://dx.doi.org/10.1088/1742-5468/abc61d}.

\bibitem[Tsigler \& Bartlett(2020)Tsigler and Bartlett]{tsigler2020benign}
Alexander Tsigler and Peter~L Bartlett.
\newblock Benign overfitting in ridge regression.
\newblock \emph{arXiv preprint arXiv:2009.14286}, 2020.

\bibitem[Varre et~al.(2021)Varre, Pillaud-Vivien, and
  Flammarion]{varre2021iterate}
Aditya Varre, Loucas Pillaud-Vivien, and Nicolas Flammarion.
\newblock Last iterate convergence of sgd for least-squares in the
  interpolation regime, 2021.

\bibitem[Velikanov \& Yarotsky(2021)Velikanov and
  Yarotsky]{velikanov2021universal}
Maksim Velikanov and Dmitry Yarotsky.
\newblock Universal scaling laws in the gradient descent training of neural
  networks, 2021.

\bibitem[Werfel et~al.(2004)Werfel, Xie, and Seung]{Werfel}
Justin Werfel, Xiaohui Xie, and H.~Seung.
\newblock Learning curves for stochastic gradient descent in linear feedforward
  networks.
\newblock In S.~Thrun, L.~Saul, and B.~Sch\"{o}lkopf (eds.), \emph{Advances in
  Neural Information Processing Systems}, volume~16. MIT Press, 2004.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2003/file/f8b932c70d0b2e6bf071729a4fa68dfc-Paper.pdf}.

\bibitem[Ying \& Pontil(2008)Ying and Pontil]{ying_nonparametric}
Yiming Ying and Massimiliano Pontil.
\newblock Online gradient descent learning algorithms.
\newblock \emph{Found. Comput. Math.}, 8\penalty0 (5):\penalty0 561–596,
  October 2008.
\newblock ISSN 1615-3375.

\bibitem[Yu et~al.(2020)Yu, Balasubramanian, Volgushev, and
  Erdogdu]{yu2020analysis}
Lu~Yu, Krishnakumar Balasubramanian, Stanislav Volgushev, and Murat~A. Erdogdu.
\newblock An analysis of constant step size sgd in the non-convex regime:
  Asymptotic normality and bias, 2020.

\bibitem[Zhang et~al.(2019)Zhang, Li, Nado, Martens, Sachdeva, Dahl, Shallue,
  and Grosse]{zhang2019algorithmic}
Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George
  Dahl, Chris Shallue, and Roger~B Grosse.
\newblock Which algorithmic choices matter at which batch sizes? insights from
  a noisy quadratic model.
\newblock \emph{Advances in neural information processing systems},
  32:\penalty0 8196--8207, 2019.

\bibitem[Zou et~al.(2021)Zou, Wu, Braverman, Gu, and Kakade]{zou2021benign}
Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, and Sham~M Kakade.
\newblock Benign overfitting of constant-stepsize sgd for linear regression.
\newblock \emph{arXiv preprint arXiv:2103.12692}, 2021.

\end{thebibliography}
