\begin{table}[t]
\centering
\caption{CIFAR100-LT, ResNet32, ablation in terms of the number of additional data
}
\resizebox{0.7\linewidth}{!}{
    \begin{tabular}{@{}lcccccc}
        \toprule
        \multirow{2}[2]{*}{\textbf{{Method}}} & \multicolumn{4}{c}{\textbf{IF=100}} & \multirow{2}[2]{*}{\textbf{{50}}} & \multirow{2}[2]{*}{\textbf{{10}}} \\
        \cmidrule{2-5}
        & \textbf{Many} & \textbf{Medium} & \textbf{Few} & \textbf{All} & & \\        
        \midrule
        LT                      & 68.31 & 36.88 & 4.87 & 37.96 & 43.54 & 59.50 \\
        \cmidrule{1-7}
        Uniform (250)           & 69.69 & 51.35 & 27.55 & 50.39 & 54.42 & 62.74 \\
        LT + 250                & 74.74 & 50.88 & 28.55 & 52.31 & 55.76 & 66.02 \\
        Uniform (500)           & 75.23 & 52.15 & 30.58 & 53.54 & 57.09 & 66.66 \\
        LT + 500                & 75.74 & 52.27 & 30.77 & 53.82 & 57.63 & 67.62 \\
        Uniform (1k)            & 75.94 & 54.91 & 33.16 & 55.53 & 59.38 & 68.59 \\
        \bottomrule 
    \end{tabular}
    }
    \label{tab:cifar100_lt_abl}
\end{table}






\paragraph{Detail Settings of Figure 1.}
We conduct toy experiments to validate our argument.
We sample training data from the normal distributions, $\calN(\bm{\mu}_{ca}, \bm{\sigma})$, where $\bm{\mu}_{00}, \bm{\mu}_{01}, \bm{\mu}_{10}, \bm{\mu}_{11} \in \Real^{2}$ are the means vectors, $c$ is the class, $a$ is the sensitive attribute, and $\bm{\sigma} \in \Real^{2\times 2}$ is the covariance matrix.
We train the 2-layer neural network on these samples and plot the learned classifier.
We visualize the learned classifier in \Fref{fig:fair_toy}.
When both class imbalance and sensitive attributed are imbalanced, the learned classifier is far from the fair classifier, which is vertical.
The next unfair classifier happens when the class imbalance exists.
In conclusion, mitigating the class imbalance is essential as the long-tail distribution.