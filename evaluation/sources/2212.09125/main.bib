@article{lite,
  title={Ultra-fine Entity Typing with Indirect Supervision from Natural Language Inference},
  author={Li, Bangzheng and Yin, Wenpeng and Chen, Muhao},
  journal={arXiv preprint arXiv:2202.06167},
  year={2022}
}

@inproceedings{fewrel,
  title={FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation},
  author={Xu Han and Hao Zhu and Pengfei Yu and Ziyun Wang and Y. Yao and Zhiyuan Liu and Maosong Sun},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2018}
}

@article{conll03,
  title={Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition},
  author={Erik Tjong Kim Sang and Fien De Meulder},
  journal={ArXiv},
  year={2003},
  volume={cs.CL/0306050}
}

@inproceedings{wu2019zero,
 title={Zero-shot Entity Linking with Dense Entity Retrieval},
 author={Ledell, Wu and Fabio, Petroni and Martin, Josifoski and Sebastian, Riedel and Luke, Zettlemoyer},
 booktitle={EMNLP},
 year={2020}
}

@inproceedings{upadhyay2021explainable,
  title={Explainable Job-Posting Recommendations Using Knowledge Graphs and Named Entity Recognition},
  author={Upadhyay, Chirayu and Abu-Rasheed, Hasan and Weber, Christian and Fathi, Madjid},
  booktitle={2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
  pages={3291--3296},
  year={2021},
  organization={IEEE}
}

@article{huang2020ner,
  title={NER-RAKE: An improved rapid automatic keyword extraction method for scientific literatures based on named entity recognition},
  author={Huang, Han and Wang, Xiaoguang and Wang, Hongyu},
  journal={Proceedings of the Association for Information Science and Technology},
  volume={57},
  number={1},
  pages={e374},
  year={2020},
  publisher={Wiley Online Library}
}

@article{fget,
  title={Fine-Grained Entity Recognition},
  author={Xiao Ling and Daniel S. Weld},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2012}
}

@article{mueller2022label,
  title={Label Semantic Aware Pre-training for Few-shot Text Classification},
  author={Mueller, Aaron and Krone, Jason and Romeo, Salvatore and Mansour, Saab and Mansimov, Elman and Zhang, Yi and Roth, Dan},
  journal={arXiv preprint arXiv:2204.07128},
  year={2022}
}

@inproceedings{crfrnn,
  title={Conditional random fields as recurrent neural networks},
  author={Zheng, Shuai and Jayasumana, Sadeep and Romera-Paredes, Bernardino and Vineet, Vibhav and Su, Zhizhong and Du, Dalong and Huang, Chang and Torr, Philip HS},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1529--1537},
  year={2015}
}

@article{ontonotes,
  title={Context-dependent fine-grained entity type tagging},
  author={Gillick, Dan and Lazic, Nevena and Ganchev, Kuzman and Kirchner, Jesse and Huynh, David},
  journal={arXiv preprint arXiv:1412.1820},
  year={2014}
}

@inproceedings{onoe-durrett-2019-learning,
    title = "Learning to Denoise Distantly-Labeled Data for Entity Typing",
    author = "Onoe, Yasumasa  and
      Durrett, Greg",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1250",
    doi = "10.18653/v1/N19-1250",
    pages = "2407--2417",
    abstract = "Distantly-labeled data can be used to scale up training of statistical models, but it is typically noisy and that noise can vary with the distant labeling technique. In this work, we propose a two-stage procedure for handling this type of data: denoise it with a learned model, then train our final model on clean and denoised distant data with standard supervised training. Our denoising approach consists of two parts. First, a filtering function discards examples from the distantly labeled data that are wholly unusable. Second, a relabeling function repairs noisy labels for the retained examples. Each of these components is a model trained on synthetically-noised examples generated from a small manually-labeled set. We investigate this approach on the ultra-fine entity typing task of Choi et al. (2018). Our baseline model is an extension of their model with pre-trained ELMo representations, which already achieves state-of-the-art performance. Adding distant data that has been denoised with our learned models gives further performance gains over this base model, outperforming models trained on raw distant data or heuristically-denoised distant data.",
}

@article{Du2022LearningTS,
  title={Learning to Select from Multiple Options},
  author={Jiangshu Du and Wenpeng Yin and Congying Xia and Philip Yu},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.00301}
}

@article{halter,
  author    = {Yanzhao Zhang and
               Dingkun Long and
               Guangwei Xu and
               Pengjun Xie},
  title     = {{HLATR:} Enhance Multi-stage Text Retrieval with Hybrid List Aware
               Transformer Reranking},
  journal   = {CoRR},
  volume    = {abs/2205.10569},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2205.10569},
  doi       = {10.48550/arXiv.2205.10569},
  eprinttype = {arXiv},
  eprint    = {2205.10569},
  timestamp = {Mon, 30 May 2022 15:47:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2205-10569.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{dfet,
  title={Automatic Noisy Label Correction for Fine-Grained Entity Typing},
  author={Pan, Weiran and Wei, Wei and Zhu, Feida},
  journal={arXiv preprint arXiv:2205.03011},
  year={2022}
}

@inproceedings{mlmet,
    title = "Ultra-Fine Entity Typing with Weak Supervision from a Masked Language Model",
    author = "Dai, Hongliang  and
      Song, Yangqiu  and
      Wang, Haixun",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.141",
    doi = "10.18653/v1/2021.acl-long.141",
    pages = "1790--1799",
    abstract = "Recently, there is an effort to extend fine-grained entity typing by using a richer and ultra-fine set of types, and labeling noun phrases including pronouns and nominal nouns instead of just named entity mentions. A key challenge for this ultra-fine entity typing task is that human annotated data are extremely scarce, and the annotation ability of existing distant or weak supervision approaches is very limited. To remedy this problem, in this paper, we propose to obtain training data for ultra-fine entity typing by using a BERT Masked Language Model (MLM). Given a mention in a sentence, our approach constructs an input for the BERT MLM so that it predicts context dependent hypernyms of the mention, which can be used as type labels. Experimental results demonstrate that, with the help of these automatically generated labels, the performance of an ultra-fine entity typing model can be improved substantially. We also show that our approach can be applied to improve traditional fine-grained entity typing after performing simple type mapping.",
}

@inproceedings{ren2016label,
  title={Label noise reduction in entity typing by heterogeneous partial-label embedding},
  author={Ren, Xiang and He, Wenqi and Qu, Meng and Voss, Clare R and Ji, Heng and Han, Jiawei},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={1825--1834},
  year={2016}
}

@article{ding2021prompt,
  title={Prompt-learning for fine-grained entity typing},
  author={Ding, Ning and Chen, Yulin and Han, Xu and Xu, Guangwei and Xie, Pengjun and Zheng, Hai-Tao and Liu, Zhiyuan and Li, Juanzi and Kim, Hong-Gee},
  journal={arXiv preprint arXiv:2108.10604},
  year={2021}
}

@InProceedings{ufet, author = {Choi, Eunsol and Levy, Omer and Choi, Yejin and Zettlemoyer}, title = {Ultra-Fine Entity Typing}, booktitle = {Proceedings of the ACL}, year = {2018}, publisher = {Association for Computational Linguistics} }


@techreport {bbn ,
author = { Weischedel, Ralph and Ada Brunstein },
year = 2005 ,
title = { BBN Pronoun Coreference and Entity Type Corpus },
institution = { inguistic Data Consortium},
type = {} ,
number = {LDC2005T33} ,
}


@inproceedings{figer,
  title={Fine-grained entity recognition},
  author={Ling, Xiao and Weld, Daniel S},
  booktitle={Twenty-Sixth AAAI Conference on Artificial Intelligence},
  year={2012}
}

@inproceedings{jin-etal-2019-fine,
    title = "Fine-Grained Entity Typing via Hierarchical Multi Graph Convolutional Networks",
    author = "Jin, Hailong  and
      Hou, Lei  and
      Li, Juanzi  and
      Dong, Tiansi",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1502",
    doi = "10.18653/v1/D19-1502",
    pages = "4969--4978",
    abstract = "This paper addresses the problem of inferring the fine-grained type of an entity from a knowledge base. We convert this problem into the task of graph-based semi-supervised classification, and propose Hierarchical Multi Graph Convolutional Network (HMGCN), a novel Deep Learning architecture to tackle this problem. We construct three kinds of connectivity matrices to capture different kinds of semantic correlations between entities. A recursive regularization is proposed to model the subClassOf relations between types in given type hierarchy. Extensive experiments with two large-scale public datasets show that our proposed method significantly outperforms four state-of-the-art methods.",
}

@inproceedings{xu-barbosa-2018-neural,
    title = "Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss",
    author = "Xu, Peng  and
      Barbosa, Denilson",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1002",
    doi = "10.18653/v1/N18-1002",
    pages = "16--25",
    abstract = "The task of Fine-grained Entity Type Classification (FETC) consists of assigning types from a hierarchy to entity mentions in text. Existing methods rely on distant supervision and are thus susceptible to noisy labels that can be out-of-context or overly-specific for the training sentence. Previous methods that attempt to address these issues do so with heuristics or with the help of hand-crafted features. Instead, we propose an end-to-end solution with a neural network model that uses a variant of cross-entropy loss function to handle out-of-context labels, and hierarchical loss normalization to cope with overly-specific ones. Also, previous work solve FETC a multi-label classification followed by ad-hoc post-processing. In contrast, our solution is more elegant: we use public word embeddings to train a single-label that jointly learns representations for entity mentions and their context. We show experimentally that our approach is robust against noise and consistently outperforms the state-of-the-art on established benchmarks for the task.",
}

@article{ir,
  title={Introduction to Information Retrieval},
  author={Ray R. Larson},
  journal={J. Assoc. Inf. Sci. Technol.},
  year={2010},
  volume={61},
  pages={852-853}
}

@article{bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  journal={ArXiv},
  year={2019},
  volume={abs/1810.04805}
}

@article{matching_info1,
  title={Building Legal Case Retrieval Systems with Lexical Matching and Summarization using A Pre-Trained Phrase Scoring Model},
  author={Vu Mai Tran and Minh Le Nguyen and Ken Satoh},
  journal={Proceedings of the Seventeenth International Conference on Artificial Intelligence and Law},
  year={2019}
}



@article{matching_info2,
  title={ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT},
  author={O. Khattab and Matei A. Zaharia},
  journal={Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
  year={2020}
}

@article{bm25,
  title={The Probabilistic Relevance Framework: BM25 and Beyond},
  author={Stephen E. Robertson and Hugo Zaragoza},
  journal={Found. Trends Inf. Retr.},
  year={2009},
  volume={3},
  pages={333-389}
}

@article{Robertson2009ThePR,
  title={The Probabilistic Relevance Framework: BM25 and Beyond},
  author={Stephen E. Robertson and Hugo Zaragoza},
  journal={Found. Trends Inf. Retr.},
  year={2009},
  volume={3},
  pages={333-389}
}

@inproceedings{cfet,
    title = "A {C}hinese Corpus for Fine-grained Entity Typing",
    author = "Lee, Chin  and
      Dai, Hongliang  and
      Song, Yangqiu  and
      Li, Xin",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.548",
    pages = "4451--4457",
    abstract = "Fine-grained entity typing is a challenging task with wide applications. However, most existing datasets for this task are in English. In this paper, we introduce a corpus for Chinese fine-grained entity typing that contains 4,800 mentions manually labeled through crowdsourcing. Each mention is annotated with free-form entity types. To make our dataset useful in more possible scenarios, we also categorize all the fine-grained types into 10 general types. Finally, we conduct experiments with some neural models whose structures are typical in fine-grained entity typing and show how well they perform on our dataset. We also show the possibility of improving Chinese fine-grained entity typing through cross-lingual transfer learning.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@inproceedings{xiong-etal-2019-imposing,
    title = "Imposing Label-Relational Inductive Bias for Extremely Fine-Grained Entity Typing",
    author = "Xiong, Wenhan  and
      Wu, Jiawei  and
      Lei, Deren  and
      Yu, Mo  and
      Chang, Shiyu  and
      Guo, Xiaoxiao  and
      Wang, William Yang",
    booktitle = "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1084",
}

@inproceedings{npcrf,
    title = "Modeling Label Correlations for Ultra-Fine Entity Typing with Neural Pairwise Conditional Random Field",
    author = "Jiang, Chengyue  and
      Jiang, Yong and
      Wu, Weiqi and
      Xie, Pengjun and
      Tu, Kewei",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    publisher = "Association for Computational Linguistics",
}

@article{便秘5,
  title={The Probabilistic Relevance Framework: BM25 and Beyond},
  author={Stephen E. Robertson and Hugo Zaragoza},
  journal={Found. Trends Inf. Retr.},
  year={2009},
  volume={3},
  pages={333-389}
}

@inproceedings{lopez-strube-2020-fully,
    title = "A Fully Hyperbolic Neural Model for Hierarchical Multi-Class Classification",
    author = "L{\'o}pez, Federico  and
      Strube, Michael",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.42",
    doi = "10.18653/v1/2020.findings-emnlp.42",
    pages = "460--475",
    abstract = "Label inventories for fine-grained entity typing have grown in size and complexity. Nonetheless, they exhibit a hierarchical structure. Hyperbolic spaces offer a mathematically appealing approach for learning hierarchical representations of symbolic data. However, it is not clear how to integrate hyperbolic components into downstream tasks. This is the first work that proposes a fully hyperbolic model for multi-class multi-label classification, which performs all operations in hyperbolic space. We evaluate the proposed model on two challenging datasets and compare to different baselines that operate under Euclidean assumptions. Our hyperbolic model infers the latent hierarchy from the class distribution, captures implicit hyponymic relations in the inventory, and shows performance on par with state-of-the-art methods on fine-grained classification with remarkable reduction of the parameter size. A thorough analysis sheds light on the impact of each component in the final prediction and showcases its ease of integration with Euclidean layers.",
}

@inproceedings{box4types,
    title = "Modeling Fine-Grained Entity Types with Box Embeddings",
    author = "Onoe, Yasumasa  and
      Boratko, Michael  and
      McCallum, Andrew  and
      Durrett, Greg",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.160",
    doi = "10.18653/v1/2021.acl-long.160",
    pages = "2051--2064",
    abstract = "Neural entity typing models typically represent fine-grained entity types as vectors in a high-dimensional space, but such spaces are not well-suited to modeling these types{'} complex interdependencies. We study the ability of box embeddings, which embed concepts as d-dimensional hyperrectangles, to capture hierarchies of types even when these relationships are not defined explicitly in the ontology. Our model represents both types and entity mentions as boxes. Each mention and its context are fed into a BERT-based model to embed that mention in our box space; essentially, this model leverages typological clues present in the surface text to hypothesize a type representation for the mention. Box containment can then be used to derive both the posterior probability of a mention exhibiting a given type and the conditional probability relations between types themselves. We compare our approach with a vector-based typing model and observe state-of-the-art performance on several entity typing benchmarks. In addition to competitive typing performance, our box-based model shows better performance in prediction consistency (predicting a supertype and a subtype together) and confidence (i.e., calibration), demonstrating that the box-based model captures the latent type hierarchies better than the vector-based model does.",
}

@inproceedings{liu-etal-2021-fine,
    title = "Fine-grained Entity Typing via Label Reasoning",
    author = "Liu, Qing  and
      Lin, Hongyu  and
      Xiao, Xinyan  and
      Han, Xianpei  and
      Sun, Le  and
      Wu, Hua",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.378",
    doi = "10.18653/v1/2021.emnlp-main.378",
    pages = "4611--4622",
    abstract = "Conventional entity typing approaches are based on independent classification paradigms, which make them difficult to recognize inter-dependent, long-tailed and fine-grained entity types. In this paper, we argue that the implicitly entailed extrinsic and intrinsic dependencies between labels can provide critical knowledge to tackle the above challenges. To this end, we propose Label Reasoning Network(LRN), which sequentially reasons fine-grained entity labels by discovering and exploiting label dependencies knowledge entailed in the data. Specifically, LRN utilizes an auto-regressive network to conduct deductive reasoning and a bipartite attribute graph to conduct inductive reasoning between labels, which can effectively model, learn and reason complex label dependencies in a sequence-to-set, end-to-end manner. Experiments show that LRN achieves the state-of-the-art performance on standard ultra fine-grained entity typing benchmarks, and can also resolve the long tail label problem effectively.",
}

@inproceedings{conll03,
    title = "Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition",
    author = "Tjong Kim Sang, Erik F.  and
      De Meulder, Fien",
    booktitle = "Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003",
    year = "2003",
    url = "https://aclanthology.org/W03-0419",
    pages = "142--147",
}

@inproceedings{06ontonotes,
    title = "{O}nto{N}otes: The 90{\%} Solution",
    author = "Hovy, Eduard  and
      Marcus, Mitchell  and
      Palmer, Martha  and
      Ramshaw, Lance  and
      Weischedel, Ralph",
    booktitle = "Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Short Papers",
    month = jun,
    year = "2006",
    address = "New York City, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N06-2015",
    pages = "57--60",
}

@inproceedings{ding-etal-2021-nerd,
    title = "Few-{NERD}: A Few-shot Named Entity Recognition Dataset",
    author = "Ding, Ning  and
      Xu, Guangwei  and
      Chen, Yulin  and
      Wang, Xiaobin  and
      Han, Xu  and
      Xie, Pengjun  and
      Zheng, Haitao  and
      Liu, Zhiyuan",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.248",
    doi = "10.18653/v1/2021.acl-long.248",
    pages = "3198--3213",
    abstract = "Recently, considerable literature has grown up around the theme of few-shot named entity recognition (NER), but little published benchmark data specifically focused on the practical and challenging task. Current approaches collect existing supervised NER datasets and re-organize them to the few-shot setting for empirical study. These strategies conventionally aim to recognize coarse-grained entity types with few examples, while in practice, most unseen entity types are fine-grained. In this paper, we present Few-NERD, a large-scale human-annotated few-shot NER dataset with a hierarchy of 8 coarse-grained and 66 fine-grained entity types. Few-NERD consists of 188,238 sentences from Wikipedia, 4,601,160 words are included and each is annotated as context or a part of the two-level entity type. To the best of our knowledge, this is the first few-shot NER dataset and the largest human-crafted NER dataset. We construct benchmark tasks with different emphases to comprehensively assess the generalization capability of models. Extensive empirical results and analysis show that Few-NERD is challenging and the problem requires further research. The Few-NERD dataset and the baselines will be publicly available to facilitate the research on this problem.",
}

@INPROCEEDINGS{2phaseNER,
  author={Li Yang and Yanhong Zhou},
  booktitle={2010 IEEE Fifth International Conference on Bio-Inspired Computing: Theories and Applications (BIC-TA)}, 
  title={Two-phase biomedical named entity recognition based on semi-CRFs}, 
  year={2010},
  volume={},
  number={},
  pages={1061-1065},
  doi={10.1109/BICTA.2010.5645108}}
  
  
 @article{coucke2018snips,
  title={Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces},
  author={Coucke, Alice and Saade, Alaa and Ball, Adrien and Bluche, Th{\'e}odore and Caulier, Alexandre and Leroy, David and Doumouro, Cl{\'e}ment and Gisselbrecht, Thibault and Caltagirone, Francesco and Lavril, Thibaut and others},
  journal={arXiv preprint arXiv:1805.10190},
  year={2018}
}

@article{joshi2017triviaqa,
  title={Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel S and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1705.03551},
  year={2017}
}

@inproceedings{yavuz-etal-2016-improving,
    title = "Improving Semantic Parsing via Answer Type Inference",
    author = "Yavuz, Semih  and
      Gur, Izzeddin  and
      Su, Yu  and
      Srivatsa, Mudhakar  and
      Yan, Xifeng",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1015",
    doi = "10.18653/v1/D16-1015",
    pages = "149--159",
}

@inproceedings{wu2019modeling,
  title={Modeling Noisy Hierarchical Types in Fine-Grained Entity Typing: A Content-Based Weighting Approach.},
  author={Wu, Junshuang and Zhang, Richong and Mao, Yongyi and Guo, Hongyu and Huai, Jinpeng},
  booktitle={IJCAI},
  pages={5264--5270},
  year={2019}
}

@inproceedings{glove,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{chen-etal-2022-learning-sibling,
    title = "Learning from Sibling Mentions with Scalable Graph Inference in Fine-Grained Entity Typing",
    author = "Chen, Yi  and
      Cheng, Jiayang  and
      Jiang, Haiyun  and
      Liu, Lemao  and
      Zhang, Haisong  and
      Shi, Shuming  and
      Xu, Ruifeng",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.147",
    pages = "2076--2087",
    abstract = "In this paper, we firstly empirically find that existing models struggle to handle hard mentions due to their insufficient contexts, which consequently limits their overall typing performance. To this end, we propose to exploit sibling mentions for enhancing the mention representations.Specifically, we present two different metrics for sibling selection and employ an attentive graph neural network to aggregate information from sibling mentions. The proposed graph model is scalable in that unseen test mentions are allowed to be added as new nodes for inference.Exhaustive experiments demonstrate the effectiveness of our sibling learning strategy, where our model outperforms ten strong baselines. Moreover, our experiments indeed prove the superiority of sibling mentions in helping clarify the types for hard mentions.",
}

@article{wainwright2008graphical,
  title={Graphical models, exponential families, and variational inference},
  author={Wainwright, Martin J and Jordan, Michael I and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={1},
  number={1--2},
  pages={1--305},
  year={2008},
  publisher={Now Publishers, Inc.}
}

@article{le2021regularized,
  title={Regularized Frank-Wolfe for Dense CRFs: Generalizing Mean Field and Beyond},
  author={L{\^e}-Huu, {\DJ} Khu{\^e} and Alahari, Karteek},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{loshchilov2018fixing,
  title={Fixing weight decay regularization in adam},
  author={Loshchilov, Ilya and Hutter, Frank},
  year={2018}
}

@inproceedings{elmo,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew E.  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1202",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
    abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
}

@inproceedings{mnli,
    title = "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
    author = "Williams, Adina  and
      Nangia, Nikita  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1101",
    doi = "10.18653/v1/N18-1101",
    pages = "1112--1122",
    abstract = "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.",
}

@inproceedings{chen-etal-2020-hierarchical,
    title = "Hierarchical Entity Typing via Multi-level Learning to Rank",
    author = "Chen, Tongfei  and
      Chen, Yunmo  and
      Van Durme, Benjamin",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.749",
    doi = "10.18653/v1/2020.acl-main.749",
    pages = "8465--8475",
    abstract = "We propose a novel method for hierarchical entity classification that embraces ontological structure at both training and during prediction. At training, our novel multi-level learning-to-rank loss compares positive types against negative siblings according to the type tree. During prediction, we define a coarse-to-fine decoder that restricts viable candidates at each level of the ontology based on already predicted parent type(s). Our approach significantly outperform prior work on strict accuracy, demonstrating the effectiveness of our method.",
}

@inproceedings{hu-etal-2020-investigation,
    title = "An Investigation of Potential Function Designs for Neural {CRF}",
    author = "Hu, Zechuan  and
      Jiang, Yong  and
      Bach, Nguyen  and
      Wang, Tao  and
      Huang, Zhongqiang  and
      Huang, Fei  and
      Tu, Kewei",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.236",
    doi = "10.18653/v1/2020.findings-emnlp.236",
    pages = "2600--2609",
    abstract = "The neural linear-chain CRF model is one of the most widely-used approach to sequence labeling. In this paper, we investigate a series of increasingly expressive potential functions for neural CRF models, which not only integrate the emission and transition functions, but also explicitly take the representations of the contextual words as input. Our extensive experiments show that the decomposed quadrilinear potential function based on the vector representations of two neighboring labels and two neighboring words consistently achieves the best performance.",
}