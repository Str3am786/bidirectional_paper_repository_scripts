\section{Related Work}
While writing this paper, we noticed that a paper \cite{Du2022LearningTS} that has similar ideas to our work was submitted to the arXiv. They propose a two-stage paradigm for selecting from multiple questions. They also propose a network similar to our {\bf \textsc{\name-B}} to select from multiple options in parallel. We summarize the differences between their work and ours as follows: (1) Different in paradigm. We have an expand stage to further improve the quality of recalled candidates (2) Different in models. {\bf \textsc{\name-S}} and {\bf \textsc{\name-B}} are both different from theirs in both input format and scoring. We additionally propose to discard the C2C attention and study the effect of removing different parts of attention. (3) We focus more on entity typing and conduct extensive experiments covering two languages and three settings (ultra-fine-grained, fine-grained, and coarse-grained). We analyze the effect of using different PLM backbones for a fairer and more comprehensive comparison.
The paradigm of our work is also inspired by works in entity linking and information retrieval. \citet{wu2019zero} uses a retrieval and rerank paradigm for entity linking, they first generate entity candidates using a bi-encoder and rerank them using a vanilla cross-encoder. Our paradigm with an additional expand stage and our proposed {\bf \textsc{\name}} models are also potentially useful for entity linking. We leave it for future work. \citet{halter} represents the query document and candidate documents as vectors and proposed to use a transformer to rerank all candidate documents in parallel for passage retrieval. Compared to them, we tackle entity typing and preserve all information of mention and context rather than represent them as a single vector, the paradigm, model architecture, and training objective are also different.



