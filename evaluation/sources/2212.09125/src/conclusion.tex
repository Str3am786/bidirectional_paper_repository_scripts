\section{Conclusion}
In conclusion, we propose a recall-expand-filter paradigm for ultra-fine entity typing. We train a recall model to generate candidates and use MLM and exact match to improve the quality of recalled candidates, then use filter models to obtain final type predictions. We also propose a filter model called multi-candidate cross-encoder ({\bf \textsc{\name}}) to concurrently encode and filter all candidates and study the influences of different input formats and attention mechanisms. Extensive experiments on entity typing show that our paradigm is effective, and the {\bf \textsc{\name}} models under our paradigm reach SOTA performances on both English and Chinese UFET datasets and are also very effective on fine and coarse-grained entity typing.  {\bf \textsc{\name}} models have comparable inference speed to simple ({\bf \textsc{\name}})  models and are thousands of times faster than previous SOTA cross-encoders.