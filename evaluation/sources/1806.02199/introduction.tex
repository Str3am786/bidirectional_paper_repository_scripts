\section{Introduction}


Interpretable representation learning on time series is a seminal problem for uncovering the latent structure in complex systems, such as chaotic dynamical systems or medical time series.
In areas where humans have to make decisions based on large amounts of data, interpretability is fundamental to ease the human task.
Especially when decisions have to be made in a timely manner and rely on observing some chaotic external process over time, such as in finance or medicine, the need for intuitive interpretations is even stronger.
%Clinicians, for instance, might need to visualize and manually process the trajectory of a patient in order to decide on their treatment \citep{johnson16_machine_learning_decision}.
%They would benefit heavily from a representation learning model, in which those patient trajectories become intuitively understandable and relevant health features are salient.
%Since clinicians are already used to classifying patients into a finite set of discrete categories (e.g. using diagnosis codes), learned discrete health state representations would fit their intuition of a patientâ€™s development.
%Clustering is a classical method for unsupervised discrete representation learning and is a natural solution for this problem at first sight.
However, many unsupervised methods, such as clustering, make misleading \emph{i.i.d.} assumptions about the data, neglecting their rich temporal structure and smooth behaviour over time.
This poses the need for a method of clustering, where the clusters assume a topological structure in a lower dimensional space, such that the representations of the time series retain their smoothness in that space.
In this work, we present a method with these properties.

We choose to employ deep neural networks, because they have a very successful tradition in representation learning \citep{Bengio2012}.
In recent years, they have increasingly been combined with generative modeling through the advent of generative adversarial networks (GANs) \citep{Goodfellow2014} and variational autoencoders (VAEs) \citep{Kingma2013}.
However, the representations learned by these models are often considered cryptic and do not offer the necessary interpretability \citep{Chen2016a}.
A lot of work has been done to improve them in this regard, in GANs \citep{Chen2016a} as well as VAEs \citep{Higgins2017, Esmaeili2018}.
Alas, these works have focused entirely on continuous representations, while discrete ones are still underexplored.

In order to define temporal smoothness in a discrete representation space, the space has to be equipped with a topological neighborhood relationship.
One type of representation space with such a structure is induced by the self-organizing map (SOM) \citep{Kohonen1998}.
The SOM allows to map states from an uninterpretable continuous space to a lower-dimensional space with a predefined topologically interpretable structure, such as an easily visualizable two-dimensional grid.
However, while yielding promising results in visualizing static state spaces, such as static patient states \citep{Tirunagari2015}, the classical SOM formulation does not offer a notion of time.
The time component can be incorporated using a probabilistic transition model, e.g.\ a Markov model, such that the representations of a single time point are enriched with information from the adjacent time points in the series.
It is therefore potentially fruitful to apply the approaches of probabilistic modeling alongside representation learning and discrete dimensionality reduction in an end-to-end model.

In this work, we propose a novel deep architecture that learns topologically interpretable discrete representations in a probabilistic fashion.
Moreover, we introduce a new method to overcome the non-differentiability in discrete representation learning architectures and develop a gradient-based version of the classical self-organizing map algorithm with improved performance.
We present extensive empirical evidence for the model's performance on synthetic and real world time series from benchmark data sets, a synthetic dynamical system with chaotic behavior and real world medical data.

Our main contributions are to

\begin{itemize}
	\item Devise a novel framework for interpretable discrete representation learning on time series.
	\item Show that the latent probabilistic model in the representation learning architecture improves clustering and interpretability of the representations on time series.
	\item Show superior clustering performance of the model on benchmark data and a real world medical data set, on which it also facilitates downstream tasks.
\end{itemize}