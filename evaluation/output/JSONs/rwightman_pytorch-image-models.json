{"somef_provenance": {"somef_version": "0.9.4", "somef_schema_version": "1.0.0", "date": "2023-12-21 19:26:30"}, "code_repository": [{"result": {"value": "https://github.com/huggingface/pytorch-image-models", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "owner": [{"result": {"value": "huggingface", "type": "Organization"}, "confidence": 1, "technique": "GitHub_API"}], "date_created": [{"result": {"value": "2019-02-02T05:51:12Z", "type": "Date"}, "confidence": 1, "technique": "GitHub_API"}], "date_updated": [{"result": {"value": "2023-12-21T17:33:24Z", "type": "Date"}, "confidence": 1, "technique": "GitHub_API"}], "license": [{"result": {"value": "https://api.github.com/licenses/apache-2.0", "type": "License", "name": "Apache License 2.0", "url": "https://api.github.com/licenses/apache-2.0", "spdx_id": "Apache-2.0"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"{}\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2019 Ross Wightman\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n", "type": "File_dump"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/LICENSE"}, {"result": {"value": "The code here is licensed Apache 2.0. I've taken care to make sure any third party code included or adapted has compatible (permissive) licenses such as MIT, BSD, etc. I've made an effort to avoid any GPL / LGPL conflicts. That said, it is your responsibility to ensure you comply with licenses here and conditions of any dependent licenses. Where applicable, I've linked the sources/references for various components in docstrings. If you think I've missed anything please create an issue.\n", "type": "Text_excerpt", "original_header": "Code", "parent_header": ["PyTorch Image Models", "Licenses"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"value": "So far all of the pretrained weights available here are pretrained on ImageNet with a select few that have some additional pretraining (see extra note below). ImageNet was released for non-commercial research purposes only (https://image-net.org/download). It's not clear what the implications of that are for the use of pretrained weights from that dataset. Any models I have trained with ImageNet are done for research purposes and one should assume that the original dataset license applies to the weights. It's best to seek legal advice if you intend to use the pretrained weights in a commercial product.\n", "type": "Text_excerpt", "original_header": "Pretrained Weights", "parent_header": ["PyTorch Image Models", "Licenses"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"value": "Several weights included or references here were pretrained with proprietary datasets that I do not have access to. These include the Facebook WSL, SSL, SWSL ResNe(Xt) and the Google Noisy Student EfficientNet models. The Facebook models have an explicit non-commercial license (CC-BY-NC 4.0, https://github.com/facebookresearch/semi-supervised-ImageNet1K-models, https://github.com/facebookresearch/WSL-Images). The Google models do not appear to have any restriction beyond the Apache 2.0 license (and ImageNet concerns). In either case, you should contact Facebook or Google with any questions.\n", "type": "Text_excerpt", "original_header": "Pretrained on more than ImageNet", "parent_header": ["PyTorch Image Models", "Licenses", "Pretrained Weights"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}], "description": [{"result": {"value": "PyTorch image models, scripts, pretrained weights -- ResNet, ResNeXT, EfficientNet, NFNet, Vision Transformer (ViT), MobileNet-V3/V2, RegNet, DPN, CSPNet, Swin Transformer, MaxViT, CoAtNet, ConvNeXt, and more", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "Py**T**orch **Im**age **M**odels (`timm`) is a collection of image models, layers, utilities, optimizers, schedulers, data-loaders / augmentations, and reference training / validation scripts that aim to pull together a wide variety of SOTA models with ability to reproduce ImageNet training results.\n\nThe work of many others is present here. I've tried to make sure all source material is acknowledged via links to github, arxiv papers, etc in the README, documentation, and code docstrings. Please let me know if I missed anything.\n", "type": "Text_excerpt", "original_header": "Introduction", "parent_header": ["PyTorch Image Models"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "\u2757Updates after Oct 10, 2022 are available in version >= 0.9\u2757\n* Many changes since the last 0.6.x stable releases. They were previewed in 0.8.x dev releases but not everyone transitioned.\n* `timm.models.layers` moved to `timm.layers`:\n  * `from timm.models.layers import name` will still work via deprecation mapping (but please transition to `timm.layers`).\n  * `import timm.models.layers.module` or `from timm.models.layers.module import name` needs to be changed now.\n* Builder, helper, non-model modules in `timm.models` have a `_` prefix added, ie `timm.models.helpers` -> `timm.models._helpers`, there are temporary deprecation mapping files but those will be removed.\n* All models now support `architecture.pretrained_tag` naming (ex `resnet50.rsb_a1`).\n  * The pretrained_tag is the specific weight variant (different head) for the architecture.\n  * Using only `architecture` defaults to the first weights in the default_cfgs for that model architecture.\n  * In adding pretrained tags, many model names that existed to differentiate were renamed to use the tag  (ex: `vit_base_patch16_224_in21k` -> `vit_base_patch16_224.augreg_in21k`). There are deprecation mappings for these.\n* A number of models had their checkpoints remaped to match architecture changes needed to better support `features_only=True`, there are `checkpoint_filter_fn` methods in any model module that was remapped. These can be passed to `timm.models.load_checkpoint(..., filter_fn=timm.models.swin_transformer_v2.checkpoint_filter_fn)` to remap your existing checkpoint.\n* The Hugging Face Hub (https://huggingface.co/timm) is now the primary source for `timm` weights. Model cards include link to papers, original source, license. \n* Previous 0.6.x can be cloned from [0.6.x](https://github.com/rwightman/pytorch-image-models/tree/0.6.x) branch or installed via pip with version.\n \n", "original_header": "What's New"}, "confidence": 0.8612762357632386, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "* [SigLIP](https://huggingface.co/papers/2303.15343) image tower weights supported in `vision_transformer.py`.\n  * Great potential for fine-tune and downstream feature use.\n* Experimental 'register' support in vit models as per [Vision Transformers Need Registers](https://huggingface.co/papers/2309.16588)\n* Updated RepViT with new weight release. Thanks [wangao](https://github.com/jameslahm)\n* Add patch resizing support (on pretrained weight load) to Swin models\n* 0.9.8 release pending\n \n", "original_header": "Oct 20, 2023"}, "confidence": 0.8905425369008998, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "* Swin, MaxViT, CoAtNet, and BEiT models support resizing of image/window size on creation with adaptation of pretrained weights\n* Example validation cmd to test w/ non-square resize `python validate.py /imagenet --model swin_base_patch4_window7_224.ms_in22k_ft_in1k --amp --amp-dtype bfloat16 --input-size 3 256 320 --model-kwargs window_size=8,10 img_size=256,320`\n  \n", "original_header": "Aug 11, 2023"}, "confidence": 0.8772380890240883, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "* Add GluonCV weights for HRNet w18_small and w18_small_v2. Converted by [SeeFun](https://github.com/seefun)\n* Fix `selecsls*` model naming regression\n* Patch and position embedding for ViT/EVA works for bfloat16/float16 weights on load (or activations for on-the-fly resize)\n* v0.9.5 release prep\n \n", "original_header": "Aug 3, 2023"}, "confidence": 0.9216207768341972, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "* Added timm trained `seresnextaa201d_32x8d.sw_in12k_ft_in1k_384` weights (and `.sw_in12k` pretrain) with 87.3% top-1 on ImageNet-1k, best ImageNet ResNet family model I'm aware of.\n* RepViT model and weights (https://arxiv.org/abs/2307.09283) added by [wangao](https://github.com/jameslahm)\n* I-JEPA ViT feature weights (no classifier) added by [SeeFun](https://github.com/seefun)\n* SAM-ViT (segment anything) feature weights (no classifier) added by [SeeFun](https://github.com/seefun)\n* Add support for alternative feat extraction methods and -ve indices to EfficientNet\n* Add NAdamW optimizer\n* Misc fixes\n \n", "original_header": "July 27, 2023"}, "confidence": 0.8268623464519607, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "* Hugging Face Hub downloading is now default, 1132 models on https://huggingface.co/timm, 1163 weights in `timm`\n* DINOv2 vit feature backbone weights added thanks to [Leng Yue](https://github.com/leng-yue)\n* FB MAE vit feature backbone weights added\n* OpenCLIP DataComp-XL L/14 feat backbone weights added\n* MetaFormer (poolformer-v2, caformer, convformer, updated poolformer (v1)) w/ weights added by [Fredo Guan](https://github.com/fffffgggg54)\n* Experimental `get_intermediate_layers` function on vit/deit models for grabbing hidden states (inspired by DINO impl). This is WIP and may change significantly... feedback welcome.\n* Model creation throws error if `pretrained=True` and no weights exist (instead of continuing with random initialization)\n* Fix regression with inception / nasnet TF sourced weights with 1001 classes in original classifiers\n* bitsandbytes (https://github.com/TimDettmers/bitsandbytes) optimizers added to factory, use `bnb` prefix, ie `bnbadam8bit`\n* Misc cleanup and fixes\n* Final testing before switching to a 0.9 and bringing `timm` out of pre-release state\n \n", "original_header": "May 10, 2023"}, "confidence": 0.9176733396406929, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "* 97% of `timm` models uploaded to HF Hub and almost all updated to support multi-weight pretrained configs\n* Minor cleanup and refactoring of another batch of models as multi-weight added. More fused_attn (F.sdpa) and features_only support, and torchscript fixes.\n \n", "original_header": "April 27, 2023"}, "confidence": 0.9827455219506531, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "* Add ONNX export script, validate script, helpers that I've had kicking around for along time. Tweak 'same' padding for better export w/ recent ONNX + pytorch.\n* Refactor dropout args for vit and vit-like models, separate drop_rate into `drop_rate` (classifier dropout), `proj_drop_rate` (block mlp / out projections), `pos_drop_rate` (position embedding drop), `attn_drop_rate` (attention dropout). Also add patch dropout (FLIP) to vit and eva models.\n* fused F.scaled_dot_product_attention support to more vit models, add env var (TIMM_FUSED_ATTN) to control, and config interface to enable/disable\n* Add EVA-CLIP backbones w/ image tower weights, all the way up to 4B param 'enormous' model, and 336x336 OpenAI ViT mode that was missed.\n \n", "original_header": "April 12, 2023"}, "confidence": 0.9451813583027499, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "* ALL ResNet models pushed to Hugging Face Hub with multi-weight support\n  * All past `timm` trained weights added with recipe based tags to differentiate\n  * All ResNet strikes back A1/A2/A3 (seed 0) and R50 example B/C1/C2/D weights available\n  * Add torchvision v2 recipe weights to existing torchvision originals\n  * See comparison table in https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288#model-comparison\n* New ImageNet-12k + ImageNet-1k fine-tunes available for a few anti-aliased ResNet models\n  * `resnetaa50d.sw_in12k_ft_in1k` - 81.7 @ 224, 82.6 @ 288\n  * `resnetaa101d.sw_in12k_ft_in1k` - 83.5 @ 224, 84.1 @ 288\n  * `seresnextaa101d_32x8d.sw_in12k_ft_in1k` - 86.0 @ 224, 86.5 @ 288 \n  * `seresnextaa101d_32x8d.sw_in12k_ft_in1k_288` - 86.5 @ 288, 86.7 @ 320\n \n", "original_header": "April 5, 2023"}, "confidence": 0.9332666596796002, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "* Add EVA-02 MIM pretrained and fine-tuned weights, push to HF hub and update model cards for all EVA models. First model over 90% top-1 (99% top-5)! Check out the original code & weights at https://github.com/baaivision/EVA for more details on their work blending MIM, CLIP w/ many model, dataset, and train recipe tweaks. \n* Multi-weight and HF hub for DeiT and MLP-Mixer based models\n \n", "original_header": "March 31, 2023"}, "confidence": 0.9417310619966149, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "* More weights pushed to HF hub along with multi-weight support, including: `regnet.py`, `rexnet.py`, `byobnet.py`, `resnetv2.py`, `swin_transformer.py`, `swin_transformer_v2.py`, `swin_transformer_v2_cr.py`\n* Swin Transformer models support feature extraction (NCHW feat maps for `swinv2_cr_*`, and NHWC for all others) and spatial embedding outputs.\n* FocalNet (from https://github.com/microsoft/FocalNet) models and weights added with significant refactoring, feature extraction, no fixed resolution / sizing constraint\n* RegNet weights increased with HF hub push, SWAG, SEER, and torchvision v2 weights. SEER is pretty poor wrt to performance for model size, but possibly useful.\n* More ImageNet-12k pretrained and 1k fine-tuned `timm` weights:\n  * `rexnetr_200.sw_in12k_ft_in1k` - 82.6 @ 224, 83.2 @ 288\n  * `rexnetr_300.sw_in12k_ft_in1k` - 84.0 @ 224, 84.5 @ 288\n  * `regnety_120.sw_in12k_ft_in1k` - 85.0 @ 224, 85.4 @ 288\n  * `regnety_160.lion_in12k_ft_in1k` - 85.6 @ 224, 86.0 @ 288\n  * `regnety_160.sw_in12k_ft_in1k` - 85.6 @ 224, 86.0 @ 288  (compare to SWAG PT + 1k FT this is same BUT much lower res, blows SEER FT away)\n* Model name deprecation + remapping functionality added (a milestone for bringing 0.8.x out of pre-release). Mappings being added...\n* Minor bug fixes and improvements.\n \n", "original_header": "March 22, 2023"}, "confidence": 0.9353747359707242, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "* Add ConvNeXt-XXLarge CLIP pretrained image tower weights for fine-tune & features (fine-tuning TBD) -- see [model card](https://huggingface.co/laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-soup)\n* Update `convnext_xxlarge` default LayerNorm eps to 1e-5 (for CLIP weights, improved stability)\n* 0.8.15dev0\n \n", "original_header": "Feb 26, 2023"}, "confidence": 0.9392611609820145, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "* Add 320x320 `convnext_large_mlp.clip_laion2b_ft_320` and `convnext_lage_mlp.clip_laion2b_ft_soup_320` CLIP image tower weights for features & fine-tune\n* 0.8.13dev0 pypi release for latest changes w/ move to huggingface org\n \n", "original_header": "Feb 20, 2023"}, "confidence": 0.8444004021628636, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "* New inference benchmark numbers added in [results](results/) folder.\n* Add convnext LAION CLIP trained weights and initial set of in1k fine-tunes\n  * `convnext_base.clip_laion2b_augreg_ft_in1k` - 86.2% @ 256x256\n  * `convnext_base.clip_laiona_augreg_ft_in1k_384` - 86.5% @ 384x384\n  * `convnext_large_mlp.clip_laion2b_augreg_ft_in1k` - 87.3% @ 256x256\n  * `convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384` - 87.9% @ 384x384\n* Add DaViT models. Supports `features_only=True`. Adapted from https://github.com/dingmyu/davit by [Fredo](https://github.com/fffffgggg54).\n* Use a common NormMlpClassifierHead across MaxViT, ConvNeXt, DaViT\n* Add EfficientFormer-V2 model, update EfficientFormer, and refactor LeViT (closely related architectures). Weights on HF hub.\n  * New EfficientFormer-V2 arch, significant refactor from original at (https://github.com/snap-research/EfficientFormer). Supports `features_only=True`.\n  * Minor updates to EfficientFormer.\n  * Refactor LeViT models to stages, add `features_only=True` support to new `conv` variants, weight remap required.\n* Move ImageNet meta-data (synsets, indices) from `/results` to [`timm/data/_info`](timm/data/_info/).\n* Add ImageNetInfo / DatasetInfo classes to provide labelling for various ImageNet classifier layouts in `timm`\n  * Update `inference.py` to use, try: `python inference.py /folder/to/images --model convnext_small.in12k --label-type detail --topk 5`\n* Ready for 0.8.10 pypi pre-release (final testing).\n \n", "original_header": "Feb 7, 2023"}, "confidence": 0.8762156818126061, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "* Push all MaxxViT weights to HF hub, and add new ImageNet-12k -> 1k fine-tunes for `rw` base MaxViT and CoAtNet 1/2 models \n", "original_header": "Jan 20, 2023"}, "confidence": 0.971397502103901, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "* Finally got around to adding `--model-kwargs` and `--opt-kwargs` to scripts to pass through rare args directly to model classes from cmd line\n  * `train.py /imagenet --model resnet50 --amp --model-kwargs output_stride=16 act_layer=silu`\n  * `train.py /imagenet --model vit_base_patch16_clip_224 --img-size 240 --amp --model-kwargs img_size=240 patch_size=12`\n* Cleanup some popular models to better support arg passthrough / merge with model configs, more to go.\n \n", "original_header": "Jan 6, 2023"}, "confidence": 0.8805859259775956, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "* Add FlexiViT models and weights from https://github.com/google-research/big_vision (check out paper at https://arxiv.org/abs/2212.08013)\n  * NOTE currently resizing is static on model creation, on-the-fly dynamic / train patch size sampling is a WIP\n* Many more models updated to multi-weight and downloadable via HF hub now (convnext, efficientnet, mobilenet, vision_transformer*, beit)\n* More model pretrained tag and adjustments, some model names changed (working on deprecation translations, consider main branch DEV branch right now, use 0.6.x for stable use)\n* More ImageNet-12k (subset of 22k) pretrain models popping up:\n  * `efficientnet_b5.in12k_ft_in1k` - 85.9 @ 448x448\n  * `vit_medium_patch16_gap_384.in12k_ft_in1k` - 85.5 @ 384x384\n  * `vit_medium_patch16_gap_256.in12k_ft_in1k` - 84.5 @ 256x256\n  * `convnext_nano.in12k_ft_in1k` - 82.9 @ 288x288\n \n", "original_header": "Dec 23, 2022 \ud83c\udf84\u2603"}, "confidence": 0.8833134522094404, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "| model                                     | top1 | param_count |  gmac | macts | hub                                     |\n|:------------------------------------------|-----:|------------:|------:|------:|:----------------------------------------|\n| eva_large_patch14_336.in22k_ft_in22k_in1k | 89.2 |       304.5 | 191.1 | 270.2 | [link](https://huggingface.co/BAAI/EVA) |\n| eva_large_patch14_336.in22k_ft_in1k       | 88.7 |       304.5 | 191.1 | 270.2 | [link](https://huggingface.co/BAAI/EVA) |\n| eva_large_patch14_196.in22k_ft_in22k_in1k | 88.6 |       304.1 |  61.6 |  63.5 | [link](https://huggingface.co/BAAI/EVA) |\n| eva_large_patch14_196.in22k_ft_in1k       | 87.9 |       304.1 |  61.6 |  63.5 | [link](https://huggingface.co/BAAI/EVA) |\n \n", "original_header": "Dec 8, 2022"}, "confidence": 0.813422624387554, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "| model                                    |   top1 |   param_count |   gmac |   macts | hub                                     |\n|:-----------------------------------------|-------:|--------------:|-------:|--------:|:----------------------------------------|\n| eva_giant_patch14_560.m30m_ft_in22k_in1k |   89.8 |        1014.4 | 1906.8 |  2577.2 | [link](https://huggingface.co/BAAI/EVA) |\n| eva_giant_patch14_336.m30m_ft_in22k_in1k |   89.6 |        1013   |  620.6 |   550.7 | [link](https://huggingface.co/BAAI/EVA) |\n| eva_giant_patch14_336.clip_ft_in1k       |   89.4 |        1013   |  620.6 |   550.7 | [link](https://huggingface.co/BAAI/EVA) |\n| eva_giant_patch14_224.clip_ft_in1k       |   89.1 |        1012.6 |  267.2 |   192.6 | [link](https://huggingface.co/BAAI/EVA) |\n \n", "original_header": "Dec 6, 2022"}, "confidence": 0.813422624387554, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "* Port of MaxViT Tensorflow Weights from official impl at https://github.com/google-research/maxvit\n  * There was larger than expected drops for the upscaled 384/512 in21k fine-tune weights, possible detail missing, but the 21k FT did seem sensitive to small preprocessing \n", "original_header": "Dec 5, 2022"}, "confidence": 0.863727383804113, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "* More weights in `maxxvit` series, incl first ConvNeXt block based `coatnext` and `maxxvit` experiments:\n  * `coatnext_nano_rw_224` - 82.0 @ 224 (G) -- (uses ConvNeXt conv block, no BatchNorm)\n  * `maxxvit_rmlp_nano_rw_256` - 83.0 @ 256, 83.7 @ 320  (G) (uses ConvNeXt conv block, no BN)\n  * `maxvit_rmlp_small_rw_224` - 84.5 @ 224, 85.1 @ 320 (G)\n  * `maxxvit_rmlp_small_rw_256` - 84.6 @ 256, 84.9 @ 288 (G) -- could be trained better, hparams need tuning (uses ConvNeXt block, no BN)\n  * `coatnet_rmlp_2_rw_224` - 84.6 @ 224, 85 @ 320  (T)\n  * NOTE: official MaxVit weights (in1k) have been released at https://github.com/google-research/maxvit -- some extra work is needed to port and adapt since my impl was created independently of theirs and has a few small differences + the whole TF same padding fun.\n \n", "original_header": "Oct 10, 2022"}, "confidence": 0.8588447612772077, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "* LAION-2B CLIP image towers supported as pretrained backbones for fine-tune or features (no classifier)\n  * vit_base_patch32_224_clip_laion2b\n  * vit_large_patch14_224_clip_laion2b\n  * vit_huge_patch14_224_clip_laion2b\n  * vit_giant_patch14_224_clip_laion2b\n \n", "original_header": "Sept 23, 2022"}, "confidence": 0.8779327420502706, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "* Hugging Face [`timm` docs](https://huggingface.co/docs/hub/timm) home now exists, look for more here in the future\n* Add BEiT-v2 weights for base and large 224x224 models from https://github.com/microsoft/unilm/tree/master/beit2\n* Add more weights in `maxxvit` series incl a `pico` (7.5M params, 1.9 GMACs), two `tiny` variants:\n  * `maxvit_rmlp_pico_rw_256` - 80.5 @ 256, 81.3 @ 320  (T)\n  * `maxvit_tiny_rw_224` - 83.5 @ 224 (G)\n  * `maxvit_rmlp_tiny_rw_256` - 84.2 @ 256, 84.8 @ 320 (T) \n", "original_header": "Sept 7, 2022"}, "confidence": 0.8013173695971039, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "All model architecture families include variants with pretrained weights. There are specific model variants without any weights, it is NOT a bug. Help training new or better weights is always appreciated. \n", "original_header": "Models"}, "confidence": 0.9605462099740867, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "Several (less common) features that I often utilize in my projects are included. Many of their additions are the reason why I maintain my own set of models, instead of using others' via PIP: \n", "original_header": "Features"}, "confidence": 0.9748042306189888, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "Model validation results can be found in the [results tables](results/README.md)\n \n", "original_header": "Results"}, "confidence": 0.8231142048819448, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "The root folder of the repository contains reference train, validation, and inference scripts that work with the included models and other features of this repository. They are adaptable for other datasets and use cases with a little hacking. See [documentation](https://huggingface.co/docs/timm/training_script).\n \n", "original_header": "Train, Validation, Inference Scripts"}, "confidence": 0.9972958265832336, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "One of the greatest assets of PyTorch is the community and their contributions. A few of my favourite resources that pair well with the models and components here are listed below.\n \n", "original_header": "Awesome PyTorch Resources"}, "confidence": 0.9921201750331988, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}], "name": [{"result": {"value": "pytorch-image-models", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}], "full_name": [{"result": {"value": "huggingface/pytorch-image-models", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}], "issue_tracker": [{"result": {"value": "https://api.github.com/repos/huggingface/pytorch-image-models/issues", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "forks_url": [{"result": {"value": "https://api.github.com/repos/huggingface/pytorch-image-models/forks", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "stargazers_count": [{"result": {"value": 28121, "type": "Number"}, "confidence": 1, "technique": "GitHub_API"}], "keywords": [{"result": {"value": "augmix, cnn-classification, distributed-training, dual-path-networks, efficientnet, efficientnet-training, imagenet-classifier, mixnet, mnasnet, mobile-deep-learning, mobilenet-v2, mobilenetv3, nfnets, normalization-free-training, pretrained-models, pretrained-weights, pytorch, randaugment, resnet, vision-transformer-models", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}], "forks_count": [{"result": {"value": 4463, "type": "Number"}, "confidence": 1, "technique": "GitHub_API"}], "download_url": [{"result": {"value": "https://github.com/rwightman/pytorch-image-models/releases", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "programming_languages": [{"result": {"value": "Python", "name": "Python", "type": "Programming_language", "size": 3102742}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "MDX", "name": "MDX", "type": "Programming_language", "size": 568439}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "Shell", "name": "Shell", "type": "Programming_language", "size": 81}, "confidence": 1, "technique": "GitHub_API"}], "releases": [{"result": {"type": "Release", "value": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/131396351", "tag": "v0.9.12", "name": "Release v0.9.12", "author": {"name": "rwightman", "type": "User"}, "description": "### Nov 23, 2023\r\n* Added EfficientViT-Large models, thanks [SeeFun](https://github.com/seefun)\r\n* Fix Python 3.7 compat, will be dropping support for it soon\r\n* Other misc fixes\r\n* Release 0.9.12", "tarball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/tarball/v0.9.12", "zipball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/zipball/v0.9.12", "html_url": "https://github.com/huggingface/pytorch-image-models/releases/tag/v0.9.12", "url": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/131396351", "release_id": 131396351, "date_created": "2023-11-24T05:48:14Z", "date_published": "2023-11-24T19:09:08Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/130505646", "tag": "v0.9.11", "name": "Release v0.9.11", "author": {"name": "rwightman", "type": "User"}, "description": "### Nov 20, 2023\r\n* Added significant flexibility for Hugging Face Hub based timm models via `model_args` config entry. `model_args` will be passed as kwargs through to models on creation. \r\n  * See example at https://huggingface.co/gaunernst/vit_base_patch16_1024_128.audiomae_as2m_ft_as20k/blob/main/config.json\r\n  * Usage: https://github.com/huggingface/pytorch-image-models/discussions/2035\r\n* Updated imagenet eval and test set csv files with latest models\r\n* `vision_transformer.py` typing and doc cleanup by [Laure\u03b7t](https://github.com/Laurent2916)\r\n* 0.9.11 release\r\n", "tarball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/tarball/v0.9.11", "zipball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/zipball/v0.9.11", "html_url": "https://github.com/huggingface/pytorch-image-models/releases/tag/v0.9.11", "url": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/130505646", "release_id": 130505646, "date_created": "2023-11-20T22:39:22Z", "date_published": "2023-11-20T23:16:35Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/127993696", "tag": "v0.9.10", "name": "Release v0.9.10", "author": {"name": "rwightman", "type": "User"}, "description": "### Nov 4\r\n* Patch fix for 0.9.9 to fix FrozenBatchnorm2d import path for old torchvision (~2 years )\r\n\r\n### Nov 3, 2023\r\n* [DFN (Data Filtering Networks)](https://huggingface.co/papers/2309.17425) and [MetaCLIP](https://huggingface.co/papers/2309.16671) ViT weights added\r\n* DINOv2 'register' ViT model weights added\r\n* Add `quickgelu` ViT variants for OpenAI, DFN, MetaCLIP weights that use it (less efficient)\r\n* Improved typing added to ResNet, MobileNet-v3 thanks to [Aryan](https://github.com/a-r-r-o-w)\r\n* ImageNet-12k fine-tuned (from LAION-2B CLIP) `convnext_xxlarge`\r\n* 0.9.9 release\r\n", "tarball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/tarball/v0.9.10", "zipball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/zipball/v0.9.10", "html_url": "https://github.com/huggingface/pytorch-image-models/releases/tag/v0.9.10", "url": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/127993696", "release_id": 127993696, "date_created": "2023-11-04T09:33:04Z", "date_published": "2023-11-04T15:23:03Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/127949757", "tag": "v0.9.9", "name": "Release v0.9.9", "author": {"name": "rwightman", "type": "User"}, "description": "### Nov 3, 2023\r\n* [DFN (Data Filtering Networks)](https://huggingface.co/papers/2309.17425) and [MetaCLIP](https://huggingface.co/papers/2309.16671) ViT weights added\r\n* DINOv2 'register' ViT model weights added\r\n* Add `quickgelu` ViT variants for OpenAI, DFN, MetaCLIP weights that use it (less efficient)\r\n* Improved typing added to ResNet, MobileNet-v3 thanks to [Aryan](https://github.com/a-r-r-o-w)\r\n* ImageNet-12k fine-tuned (from LAION-2B CLIP) `convnext_xxlarge`\r\n* 0.9.9 release\r\n", "tarball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/tarball/v0.9.9", "zipball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/zipball/v0.9.9", "html_url": "https://github.com/huggingface/pytorch-image-models/releases/tag/v0.9.9", "url": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/127949757", "release_id": 127949757, "date_created": "2023-11-03T21:35:01Z", "date_published": "2023-11-03T22:24:26Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/126127784", "tag": "v0.9.8", "name": "Release v0.9.8", "author": {"name": "rwightman", "type": "User"}, "description": "### Oct 20, 2023\r\n* [SigLIP](https://huggingface.co/papers/2303.15343) image tower weights supported in `vision_transformer.py`.\r\n  * Great potential for fine-tune and downstream feature use.\r\n* Experimental 'register' support in vit models as per [Vision Transformers Need Registers](https://huggingface.co/papers/2309.16588)\r\n* Updated RepViT with new weight release. Thanks [wangao](https://github.com/jameslahm)\r\n* Add patch resizing support (on pretrained weight load) to Swin models\r\n* 0.9.8 release", "tarball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/tarball/v0.9.8", "zipball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/zipball/v0.9.8", "html_url": "https://github.com/huggingface/pytorch-image-models/releases/tag/v0.9.8", "url": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/126127784", "release_id": 126127784, "date_created": "2023-10-20T20:57:23Z", "date_published": "2023-10-21T20:52:29Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/119710741", "tag": "v0.9.7", "name": "Release v0.9.7", "author": {"name": "rwightman", "type": "User"}, "description": "Small bug fix & extra model from [v0.9.6](https://github.com/huggingface/pytorch-image-models/releases/tag/v0.9.6)\r\n\r\n### Sep 1, 2023\r\n* TinyViT added by [SeeFun](https://github.com/seefun)\r\n* Fix EfficientViT (MIT) to use torch.autocast so it works back to PT 1.10\r\n* 0.9.7 release\r\n", "tarball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/tarball/v0.9.7", "zipball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/zipball/v0.9.7", "html_url": "https://github.com/huggingface/pytorch-image-models/releases/tag/v0.9.7", "url": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/119710741", "release_id": 119710741, "date_created": "2023-09-02T18:16:21Z", "date_published": "2023-09-02T19:49:58Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/119126609", "tag": "v0.9.6", "name": "Release v0.9.6", "author": {"name": "rwightman", "type": "User"}, "description": "### Aug 28, 2023\r\n* Add dynamic img size support to models in `vision_transformer.py`, `vision_transformer_hybrid.py`, `deit.py`, and `eva.py` w/o breaking backward compat.\r\n  * Add `dynamic_img_size=True` to args at model creation time to allow changing the grid size (interpolate abs and/or ROPE pos embed each forward pass).\r\n  * Add `dynamic_img_pad=True` to allow image sizes that aren't divisible by patch size (pad bottom right to patch size each forward pass).\r\n  * Enabling either dynamic mode will break FX tracing unless PatchEmbed module added as leaf.\r\n  * Existing method of resizing position embedding by passing different `img_size` (interpolate pretrained embed weights once) on creation still works.\r\n  * Existing method of changing `patch_size` (resize pretrained patch_embed weights once) on creation still works.\r\n  * Example validation cmd `python validate.py /imagenet --model vit_base_patch16_224 --amp --amp-dtype bfloat16 --img-size 255 --crop-pct 1.0 --model-kwargs dynamic_img_size=True dyamic_img_pad=True`\r\n\r\n### Aug 25, 2023\r\n* Many new models since last release\r\n  * FastViT - https://arxiv.org/abs/2303.14189\r\n  * MobileOne - https://arxiv.org/abs/2206.04040\r\n  * InceptionNeXt - https://arxiv.org/abs/2303.16900\r\n  * RepGhostNet - https://arxiv.org/abs/2211.06088 (thanks https://github.com/ChengpengChen)\r\n  * GhostNetV2 - https://arxiv.org/abs/2211.12905 (thanks https://github.com/yehuitang)\r\n  * EfficientViT (MSRA) - https://arxiv.org/abs/2305.07027 (thanks https://github.com/seefun)\r\n  * EfficientViT (MIT) - https://arxiv.org/abs/2205.14756 (thanks https://github.com/seefun)\r\n* Add `--reparam` arg to `benchmark.py`, `onnx_export.py`, and `validate.py` to trigger layer reparameterization / fusion for models with any one of `reparameterize()`, `switch_to_deploy()` or `fuse()`\r\n  * Including FastViT, MobileOne, RepGhostNet, EfficientViT (MSRA), RepViT, RepVGG, and LeViT\r\n* Preparing 0.9.6 'back to school' release\r\n\r\n### Aug 11, 2023\r\n* Swin, MaxViT, CoAtNet, and BEiT models support resizing of image/window size on creation with adaptation of pretrained weights\r\n* Example validation cmd to test w/ non-square resize `python validate.py /imagenet --model swin_base_patch4_window7_224.ms_in22k_ft_in1k --amp --amp-dtype bfloat16 --input-size 3 256 320 --model-kwargs window_size=8,10 img_size=256,320`\r\n", "tarball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/tarball/v0.9.6", "zipball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/zipball/v0.9.6", "html_url": "https://github.com/huggingface/pytorch-image-models/releases/tag/v0.9.6", "url": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/119126609", "release_id": 119126609, "date_created": "2023-08-29T16:14:13Z", "date_published": "2023-08-29T19:06:35Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/115045401", "tag": "v0.9.5", "name": "Release v0.9.5", "author": {"name": "rwightman", "type": "User"}, "description": "Minor updates and bug fixes. New ResNeXT w/ highest ImageNet eval I'm aware of in the ResNe(X)t family (`seresnextaa201d_32x8d.sw_in12k_ft_in1k_384`)\r\n\r\n### Aug 3, 2023\r\n* Add GluonCV weights for HRNet w18_small and w18_small_v2. Converted by [SeeFun](https://github.com/seefun)\r\n* Fix `selecsls*` model naming regression\r\n* Patch and position embedding for ViT/EVA works for bfloat16/float16 weights on load (or activations for on-the-fly resize)\r\n* v0.9.5 release prep\r\n\r\n### July 27, 2023\r\n* Added timm trained `seresnextaa201d_32x8d.sw_in12k_ft_in1k_384` weights (and `.sw_in12k` pretrain) with 87.3% top-1 on ImageNet-1k, best ImageNet ResNet family model I'm aware of.\r\n* RepViT model and weights (https://arxiv.org/abs/2307.09283) added by [wangao](https://github.com/jameslahm)\r\n* I-JEPA ViT feature weights (no classifier) added by [SeeFun](https://github.com/seefun)\r\n* SAM-ViT (segment anything) feature weights (no classifier) added by [SeeFun](https://github.com/seefun)\r\n* Add support for alternative feat extraction methods and -ve indices to EfficientNet\r\n* Add NAdamW optimizer\r\n* Misc fixes\r\n", "tarball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/tarball/v0.9.5", "zipball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/zipball/v0.9.5", "html_url": "https://github.com/huggingface/pytorch-image-models/releases/tag/v0.9.5", "url": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/115045401", "release_id": 115045401, "date_created": "2023-08-03T23:38:53Z", "date_published": "2023-08-03T23:55:13Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/102905126", "tag": "v0.9.2", "name": "Release v0.9.2", "author": {"name": "rwightman", "type": "User"}, "description": "* Fix _hub deprecation pass through import", "tarball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/tarball/v0.9.2", "zipball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/zipball/v0.9.2", "html_url": "https://github.com/huggingface/pytorch-image-models/releases/tag/v0.9.2", "url": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/102905126", "release_id": 102905126, "date_created": "2023-05-14T15:03:04Z", "date_published": "2023-05-14T15:08:21Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/102814801", "tag": "v0.9.1", "name": "Release v0.9.1", "author": {"name": "rwightman", "type": "User"}, "description": "The first non pre-release since Oct 2022 with a long list of changes from 0.6.x releases...\r\n\r\n### May 12, 2023\r\n* Fix Python 3.7 import error re Final[] typing annotation\r\n\r\n### May 11, 2023\r\n* `timm` 0.9 released, transition from 0.8.xdev releases\r\n\r\n### May 10, 2023\r\n* Hugging Face Hub downloading is now default, 1132 models on https://huggingface.co/timm, 1163 weights in `timm`\r\n* DINOv2 vit feature backbone weights added thanks to [Leng Yue](https://github.com/leng-yue)\r\n* FB MAE vit feature backbone weights added\r\n* OpenCLIP DataComp-XL L/14 feat backbone weights added\r\n* MetaFormer (poolformer-v2, caformer, convformer, updated poolformer (v1)) w/ weights added by [Fredo Guan](https://github.com/fffffgggg54)\r\n* Experimental `get_intermediate_layers` function on vit/deit models for grabbing hidden states (inspired by DINO impl). This is WIP and may change significantly... feedback welcome.\r\n* Model creation throws error if `pretrained=True` and no weights exist (instead of continuing with random initialization)\r\n* Fix regression with inception / nasnet TF sourced weights with 1001 classes in original classifiers\r\n* bitsandbytes (https://github.com/TimDettmers/bitsandbytes) optimizers added to factory, use `bnb` prefix, ie `bnbadam8bit`\r\n* Misc cleanup and fixes\r\n* Final testing before switching to a 0.9 and bringing `timm` out of pre-release state\r\n\r\n### April 27, 2023\r\n* 97% of `timm` models uploaded to HF Hub and almost all updated to support multi-weight pretrained configs\r\n* Minor cleanup and refactoring of another batch of models as multi-weight added. More fused_attn (F.sdpa) and features_only support, and torchscript fixes.\r\n\r\n### April 21, 2023\r\n* Gradient accumulation support added to train script and tested (`--grad-accum-steps`), thanks [Taeksang Kim](https://github.com/voidbag)\r\n* More weights on HF Hub (cspnet, cait, volo, xcit, tresnet, hardcorenas, densenet, dpn, vovnet, xception_aligned)\r\n* Added `--head-init-scale` and `--head-init-bias` to train.py to scale classiifer head and set fixed bias for fine-tune\r\n* Remove all InplaceABN (`inplace_abn`) use, replaced use in tresnet with standard BatchNorm (modified weights accordingly). \r\n\r\n### April 12, 2023\r\n* Add ONNX export script, validate script, helpers that I've had kicking around for along time. Tweak 'same' padding for better export w/ recent ONNX + pytorch.\r\n* Refactor dropout args for vit and vit-like models, separate drop_rate into `drop_rate` (classifier dropout), `proj_drop_rate` (block mlp / out projections), `pos_drop_rate` (position embedding drop), `attn_drop_rate` (attention dropout). Also add patch dropout (FLIP) to vit and eva models.\r\n* fused F.scaled_dot_product_attention support to more vit models, add env var (TIMM_FUSED_ATTN) to control, and config interface to enable/disable\r\n* Add EVA-CLIP backbones w/ image tower weights, all the way up to 4B param 'enormous' model, and 336x336 OpenAI ViT mode that was missed.\r\n\r\n### April 5, 2023\r\n* ALL ResNet models pushed to Hugging Face Hub with multi-weight support\r\n  * All past `timm` trained weights added with recipe based tags to differentiate\r\n  * All ResNet strikes back A1/A2/A3 (seed 0) and R50 example B/C1/C2/D weights available\r\n  * Add torchvision v2 recipe weights to existing torchvision originals\r\n  * See comparison table in https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288#model-comparison\r\n* New ImageNet-12k + ImageNet-1k fine-tunes available for a few anti-aliased ResNet models\r\n  * `resnetaa50d.sw_in12k_ft_in1k` - 81.7 @ 224, 82.6 @ 288\r\n  * `resnetaa101d.sw_in12k_ft_in1k` - 83.5 @ 224, 84.1 @ 288\r\n  * `seresnextaa101d_32x8d.sw_in12k_ft_in1k` - 86.0 @ 224, 86.5 @ 288 \r\n  * `seresnextaa101d_32x8d.sw_in12k_ft_in1k_288` - 86.5 @ 288, 86.7 @ 320\r\n\r\n### March 31, 2023\r\n* Add first ConvNext-XXLarge CLIP -> IN-1k fine-tune and IN-12k intermediate fine-tunes for convnext-base/large CLIP models.\r\n\r\n| model                                                                                                                |top1  |top5  |img_size|param_count|gmacs |macts |\r\n|----------------------------------------------------------------------------------------------------------------------|------|------|--------|-----------|------|------|\r\n| [convnext_xxlarge.clip_laion2b_soup_ft_in1k](https://huggingface.co/timm/convnext_xxlarge.clip_laion2b_soup_ft_in1k) |88.612|98.704|256     |846.47     |198.09|124.45|\r\n| convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384                                                               |88.312|98.578|384     |200.13     |101.11|126.74|\r\n| convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320                                                               |87.968|98.47 |320     |200.13     |70.21 |88.02 |\r\n| convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384                                                                  |87.138|98.212|384     |88.59      |45.21 |84.49 |\r\n| convnext_base.clip_laion2b_augreg_ft_in12k_in1k                                                                      |86.344|97.97 |256     |88.59      |20.09 |37.55 |\r\n\r\n* Add EVA-02 MIM pretrained and fine-tuned weights, push to HF hub and update model cards for all EVA models. First model over 90% top-1 (99% top-5)! Check out the original code & weights at https://github.com/baaivision/EVA for more details on their work blending MIM, CLIP w/ many model, dataset, and train recipe tweaks.\r\n\r\n| model                                              |top1  |top5  |param_count|img_size|\r\n|----------------------------------------------------|------|------|-----------|--------|\r\n| [eva02_large_patch14_448.mim_m38m_ft_in22k_in1k](https://huggingface.co/timm/eva02_large_patch14_448.mim_m38m_ft_in1k) |90.054|99.042|305.08     |448     |\r\n| eva02_large_patch14_448.mim_in22k_ft_in22k_in1k    |89.946|99.01 |305.08     |448     |\r\n| eva_giant_patch14_560.m30m_ft_in22k_in1k           |89.792|98.992|1014.45    |560     |\r\n| eva02_large_patch14_448.mim_in22k_ft_in1k          |89.626|98.954|305.08     |448     |\r\n| eva02_large_patch14_448.mim_m38m_ft_in1k           |89.57 |98.918|305.08     |448     |\r\n| eva_giant_patch14_336.m30m_ft_in22k_in1k           |89.56 |98.956|1013.01    |336     |\r\n| eva_giant_patch14_336.clip_ft_in1k                 |89.466|98.82 |1013.01    |336     |\r\n| eva_large_patch14_336.in22k_ft_in22k_in1k          |89.214|98.854|304.53     |336     |\r\n| eva_giant_patch14_224.clip_ft_in1k                 |88.882|98.678|1012.56    |224     |\r\n| eva02_base_patch14_448.mim_in22k_ft_in22k_in1k     |88.692|98.722|87.12      |448     |\r\n| eva_large_patch14_336.in22k_ft_in1k                |88.652|98.722|304.53     |336     |\r\n| eva_large_patch14_196.in22k_ft_in22k_in1k          |88.592|98.656|304.14     |196     |\r\n| eva02_base_patch14_448.mim_in22k_ft_in1k           |88.23 |98.564|87.12      |448     |\r\n| eva_large_patch14_196.in22k_ft_in1k                |87.934|98.504|304.14     |196     |\r\n| eva02_small_patch14_336.mim_in22k_ft_in1k          |85.74 |97.614|22.13      |336     |\r\n| eva02_tiny_patch14_336.mim_in22k_ft_in1k           |80.658|95.524|5.76       |336     |\r\n\r\n* Multi-weight and HF hub for DeiT and MLP-Mixer based models\r\n\r\n### March 22, 2023\r\n* More weights pushed to HF hub along with multi-weight support, including: `regnet.py`, `rexnet.py`, `byobnet.py`, `resnetv2.py`, `swin_transformer.py`, `swin_transformer_v2.py`, `swin_transformer_v2_cr.py`\r\n* Swin Transformer models support feature extraction (NCHW feat maps for `swinv2_cr_*`, and NHWC for all others) and spatial embedding outputs.\r\n* FocalNet (from https://github.com/microsoft/FocalNet) models and weights added with significant refactoring, feature extraction, no fixed resolution / sizing constraint\r\n* RegNet weights increased with HF hub push, SWAG, SEER, and torchvision v2 weights. SEER is pretty poor wrt to performance for model size, but possibly useful.\r\n* More ImageNet-12k pretrained and 1k fine-tuned `timm` weights:\r\n  * `rexnetr_200.sw_in12k_ft_in1k` - 82.6 @ 224, 83.2 @ 288\r\n  * `rexnetr_300.sw_in12k_ft_in1k` - 84.0 @ 224, 84.5 @ 288\r\n  * `regnety_120.sw_in12k_ft_in1k` - 85.0 @ 224, 85.4 @ 288\r\n  * `regnety_160.lion_in12k_ft_in1k` - 85.6 @ 224, 86.0 @ 288\r\n  * `regnety_160.sw_in12k_ft_in1k` - 85.6 @ 224, 86.0 @ 288  (compare to SWAG PT + 1k FT this is same BUT much lower res, blows SEER FT away)\r\n* Model name deprecation + remapping functionality added (a milestone for bringing 0.8.x out of pre-release). Mappings being added...\r\n* Minor bug fixes and improvements.\r\n\r\n### Feb 26, 2023\r\n* Add ConvNeXt-XXLarge CLIP pretrained image tower weights for fine-tune & features (fine-tuning TBD) -- see [model card](https://huggingface.co/laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-soup)\r\n* Update `convnext_xxlarge` default LayerNorm eps to 1e-5 (for CLIP weights, improved stability)\r\n* 0.8.15dev0\r\n\r\n### Feb 20, 2023\r\n* Add 320x320 `convnext_large_mlp.clip_laion2b_ft_320` and `convnext_lage_mlp.clip_laion2b_ft_soup_320` CLIP image tower weights for features & fine-tune\r\n* 0.8.13dev0 pypi release for latest changes w/ move to huggingface org\r\n\r\n### Feb 16, 2023\r\n* `safetensor` checkpoint support added\r\n* Add ideas from 'Scaling Vision Transformers to 22 B. Params' (https://arxiv.org/abs/2302.05442) -- qk norm, RmsNorm, parallel block\r\n* Add F.scaled_dot_product_attention support (PyTorch 2.0 only) to `vit_*`, `vit_relpos*`, `coatnet` / `maxxvit` (to start)\r\n* Lion optimizer (w/ multi-tensor option) added (https://arxiv.org/abs/2302.06675)\r\n* gradient checkpointing works with `features_only=True`\r\n\r\n### Feb 7, 2023\r\n* New inference benchmark numbers added in [results](results/) folder.\r\n* Add convnext LAION CLIP trained weights and initial set of in1k fine-tunes\r\n  * `convnext_base.clip_laion2b_augreg_ft_in1k` - 86.2% @ 256x256\r\n  * `convnext_base.clip_laiona_augreg_ft_in1k_384` - 86.5% @ 384x384\r\n  * `convnext_large_mlp.clip_laion2b_augreg_ft_in1k` - 87.3% @ 256x256\r\n  * `convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384` - 87.9% @ 384x384\r\n* Add DaViT models. Supports `features_only=True`. Adapted from https://github.com/dingmyu/davit by [Fredo](https://github.com/fffffgggg54).\r\n* Use a common NormMlpClassifierHead across MaxViT, ConvNeXt, DaViT\r\n* Add EfficientFormer-V2 model, update EfficientFormer, and refactor LeViT (closely related architectures). Weights on HF hub.\r\n  * New EfficientFormer-V2 arch, significant refactor from original at (https://github.com/snap-research/EfficientFormer). Supports `features_only=True`.\r\n  * Minor updates to EfficientFormer.\r\n  * Refactor LeViT models to stages, add `features_only=True` support to new `conv` variants, weight remap required.\r\n* Move ImageNet meta-data (synsets, indices) from `/results` to [`timm/data/_info`](timm/data/_info/).\r\n* Add ImageNetInfo / DatasetInfo classes to provide labelling for various ImageNet classifier layouts in `timm`\r\n  * Update `inference.py` to use, try: `python inference.py /folder/to/images --model convnext_small.in12k --label-type detail --topk 5`\r\n* Ready for 0.8.10 pypi pre-release (final testing).\r\n\r\n### Jan 20, 2023\r\n* Add two convnext 12k -> 1k fine-tunes at 384x384\r\n  * `convnext_tiny.in12k_ft_in1k_384` - 85.1 @ 384\r\n  * `convnext_small.in12k_ft_in1k_384` - 86.2 @ 384\r\n\r\n* Push all MaxxViT weights to HF hub, and add new ImageNet-12k -> 1k fine-tunes for `rw` base MaxViT and CoAtNet 1/2 models\r\n\r\n|model                                                                                                                   |top1 |top5 |samples / sec  |Params (M)     |GMAC  |Act (M)|\r\n|------------------------------------------------------------------------------------------------------------------------|----:|----:|--------------:|--------------:|-----:|------:|\r\n|[maxvit_xlarge_tf_512.in21k_ft_in1k](https://huggingface.co/timm/maxvit_xlarge_tf_512.in21k_ft_in1k)                    |88.53|98.64|          21.76|         475.77|534.14|1413.22|\r\n|[maxvit_xlarge_tf_384.in21k_ft_in1k](https://huggingface.co/timm/maxvit_xlarge_tf_384.in21k_ft_in1k)                    |88.32|98.54|          42.53|         475.32|292.78| 668.76|\r\n|[maxvit_base_tf_512.in21k_ft_in1k](https://huggingface.co/timm/maxvit_base_tf_512.in21k_ft_in1k)                        |88.20|98.53|          50.87|         119.88|138.02| 703.99|\r\n|[maxvit_large_tf_512.in21k_ft_in1k](https://huggingface.co/timm/maxvit_large_tf_512.in21k_ft_in1k)                      |88.04|98.40|          36.42|         212.33|244.75| 942.15|\r\n|[maxvit_large_tf_384.in21k_ft_in1k](https://huggingface.co/timm/maxvit_large_tf_384.in21k_ft_in1k)                      |87.98|98.56|          71.75|         212.03|132.55| 445.84|\r\n|[maxvit_base_tf_384.in21k_ft_in1k](https://huggingface.co/timm/maxvit_base_tf_384.in21k_ft_in1k)                        |87.92|98.54|         104.71|         119.65| 73.80| 332.90|\r\n|[maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k](https://huggingface.co/timm/maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k)        |87.81|98.37|         106.55|         116.14| 70.97| 318.95|\r\n|[maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k](https://huggingface.co/timm/maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k)  |87.47|98.37|         149.49|         116.09| 72.98| 213.74|\r\n|[coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k](https://huggingface.co/timm/coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k)            |87.39|98.31|         160.80|          73.88| 47.69| 209.43|\r\n|[maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k](https://huggingface.co/timm/maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k)        |86.89|98.02|         375.86|         116.14| 23.15|  92.64|\r\n|[maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k](https://huggingface.co/timm/maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k)  |86.64|98.02|         501.03|         116.09| 24.20|  62.77|\r\n|[maxvit_base_tf_512.in1k](https://huggingface.co/timm/maxvit_base_tf_512.in1k)                                          |86.60|97.92|          50.75|         119.88|138.02| 703.99|\r\n|[coatnet_2_rw_224.sw_in12k_ft_in1k](https://huggingface.co/timm/coatnet_2_rw_224.sw_in12k_ft_in1k)                      |86.57|97.89|         631.88|          73.87| 15.09|  49.22|\r\n|[maxvit_large_tf_512.in1k](https://huggingface.co/timm/maxvit_large_tf_512.in1k)                                        |86.52|97.88|          36.04|         212.33|244.75| 942.15|\r\n|[coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k](https://huggingface.co/timm/coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k)            |86.49|97.90|         620.58|          73.88| 15.18|  54.78|\r\n|[maxvit_base_tf_384.in1k](https://huggingface.co/timm/maxvit_base_tf_384.in1k)                                          |86.29|97.80|         101.09|         119.65| 73.80| 332.90|\r\n|[maxvit_large_tf_384.in1k](https://huggingface.co/timm/maxvit_large_tf_384.in1k)                                        |86.23|97.69|          70.56|         212.03|132.55| 445.84|\r\n|[maxvit_small_tf_512.in1k](https://huggingface.co/timm/maxvit_small_tf_512.in1k)                                        |86.10|97.76|          88.63|          69.13| 67.26| 383.77|\r\n|[maxvit_tiny_tf_512.in1k](https://huggingface.co/timm/maxvit_tiny_tf_512.in1k)                                          |85.67|97.58|         144.25|          31.05| 33.49| 257.59|\r\n|[maxvit_small_tf_384.in1k](https://huggingface.co/timm/maxvit_small_tf_384.in1k)                                        |85.54|97.46|         188.35|          69.02| 35.87| 183.65|\r\n|[maxvit_tiny_tf_384.in1k](https://huggingface.co/timm/maxvit_tiny_tf_384.in1k)                                          |85.11|97.38|         293.46|          30.98| 17.53| 123.42|\r\n|[maxvit_large_tf_224.in1k](https://huggingface.co/timm/maxvit_large_tf_224.in1k)                                        |84.93|96.97|         247.71|         211.79| 43.68| 127.35|\r\n|[coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k](https://huggingface.co/timm/coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k)          |84.90|96.96|        1025.45|          41.72|  8.11|  40.13|\r\n|[maxvit_base_tf_224.in1k](https://huggingface.co/timm/maxvit_base_tf_224.in1k)                                          |84.85|96.99|         358.25|         119.47| 24.04|  95.01|\r\n|[maxxvit_rmlp_small_rw_256.sw_in1k](https://huggingface.co/timm/maxxvit_rmlp_small_rw_256.sw_in1k)                      |84.63|97.06|         575.53|          66.01| 14.67|  58.38|\r\n|[coatnet_rmlp_2_rw_224.sw_in1k](https://huggingface.co/timm/coatnet_rmlp_2_rw_224.sw_in1k)                              |84.61|96.74|         625.81|          73.88| 15.18|  54.78|\r\n|[maxvit_rmlp_small_rw_224.sw_in1k](https://huggingface.co/timm/maxvit_rmlp_small_rw_224.sw_in1k)                        |84.49|96.76|         693.82|          64.90| 10.75|  49.30|\r\n|[maxvit_small_tf_224.in1k](https://huggingface.co/timm/maxvit_small_tf_224.in1k)                                        |84.43|96.83|         647.96|          68.93| 11.66|  53.17|\r\n|[maxvit_rmlp_tiny_rw_256.sw_in1k](https://huggingface.co/timm/maxvit_rmlp_tiny_rw_256.sw_in1k)                          |84.23|96.78|         807.21|          29.15|  6.77|  46.92|\r\n|[coatnet_1_rw_224.sw_in1k](https://huggingface.co/timm/coatnet_1_rw_224.sw_in1k)                                        |83.62|96.38|         989.59|          41.72|  8.04|  34.60|\r\n|[maxvit_tiny_rw_224.sw_in1k](https://huggingface.co/timm/maxvit_tiny_rw_224.sw_in1k)                                    |83.50|96.50|        1100.53|          29.06|  5.11|  33.11|\r\n|[maxvit_tiny_tf_224.in1k](https://huggingface.co/timm/maxvit_tiny_tf_224.in1k)                                          |83.41|96.59|        1004.94|          30.92|  5.60|  35.78|\r\n|[coatnet_rmlp_1_rw_224.sw_in1k](https://huggingface.co/timm/coatnet_rmlp_1_rw_224.sw_in1k)                              |83.36|96.45|        1093.03|          41.69|  7.85|  35.47|\r\n|[maxxvitv2_nano_rw_256.sw_in1k](https://huggingface.co/timm/maxxvitv2_nano_rw_256.sw_in1k)                              |83.11|96.33|        1276.88|          23.70|  6.26|  23.05|\r\n|[maxxvit_rmlp_nano_rw_256.sw_in1k](https://huggingface.co/timm/maxxvit_rmlp_nano_rw_256.sw_in1k)                        |83.03|96.34|        1341.24|          16.78|  4.37|  26.05|\r\n|[maxvit_rmlp_nano_rw_256.sw_in1k](https://huggingface.co/timm/maxvit_rmlp_nano_rw_256.sw_in1k)                          |82.96|96.26|        1283.24|          15.50|  4.47|  31.92|\r\n|[maxvit_nano_rw_256.sw_in1k](https://huggingface.co/timm/maxvit_nano_rw_256.sw_in1k)                                    |82.93|96.23|        1218.17|          15.45|  4.46|  30.28|\r\n|[coatnet_bn_0_rw_224.sw_in1k](https://huggingface.co/timm/coatnet_bn_0_rw_224.sw_in1k)                                  |82.39|96.19|        1600.14|          27.44|  4.67|  22.04|\r\n|[coatnet_0_rw_224.sw_in1k](https://huggingface.co/timm/coatnet_0_rw_224.sw_in1k)                                        |82.39|95.84|        1831.21|          27.44|  4.43|  18.73|\r\n|[coatnet_rmlp_nano_rw_224.sw_in1k](https://huggingface.co/timm/coatnet_rmlp_nano_rw_224.sw_in1k)                        |82.05|95.87|        2109.09|          15.15|  2.62|  20.34|\r\n|[coatnext_nano_rw_224.sw_in1k](https://huggingface.co/timm/coatnext_nano_rw_224.sw_in1k)                                |81.95|95.92|        2525.52|          14.70|  2.47|  12.80|\r\n|[coatnet_nano_rw_224.sw_in1k](https://huggingface.co/timm/coatnet_nano_rw_224.sw_in1k)                                  |81.70|95.64|        2344.52|          15.14|  2.41|  15.41|\r\n|[maxvit_rmlp_pico_rw_256.sw_in1k](https://huggingface.co/timm/maxvit_rmlp_pico_rw_256.sw_in1k)                          |80.53|95.21|        1594.71|           7.52|  1.85|  24.86|\r\n\r\n### Jan 11, 2023\r\n* Update ConvNeXt ImageNet-12k pretrain series w/ two new fine-tuned weights (and pre FT `.in12k` tags)\r\n  * `convnext_nano.in12k_ft_in1k` - 82.3 @ 224, 82.9 @ 288  (previously released)\r\n  * `convnext_tiny.in12k_ft_in1k` - 84.2 @ 224, 84.5 @ 288\r\n  * `convnext_small.in12k_ft_in1k` - 85.2 @ 224, 85.3 @ 288\r\n\r\n### Jan 6, 2023\r\n* Finally got around to adding `--model-kwargs` and `--opt-kwargs` to scripts to pass through rare args directly to model classes from cmd line\r\n  * `train.py /imagenet --model resnet50 --amp --model-kwargs output_stride=16 act_layer=silu`\r\n  * `train.py /imagenet --model vit_base_patch16_clip_224 --img-size 240 --amp --model-kwargs img_size=240 patch_size=12`\r\n* Cleanup some popular models to better support arg passthrough / merge with model configs, more to go.\r\n\r\n### Jan 5, 2023\r\n* ConvNeXt-V2 models and weights added to existing `convnext.py`\r\n  * Paper: [ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders](http://arxiv.org/abs/2301.00808)\r\n  * Reference impl: https://github.com/facebookresearch/ConvNeXt-V2 (NOTE: weights currently CC-BY-NC)\r\n\r\n### Dec 23, 2022 \ud83c\udf84\u2603\r\n* Add FlexiViT models and weights from https://github.com/google-research/big_vision (check out paper at https://arxiv.org/abs/2212.08013)\r\n  * NOTE currently resizing is static on model creation, on-the-fly dynamic / train patch size sampling is a WIP\r\n* Many more models updated to multi-weight and downloadable via HF hub now (convnext, efficientnet, mobilenet, vision_transformer*, beit)\r\n* More model pretrained tag and adjustments, some model names changed (working on deprecation translations, consider main branch DEV branch right now, use 0.6.x for stable use)\r\n* More ImageNet-12k (subset of 22k) pretrain models popping up:\r\n  * `efficientnet_b5.in12k_ft_in1k` - 85.9 @ 448x448\r\n  * `vit_medium_patch16_gap_384.in12k_ft_in1k` - 85.5 @ 384x384\r\n  * `vit_medium_patch16_gap_256.in12k_ft_in1k` - 84.5 @ 256x256\r\n  * `convnext_nano.in12k_ft_in1k` - 82.9 @ 288x288\r\n\r\n### Dec 8, 2022\r\n* Add 'EVA l' to `vision_transformer.py`, MAE style ViT-L/14 MIM pretrain w/ EVA-CLIP targets, FT on ImageNet-1k (w/ ImageNet-22k intermediate for some)\r\n  * original source: https://github.com/baaivision/EVA\r\n\r\n| model                                     | top1 | param_count |  gmac | macts | hub                                     |\r\n|:------------------------------------------|-----:|------------:|------:|------:|:----------------------------------------|\r\n| eva_large_patch14_336.in22k_ft_in22k_in1k | 89.2 |       304.5 | 191.1 | 270.2 | [link](https://huggingface.co/BAAI/EVA) |\r\n| eva_large_patch14_336.in22k_ft_in1k       | 88.7 |       304.5 | 191.1 | 270.2 | [link](https://huggingface.co/BAAI/EVA) |\r\n| eva_large_patch14_196.in22k_ft_in22k_in1k | 88.6 |       304.1 |  61.6 |  63.5 | [link](https://huggingface.co/BAAI/EVA) |\r\n| eva_large_patch14_196.in22k_ft_in1k       | 87.9 |       304.1 |  61.6 |  63.5 | [link](https://huggingface.co/BAAI/EVA) |\r\n\r\n### Dec 6, 2022\r\n* Add 'EVA g', BEiT style ViT-g/14 model weights w/ both MIM pretrain and CLIP pretrain to `beit.py`.\r\n  * original source: https://github.com/baaivision/EVA\r\n  * paper: https://arxiv.org/abs/2211.07636\r\n\r\n| model                                    |   top1 |   param_count |   gmac |   macts | hub                                     |\r\n|:-----------------------------------------|-------:|--------------:|-------:|--------:|:----------------------------------------|\r\n| eva_giant_patch14_560.m30m_ft_in22k_in1k |   89.8 |        1014.4 | 1906.8 |  2577.2 | [link](https://huggingface.co/BAAI/EVA) |\r\n| eva_giant_patch14_336.m30m_ft_in22k_in1k |   89.6 |        1013   |  620.6 |   550.7 | [link](https://huggingface.co/BAAI/EVA) |\r\n| eva_giant_patch14_336.clip_ft_in1k       |   89.4 |        1013   |  620.6 |   550.7 | [link](https://huggingface.co/BAAI/EVA) |\r\n| eva_giant_patch14_224.clip_ft_in1k       |   89.1 |        1012.6 |  267.2 |   192.6 | [link](https://huggingface.co/BAAI/EVA) |\r\n\r\n### Dec 5, 2022\r\n\r\n* Pre-release (`0.8.0dev0`) of multi-weight support (`model_arch.pretrained_tag`). Install with `pip install --pre timm`\r\n  * vision_transformer, maxvit, convnext are the first three model impl w/ support\r\n  * model names are changing with this (previous _21k, etc. fn will merge), still sorting out deprecation handling\r\n  * bugs are likely, but I need feedback so please try it out\r\n  * if stability is needed, please use 0.6.x pypi releases or clone from [0.6.x branch](https://github.com/rwightman/pytorch-image-models/tree/0.6.x)\r\n* Support for PyTorch 2.0 compile is added in train/validate/inference/benchmark, use `--torchcompile` argument\r\n* Inference script allows more control over output, select k for top-class index + prob json, csv or parquet output\r\n* Add a full set of fine-tuned CLIP image tower weights from both LAION-2B and original OpenAI CLIP models\r\n\r\n| model                                            |   top1 |   param_count |   gmac |   macts | hub                                                                                  |\r\n|:-------------------------------------------------|-------:|--------------:|-------:|--------:|:-------------------------------------------------------------------------------------|\r\n| vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k  |   88.6 |         632.5 |  391   |   407.5 | [link](https://huggingface.co/timm/vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k)  |\r\n| vit_large_patch14_clip_336.openai_ft_in12k_in1k  |   88.3 |         304.5 |  191.1 |   270.2 | [link](https://huggingface.co/timm/vit_large_patch14_clip_336.openai_ft_in12k_in1k)  |\r\n| vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k  |   88.2 |         632   |  167.4 |   139.4 | [link](https://huggingface.co/timm/vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k)  |\r\n| vit_large_patch14_clip_336.laion2b_ft_in12k_in1k |   88.2 |         304.5 |  191.1 |   270.2 | [link](https://huggingface.co/timm/vit_large_patch14_clip_336.laion2b_ft_in12k_in1k) |\r\n| vit_large_patch14_clip_224.openai_ft_in12k_in1k  |   88.2 |         304.2 |   81.1 |    88.8 | [link](https://huggingface.co/timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k)  |\r\n| vit_large_patch14_clip_224.laion2b_ft_in12k_in1k |   87.9 |         304.2 |   81.1 |    88.8 | [link](https://huggingface.co/timm/vit_large_patch14_clip_224.laion2b_ft_in12k_in1k) |\r\n| vit_large_patch14_clip_224.openai_ft_in1k        |   87.9 |         304.2 |   81.1 |    88.8 | [link](https://huggingface.co/timm/vit_large_patch14_clip_224.openai_ft_in1k)        |\r\n| vit_large_patch14_clip_336.laion2b_ft_in1k       |   87.9 |         304.5 |  191.1 |   270.2 | [link](https://huggingface.co/timm/vit_large_patch14_clip_336.laion2b_ft_in1k)       |\r\n| vit_huge_patch14_clip_224.laion2b_ft_in1k        |   87.6 |         632   |  167.4 |   139.4 | [link](https://huggingface.co/timm/vit_huge_patch14_clip_224.laion2b_ft_in1k)        |\r\n| vit_large_patch14_clip_224.laion2b_ft_in1k       |   87.3 |         304.2 |   81.1 |    88.8 | [link](https://huggingface.co/timm/vit_large_patch14_clip_224.laion2b_ft_in1k)       |\r\n| vit_base_patch16_clip_384.laion2b_ft_in12k_in1k  |   87.2 |          86.9 |   55.5 |   101.6 | [link](https://huggingface.co/timm/vit_base_patch16_clip_384.laion2b_ft_in12k_in1k)  |\r\n| vit_base_patch16_clip_384.openai_ft_in12k_in1k   |   87   |          86.9 |   55.5 |   101.6 | [link](https://huggingface.co/timm/vit_base_patch16_clip_384.openai_ft_in12k_in1k)   |\r\n| vit_base_patch16_clip_384.laion2b_ft_in1k        |   86.6 |          86.9 |   55.5 |   101.6 | [link](https://huggingface.co/timm/vit_base_patch16_clip_384.laion2b_ft_in1k)        |\r\n| vit_base_patch16_clip_384.openai_ft_in1k         |   86.2 |          86.9 |   55.5 |   101.6 | [link](https://huggingface.co/timm/vit_base_patch16_clip_384.openai_ft_in1k)         |\r\n| vit_base_patch16_clip_224.laion2b_ft_in12k_in1k  |   86.2 |          86.6 |   17.6 |    23.9 | [link](https://huggingface.co/timm/vit_base_patch16_clip_224.laion2b_ft_in12k_in1k)  |\r\n| vit_base_patch16_clip_224.openai_ft_in12k_in1k   |   85.9 |          86.6 |   17.6 |    23.9 | [link](https://huggingface.co/timm/vit_base_patch16_clip_224.openai_ft_in12k_in1k)   |\r\n| vit_base_patch32_clip_448.laion2b_ft_in12k_in1k  |   85.8 |          88.3 |   17.9 |    23.9 | [link](https://huggingface.co/timm/vit_base_patch32_clip_448.laion2b_ft_in12k_in1k)  |\r\n| vit_base_patch16_clip_224.laion2b_ft_in1k        |   85.5 |          86.6 |   17.6 |    23.9 | [link](https://huggingface.co/timm/vit_base_patch16_clip_224.laion2b_ft_in1k)        |\r\n| vit_base_patch32_clip_384.laion2b_ft_in12k_in1k  |   85.4 |          88.3 |   13.1 |    16.5 | [link](https://huggingface.co/timm/vit_base_patch32_clip_384.laion2b_ft_in12k_in1k)  |\r\n| vit_base_patch16_clip_224.openai_ft_in1k         |   85.3 |          86.6 |   17.6 |    23.9 | [link](https://huggingface.co/timm/vit_base_patch16_clip_224.openai_ft_in1k)         |\r\n| vit_base_patch32_clip_384.openai_ft_in12k_in1k   |   85.2 |          88.3 |   13.1 |    16.5 | [link](https://huggingface.co/timm/vit_base_patch32_clip_384.openai_ft_in12k_in1k)   |\r\n| vit_base_patch32_clip_224.laion2b_ft_in12k_in1k  |   83.3 |          88.2 |    4.4 |     5   | [link](https://huggingface.co/timm/vit_base_patch32_clip_224.laion2b_ft_in12k_in1k)  |\r\n| vit_base_patch32_clip_224.laion2b_ft_in1k        |   82.6 |          88.2 |    4.4 |     5   | [link](https://huggingface.co/timm/vit_base_patch32_clip_224.laion2b_ft_in1k)        |\r\n| vit_base_patch32_clip_224.openai_ft_in1k         |   81.9 |          88.2 |    4.4 |     5   | [link](https://huggingface.co/timm/vit_base_patch32_clip_224.openai_ft_in1k)         |\r\n\r\n* Port of MaxViT Tensorflow Weights from official impl at https://github.com/google-research/maxvit\r\n  * There was larger than expected drops for the upscaled 384/512 in21k fine-tune weights, possible detail missing, but the 21k FT did seem sensitive to small preprocessing\r\n\r\n| model                              |   top1 |   param_count |   gmac |   macts | hub                                                                    |\r\n|:-----------------------------------|-------:|--------------:|-------:|--------:|:-----------------------------------------------------------------------|\r\n| maxvit_xlarge_tf_512.in21k_ft_in1k |   88.5 |         475.8 |  534.1 |  1413.2 | [link](https://huggingface.co/timm/maxvit_xlarge_tf_512.in21k_ft_in1k) |\r\n| maxvit_xlarge_tf_384.in21k_ft_in1k |   88.3 |         475.3 |  292.8 |   668.8 | [link](https://huggingface.co/timm/maxvit_xlarge_tf_384.in21k_ft_in1k) |\r\n| maxvit_base_tf_512.in21k_ft_in1k   |   88.2 |         119.9 |  138   |   704   | [link](https://huggingface.co/timm/maxvit_base_tf_512.in21k_ft_in1k)   |\r\n| maxvit_large_tf_512.in21k_ft_in1k  |   88   |         212.3 |  244.8 |   942.2 | [link](https://huggingface.co/timm/maxvit_large_tf_512.in21k_ft_in1k)  |\r\n| maxvit_large_tf_384.in21k_ft_in1k  |   88   |         212   |  132.6 |   445.8 | [link](https://huggingface.co/timm/maxvit_large_tf_384.in21k_ft_in1k)  |\r\n| maxvit_base_tf_384.in21k_ft_in1k   |   87.9 |         119.6 |   73.8 |   332.9 | [link](https://huggingface.co/timm/maxvit_base_tf_384.in21k_ft_in1k)   |\r\n| maxvit_base_tf_512.in1k            |   86.6 |         119.9 |  138   |   704   | [link](https://huggingface.co/timm/maxvit_base_tf_512.in1k)            |\r\n| maxvit_large_tf_512.in1k           |   86.5 |         212.3 |  244.8 |   942.2 | [link](https://huggingface.co/timm/maxvit_large_tf_512.in1k)           |\r\n| maxvit_base_tf_384.in1k            |   86.3 |         119.6 |   73.8 |   332.9 | [link](https://huggingface.co/timm/maxvit_base_tf_384.in1k)            |\r\n| maxvit_large_tf_384.in1k           |   86.2 |         212   |  132.6 |   445.8 | [link](https://huggingface.co/timm/maxvit_large_tf_384.in1k)           |\r\n| maxvit_small_tf_512.in1k           |   86.1 |          69.1 |   67.3 |   383.8 | [link](https://huggingface.co/timm/maxvit_small_tf_512.in1k)           |\r\n| maxvit_tiny_tf_512.in1k            |   85.7 |          31   |   33.5 |   257.6 | [link](https://huggingface.co/timm/maxvit_tiny_tf_512.in1k)            |\r\n| maxvit_small_tf_384.in1k           |   85.5 |          69   |   35.9 |   183.6 | [link](https://huggingface.co/timm/maxvit_small_tf_384.in1k)           |\r\n| maxvit_tiny_tf_384.in1k            |   85.1 |          31   |   17.5 |   123.4 | [link](https://huggingface.co/timm/maxvit_tiny_tf_384.in1k)            |\r\n| maxvit_large_tf_224.in1k           |   84.9 |         211.8 |   43.7 |   127.4 | [link](https://huggingface.co/timm/maxvit_large_tf_224.in1k)           |\r\n| maxvit_base_tf_224.in1k            |   84.9 |         119.5 |   24   |    95   | [link](https://huggingface.co/timm/maxvit_base_tf_224.in1k)            |\r\n| maxvit_small_tf_224.in1k           |   84.4 |          68.9 |   11.7 |    53.2 | [link](https://huggingface.co/timm/maxvit_small_tf_224.in1k)           |\r\n| maxvit_tiny_tf_224.in1k            |   83.4 |          30.9 |    5.6 |    35.8 | [link](https://huggingface.co/timm/maxvit_tiny_tf_224.in1k)            |\r\n\r\n### Oct 15, 2022\r\n* Train and validation script enhancements\r\n* Non-GPU (ie CPU) device support\r\n* SLURM compatibility for train script\r\n* HF datasets support (via ReaderHfds)\r\n* TFDS/WDS dataloading improvements (sample padding/wrap for distributed use fixed wrt sample count estimate)\r\n* in_chans !=3 support for scripts / loader\r\n* Adan optimizer\r\n* Can enable per-step LR scheduling via args\r\n* Dataset 'parsers' renamed to 'readers', more descriptive of purpose\r\n* AMP args changed, APEX via `--amp-impl apex`, bfloat16 supportedf via `--amp-dtype bfloat16`\r\n* main branch switched to 0.7.x version, 0.6x forked for stable release of weight only adds\r\n* master -> main branch rename", "tarball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/tarball/v0.9.1", "zipball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/zipball/v0.9.1", "html_url": "https://github.com/huggingface/pytorch-image-models/releases/tag/v0.9.1", "url": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/102814801", "release_id": 102814801, "date_created": "2023-05-12T16:47:47Z", "date_published": "2023-05-12T16:52:52Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/102806602", "tag": "v0.9.0", "name": "Release v0.9.0", "author": {"name": "rwightman", "type": "User"}, "description": "First non pre-release in a loooong while, changelog from 0.6.x below...\r\n\r\n\r\n### May 11, 2023\r\n* `timm` 0.9 released, transition from 0.8.xdev releases\r\n\r\n### May 10, 2023\r\n* Hugging Face Hub downloading is now default, 1132 models on https://huggingface.co/timm, 1163 weights in `timm`\r\n* DINOv2 vit feature backbone weights added thanks to [Leng Yue](https://github.com/leng-yue)\r\n* FB MAE vit feature backbone weights added\r\n* OpenCLIP DataComp-XL L/14 feat backbone weights added\r\n* MetaFormer (poolformer-v2, caformer, convformer, updated poolformer (v1)) w/ weights added by [Fredo Guan](https://github.com/fffffgggg54)\r\n* Experimental `get_intermediate_layers` function on vit/deit models for grabbing hidden states (inspired by DINO impl). This is WIP and may change significantly... feedback welcome.\r\n* Model creation throws error if `pretrained=True` and no weights exist (instead of continuing with random initialization)\r\n* Fix regression with inception / nasnet TF sourced weights with 1001 classes in original classifiers\r\n* bitsandbytes (https://github.com/TimDettmers/bitsandbytes) optimizers added to factory, use `bnb` prefix, ie `bnbadam8bit`\r\n* Misc cleanup and fixes\r\n* Final testing before switching to a 0.9 and bringing `timm` out of pre-release state\r\n\r\n### April 27, 2023\r\n* 97% of `timm` models uploaded to HF Hub and almost all updated to support multi-weight pretrained configs\r\n* Minor cleanup and refactoring of another batch of models as multi-weight added. More fused_attn (F.sdpa) and features_only support, and torchscript fixes.\r\n\r\n### April 21, 2023\r\n* Gradient accumulation support added to train script and tested (`--grad-accum-steps`), thanks [Taeksang Kim](https://github.com/voidbag)\r\n* More weights on HF Hub (cspnet, cait, volo, xcit, tresnet, hardcorenas, densenet, dpn, vovnet, xception_aligned)\r\n* Added `--head-init-scale` and `--head-init-bias` to train.py to scale classiifer head and set fixed bias for fine-tune\r\n* Remove all InplaceABN (`inplace_abn`) use, replaced use in tresnet with standard BatchNorm (modified weights accordingly). \r\n\r\n### April 12, 2023\r\n* Add ONNX export script, validate script, helpers that I've had kicking around for along time. Tweak 'same' padding for better export w/ recent ONNX + pytorch.\r\n* Refactor dropout args for vit and vit-like models, separate drop_rate into `drop_rate` (classifier dropout), `proj_drop_rate` (block mlp / out projections), `pos_drop_rate` (position embedding drop), `attn_drop_rate` (attention dropout). Also add patch dropout (FLIP) to vit and eva models.\r\n* fused F.scaled_dot_product_attention support to more vit models, add env var (TIMM_FUSED_ATTN) to control, and config interface to enable/disable\r\n* Add EVA-CLIP backbones w/ image tower weights, all the way up to 4B param 'enormous' model, and 336x336 OpenAI ViT mode that was missed.\r\n\r\n### April 5, 2023\r\n* ALL ResNet models pushed to Hugging Face Hub with multi-weight support\r\n  * All past `timm` trained weights added with recipe based tags to differentiate\r\n  * All ResNet strikes back A1/A2/A3 (seed 0) and R50 example B/C1/C2/D weights available\r\n  * Add torchvision v2 recipe weights to existing torchvision originals\r\n  * See comparison table in https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288#model-comparison\r\n* New ImageNet-12k + ImageNet-1k fine-tunes available for a few anti-aliased ResNet models\r\n  * `resnetaa50d.sw_in12k_ft_in1k` - 81.7 @ 224, 82.6 @ 288\r\n  * `resnetaa101d.sw_in12k_ft_in1k` - 83.5 @ 224, 84.1 @ 288\r\n  * `seresnextaa101d_32x8d.sw_in12k_ft_in1k` - 86.0 @ 224, 86.5 @ 288 \r\n  * `seresnextaa101d_32x8d.sw_in12k_ft_in1k_288` - 86.5 @ 288, 86.7 @ 320\r\n\r\n### March 31, 2023\r\n* Add first ConvNext-XXLarge CLIP -> IN-1k fine-tune and IN-12k intermediate fine-tunes for convnext-base/large CLIP models.\r\n\r\n| model                                                                                                                |top1  |top5  |img_size|param_count|gmacs |macts |\r\n|----------------------------------------------------------------------------------------------------------------------|------|------|--------|-----------|------|------|\r\n| [convnext_xxlarge.clip_laion2b_soup_ft_in1k](https://huggingface.co/timm/convnext_xxlarge.clip_laion2b_soup_ft_in1k) |88.612|98.704|256     |846.47     |198.09|124.45|\r\n| convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384                                                               |88.312|98.578|384     |200.13     |101.11|126.74|\r\n| convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320                                                               |87.968|98.47 |320     |200.13     |70.21 |88.02 |\r\n| convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384                                                                  |87.138|98.212|384     |88.59      |45.21 |84.49 |\r\n| convnext_base.clip_laion2b_augreg_ft_in12k_in1k                                                                      |86.344|97.97 |256     |88.59      |20.09 |37.55 |\r\n\r\n* Add EVA-02 MIM pretrained and fine-tuned weights, push to HF hub and update model cards for all EVA models. First model over 90% top-1 (99% top-5)! Check out the original code & weights at https://github.com/baaivision/EVA for more details on their work blending MIM, CLIP w/ many model, dataset, and train recipe tweaks.\r\n\r\n| model                                              |top1  |top5  |param_count|img_size|\r\n|----------------------------------------------------|------|------|-----------|--------|\r\n| [eva02_large_patch14_448.mim_m38m_ft_in22k_in1k](https://huggingface.co/timm/eva02_large_patch14_448.mim_m38m_ft_in1k) |90.054|99.042|305.08     |448     |\r\n| eva02_large_patch14_448.mim_in22k_ft_in22k_in1k    |89.946|99.01 |305.08     |448     |\r\n| eva_giant_patch14_560.m30m_ft_in22k_in1k           |89.792|98.992|1014.45    |560     |\r\n| eva02_large_patch14_448.mim_in22k_ft_in1k          |89.626|98.954|305.08     |448     |\r\n| eva02_large_patch14_448.mim_m38m_ft_in1k           |89.57 |98.918|305.08     |448     |\r\n| eva_giant_patch14_336.m30m_ft_in22k_in1k           |89.56 |98.956|1013.01    |336     |\r\n| eva_giant_patch14_336.clip_ft_in1k                 |89.466|98.82 |1013.01    |336     |\r\n| eva_large_patch14_336.in22k_ft_in22k_in1k          |89.214|98.854|304.53     |336     |\r\n| eva_giant_patch14_224.clip_ft_in1k                 |88.882|98.678|1012.56    |224     |\r\n| eva02_base_patch14_448.mim_in22k_ft_in22k_in1k     |88.692|98.722|87.12      |448     |\r\n| eva_large_patch14_336.in22k_ft_in1k                |88.652|98.722|304.53     |336     |\r\n| eva_large_patch14_196.in22k_ft_in22k_in1k          |88.592|98.656|304.14     |196     |\r\n| eva02_base_patch14_448.mim_in22k_ft_in1k           |88.23 |98.564|87.12      |448     |\r\n| eva_large_patch14_196.in22k_ft_in1k                |87.934|98.504|304.14     |196     |\r\n| eva02_small_patch14_336.mim_in22k_ft_in1k          |85.74 |97.614|22.13      |336     |\r\n| eva02_tiny_patch14_336.mim_in22k_ft_in1k           |80.658|95.524|5.76       |336     |\r\n\r\n* Multi-weight and HF hub for DeiT and MLP-Mixer based models\r\n\r\n### March 22, 2023\r\n* More weights pushed to HF hub along with multi-weight support, including: `regnet.py`, `rexnet.py`, `byobnet.py`, `resnetv2.py`, `swin_transformer.py`, `swin_transformer_v2.py`, `swin_transformer_v2_cr.py`\r\n* Swin Transformer models support feature extraction (NCHW feat maps for `swinv2_cr_*`, and NHWC for all others) and spatial embedding outputs.\r\n* FocalNet (from https://github.com/microsoft/FocalNet) models and weights added with significant refactoring, feature extraction, no fixed resolution / sizing constraint\r\n* RegNet weights increased with HF hub push, SWAG, SEER, and torchvision v2 weights. SEER is pretty poor wrt to performance for model size, but possibly useful.\r\n* More ImageNet-12k pretrained and 1k fine-tuned `timm` weights:\r\n  * `rexnetr_200.sw_in12k_ft_in1k` - 82.6 @ 224, 83.2 @ 288\r\n  * `rexnetr_300.sw_in12k_ft_in1k` - 84.0 @ 224, 84.5 @ 288\r\n  * `regnety_120.sw_in12k_ft_in1k` - 85.0 @ 224, 85.4 @ 288\r\n  * `regnety_160.lion_in12k_ft_in1k` - 85.6 @ 224, 86.0 @ 288\r\n  * `regnety_160.sw_in12k_ft_in1k` - 85.6 @ 224, 86.0 @ 288  (compare to SWAG PT + 1k FT this is same BUT much lower res, blows SEER FT away)\r\n* Model name deprecation + remapping functionality added (a milestone for bringing 0.8.x out of pre-release). Mappings being added...\r\n* Minor bug fixes and improvements.\r\n\r\n### Feb 26, 2023\r\n* Add ConvNeXt-XXLarge CLIP pretrained image tower weights for fine-tune & features (fine-tuning TBD) -- see [model card](https://huggingface.co/laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-soup)\r\n* Update `convnext_xxlarge` default LayerNorm eps to 1e-5 (for CLIP weights, improved stability)\r\n* 0.8.15dev0\r\n\r\n### Feb 20, 2023\r\n* Add 320x320 `convnext_large_mlp.clip_laion2b_ft_320` and `convnext_lage_mlp.clip_laion2b_ft_soup_320` CLIP image tower weights for features & fine-tune\r\n* 0.8.13dev0 pypi release for latest changes w/ move to huggingface org\r\n\r\n### Feb 16, 2023\r\n* `safetensor` checkpoint support added\r\n* Add ideas from 'Scaling Vision Transformers to 22 B. Params' (https://arxiv.org/abs/2302.05442) -- qk norm, RmsNorm, parallel block\r\n* Add F.scaled_dot_product_attention support (PyTorch 2.0 only) to `vit_*`, `vit_relpos*`, `coatnet` / `maxxvit` (to start)\r\n* Lion optimizer (w/ multi-tensor option) added (https://arxiv.org/abs/2302.06675)\r\n* gradient checkpointing works with `features_only=True`\r\n\r\n### Feb 7, 2023\r\n* New inference benchmark numbers added in [results](results/) folder.\r\n* Add convnext LAION CLIP trained weights and initial set of in1k fine-tunes\r\n  * `convnext_base.clip_laion2b_augreg_ft_in1k` - 86.2% @ 256x256\r\n  * `convnext_base.clip_laiona_augreg_ft_in1k_384` - 86.5% @ 384x384\r\n  * `convnext_large_mlp.clip_laion2b_augreg_ft_in1k` - 87.3% @ 256x256\r\n  * `convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384` - 87.9% @ 384x384\r\n* Add DaViT models. Supports `features_only=True`. Adapted from https://github.com/dingmyu/davit by [Fredo](https://github.com/fffffgggg54).\r\n* Use a common NormMlpClassifierHead across MaxViT, ConvNeXt, DaViT\r\n* Add EfficientFormer-V2 model, update EfficientFormer, and refactor LeViT (closely related architectures). Weights on HF hub.\r\n  * New EfficientFormer-V2 arch, significant refactor from original at (https://github.com/snap-research/EfficientFormer). Supports `features_only=True`.\r\n  * Minor updates to EfficientFormer.\r\n  * Refactor LeViT models to stages, add `features_only=True` support to new `conv` variants, weight remap required.\r\n* Move ImageNet meta-data (synsets, indices) from `/results` to [`timm/data/_info`](timm/data/_info/).\r\n* Add ImageNetInfo / DatasetInfo classes to provide labelling for various ImageNet classifier layouts in `timm`\r\n  * Update `inference.py` to use, try: `python inference.py /folder/to/images --model convnext_small.in12k --label-type detail --topk 5`\r\n* Ready for 0.8.10 pypi pre-release (final testing).\r\n\r\n### Jan 20, 2023\r\n* Add two convnext 12k -> 1k fine-tunes at 384x384\r\n  * `convnext_tiny.in12k_ft_in1k_384` - 85.1 @ 384\r\n  * `convnext_small.in12k_ft_in1k_384` - 86.2 @ 384\r\n\r\n* Push all MaxxViT weights to HF hub, and add new ImageNet-12k -> 1k fine-tunes for `rw` base MaxViT and CoAtNet 1/2 models\r\n\r\n|model                                                                                                                   |top1 |top5 |samples / sec  |Params (M)     |GMAC  |Act (M)|\r\n|------------------------------------------------------------------------------------------------------------------------|----:|----:|--------------:|--------------:|-----:|------:|\r\n|[maxvit_xlarge_tf_512.in21k_ft_in1k](https://huggingface.co/timm/maxvit_xlarge_tf_512.in21k_ft_in1k)                    |88.53|98.64|          21.76|         475.77|534.14|1413.22|\r\n|[maxvit_xlarge_tf_384.in21k_ft_in1k](https://huggingface.co/timm/maxvit_xlarge_tf_384.in21k_ft_in1k)                    |88.32|98.54|          42.53|         475.32|292.78| 668.76|\r\n|[maxvit_base_tf_512.in21k_ft_in1k](https://huggingface.co/timm/maxvit_base_tf_512.in21k_ft_in1k)                        |88.20|98.53|          50.87|         119.88|138.02| 703.99|\r\n|[maxvit_large_tf_512.in21k_ft_in1k](https://huggingface.co/timm/maxvit_large_tf_512.in21k_ft_in1k)                      |88.04|98.40|          36.42|         212.33|244.75| 942.15|\r\n|[maxvit_large_tf_384.in21k_ft_in1k](https://huggingface.co/timm/maxvit_large_tf_384.in21k_ft_in1k)                      |87.98|98.56|          71.75|         212.03|132.55| 445.84|\r\n|[maxvit_base_tf_384.in21k_ft_in1k](https://huggingface.co/timm/maxvit_base_tf_384.in21k_ft_in1k)                        |87.92|98.54|         104.71|         119.65| 73.80| 332.90|\r\n|[maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k](https://huggingface.co/timm/maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k)        |87.81|98.37|         106.55|         116.14| 70.97| 318.95|\r\n|[maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k](https://huggingface.co/timm/maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k)  |87.47|98.37|         149.49|         116.09| 72.98| 213.74|\r\n|[coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k](https://huggingface.co/timm/coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k)            |87.39|98.31|         160.80|          73.88| 47.69| 209.43|\r\n|[maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k](https://huggingface.co/timm/maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k)        |86.89|98.02|         375.86|         116.14| 23.15|  92.64|\r\n|[maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k](https://huggingface.co/timm/maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k)  |86.64|98.02|         501.03|         116.09| 24.20|  62.77|\r\n|[maxvit_base_tf_512.in1k](https://huggingface.co/timm/maxvit_base_tf_512.in1k)                                          |86.60|97.92|          50.75|         119.88|138.02| 703.99|\r\n|[coatnet_2_rw_224.sw_in12k_ft_in1k](https://huggingface.co/timm/coatnet_2_rw_224.sw_in12k_ft_in1k)                      |86.57|97.89|         631.88|          73.87| 15.09|  49.22|\r\n|[maxvit_large_tf_512.in1k](https://huggingface.co/timm/maxvit_large_tf_512.in1k)                                        |86.52|97.88|          36.04|         212.33|244.75| 942.15|\r\n|[coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k](https://huggingface.co/timm/coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k)            |86.49|97.90|         620.58|          73.88| 15.18|  54.78|\r\n|[maxvit_base_tf_384.in1k](https://huggingface.co/timm/maxvit_base_tf_384.in1k)                                          |86.29|97.80|         101.09|         119.65| 73.80| 332.90|\r\n|[maxvit_large_tf_384.in1k](https://huggingface.co/timm/maxvit_large_tf_384.in1k)                                        |86.23|97.69|          70.56|         212.03|132.55| 445.84|\r\n|[maxvit_small_tf_512.in1k](https://huggingface.co/timm/maxvit_small_tf_512.in1k)                                        |86.10|97.76|          88.63|          69.13| 67.26| 383.77|\r\n|[maxvit_tiny_tf_512.in1k](https://huggingface.co/timm/maxvit_tiny_tf_512.in1k)                                          |85.67|97.58|         144.25|          31.05| 33.49| 257.59|\r\n|[maxvit_small_tf_384.in1k](https://huggingface.co/timm/maxvit_small_tf_384.in1k)                                        |85.54|97.46|         188.35|          69.02| 35.87| 183.65|\r\n|[maxvit_tiny_tf_384.in1k](https://huggingface.co/timm/maxvit_tiny_tf_384.in1k)                                          |85.11|97.38|         293.46|          30.98| 17.53| 123.42|\r\n|[maxvit_large_tf_224.in1k](https://huggingface.co/timm/maxvit_large_tf_224.in1k)                                        |84.93|96.97|         247.71|         211.79| 43.68| 127.35|\r\n|[coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k](https://huggingface.co/timm/coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k)          |84.90|96.96|        1025.45|          41.72|  8.11|  40.13|\r\n|[maxvit_base_tf_224.in1k](https://huggingface.co/timm/maxvit_base_tf_224.in1k)                                          |84.85|96.99|         358.25|         119.47| 24.04|  95.01|\r\n|[maxxvit_rmlp_small_rw_256.sw_in1k](https://huggingface.co/timm/maxxvit_rmlp_small_rw_256.sw_in1k)                      |84.63|97.06|         575.53|          66.01| 14.67|  58.38|\r\n|[coatnet_rmlp_2_rw_224.sw_in1k](https://huggingface.co/timm/coatnet_rmlp_2_rw_224.sw_in1k)                              |84.61|96.74|         625.81|          73.88| 15.18|  54.78|\r\n|[maxvit_rmlp_small_rw_224.sw_in1k](https://huggingface.co/timm/maxvit_rmlp_small_rw_224.sw_in1k)                        |84.49|96.76|         693.82|          64.90| 10.75|  49.30|\r\n|[maxvit_small_tf_224.in1k](https://huggingface.co/timm/maxvit_small_tf_224.in1k)                                        |84.43|96.83|         647.96|          68.93| 11.66|  53.17|\r\n|[maxvit_rmlp_tiny_rw_256.sw_in1k](https://huggingface.co/timm/maxvit_rmlp_tiny_rw_256.sw_in1k)                          |84.23|96.78|         807.21|          29.15|  6.77|  46.92|\r\n|[coatnet_1_rw_224.sw_in1k](https://huggingface.co/timm/coatnet_1_rw_224.sw_in1k)                                        |83.62|96.38|         989.59|          41.72|  8.04|  34.60|\r\n|[maxvit_tiny_rw_224.sw_in1k](https://huggingface.co/timm/maxvit_tiny_rw_224.sw_in1k)                                    |83.50|96.50|        1100.53|          29.06|  5.11|  33.11|\r\n|[maxvit_tiny_tf_224.in1k](https://huggingface.co/timm/maxvit_tiny_tf_224.in1k)                                          |83.41|96.59|        1004.94|          30.92|  5.60|  35.78|\r\n|[coatnet_rmlp_1_rw_224.sw_in1k](https://huggingface.co/timm/coatnet_rmlp_1_rw_224.sw_in1k)                              |83.36|96.45|        1093.03|          41.69|  7.85|  35.47|\r\n|[maxxvitv2_nano_rw_256.sw_in1k](https://huggingface.co/timm/maxxvitv2_nano_rw_256.sw_in1k)                              |83.11|96.33|        1276.88|          23.70|  6.26|  23.05|\r\n|[maxxvit_rmlp_nano_rw_256.sw_in1k](https://huggingface.co/timm/maxxvit_rmlp_nano_rw_256.sw_in1k)                        |83.03|96.34|        1341.24|          16.78|  4.37|  26.05|\r\n|[maxvit_rmlp_nano_rw_256.sw_in1k](https://huggingface.co/timm/maxvit_rmlp_nano_rw_256.sw_in1k)                          |82.96|96.26|        1283.24|          15.50|  4.47|  31.92|\r\n|[maxvit_nano_rw_256.sw_in1k](https://huggingface.co/timm/maxvit_nano_rw_256.sw_in1k)                                    |82.93|96.23|        1218.17|          15.45|  4.46|  30.28|\r\n|[coatnet_bn_0_rw_224.sw_in1k](https://huggingface.co/timm/coatnet_bn_0_rw_224.sw_in1k)                                  |82.39|96.19|        1600.14|          27.44|  4.67|  22.04|\r\n|[coatnet_0_rw_224.sw_in1k](https://huggingface.co/timm/coatnet_0_rw_224.sw_in1k)                                        |82.39|95.84|        1831.21|          27.44|  4.43|  18.73|\r\n|[coatnet_rmlp_nano_rw_224.sw_in1k](https://huggingface.co/timm/coatnet_rmlp_nano_rw_224.sw_in1k)                        |82.05|95.87|        2109.09|          15.15|  2.62|  20.34|\r\n|[coatnext_nano_rw_224.sw_in1k](https://huggingface.co/timm/coatnext_nano_rw_224.sw_in1k)                                |81.95|95.92|        2525.52|          14.70|  2.47|  12.80|\r\n|[coatnet_nano_rw_224.sw_in1k](https://huggingface.co/timm/coatnet_nano_rw_224.sw_in1k)                                  |81.70|95.64|        2344.52|          15.14|  2.41|  15.41|\r\n|[maxvit_rmlp_pico_rw_256.sw_in1k](https://huggingface.co/timm/maxvit_rmlp_pico_rw_256.sw_in1k)                          |80.53|95.21|        1594.71|           7.52|  1.85|  24.86|\r\n\r\n### Jan 11, 2023\r\n* Update ConvNeXt ImageNet-12k pretrain series w/ two new fine-tuned weights (and pre FT `.in12k` tags)\r\n  * `convnext_nano.in12k_ft_in1k` - 82.3 @ 224, 82.9 @ 288  (previously released)\r\n  * `convnext_tiny.in12k_ft_in1k` - 84.2 @ 224, 84.5 @ 288\r\n  * `convnext_small.in12k_ft_in1k` - 85.2 @ 224, 85.3 @ 288\r\n\r\n### Jan 6, 2023\r\n* Finally got around to adding `--model-kwargs` and `--opt-kwargs` to scripts to pass through rare args directly to model classes from cmd line\r\n  * `train.py /imagenet --model resnet50 --amp --model-kwargs output_stride=16 act_layer=silu`\r\n  * `train.py /imagenet --model vit_base_patch16_clip_224 --img-size 240 --amp --model-kwargs img_size=240 patch_size=12`\r\n* Cleanup some popular models to better support arg passthrough / merge with model configs, more to go.\r\n\r\n### Jan 5, 2023\r\n* ConvNeXt-V2 models and weights added to existing `convnext.py`\r\n  * Paper: [ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders](http://arxiv.org/abs/2301.00808)\r\n  * Reference impl: https://github.com/facebookresearch/ConvNeXt-V2 (NOTE: weights currently CC-BY-NC)\r\n\r\n### Dec 23, 2022 \ud83c\udf84\u2603\r\n* Add FlexiViT models and weights from https://github.com/google-research/big_vision (check out paper at https://arxiv.org/abs/2212.08013)\r\n  * NOTE currently resizing is static on model creation, on-the-fly dynamic / train patch size sampling is a WIP\r\n* Many more models updated to multi-weight and downloadable via HF hub now (convnext, efficientnet, mobilenet, vision_transformer*, beit)\r\n* More model pretrained tag and adjustments, some model names changed (working on deprecation translations, consider main branch DEV branch right now, use 0.6.x for stable use)\r\n* More ImageNet-12k (subset of 22k) pretrain models popping up:\r\n  * `efficientnet_b5.in12k_ft_in1k` - 85.9 @ 448x448\r\n  * `vit_medium_patch16_gap_384.in12k_ft_in1k` - 85.5 @ 384x384\r\n  * `vit_medium_patch16_gap_256.in12k_ft_in1k` - 84.5 @ 256x256\r\n  * `convnext_nano.in12k_ft_in1k` - 82.9 @ 288x288\r\n\r\n### Dec 8, 2022\r\n* Add 'EVA l' to `vision_transformer.py`, MAE style ViT-L/14 MIM pretrain w/ EVA-CLIP targets, FT on ImageNet-1k (w/ ImageNet-22k intermediate for some)\r\n  * original source: https://github.com/baaivision/EVA\r\n\r\n| model                                     | top1 | param_count |  gmac | macts | hub                                     |\r\n|:------------------------------------------|-----:|------------:|------:|------:|:----------------------------------------|\r\n| eva_large_patch14_336.in22k_ft_in22k_in1k | 89.2 |       304.5 | 191.1 | 270.2 | [link](https://huggingface.co/BAAI/EVA) |\r\n| eva_large_patch14_336.in22k_ft_in1k       | 88.7 |       304.5 | 191.1 | 270.2 | [link](https://huggingface.co/BAAI/EVA) |\r\n| eva_large_patch14_196.in22k_ft_in22k_in1k | 88.6 |       304.1 |  61.6 |  63.5 | [link](https://huggingface.co/BAAI/EVA) |\r\n| eva_large_patch14_196.in22k_ft_in1k       | 87.9 |       304.1 |  61.6 |  63.5 | [link](https://huggingface.co/BAAI/EVA) |\r\n\r\n### Dec 6, 2022\r\n* Add 'EVA g', BEiT style ViT-g/14 model weights w/ both MIM pretrain and CLIP pretrain to `beit.py`.\r\n  * original source: https://github.com/baaivision/EVA\r\n  * paper: https://arxiv.org/abs/2211.07636\r\n\r\n| model                                    |   top1 |   param_count |   gmac |   macts | hub                                     |\r\n|:-----------------------------------------|-------:|--------------:|-------:|--------:|:----------------------------------------|\r\n| eva_giant_patch14_560.m30m_ft_in22k_in1k |   89.8 |        1014.4 | 1906.8 |  2577.2 | [link](https://huggingface.co/BAAI/EVA) |\r\n| eva_giant_patch14_336.m30m_ft_in22k_in1k |   89.6 |        1013   |  620.6 |   550.7 | [link](https://huggingface.co/BAAI/EVA) |\r\n| eva_giant_patch14_336.clip_ft_in1k       |   89.4 |        1013   |  620.6 |   550.7 | [link](https://huggingface.co/BAAI/EVA) |\r\n| eva_giant_patch14_224.clip_ft_in1k       |   89.1 |        1012.6 |  267.2 |   192.6 | [link](https://huggingface.co/BAAI/EVA) |\r\n\r\n### Dec 5, 2022\r\n\r\n* Pre-release (`0.8.0dev0`) of multi-weight support (`model_arch.pretrained_tag`). Install with `pip install --pre timm`\r\n  * vision_transformer, maxvit, convnext are the first three model impl w/ support\r\n  * model names are changing with this (previous _21k, etc. fn will merge), still sorting out deprecation handling\r\n  * bugs are likely, but I need feedback so please try it out\r\n  * if stability is needed, please use 0.6.x pypi releases or clone from [0.6.x branch](https://github.com/rwightman/pytorch-image-models/tree/0.6.x)\r\n* Support for PyTorch 2.0 compile is added in train/validate/inference/benchmark, use `--torchcompile` argument\r\n* Inference script allows more control over output, select k for top-class index + prob json, csv or parquet output\r\n* Add a full set of fine-tuned CLIP image tower weights from both LAION-2B and original OpenAI CLIP models\r\n\r\n| model                                            |   top1 |   param_count |   gmac |   macts | hub                                                                                  |\r\n|:-------------------------------------------------|-------:|--------------:|-------:|--------:|:-------------------------------------------------------------------------------------|\r\n| vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k  |   88.6 |         632.5 |  391   |   407.5 | [link](https://huggingface.co/timm/vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k)  |\r\n| vit_large_patch14_clip_336.openai_ft_in12k_in1k  |   88.3 |         304.5 |  191.1 |   270.2 | [link](https://huggingface.co/timm/vit_large_patch14_clip_336.openai_ft_in12k_in1k)  |\r\n| vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k  |   88.2 |         632   |  167.4 |   139.4 | [link](https://huggingface.co/timm/vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k)  |\r\n| vit_large_patch14_clip_336.laion2b_ft_in12k_in1k |   88.2 |         304.5 |  191.1 |   270.2 | [link](https://huggingface.co/timm/vit_large_patch14_clip_336.laion2b_ft_in12k_in1k) |\r\n| vit_large_patch14_clip_224.openai_ft_in12k_in1k  |   88.2 |         304.2 |   81.1 |    88.8 | [link](https://huggingface.co/timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k)  |\r\n| vit_large_patch14_clip_224.laion2b_ft_in12k_in1k |   87.9 |         304.2 |   81.1 |    88.8 | [link](https://huggingface.co/timm/vit_large_patch14_clip_224.laion2b_ft_in12k_in1k) |\r\n| vit_large_patch14_clip_224.openai_ft_in1k        |   87.9 |         304.2 |   81.1 |    88.8 | [link](https://huggingface.co/timm/vit_large_patch14_clip_224.openai_ft_in1k)        |\r\n| vit_large_patch14_clip_336.laion2b_ft_in1k       |   87.9 |         304.5 |  191.1 |   270.2 | [link](https://huggingface.co/timm/vit_large_patch14_clip_336.laion2b_ft_in1k)       |\r\n| vit_huge_patch14_clip_224.laion2b_ft_in1k        |   87.6 |         632   |  167.4 |   139.4 | [link](https://huggingface.co/timm/vit_huge_patch14_clip_224.laion2b_ft_in1k)        |\r\n| vit_large_patch14_clip_224.laion2b_ft_in1k       |   87.3 |         304.2 |   81.1 |    88.8 | [link](https://huggingface.co/timm/vit_large_patch14_clip_224.laion2b_ft_in1k)       |\r\n| vit_base_patch16_clip_384.laion2b_ft_in12k_in1k  |   87.2 |          86.9 |   55.5 |   101.6 | [link](https://huggingface.co/timm/vit_base_patch16_clip_384.laion2b_ft_in12k_in1k)  |\r\n| vit_base_patch16_clip_384.openai_ft_in12k_in1k   |   87   |          86.9 |   55.5 |   101.6 | [link](https://huggingface.co/timm/vit_base_patch16_clip_384.openai_ft_in12k_in1k)   |\r\n| vit_base_patch16_clip_384.laion2b_ft_in1k        |   86.6 |          86.9 |   55.5 |   101.6 | [link](https://huggingface.co/timm/vit_base_patch16_clip_384.laion2b_ft_in1k)        |\r\n| vit_base_patch16_clip_384.openai_ft_in1k         |   86.2 |          86.9 |   55.5 |   101.6 | [link](https://huggingface.co/timm/vit_base_patch16_clip_384.openai_ft_in1k)         |\r\n| vit_base_patch16_clip_224.laion2b_ft_in12k_in1k  |   86.2 |          86.6 |   17.6 |    23.9 | [link](https://huggingface.co/timm/vit_base_patch16_clip_224.laion2b_ft_in12k_in1k)  |\r\n| vit_base_patch16_clip_224.openai_ft_in12k_in1k   |   85.9 |          86.6 |   17.6 |    23.9 | [link](https://huggingface.co/timm/vit_base_patch16_clip_224.openai_ft_in12k_in1k)   |\r\n| vit_base_patch32_clip_448.laion2b_ft_in12k_in1k  |   85.8 |          88.3 |   17.9 |    23.9 | [link](https://huggingface.co/timm/vit_base_patch32_clip_448.laion2b_ft_in12k_in1k)  |\r\n| vit_base_patch16_clip_224.laion2b_ft_in1k        |   85.5 |          86.6 |   17.6 |    23.9 | [link](https://huggingface.co/timm/vit_base_patch16_clip_224.laion2b_ft_in1k)        |\r\n| vit_base_patch32_clip_384.laion2b_ft_in12k_in1k  |   85.4 |          88.3 |   13.1 |    16.5 | [link](https://huggingface.co/timm/vit_base_patch32_clip_384.laion2b_ft_in12k_in1k)  |\r\n| vit_base_patch16_clip_224.openai_ft_in1k         |   85.3 |          86.6 |   17.6 |    23.9 | [link](https://huggingface.co/timm/vit_base_patch16_clip_224.openai_ft_in1k)         |\r\n| vit_base_patch32_clip_384.openai_ft_in12k_in1k   |   85.2 |          88.3 |   13.1 |    16.5 | [link](https://huggingface.co/timm/vit_base_patch32_clip_384.openai_ft_in12k_in1k)   |\r\n| vit_base_patch32_clip_224.laion2b_ft_in12k_in1k  |   83.3 |          88.2 |    4.4 |     5   | [link](https://huggingface.co/timm/vit_base_patch32_clip_224.laion2b_ft_in12k_in1k)  |\r\n| vit_base_patch32_clip_224.laion2b_ft_in1k        |   82.6 |          88.2 |    4.4 |     5   | [link](https://huggingface.co/timm/vit_base_patch32_clip_224.laion2b_ft_in1k)        |\r\n| vit_base_patch32_clip_224.openai_ft_in1k         |   81.9 |          88.2 |    4.4 |     5   | [link](https://huggingface.co/timm/vit_base_patch32_clip_224.openai_ft_in1k)         |\r\n\r\n* Port of MaxViT Tensorflow Weights from official impl at https://github.com/google-research/maxvit\r\n  * There was larger than expected drops for the upscaled 384/512 in21k fine-tune weights, possible detail missing, but the 21k FT did seem sensitive to small preprocessing\r\n\r\n| model                              |   top1 |   param_count |   gmac |   macts | hub                                                                    |\r\n|:-----------------------------------|-------:|--------------:|-------:|--------:|:-----------------------------------------------------------------------|\r\n| maxvit_xlarge_tf_512.in21k_ft_in1k |   88.5 |         475.8 |  534.1 |  1413.2 | [link](https://huggingface.co/timm/maxvit_xlarge_tf_512.in21k_ft_in1k) |\r\n| maxvit_xlarge_tf_384.in21k_ft_in1k |   88.3 |         475.3 |  292.8 |   668.8 | [link](https://huggingface.co/timm/maxvit_xlarge_tf_384.in21k_ft_in1k) |\r\n| maxvit_base_tf_512.in21k_ft_in1k   |   88.2 |         119.9 |  138   |   704   | [link](https://huggingface.co/timm/maxvit_base_tf_512.in21k_ft_in1k)   |\r\n| maxvit_large_tf_512.in21k_ft_in1k  |   88   |         212.3 |  244.8 |   942.2 | [link](https://huggingface.co/timm/maxvit_large_tf_512.in21k_ft_in1k)  |\r\n| maxvit_large_tf_384.in21k_ft_in1k  |   88   |         212   |  132.6 |   445.8 | [link](https://huggingface.co/timm/maxvit_large_tf_384.in21k_ft_in1k)  |\r\n| maxvit_base_tf_384.in21k_ft_in1k   |   87.9 |         119.6 |   73.8 |   332.9 | [link](https://huggingface.co/timm/maxvit_base_tf_384.in21k_ft_in1k)   |\r\n| maxvit_base_tf_512.in1k            |   86.6 |         119.9 |  138   |   704   | [link](https://huggingface.co/timm/maxvit_base_tf_512.in1k)            |\r\n| maxvit_large_tf_512.in1k           |   86.5 |         212.3 |  244.8 |   942.2 | [link](https://huggingface.co/timm/maxvit_large_tf_512.in1k)           |\r\n| maxvit_base_tf_384.in1k            |   86.3 |         119.6 |   73.8 |   332.9 | [link](https://huggingface.co/timm/maxvit_base_tf_384.in1k)            |\r\n| maxvit_large_tf_384.in1k           |   86.2 |         212   |  132.6 |   445.8 | [link](https://huggingface.co/timm/maxvit_large_tf_384.in1k)           |\r\n| maxvit_small_tf_512.in1k           |   86.1 |          69.1 |   67.3 |   383.8 | [link](https://huggingface.co/timm/maxvit_small_tf_512.in1k)           |\r\n| maxvit_tiny_tf_512.in1k            |   85.7 |          31   |   33.5 |   257.6 | [link](https://huggingface.co/timm/maxvit_tiny_tf_512.in1k)            |\r\n| maxvit_small_tf_384.in1k           |   85.5 |          69   |   35.9 |   183.6 | [link](https://huggingface.co/timm/maxvit_small_tf_384.in1k)           |\r\n| maxvit_tiny_tf_384.in1k            |   85.1 |          31   |   17.5 |   123.4 | [link](https://huggingface.co/timm/maxvit_tiny_tf_384.in1k)            |\r\n| maxvit_large_tf_224.in1k           |   84.9 |         211.8 |   43.7 |   127.4 | [link](https://huggingface.co/timm/maxvit_large_tf_224.in1k)           |\r\n| maxvit_base_tf_224.in1k            |   84.9 |         119.5 |   24   |    95   | [link](https://huggingface.co/timm/maxvit_base_tf_224.in1k)            |\r\n| maxvit_small_tf_224.in1k           |   84.4 |          68.9 |   11.7 |    53.2 | [link](https://huggingface.co/timm/maxvit_small_tf_224.in1k)           |\r\n| maxvit_tiny_tf_224.in1k            |   83.4 |          30.9 |    5.6 |    35.8 | [link](https://huggingface.co/timm/maxvit_tiny_tf_224.in1k)            |\r\n\r\n### Oct 15, 2022\r\n* Train and validation script enhancements\r\n* Non-GPU (ie CPU) device support\r\n* SLURM compatibility for train script\r\n* HF datasets support (via ReaderHfds)\r\n* TFDS/WDS dataloading improvements (sample padding/wrap for distributed use fixed wrt sample count estimate)\r\n* in_chans !=3 support for scripts / loader\r\n* Adan optimizer\r\n* Can enable per-step LR scheduling via args\r\n* Dataset 'parsers' renamed to 'readers', more descriptive of purpose\r\n* AMP args changed, APEX via `--amp-impl apex`, bfloat16 supportedf via `--amp-dtype bfloat16`\r\n* main branch switched to 0.7.x version, 0.6x forked for stable release of weight only adds\r\n* master -> main branch rename", "tarball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/tarball/v0.9.0", "zipball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/zipball/v0.9.0", "html_url": "https://github.com/huggingface/pytorch-image-models/releases/tag/v0.9.0", "url": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/102806602", "release_id": 102806602, "date_created": "2023-05-11T22:32:01Z", "date_published": "2023-05-12T15:34:41Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/99664840", "tag": "v0.6.13", "name": "Release v0.6.13", "author": {"name": "rwightman", "type": "User"}, "description": "Release from 0.6.x stable branch with fix for Python 3.11. NOTE original 0.6.13 release tag was against wrong branch.", "tarball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/tarball/v0.6.13", "zipball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/zipball/v0.6.13", "html_url": "https://github.com/huggingface/pytorch-image-models/releases/tag/v0.6.13", "url": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/99664840", "release_id": 99664840, "date_created": "2023-03-24T00:55:24Z", "date_published": "2023-04-16T15:27:19Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/96738760", "tag": "v0.8.17dev0", "name": "Release v0.8.17dev0", "author": {"name": "rwightman", "type": "User"}, "description": "\r\n### March 22, 2023\r\n* More weights pushed to HF hub along with multi-weight support, including: `regnet.py`, `rexnet.py`, `byobnet.py`, `resnetv2.py`, `swin_transformer.py`, `swin_transformer_v2.py`, `swin_transformer_v2_cr.py`\r\n* Swin Transformer models support feature extraction (NCHW feat maps for `swinv2_cr_*`, and NHWC for all others) and spatial embedding outputs.\r\n* FocalNet (from https://github.com/microsoft/FocalNet) models and weights added with significant refactoring, feature extraction, no fixed resolution / sizing constraint\r\n* RegNet weights increased with HF hub push, SWAG, SEER, and torchvision v2 weights. SEER is pretty poor wrt to performance for model size, but possibly useful.\r\n* More ImageNet-12k pretrained and 1k fine-tuned `timm` weights:\r\n  * `rexnetr_200.sw_in12k_ft_in1k` - 82.6 @ 224, 83.2 @ 288\r\n  * `rexnetr_300.sw_in12k_ft_in1k` - 84.0 @ 224, 84.5 @ 288\r\n  * `regnety_120.sw_in12k_ft_in1k` - 85.0 @ 224, 85.4 @ 288\r\n  * `regnety_160.lion_in12k_ft_in1k` - 85.6 @ 224, 86.0 @ 288\r\n  * `regnety_160.sw_in12k_ft_in1k` - 85.6 @ 224, 86.0 @ 288  (compare to SWAG PT + 1k FT this is same BUT much lower res, blows SEER FT away)\r\n* Model name deprecation + remapping functionality added (a milestone for bringing 0.8.x out of pre-release). Mappings being added...\r\n* Minor bug fixes and improvements.\r\n\r\n### Feb 26, 2023\r\n* Add ConvNeXt-XXLarge CLIP pretrained image tower weights for fine-tune & features (fine-tuning TBD) -- see [model card](https://huggingface.co/laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-soup)\r\n* Update `convnext_xxlarge` default LayerNorm eps to 1e-5 (for CLIP weights, improved stability)\r\n* 0.8.15dev0", "tarball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/tarball/v0.8.17dev0", "zipball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/zipball/v0.8.17dev0", "html_url": "https://github.com/huggingface/pytorch-image-models/releases/tag/v0.8.17dev0", "url": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/96738760", "release_id": 96738760, "date_created": "2023-03-22T22:40:23Z", "date_published": "2023-03-24T00:59:22Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/93061900", "tag": "v0.8.13dev0", "name": "v0.8.13dev0 Release", "author": {"name": "rwightman", "type": "User"}, "description": "\r\n### Feb 20, 2023\r\n* Add 320x320 `convnext_large_mlp.clip_laion2b_ft_320` and `convnext_lage_mlp.clip_laion2b_ft_soup_320` CLIP image tower weights for features & fine-tune\r\n* 0.8.13dev0 pypi release for latest changes w/ move to huggingface org\r\n\r\n### Feb 16, 2023\r\n* `safetensor` checkpoint support added\r\n* Add ideas from 'Scaling Vision Transformers to 22 B. Params' (https://arxiv.org/abs/2302.05442) -- qk norm, RmsNorm, parallel block\r\n* Add F.scaled_dot_product_attention support (PyTorch 2.0 only) to `vit_*`, `vit_relpos*`, `coatnet` / `maxxvit` (to start)\r\n* Lion optimizer (w/ multi-tensor option) added (https://arxiv.org/abs/2302.06675)", "tarball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/tarball/v0.8.13dev0", "zipball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/zipball/v0.8.13dev0", "html_url": "https://github.com/huggingface/pytorch-image-models/releases/tag/v0.8.13dev0", "url": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/93061900", "release_id": 93061900, "date_created": "2023-02-20T18:26:09Z", "date_published": "2023-02-20T18:26:36Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/91670709", "tag": "v0.8.10dev0", "name": "v0.8.10dev0 Release", "author": {"name": "rwightman", "type": "User"}, "description": "\r\n### Feb 7, 2023\r\n* New inference benchmark numbers added in [results](results/) folder.\r\n* Add convnext LAION CLIP trained weights and initial set of in1k fine-tunes\r\n  * `convnext_base.clip_laion2b_augreg_ft_in1k` - 86.2% @ 256x256\r\n  * `convnext_base.clip_laiona_augreg_ft_in1k_384` - 86.5% @ 384x384\r\n  * `convnext_large_mlp.clip_laion2b_augreg_ft_in1k` - 87.3% @ 256x256\r\n  * `convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384` - 87.9% @ 384x384\r\n* Add DaViT models. Supports `features_only=True`. Adapted from https://github.com/dingmyu/davit by [Fredo](https://github.com/fffffgggg54).\r\n* Use a common NormMlpClassifierHead across MaxViT, ConvNeXt, DaViT\r\n* Add EfficientFormer-V2 model, update EfficientFormer, and refactor LeViT (closely related architectures). Weights on HF hub.\r\n  * New EfficientFormer-V2 arch, significant refactor from original at (https://github.com/snap-research/EfficientFormer). Supports `features_only=True`.\r\n  * Minor updates to EfficientFormer.\r\n  * Refactor LeViT models to stages, add `features_only=True` support to new `conv` variants, weight remap required.\r\n* Move ImageNet meta-data (synsets, indices) from `/results` to [`timm/data/_info`](timm/data/_info/).\r\n* Add ImageNetInfo / DatasetInfo classes to provide labelling for various ImageNet classifier layouts in `timm`\r\n  * Update `inference.py` to use, try: `python inference.py /folder/to/images --model convnext_small.in12k --label-type detail --topk 5`\r\n* Ready for 0.8.10 pypi pre-release (final testing).\r\n\r\n### Jan 20, 2023\r\n* Add two convnext 12k -> 1k fine-tunes at 384x384\r\n  * `convnext_tiny.in12k_ft_in1k_384` - 85.1 @ 384\r\n  * `convnext_small.in12k_ft_in1k_384` - 86.2 @ 384\r\n\r\n* Push all MaxxViT weights to HF hub, and add new ImageNet-12k -> 1k fine-tunes for `rw` base MaxViT and CoAtNet 1/2 models\r\n\r\n|model                                                                                                                   |top1 |top5 |samples / sec  |Params (M)     |GMAC  |Act (M)|\r\n|------------------------------------------------------------------------------------------------------------------------|----:|----:|--------------:|--------------:|-----:|------:|\r\n|[maxvit_xlarge_tf_512.in21k_ft_in1k](https://huggingface.co/timm/maxvit_xlarge_tf_512.in21k_ft_in1k)                    |88.53|98.64|          21.76|         475.77|534.14|1413.22|\r\n|[maxvit_xlarge_tf_384.in21k_ft_in1k](https://huggingface.co/timm/maxvit_xlarge_tf_384.in21k_ft_in1k)                    |88.32|98.54|          42.53|         475.32|292.78| 668.76|\r\n|[maxvit_base_tf_512.in21k_ft_in1k](https://huggingface.co/timm/maxvit_base_tf_512.in21k_ft_in1k)                        |88.20|98.53|          50.87|         119.88|138.02| 703.99|\r\n|[maxvit_large_tf_512.in21k_ft_in1k](https://huggingface.co/timm/maxvit_large_tf_512.in21k_ft_in1k)                      |88.04|98.40|          36.42|         212.33|244.75| 942.15|\r\n|[maxvit_large_tf_384.in21k_ft_in1k](https://huggingface.co/timm/maxvit_large_tf_384.in21k_ft_in1k)                      |87.98|98.56|          71.75|         212.03|132.55| 445.84|\r\n|[maxvit_base_tf_384.in21k_ft_in1k](https://huggingface.co/timm/maxvit_base_tf_384.in21k_ft_in1k)                        |87.92|98.54|         104.71|         119.65| 73.80| 332.90|\r\n|[maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k](https://huggingface.co/timm/maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k)        |87.81|98.37|         106.55|         116.14| 70.97| 318.95|\r\n|[maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k](https://huggingface.co/timm/maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k)  |87.47|98.37|         149.49|         116.09| 72.98| 213.74|\r\n|[coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k](https://huggingface.co/timm/coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k)            |87.39|98.31|         160.80|          73.88| 47.69| 209.43|\r\n|[maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k](https://huggingface.co/timm/maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k)        |86.89|98.02|         375.86|         116.14| 23.15|  92.64|\r\n|[maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k](https://huggingface.co/timm/maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k)  |86.64|98.02|         501.03|         116.09| 24.20|  62.77|\r\n|[maxvit_base_tf_512.in1k](https://huggingface.co/timm/maxvit_base_tf_512.in1k)                                          |86.60|97.92|          50.75|         119.88|138.02| 703.99|\r\n|[coatnet_2_rw_224.sw_in12k_ft_in1k](https://huggingface.co/timm/coatnet_2_rw_224.sw_in12k_ft_in1k)                      |86.57|97.89|         631.88|          73.87| 15.09|  49.22|\r\n|[maxvit_large_tf_512.in1k](https://huggingface.co/timm/maxvit_large_tf_512.in1k)                                        |86.52|97.88|          36.04|         212.33|244.75| 942.15|\r\n|[coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k](https://huggingface.co/timm/coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k)            |86.49|97.90|         620.58|          73.88| 15.18|  54.78|\r\n|[maxvit_base_tf_384.in1k](https://huggingface.co/timm/maxvit_base_tf_384.in1k)                                          |86.29|97.80|         101.09|         119.65| 73.80| 332.90|\r\n|[maxvit_large_tf_384.in1k](https://huggingface.co/timm/maxvit_large_tf_384.in1k)                                        |86.23|97.69|          70.56|         212.03|132.55| 445.84|\r\n|[maxvit_small_tf_512.in1k](https://huggingface.co/timm/maxvit_small_tf_512.in1k)                                        |86.10|97.76|          88.63|          69.13| 67.26| 383.77|\r\n|[maxvit_tiny_tf_512.in1k](https://huggingface.co/timm/maxvit_tiny_tf_512.in1k)                                          |85.67|97.58|         144.25|          31.05| 33.49| 257.59|\r\n|[maxvit_small_tf_384.in1k](https://huggingface.co/timm/maxvit_small_tf_384.in1k)                                        |85.54|97.46|         188.35|          69.02| 35.87| 183.65|\r\n|[maxvit_tiny_tf_384.in1k](https://huggingface.co/timm/maxvit_tiny_tf_384.in1k)                                          |85.11|97.38|         293.46|          30.98| 17.53| 123.42|\r\n|[maxvit_large_tf_224.in1k](https://huggingface.co/timm/maxvit_large_tf_224.in1k)                                        |84.93|96.97|         247.71|         211.79| 43.68| 127.35|\r\n|[coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k](https://huggingface.co/timm/coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k)          |84.90|96.96|        1025.45|          41.72|  8.11|  40.13|\r\n|[maxvit_base_tf_224.in1k](https://huggingface.co/timm/maxvit_base_tf_224.in1k)                                          |84.85|96.99|         358.25|         119.47| 24.04|  95.01|\r\n|[maxxvit_rmlp_small_rw_256.sw_in1k](https://huggingface.co/timm/maxxvit_rmlp_small_rw_256.sw_in1k)                      |84.63|97.06|         575.53|          66.01| 14.67|  58.38|\r\n|[coatnet_rmlp_2_rw_224.sw_in1k](https://huggingface.co/timm/coatnet_rmlp_2_rw_224.sw_in1k)                              |84.61|96.74|         625.81|          73.88| 15.18|  54.78|\r\n|[maxvit_rmlp_small_rw_224.sw_in1k](https://huggingface.co/timm/maxvit_rmlp_small_rw_224.sw_in1k)                        |84.49|96.76|         693.82|          64.90| 10.75|  49.30|\r\n|[maxvit_small_tf_224.in1k](https://huggingface.co/timm/maxvit_small_tf_224.in1k)                                        |84.43|96.83|         647.96|          68.93| 11.66|  53.17|\r\n|[maxvit_rmlp_tiny_rw_256.sw_in1k](https://huggingface.co/timm/maxvit_rmlp_tiny_rw_256.sw_in1k)                          |84.23|96.78|         807.21|          29.15|  6.77|  46.92|\r\n|[coatnet_1_rw_224.sw_in1k](https://huggingface.co/timm/coatnet_1_rw_224.sw_in1k)                                        |83.62|96.38|         989.59|          41.72|  8.04|  34.60|\r\n|[maxvit_tiny_rw_224.sw_in1k](https://huggingface.co/timm/maxvit_tiny_rw_224.sw_in1k)                                    |83.50|96.50|        1100.53|          29.06|  5.11|  33.11|\r\n|[maxvit_tiny_tf_224.in1k](https://huggingface.co/timm/maxvit_tiny_tf_224.in1k)                                          |83.41|96.59|        1004.94|          30.92|  5.60|  35.78|\r\n|[coatnet_rmlp_1_rw_224.sw_in1k](https://huggingface.co/timm/coatnet_rmlp_1_rw_224.sw_in1k)                              |83.36|96.45|        1093.03|          41.69|  7.85|  35.47|\r\n|[maxxvitv2_nano_rw_256.sw_in1k](https://huggingface.co/timm/maxxvitv2_nano_rw_256.sw_in1k)                              |83.11|96.33|        1276.88|          23.70|  6.26|  23.05|\r\n|[maxxvit_rmlp_nano_rw_256.sw_in1k](https://huggingface.co/timm/maxxvit_rmlp_nano_rw_256.sw_in1k)                        |83.03|96.34|        1341.24|          16.78|  4.37|  26.05|\r\n|[maxvit_rmlp_nano_rw_256.sw_in1k](https://huggingface.co/timm/maxvit_rmlp_nano_rw_256.sw_in1k)                          |82.96|96.26|        1283.24|          15.50|  4.47|  31.92|\r\n|[maxvit_nano_rw_256.sw_in1k](https://huggingface.co/timm/maxvit_nano_rw_256.sw_in1k)                                    |82.93|96.23|        1218.17|          15.45|  4.46|  30.28|\r\n|[coatnet_bn_0_rw_224.sw_in1k](https://huggingface.co/timm/coatnet_bn_0_rw_224.sw_in1k)                                  |82.39|96.19|        1600.14|          27.44|  4.67|  22.04|\r\n|[coatnet_0_rw_224.sw_in1k](https://huggingface.co/timm/coatnet_0_rw_224.sw_in1k)                                        |82.39|95.84|        1831.21|          27.44|  4.43|  18.73|\r\n|[coatnet_rmlp_nano_rw_224.sw_in1k](https://huggingface.co/timm/coatnet_rmlp_nano_rw_224.sw_in1k)                        |82.05|95.87|        2109.09|          15.15|  2.62|  20.34|\r\n|[coatnext_nano_rw_224.sw_in1k](https://huggingface.co/timm/coatnext_nano_rw_224.sw_in1k)                                |81.95|95.92|        2525.52|          14.70|  2.47|  12.80|\r\n|[coatnet_nano_rw_224.sw_in1k](https://huggingface.co/timm/coatnet_nano_rw_224.sw_in1k)                                  |81.70|95.64|        2344.52|          15.14|  2.41|  15.41|\r\n|[maxvit_rmlp_pico_rw_256.sw_in1k](https://huggingface.co/timm/maxvit_rmlp_pico_rw_256.sw_in1k)                          |80.53|95.21|        1594.71|           7.52|  1.85|  24.86|\r\n", "tarball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/tarball/v0.8.10dev0", "zipball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/zipball/v0.8.10dev0", "html_url": "https://github.com/huggingface/pytorch-image-models/releases/tag/v0.8.10dev0", "url": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/91670709", "release_id": 91670709, "date_created": "2023-02-07T07:46:26Z", "date_published": "2023-02-07T22:37:54Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/88677694", "tag": "v0.8.6dev0", "name": "v0.8.6dev0 Release", "author": {"name": "rwightman", "type": "User"}, "description": "### Jan 11, 2023\r\n* Update ConvNeXt ImageNet-12k pretrain series w/ two new fine-tuned weights (and pre FT `.in12k` tags)\r\n  * `convnext_nano.in12k_ft_in1k` - 82.3 @ 224, 82.9 @ 288  (previously released)\r\n  * `convnext_tiny.in12k_ft_in1k` - 84.2 @ 224, 84.5 @ 288\r\n  * `convnext_small.in12k_ft_in1k` - 85.2 @ 224, 85.3 @ 288\r\n\r\n### Jan 6, 2023\r\n* Finally got around to adding `--model-kwargs` and `--opt-kwargs` to scripts to pass through rare args directly to model classes from cmd line\r\n  * `train.py /imagenet --model resnet50 --amp --model-kwargs output_stride=16 act_layer=silu`\r\n  * `train.py /imagenet --model vit_base_patch16_clip_224 --img-size 240 --amp --model-kwargs img_size=240 patch_size=12`\r\n* Cleanup some popular models to better support arg passthrough / merge with model configs, more to go. \r\n\r\n### Jan 5, 2023\r\n* ConvNeXt-V2 models and weights added to existing `convnext.py`\r\n  * Paper: [ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders](http://arxiv.org/abs/2301.00808)\r\n  * Reference impl: https://github.com/facebookresearch/ConvNeXt-V2 (NOTE: weights currently CC-BY-NC)\r\n", "tarball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/tarball/v0.8.6dev0", "zipball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/zipball/v0.8.6dev0", "html_url": "https://github.com/huggingface/pytorch-image-models/releases/tag/v0.8.6dev0", "url": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/88677694", "release_id": 88677694, "date_created": "2023-01-11T22:50:39Z", "date_published": "2023-01-12T05:36:53Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/87131525", "tag": "v0.8.2dev0", "name": "v0.8.2dev0 Release", "author": {"name": "rwightman", "type": "User"}, "description": "Part way through the conversion of models to multi-weight support (`model_arch.pretrain_tag`), module reorg for future building, and lots of new weights and model additions as we go...\r\n\r\nThis is considered a development release. Please stick to 0.6.x if you need stability. Some of the model names, tags will shift a bit, some old names have already been deprecated and remapping support not added yet. For code 0.6.x branch is considered 'stable' https://github.com/rwightman/pytorch-image-models/tree/0.6.x\r\n\r\n### Dec 23, 2022 \ud83c\udf84\u2603\r\n* Add FlexiViT models and weights from https://github.com/google-research/big_vision (check out paper at https://arxiv.org/abs/2212.08013)\r\n  * NOTE currently resizing is static on model creation, on-the-fly dynamic / train patch size sampling is a WIP\r\n* Many more models updated to multi-weight and downloadable via HF hub now (convnext, efficientnet, mobilenet, vision_transformer*, beit)\r\n* More model pretrained tag and adjustments, some model names changed (working on deprecation translations, consider main branch DEV branch right now, use 0.6.x for stable use)\r\n* More ImageNet-12k (subset of 22k) pretrain models popping up:\r\n  * `efficientnet_b5.in12k_ft_in1k` - 85.9 @ 448x448\r\n  * `vit_medium_patch16_gap_384.in12k_ft_in1k` - 85.5 @ 384x384\r\n  * `vit_medium_patch16_gap_256.in12k_ft_in1k` - 84.5 @ 256x256\r\n  * `convnext_nano.in12k_ft_in1k` - 82.9 @ 288x288\r\n\r\n### Dec 8, 2022\r\n* Add 'EVA l' to `vision_transformer.py`, MAE style ViT-L/14 MIM pretrain w/ EVA-CLIP targets, FT on ImageNet-1k (w/ ImageNet-22k intermediate for some)\r\n  * original source: https://github.com/baaivision/EVA\r\n\r\n| model                                     | top1 | param_count |  gmac | macts | hub                                     |\r\n|:------------------------------------------|-----:|------------:|------:|------:|:----------------------------------------|\r\n| eva_large_patch14_336.in22k_ft_in22k_in1k | 89.2 |       304.5 | 191.1 | 270.2 | [link](https://huggingface.co/BAAI/EVA) |\r\n| eva_large_patch14_336.in22k_ft_in1k       | 88.7 |       304.5 | 191.1 | 270.2 | [link](https://huggingface.co/BAAI/EVA) |\r\n| eva_large_patch14_196.in22k_ft_in22k_in1k | 88.6 |       304.1 |  61.6 |  63.5 | [link](https://huggingface.co/BAAI/EVA) |\r\n| eva_large_patch14_196.in22k_ft_in1k       | 87.9 |       304.1 |  61.6 |  63.5 | [link](https://huggingface.co/BAAI/EVA) |\r\n\r\n### Dec 6, 2022\r\n* Add 'EVA g', BEiT style ViT-g/14 model weights w/ both MIM pretrain and CLIP pretrain to `beit.py`. \r\n  * original source: https://github.com/baaivision/EVA\r\n  * paper: https://arxiv.org/abs/2211.07636\r\n\r\n| model                                    |   top1 |   param_count |   gmac |   macts | hub                                     |\r\n|:-----------------------------------------|-------:|--------------:|-------:|--------:|:----------------------------------------|\r\n| eva_giant_patch14_560.m30m_ft_in22k_in1k |   89.8 |        1014.4 | 1906.8 |  2577.2 | [link](https://huggingface.co/BAAI/EVA) |\r\n| eva_giant_patch14_336.m30m_ft_in22k_in1k |   89.6 |        1013   |  620.6 |   550.7 | [link](https://huggingface.co/BAAI/EVA) |\r\n| eva_giant_patch14_336.clip_ft_in1k       |   89.4 |        1013   |  620.6 |   550.7 | [link](https://huggingface.co/BAAI/EVA) |\r\n| eva_giant_patch14_224.clip_ft_in1k       |   89.1 |        1012.6 |  267.2 |   192.6 | [link](https://huggingface.co/BAAI/EVA) |\r\n\r\n### Dec 5, 2022\r\n\r\n* Pre-release (`0.8.0dev0`) of multi-weight support (`model_arch.pretrained_tag`). Install with `pip install --pre timm`\r\n  * vision_transformer, maxvit, convnext are the first three model impl w/ support\r\n  * model names are changing with this (previous _21k, etc. fn will merge), still sorting out deprecation handling\r\n  * bugs are likely, but I need feedback so please try it out\r\n  * if stability is needed, please use 0.6.x pypi releases or clone from [0.6.x branch](https://github.com/rwightman/pytorch-image-models/tree/0.6.x)\r\n* Support for PyTorch 2.0 compile is added in train/validate/inference/benchmark, use `--torchcompile` argument\r\n* Inference script allows more control over output, select k for top-class index + prob json, csv or parquet output\r\n* Add a full set of fine-tuned CLIP image tower weights from both LAION-2B and original OpenAI CLIP models\r\n\r\n| model                                            |   top1 |   param_count |   gmac |   macts | hub                                                                                  |\r\n|:-------------------------------------------------|-------:|--------------:|-------:|--------:|:-------------------------------------------------------------------------------------|\r\n| vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k  |   88.6 |         632.5 |  391   |   407.5 | [link](https://huggingface.co/timm/vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k)  |\r\n| vit_large_patch14_clip_336.openai_ft_in12k_in1k  |   88.3 |         304.5 |  191.1 |   270.2 | [link](https://huggingface.co/timm/vit_large_patch14_clip_336.openai_ft_in12k_in1k)  |\r\n| vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k  |   88.2 |         632   |  167.4 |   139.4 | [link](https://huggingface.co/timm/vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k)  |\r\n| vit_large_patch14_clip_336.laion2b_ft_in12k_in1k |   88.2 |         304.5 |  191.1 |   270.2 | [link](https://huggingface.co/timm/vit_large_patch14_clip_336.laion2b_ft_in12k_in1k) |\r\n| vit_large_patch14_clip_224.openai_ft_in12k_in1k  |   88.2 |         304.2 |   81.1 |    88.8 | [link](https://huggingface.co/timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k)  |\r\n| vit_large_patch14_clip_224.laion2b_ft_in12k_in1k |   87.9 |         304.2 |   81.1 |    88.8 | [link](https://huggingface.co/timm/vit_large_patch14_clip_224.laion2b_ft_in12k_in1k) |\r\n| vit_large_patch14_clip_224.openai_ft_in1k        |   87.9 |         304.2 |   81.1 |    88.8 | [link](https://huggingface.co/timm/vit_large_patch14_clip_224.openai_ft_in1k)        |\r\n| vit_large_patch14_clip_336.laion2b_ft_in1k       |   87.9 |         304.5 |  191.1 |   270.2 | [link](https://huggingface.co/timm/vit_large_patch14_clip_336.laion2b_ft_in1k)       |\r\n| vit_huge_patch14_clip_224.laion2b_ft_in1k        |   87.6 |         632   |  167.4 |   139.4 | [link](https://huggingface.co/timm/vit_huge_patch14_clip_224.laion2b_ft_in1k)        |\r\n| vit_large_patch14_clip_224.laion2b_ft_in1k       |   87.3 |         304.2 |   81.1 |    88.8 | [link](https://huggingface.co/timm/vit_large_patch14_clip_224.laion2b_ft_in1k)       |\r\n| vit_base_patch16_clip_384.laion2b_ft_in12k_in1k  |   87.2 |          86.9 |   55.5 |   101.6 | [link](https://huggingface.co/timm/vit_base_patch16_clip_384.laion2b_ft_in12k_in1k)  |\r\n| vit_base_patch16_clip_384.openai_ft_in12k_in1k   |   87   |          86.9 |   55.5 |   101.6 | [link](https://huggingface.co/timm/vit_base_patch16_clip_384.openai_ft_in12k_in1k)   |\r\n| vit_base_patch16_clip_384.laion2b_ft_in1k        |   86.6 |          86.9 |   55.5 |   101.6 | [link](https://huggingface.co/timm/vit_base_patch16_clip_384.laion2b_ft_in1k)        |\r\n| vit_base_patch16_clip_384.openai_ft_in1k         |   86.2 |          86.9 |   55.5 |   101.6 | [link](https://huggingface.co/timm/vit_base_patch16_clip_384.openai_ft_in1k)         |\r\n| vit_base_patch16_clip_224.laion2b_ft_in12k_in1k  |   86.2 |          86.6 |   17.6 |    23.9 | [link](https://huggingface.co/timm/vit_base_patch16_clip_224.laion2b_ft_in12k_in1k)  |\r\n| vit_base_patch16_clip_224.openai_ft_in12k_in1k   |   85.9 |          86.6 |   17.6 |    23.9 | [link](https://huggingface.co/timm/vit_base_patch16_clip_224.openai_ft_in12k_in1k)   |\r\n| vit_base_patch32_clip_448.laion2b_ft_in12k_in1k  |   85.8 |          88.3 |   17.9 |    23.9 | [link](https://huggingface.co/timm/vit_base_patch32_clip_448.laion2b_ft_in12k_in1k)  |\r\n| vit_base_patch16_clip_224.laion2b_ft_in1k        |   85.5 |          86.6 |   17.6 |    23.9 | [link](https://huggingface.co/timm/vit_base_patch16_clip_224.laion2b_ft_in1k)        |\r\n| vit_base_patch32_clip_384.laion2b_ft_in12k_in1k  |   85.4 |          88.3 |   13.1 |    16.5 | [link](https://huggingface.co/timm/vit_base_patch32_clip_384.laion2b_ft_in12k_in1k)  |\r\n| vit_base_patch16_clip_224.openai_ft_in1k         |   85.3 |          86.6 |   17.6 |    23.9 | [link](https://huggingface.co/timm/vit_base_patch16_clip_224.openai_ft_in1k)         |\r\n| vit_base_patch32_clip_384.openai_ft_in12k_in1k   |   85.2 |          88.3 |   13.1 |    16.5 | [link](https://huggingface.co/timm/vit_base_patch32_clip_384.openai_ft_in12k_in1k)   |\r\n| vit_base_patch32_clip_224.laion2b_ft_in12k_in1k  |   83.3 |          88.2 |    4.4 |     5   | [link](https://huggingface.co/timm/vit_base_patch32_clip_224.laion2b_ft_in12k_in1k)  |\r\n| vit_base_patch32_clip_224.laion2b_ft_in1k        |   82.6 |          88.2 |    4.4 |     5   | [link](https://huggingface.co/timm/vit_base_patch32_clip_224.laion2b_ft_in1k)        |\r\n| vit_base_patch32_clip_224.openai_ft_in1k         |   81.9 |          88.2 |    4.4 |     5   | [link](https://huggingface.co/timm/vit_base_patch32_clip_224.openai_ft_in1k)         |\r\n\r\n* Port of MaxViT Tensorflow Weights from official impl at https://github.com/google-research/maxvit\r\n  * There was larger than expected drops for the upscaled 384/512 in21k fine-tune weights, possible detail missing, but the 21k FT did seem sensitive to small preprocessing\r\n\r\n| model                              |   top1 |   param_count |   gmac |   macts | hub                                                                    |\r\n|:-----------------------------------|-------:|--------------:|-------:|--------:|:-----------------------------------------------------------------------|\r\n| maxvit_xlarge_tf_512.in21k_ft_in1k |   88.5 |         475.8 |  534.1 |  1413.2 | [link](https://huggingface.co/timm/maxvit_xlarge_tf_512.in21k_ft_in1k) |\r\n| maxvit_xlarge_tf_384.in21k_ft_in1k |   88.3 |         475.3 |  292.8 |   668.8 | [link](https://huggingface.co/timm/maxvit_xlarge_tf_384.in21k_ft_in1k) |\r\n| maxvit_base_tf_512.in21k_ft_in1k   |   88.2 |         119.9 |  138   |   704   | [link](https://huggingface.co/timm/maxvit_base_tf_512.in21k_ft_in1k)   |\r\n| maxvit_large_tf_512.in21k_ft_in1k  |   88   |         212.3 |  244.8 |   942.2 | [link](https://huggingface.co/timm/maxvit_large_tf_512.in21k_ft_in1k)  |\r\n| maxvit_large_tf_384.in21k_ft_in1k  |   88   |         212   |  132.6 |   445.8 | [link](https://huggingface.co/timm/maxvit_large_tf_384.in21k_ft_in1k)  |\r\n| maxvit_base_tf_384.in21k_ft_in1k   |   87.9 |         119.6 |   73.8 |   332.9 | [link](https://huggingface.co/timm/maxvit_base_tf_384.in21k_ft_in1k)   |\r\n| maxvit_base_tf_512.in1k            |   86.6 |         119.9 |  138   |   704   | [link](https://huggingface.co/timm/maxvit_base_tf_512.in1k)            |\r\n| maxvit_large_tf_512.in1k           |   86.5 |         212.3 |  244.8 |   942.2 | [link](https://huggingface.co/timm/maxvit_large_tf_512.in1k)           |\r\n| maxvit_base_tf_384.in1k            |   86.3 |         119.6 |   73.8 |   332.9 | [link](https://huggingface.co/timm/maxvit_base_tf_384.in1k)            |\r\n| maxvit_large_tf_384.in1k           |   86.2 |         212   |  132.6 |   445.8 | [link](https://huggingface.co/timm/maxvit_large_tf_384.in1k)           |\r\n| maxvit_small_tf_512.in1k           |   86.1 |          69.1 |   67.3 |   383.8 | [link](https://huggingface.co/timm/maxvit_small_tf_512.in1k)           |\r\n| maxvit_tiny_tf_512.in1k            |   85.7 |          31   |   33.5 |   257.6 | [link](https://huggingface.co/timm/maxvit_tiny_tf_512.in1k)            |\r\n| maxvit_small_tf_384.in1k           |   85.5 |          69   |   35.9 |   183.6 | [link](https://huggingface.co/timm/maxvit_small_tf_384.in1k)           |\r\n| maxvit_tiny_tf_384.in1k            |   85.1 |          31   |   17.5 |   123.4 | [link](https://huggingface.co/timm/maxvit_tiny_tf_384.in1k)            |\r\n| maxvit_large_tf_224.in1k           |   84.9 |         211.8 |   43.7 |   127.4 | [link](https://huggingface.co/timm/maxvit_large_tf_224.in1k)           |\r\n| maxvit_base_tf_224.in1k            |   84.9 |         119.5 |   24   |    95   | [link](https://huggingface.co/timm/maxvit_base_tf_224.in1k)            |\r\n| maxvit_small_tf_224.in1k           |   84.4 |          68.9 |   11.7 |    53.2 | [link](https://huggingface.co/timm/maxvit_small_tf_224.in1k)           |\r\n| maxvit_tiny_tf_224.in1k            |   83.4 |          30.9 |    5.6 |    35.8 | [link](https://huggingface.co/timm/maxvit_tiny_tf_224.in1k)            |\r\n\r\n### Oct 15, 2022\r\n* Train and validation script enhancements\r\n* Non-GPU (ie CPU) device support\r\n* SLURM compatibility for train script\r\n* HF datasets support (via ReaderHfds)\r\n* TFDS/WDS dataloading improvements (sample padding/wrap for distributed use fixed wrt sample count estimate)\r\n* in_chans !=3 support for scripts / loader\r\n* Adan optimizer\r\n* Can enable per-step LR scheduling via args\r\n* Dataset 'parsers' renamed to 'readers', more descriptive of purpose\r\n* AMP args changed, APEX via `--amp-impl apex`, bfloat16 supportedf via `--amp-dtype bfloat16`\r\n* main branch switched to 0.7.x version, 0.6x forked for stable release of weight only adds\r\n* master -> main branch rename", "tarball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/tarball/v0.8.2dev0", "zipball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/zipball/v0.8.2dev0", "html_url": "https://github.com/huggingface/pytorch-image-models/releases/tag/v0.8.2dev0", "url": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/87131525", "release_id": 87131525, "date_created": "2022-12-23T23:20:43Z", "date_published": "2022-12-24T00:19:44Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/84042745", "tag": "v0.6.12", "name": "v0.6.12 Release", "author": {"name": "rwightman", "type": "User"}, "description": "Minor bug fixes to HF push_to_hub, plus some more MaxVit weights\r\n\r\n### Oct 10, 2022\r\n* More weights in `maxxvit` series, incl first ConvNeXt block based `coatnext` and `maxxvit` experiments:\r\n  * `coatnext_nano_rw_224` - 82.0 @ 224 (G) -- (uses ConvNeXt conv block, no BatchNorm)\r\n  * `maxxvit_rmlp_nano_rw_256` - 83.0 @ 256, 83.7 @ 320  (G) (uses ConvNeXt conv block, no BN)\r\n  * `maxvit_rmlp_small_rw_224` - 84.5 @ 224, 85.1 @ 320 (G)\r\n  * `maxxvit_rmlp_small_rw_256` - 84.6 @ 256, 84.9 @ 288 (G) -- could be trained better, hparams need tuning (uses ConvNeXt block, no BN)\r\n  * `coatnet_rmlp_2_rw_224` - 84.6 @ 224, 85 @ 320  (T)\r\n", "tarball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/tarball/v0.6.12", "zipball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/zipball/v0.6.12", "html_url": "https://github.com/huggingface/pytorch-image-models/releases/tag/v0.6.12", "url": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/84042745", "release_id": 84042745, "date_created": "2022-11-23T18:29:06Z", "date_published": "2022-11-23T22:11:26Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/78890176", "tag": "v0.6.11", "name": "v0.6.11 Release", "author": {"name": "rwightman", "type": "User"}, "description": "\r\n## Changes Since 0.6.7\r\n\r\n### Sept 23, 2022 \r\n* CLIP LAION-2B pretrained B/32, L/14, H/14, and g/14 image tower weights as vit models (for fine-tune)\r\n\r\n### Sept 7, 2022\r\n* Hugging Face [`timm` docs](https://huggingface.co/docs/hub/timm) home now exists, look for more here in the future\r\n* Add BEiT-v2 weights for base and large 224x224 models from https://github.com/microsoft/unilm/tree/master/beit2\r\n* Add more weights in `maxxvit` series incl a `pico` (7.5M params, 1.9 GMACs), two `tiny` variants:\r\n  * `maxvit_rmlp_pico_rw_256` - 80.5 @ 256, 81.3 @ 320  (T)\r\n  * `maxvit_tiny_rw_224` - 83.5 @ 224 (G)\r\n  * `maxvit_rmlp_tiny_rw_256` - 84.2 @ 256, 84.8 @ 320 (T)\r\n\r\n### Aug 29, 2022\r\n* MaxVit window size scales with img_size by default. Add new RelPosMlp MaxViT weight that leverages this:\r\n  * `maxvit_rmlp_nano_rw_256` - 83.0 @ 256, 83.6 @ 320  (T)\r\n\r\n### Aug 26, 2022\r\n* CoAtNet (https://arxiv.org/abs/2106.04803) and MaxVit (https://arxiv.org/abs/2204.01697) `timm` original models\r\n  * both found in [`maxxvit.py`](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/maxxvit.py) model def, contains numerous experiments outside scope of original papers\r\n  * an unfinished Tensorflow version from MaxVit authors can be found https://github.com/google-research/maxvit\r\n* Initial CoAtNet and MaxVit timm pretrained weights (working on more):\r\n  * `coatnet_nano_rw_224` - 81.7 @ 224  (T)\r\n  * `coatnet_rmlp_nano_rw_224` - 82.0 @ 224, 82.8 @ 320 (T)\r\n  * `coatnet_0_rw_224` - 82.4  (T)  -- NOTE timm '0' coatnets have 2 more 3rd stage blocks\r\n  * `coatnet_bn_0_rw_224` - 82.4  (T)\r\n  * `maxvit_nano_rw_256` - 82.9 @ 256  (T)\r\n  * `coatnet_rmlp_1_rw_224` - 83.4 @ 224, 84 @ 320  (T)\r\n  * `coatnet_1_rw_224` - 83.6 @ 224 (G) \r\n  * (T) = TPU trained with `bits_and_tpu` branch training code, (G) = GPU trained\r\n* GCVit (weights adapted from https://github.com/NVlabs/GCVit, code 100% `timm` re-write for license purposes)\r\n* MViT-V2 (multi-scale vit, adapted from https://github.com/facebookresearch/mvit)\r\n* EfficientFormer (adapted from https://github.com/snap-research/EfficientFormer)\r\n* PyramidVisionTransformer-V2 (adapted from https://github.com/whai362/PVT)\r\n* 'Fast Norm' support for LayerNorm and GroupNorm that avoids float32 upcast w/ AMP (uses APEX LN if available for further boost)\r\n\r\n\r\n### Aug 15, 2022\r\n* ConvNeXt atto weights added\r\n  * `convnext_atto` - 75.7 @ 224, 77.0 @ 288\r\n  * `convnext_atto_ols` - 75.9  @ 224, 77.2 @ 288\r\n\r\n### Aug 5, 2022\r\n* More custom ConvNeXt smaller model defs with weights \r\n  * `convnext_femto` - 77.5 @ 224, 78.7 @ 288\r\n  * `convnext_femto_ols` - 77.9  @ 224, 78.9 @ 288\r\n  * `convnext_pico` - 79.5 @ 224, 80.4 @ 288\r\n  * `convnext_pico_ols` - 79.5 @ 224, 80.5 @ 288\r\n  * `convnext_nano_ols` - 80.9 @ 224, 81.6 @ 288\r\n* Updated EdgeNeXt to improve ONNX export, add new base variant and weights from original (https://github.com/mmaaz60/EdgeNeXt)\r\n\r\n### July 28, 2022\r\n* Add freshly minted DeiT-III Medium (width=512, depth=12, num_heads=8) model weights. Thanks [Hugo Touvron](https://github.com/TouvronHugo)!\r\n", "tarball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/tarball/v0.6.11", "zipball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/zipball/v0.6.11", "html_url": "https://github.com/huggingface/pytorch-image-models/releases/tag/v0.6.11", "url": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/78890176", "release_id": 78890176, "date_created": "2022-09-27T20:26:34Z", "date_published": "2022-10-03T21:44:35Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/75258307", "tag": "v0.1-weights-maxx", "name": "MaxxVit (CoAtNet, MaxVit, and related experimental weights)", "author": {"name": "rwightman", "type": "User"}, "description": "# CoAtNet (https://arxiv.org/abs/2106.04803) and MaxVit (https://arxiv.org/abs/2204.01697) `timm` trained weights\r\n\r\nWeights were created reproducing the paper architectures and exploring timm sepcific additions such as ConvNeXt blocks, parallel partitioning, and other experiments.\r\n\r\nWeights were trained on a mix of TPU and GPU systems. Bulk of weights were trained on TPU via the TRC program (https://sites.research.google/trc/about/). \r\n\r\nCoAtNet variants run particularly well on TPU, it's a great combination. MaxVit is better suited to GPU due to the window partitioning, although there are some optimizations that can be made to improve TPU padding/utilization incl using 256x256 image size (8, 8) windo/grid size, and keeping format in NCHW for partition attention when using PyTorch XLA.\r\n\r\nGlossary:\r\n* `coatnet` - CoAtNet (MBConv + transformer blocks)\r\n* `coatnext` - CoAtNet w/ ConvNeXt conv blocks\r\n* `maxvit` - MaxViT (MBConv + block (ala swin) and grid partioning transformer blocks)\r\n* `maxxvit` - MaxViT w/ ConvNeXt conv blocks \r\n* `rmlp` - relative position embedding w/ MLP (can be resized) -- if this isn't in model name, it's using relative position bias (ala swin)\r\n* `rw` - my variations on the model, slight differences in sizing / pooling / etc from Google paper spec\r\n\r\nResults:\r\n* `maxvit_rmlp_pico_rw_256` - 80.5 @ 256, 81.3 @ 320  (T)\r\n* `coatnet_nano_rw_224` - 81.7 @ 224  (T)\r\n* `coatnext_nano_rw_224` - 82.0 @ 224 (G) -- (uses convnext block, no BatchNorm)\r\n* `coatnet_rmlp_nano_rw_224` - 82.0 @ 224, 82.8 @ 320 (T)\r\n* `coatnet_0_rw_224` - 82.4  (T)  -- NOTE timm '0' coatnets have 2 more 3rd stage blocks\r\n* `coatnet_bn_0_rw_224` - 82.4  (T) -- all BatchNorm, no LayerNorm\r\n* `maxvit_nano_rw_256` - 82.9 @ 256  (T)\r\n* `maxvit_rmlp_nano_rw_256` - 83.0 @ 256, 83.6 @ 320  (T)\r\n* `maxxvit_rmlp_nano_rw_256` - 83.0 @ 256, 83.7 @ 320  (G) (uses convnext conv block, no BatchNorm)\r\n* `coatnet_rmlp_1_rw_224` - 83.4 @ 224, 84 @ 320  (T)\r\n* `maxvit_tiny_rw_224` - 83.5 @ 224 (G)\r\n* `coatnet_1_rw_224` - 83.6 @ 224 (G)\r\n* `maxvit_rmlp_tiny_rw_256` - 84.2 @ 256, 84.8 @ 320 (T)\r\n* `maxvit_rmlp_small_rw_224` - 84.5 @ 224, 85.1 @ 320 (G)\r\n* `maxxvit_rmlp_small_rw_256` - 84.6 @ 256, 84.9 @ 288 (G) -- could be trained better, hparms need tuning (uses convnext conv block, no BN)\r\n* `coatnet_rmlp_2_rw_224` - 84.6 @ 224, 85 @ 320  (T)\r\n\r\n(T) = TPU trained with `bits_and_tpu` branch training code, (G) = GPU trained\r\n", "tarball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/tarball/v0.1-weights-maxx", "zipball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/zipball/v0.1-weights-maxx", "html_url": "https://github.com/huggingface/pytorch-image-models/releases/tag/v0.1-weights-maxx", "url": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/75258307", "release_id": 75258307, "date_created": "2022-08-24T18:01:20Z", "date_published": "2022-08-24T18:03:56Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/74653799", "tag": "v0.1-weights-morevit", "name": "More 3rd party ViT / ViT-hybrid weights", "author": {"name": "rwightman", "type": "User"}, "description": "# More weights for 3rd party ViT / ViT-CNN hybrids that needed remapping / re-hosting\r\n\r\n## EfficientFormer\r\n\r\nRehosted and remaped checkpoints from https://github.com/snap-research/EfficientFormer (originals in Google Drive)\r\n\r\n## GCViT\r\n\r\nHeavily remaped from originals at https://github.com/NVlabs/GCVit due to from-scratch re-write of model code\r\n\r\nNOTE: these checkpoints have a non-commercial [CC-BY-NC-SA-4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license.\r\n", "tarball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/tarball/v0.1-weights-morevit", "zipball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/zipball/v0.1-weights-morevit", "html_url": "https://github.com/huggingface/pytorch-image-models/releases/tag/v0.1-weights-morevit", "url": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/74653799", "release_id": 74653799, "date_created": "2022-08-16T00:56:08Z", "date_published": "2022-08-17T18:45:05Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/73005307", "tag": "v0.6.7", "name": "v0.6.7 Release", "author": {"name": "rwightman", "type": "User"}, "description": "Minor bug fixes and a few more weights since 0.6.5\r\n\r\n* A few more weights & model defs added:\r\n  * `darknetaa53` -  79.8 @ 256, 80.5 @ 288\r\n  * `convnext_nano` - 80.8 @ 224, 81.5 @ 288\r\n  * `cs3sedarknet_l` - 81.2 @ 256, 81.8 @ 288\r\n  * `cs3darknet_x` - 81.8 @ 256, 82.2 @ 288\r\n  * `cs3sedarknet_x` - 82.2 @ 256, 82.7 @ 288\r\n  * `cs3edgenet_x` - 82.2 @ 256, 82.7 @ 288\r\n  * `cs3se_edgenet_x` - 82.8 @ 256, 83.5 @ 320\r\n* `cs3*` weights above all trained on TPU w/ `bits_and_tpu` branch. Thanks to TRC program! \r\n* Add output_stride=8 and 16 support to ConvNeXt (dilation)\r\n* deit3 models not being able to resize pos_emb fixed", "tarball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/tarball/v0.6.7", "zipball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/zipball/v0.6.7", "html_url": "https://github.com/huggingface/pytorch-image-models/releases/tag/v0.6.7", "url": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/73005307", "release_id": 73005307, "date_created": "2022-07-27T21:07:37Z", "date_published": "2022-07-27T21:12:16Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/71657033", "tag": "v0.6.5", "name": "v0.6.5 Release", "author": {"name": "rwightman", "type": "User"}, "description": "First official release in a long while (since 0.5.4). All change log since 0.5.4 below,\r\n\r\n### July 8, 2022\r\nMore models, more fixes\r\n* Official research models (w/ weights) added:\r\n  * EdgeNeXt from (https://github.com/mmaaz60/EdgeNeXt)\r\n  * MobileViT-V2 from (https://github.com/apple/ml-cvnets)\r\n  * DeiT III (Revenge of the ViT) from (https://github.com/facebookresearch/deit)\r\n* My own models:\r\n  * Small `ResNet` defs added by request with 1 block repeats for both basic and bottleneck (resnet10 and resnet14)\r\n  * `CspNet` refactored with dataclass config, simplified CrossStage3 (`cs3`) option. These are closer to YOLO-v5+ backbone defs.\r\n  * More relative position vit fiddling. Two `srelpos` (shared relative position) models trained, and a medium w/ class token.\r\n  * Add an alternate downsample mode to EdgeNeXt and train a `small` model. Better than original small, but not their new USI trained weights.\r\n* My own model weight results (all ImageNet-1k training)\r\n  * `resnet10t` - 66.5 @ 176, 68.3 @ 224\r\n  * `resnet14t` - 71.3 @ 176, 72.3 @ 224\r\n  * `resnetaa50` - 80.6 @ 224 , 81.6 @ 288\r\n  * `darknet53` -  80.0 @ 256, 80.5 @ 288\r\n  * `cs3darknet_m` - 77.0 @ 256, 77.6 @ 288\r\n  * `cs3darknet_focus_m` - 76.7 @ 256, 77.3 @ 288\r\n  * `cs3darknet_l` - 80.4 @ 256, 80.9 @ 288\r\n  * `cs3darknet_focus_l` - 80.3 @ 256, 80.9 @ 288\r\n  * `vit_srelpos_small_patch16_224` - 81.1 @ 224, 82.1 @ 320\r\n  * `vit_srelpos_medium_patch16_224` - 82.3 @ 224, 83.1 @ 320\r\n  * `vit_relpos_small_patch16_cls_224` - 82.6 @ 224, 83.6 @ 320\r\n  * `edgnext_small_rw` - 79.6 @ 224, 80.4 @ 320\r\n* `cs3`, `darknet`, and `vit_*relpos` weights above all trained on TPU thanks to TRC program! Rest trained on overheating GPUs.\r\n* Hugging Face Hub support fixes verified, demo notebook TBA\r\n* Pretrained weights / configs can be loaded externally (ie from local disk) w/ support for head adaptation.\r\n* Add support to change image extensions scanned by `timm` datasets/parsers. See (https://github.com/rwightman/pytorch-image-models/pull/1274#issuecomment-1178303103)\r\n* Default ConvNeXt LayerNorm impl to use `F.layer_norm(x.permute(0, 2, 3, 1), ...).permute(0, 3, 1, 2)` via `LayerNorm2d` in all cases. \r\n  * a bit slower than previous custom impl on some hardware (ie Ampere w/ CL), but overall fewer regressions across wider HW / PyTorch version ranges. \r\n  * previous impl exists as `LayerNormExp2d` in `models/layers/norm.py`\r\n* Numerous bug fixes\r\n* Currently testing for imminent PyPi 0.6.x release\r\n* LeViT pretraining of larger models still a WIP, they don't train well / easily without distillation. Time to add distill support (finally)?\r\n* ImageNet-22k weight training + finetune ongoing, work on multi-weight support (slowly) chugging along (there are a LOT of weights, sigh) ...\r\n\r\n### May 13, 2022\r\n* Official Swin-V2 models and weights added from (https://github.com/microsoft/Swin-Transformer). Cleaned up to support torchscript.\r\n* Some refactoring for existing `timm` Swin-V2-CR impl, will likely do a bit more to bring parts closer to official and decide whether to merge some aspects.\r\n* More Vision Transformer relative position / residual post-norm experiments (all trained on TPU thanks to TRC program)\r\n  * `vit_relpos_small_patch16_224` - 81.5 @ 224, 82.5 @ 320 -- rel pos, layer scale, no class token, avg pool\r\n  * `vit_relpos_medium_patch16_rpn_224` - 82.3 @ 224, 83.1 @ 320 -- rel pos + res-post-norm, no class token, avg pool\r\n  * `vit_relpos_medium_patch16_224` - 82.5 @ 224, 83.3 @ 320 -- rel pos, layer scale, no class token, avg pool\r\n  * `vit_relpos_base_patch16_gapcls_224` - 82.8 @ 224, 83.9 @ 320 -- rel pos, layer scale, class token, avg pool (by mistake)\r\n* Bring 512 dim, 8-head 'medium' ViT model variant back to life (after using in a pre DeiT 'small' model for first ViT impl back in 2020)\r\n* Add ViT relative position support for switching btw existing impl and some additions in official Swin-V2 impl for future trials\r\n* Sequencer2D impl (https://arxiv.org/abs/2205.01972), added via PR from author (https://github.com/okojoalg)\r\n\r\n### May 2, 2022\r\n* Vision Transformer experiments adding Relative Position (Swin-V2 log-coord) (`vision_transformer_relpos.py`) and Residual Post-Norm branches (from Swin-V2) (`vision_transformer*.py`)\r\n  * `vit_relpos_base_patch32_plus_rpn_256` - 79.5 @ 256, 80.6 @ 320 -- rel pos + extended width + res-post-norm, no class token, avg pool\r\n  * `vit_relpos_base_patch16_224` - 82.5 @ 224, 83.6 @ 320 -- rel pos, layer scale, no class token, avg pool\r\n  * `vit_base_patch16_rpn_224` - 82.3 @ 224 -- rel pos + res-post-norm, no class token, avg pool\r\n* Vision Transformer refactor to remove representation layer that was only used in initial vit and rarely used since with newer pretrain (ie `How to Train Your ViT`)\r\n* `vit_*` models support removal of class token, use of global average pool, use of fc_norm (ala beit, mae).\r\n\r\n### April 22, 2022\r\n* `timm` models are now officially supported in [fast.ai](https://www.fast.ai/)! Just in time for the new Practical Deep Learning course. `timmdocs` documentation link updated to [timm.fast.ai](http://timm.fast.ai/).\r\n* Two more model weights added in the TPU trained [series](https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-tpu-weights). Some In22k pretrain still in progress.\r\n  * `seresnext101d_32x8d` - 83.69 @ 224, 84.35 @ 288\r\n  * `seresnextaa101d_32x8d` (anti-aliased w/ AvgPool2d) - 83.85 @ 224, 84.57 @ 288\r\n\r\n### March 23, 2022\r\n* Add `ParallelBlock` and `LayerScale` option to base vit models to support model configs in [Three things everyone should know about ViT](https://arxiv.org/abs/2203.09795)\r\n* `convnext_tiny_hnf` (head norm first) weights trained with (close to) A2 recipe, 82.2% top-1, could do better with more epochs.\r\n\r\n### March 21, 2022\r\n* Merge `norm_norm_norm`. **IMPORTANT** this update for a coming 0.6.x release will likely de-stabilize the master branch for a while. Branch [`0.5.x`](https://github.com/rwightman/pytorch-image-models/tree/0.5.x) or a previous 0.5.x release can be used if stability is required.\r\n* Significant weights update (all TPU trained) as described in this [release](https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-tpu-weights)\r\n  * `regnety_040` - 82.3 @ 224, 82.96 @ 288\r\n  * `regnety_064` - 83.0 @ 224, 83.65 @ 288\r\n  * `regnety_080` - 83.17 @ 224, 83.86 @ 288\r\n  * `regnetv_040` - 82.44 @ 224, 83.18 @ 288   (timm pre-act)\r\n  * `regnetv_064` - 83.1 @ 224, 83.71 @ 288   (timm pre-act)\r\n  * `regnetz_040` - 83.67 @ 256, 84.25 @ 320\r\n  * `regnetz_040h` - 83.77 @ 256, 84.5 @ 320 (w/ extra fc in head)\r\n  * `resnetv2_50d_gn` - 80.8 @ 224, 81.96 @ 288 (pre-act GroupNorm)\r\n  * `resnetv2_50d_evos` 80.77 @ 224, 82.04 @ 288 (pre-act EvoNormS)\r\n  * `regnetz_c16_evos`  - 81.9 @ 256, 82.64 @ 320 (EvoNormS)\r\n  * `regnetz_d8_evos`  - 83.42 @ 256, 84.04 @ 320 (EvoNormS)\r\n  * `xception41p` - 82 @ 299   (timm pre-act)\r\n  * `xception65` -  83.17 @ 299\r\n  * `xception65p` -  83.14 @ 299   (timm pre-act)\r\n  * `resnext101_64x4d` - 82.46 @ 224, 83.16 @ 288\r\n  * `seresnext101_32x8d` - 83.57 @ 224, 84.270 @ 288\r\n  * `resnetrs200` - 83.85 @ 256, 84.44 @ 320\r\n* HuggingFace hub support fixed w/ initial groundwork for allowing alternative 'config sources' for pretrained model definitions and weights (generic local file / remote url support soon)\r\n* SwinTransformer-V2 implementation added. Submitted by [Christoph Reich](https://github.com/ChristophReich1996). Training experiments and model changes by myself are ongoing so expect compat breaks.\r\n* Swin-S3 (AutoFormerV2) models / weights added from https://github.com/microsoft/Cream/tree/main/AutoFormerV2\r\n* MobileViT models w/ weights adapted from https://github.com/apple/ml-cvnets\r\n* PoolFormer models w/ weights adapted from https://github.com/sail-sg/poolformer\r\n* VOLO models w/ weights adapted from https://github.com/sail-sg/volo\r\n* Significant work experimenting with non-BatchNorm norm layers such as EvoNorm, FilterResponseNorm, GroupNorm, etc\r\n* Enhance support for alternate norm + act ('NormAct') layers added to a number of models, esp EfficientNet/MobileNetV3, RegNet, and aligned Xception\r\n* Grouped conv support added to EfficientNet family\r\n* Add 'group matching' API to all models to allow grouping model parameters for application of 'layer-wise' LR decay, lr scale added to LR scheduler\r\n* Gradient checkpointing support added to many models\r\n* `forward_head(x, pre_logits=False)` fn added to all models to allow separate calls of `forward_features` + `forward_head`\r\n* All vision transformer and vision MLP models update to return non-pooled / non-token selected features from `foward_features`, for consistency with CNN models, token selection or pooling now applied in `forward_head`\r\n\r\n### Feb 2, 2022\r\n* [Chris Hughes](https://github.com/Chris-hughes10) posted an exhaustive run through of `timm` on his blog yesterday. Well worth a read. [Getting Started with PyTorch Image Models (timm): A Practitioner\u2019s Guide](https://towardsdatascience.com/getting-started-with-pytorch-image-models-timm-a-practitioners-guide-4e77b4bf9055)\r\n* I'm currently prepping to merge the `norm_norm_norm` branch back to master (ver 0.6.x) in next week or so.\r\n  * The changes are more extensive than usual and may destabilize and break some model API use (aiming for full backwards compat). So, beware `pip install git+https://github.com/rwightman/pytorch-image-models` installs!\r\n  * `0.5.x` releases and a `0.5.x` branch will remain stable with a cherry pick or two until dust clears. Recommend sticking to pypi install for a bit if you want stable.", "tarball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/tarball/v0.6.5", "zipball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/zipball/v0.6.5", "html_url": "https://github.com/huggingface/pytorch-image-models/releases/tag/v0.6.5", "url": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/71657033", "release_id": 71657033, "date_created": "2022-07-10T23:43:23Z", "date_published": "2022-07-10T23:53:09Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/63478244", "tag": "v0.1-weights-swinv2", "name": "Swin Transformer V2 (CR) weights and experiments ", "author": {"name": "rwightman", "type": "User"}, "description": "This release holds weights for timm's variant of Swin V2 (from @ChristophReich1996 impl, https://github.com/ChristophReich1996/Swin-Transformer-V2)\r\n\r\nNOTE: `ns` variants of the models have extra norms on the main branch at the end of each stage, this seems to help training. The current `small` model is not using this, but currently training one. Will have a non-ns tiny soon as well as a comparsion. in21k and 1k base models are also in the works...\r\n\r\n`small` checkpoints trained on TPU-VM instances via the TPU-Research Cloud (https://sites.research.google/trc/about/)\r\n\r\n* `swin_v2_tiny_ns_224` - 81.80 top-1\r\n* `swin_v2_small_224` - 83.13 top-1\r\n* `swin_v2_small_ns_224` - 83.5 top-1", "tarball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/tarball/v0.1-weights-swinv2", "zipball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/zipball/v0.1-weights-swinv2", "html_url": "https://github.com/huggingface/pytorch-image-models/releases/tag/v0.1-weights-swinv2", "url": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/63478244", "release_id": 63478244, "date_created": "2022-04-03T21:32:40Z", "date_published": "2022-04-03T22:14:28Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/62231703", "tag": "v0.1-tpu-weights", "name": "TPU VM trained weight release w/ PyTorch XLA", "author": {"name": "rwightman", "type": "User"}, "description": "A wide range of mid-large sized models trained in PyTorch XLA on TPU VM instances. Demonstrating viability of the TPU + PyTorch combo for excellent image model results. All models trained w/ the `bits_and_tpu` branch of this codebase. \r\n\r\nA big thanks to the TPU Research Cloud (https://sites.research.google/trc/about/) for the compute used in these experiments.\r\n\r\nThis set includes several novel weights, including EvoNorm-S RegNetZ (C/D timm variants) and ResNet-V2 model experiments, as well as  custom pre-activation model variants of RegNet-Y (called RegNet-V) and Xception (Xception-P) models.\r\n\r\nMany if not all of the included RegNet weights surpass original paper results by a wide margin and remain above other known results (e.g. recent torchvision updates) in ImageNet-1k validation and especially OOD test set / robustness performance and scaling to higher resolutions.\r\n\r\n## RegNets\r\n* `regnety_040` - 82.3 @ 224, 82.96 @ 288\r\n* `regnety_064` - 83.0 @ 224, 83.65 @ 288\r\n* `regnety_080` - 83.17 @ 224, 83.86 @ 288\r\n* `regnetv_040` - 82.44 @ 224, 83.18 @ 288   (timm pre-act)\r\n* `regnetv_064` - 83.1 @ 224, 83.71 @ 288   (timm pre-act)\r\n* `regnetz_040` - 83.67 @ 256, 84.25 @ 320 \r\n* `regnetz_040h` - 83.77 @ 256, 84.5 @ 320 (w/ extra fc in head)\r\n\r\n## Alternative norm layers (no BN!)\r\n* `resnetv2_50d_gn` - 80.8 @ 224, 81.96 @ 288 (pre-act GroupNorm)\r\n* `resnetv2_50d_evos` 80.77 @ 224, 82.04 @ 288 (pre-act EvoNormS)\r\n* `regnetz_c16_evos`  - 81.9 @ 256, 82.64 @ 320 (EvoNormS)\r\n* `regnetz_d8_evos`  - 83.42 @ 256, 84.04 @ 320 (EvoNormS) \r\n\r\n## Xception redux\r\n* `xception41p` - 82 @ 299   (timm pre-act)\r\n* `xception65` -  83.17 @ 299\r\n* `xception65p` -  83.14 @ 299   (timm pre-act)\r\n\r\n## ResNets (w/ SE and/or NeXT)\r\n* `resnext101_64x4d` - 82.46 @ 224, 83.16 @ 288\r\n* `seresnext101_32x8d` - 83.57 @ 224, 84.27 @ 288\r\n* `seresnext101d_32x8d` - 83.69 @ 224, 84.35 @ 288\r\n* `seresnextaa101d_32x8d` - 83.85 @ 224, 84.57 @ 288\r\n* `resnetrs200` - 83.85 @ 256, 84.44 @ 320\r\n\r\n\r\n## Vision transformer experiments -- relpos, residual-post-norm, layer-scale, fc-norm, and GAP\r\n* `vit_relpos_base_patch32_plus_rpn_256` - 79.5 @ 256, 80.6 @ 320 -- rel pos + extended width + res-post-norm, no class token, avg pool\r\n* `vit_relpos_small_patch16_224` - 81.5 @ 224, 82.5 @ 320 -- rel pos, layer scale, no class token, avg pool\r\n* `vit_relpos_medium_patch16_rpn_224` - 82.3 @ 224, 83.1 @ 320 -- rel pos + res-post-norm, no class token, avg pool\r\n* `vit_base_patch16_rpn_224` - 82.3 @ 224 -- rel pos + res-post-norm, no class token, avg pool\r\n* `vit_relpos_medium_patch16_224` - 82.5 @ 224, 83.3 @ 320 -- rel pos, layer scale, no class token, avg pool\r\n* `vit_relpos_base_patch16_224` - 82.5 @ 224, 83.6 @ 320 -- rel pos, layer scale, no class token, avg pool\r\n* `vit_relpos_base_patch16_gapcls_224` - 82.8 @ 224, 83.9 @ 320 -- rel pos, layer scale, class token, avg pool (by mistake)", "tarball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/tarball/v0.1-tpu-weights", "zipball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/zipball/v0.1-tpu-weights", "html_url": "https://github.com/huggingface/pytorch-image-models/releases/tag/v0.1-tpu-weights", "url": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/62231703", "release_id": 62231703, "date_created": "2022-02-02T17:15:20Z", "date_published": "2022-03-18T22:50:29Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/58385122", "tag": "v0.1-mvit-weights", "name": "MobileViT weights", "author": {"name": "rwightman", "type": "User"}, "description": "Pretrained weights for MobileViT and MobileViT-V2 adapted from Apple impl at https://github.com/apple/ml-cvnets\r\n\r\nCheckpoints remapped to `timm` impl of the model with BGR corrected to RGB (for V1).", "tarball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/tarball/v0.1-mvit-weights", "zipball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/zipball/v0.1-mvit-weights", "html_url": "https://github.com/huggingface/pytorch-image-models/releases/tag/v0.1-mvit-weights", "url": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/58385122", "release_id": 58385122, "date_created": "2022-01-24T22:46:47Z", "date_published": "2022-01-31T23:25:04Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/57258054", "tag": "v0.5.4", "name": "v0.5.4 - More weights, models. ResNet strikes back, self-attn - convnet hybrids, optimizers and more", "author": {"name": "rwightman", "type": "User"}, "tarball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/tarball/v0.5.4", "zipball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/zipball/v0.5.4", "html_url": "https://github.com/huggingface/pytorch-image-models/releases/tag/v0.5.4", "url": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/57258054", "release_id": 57258054, "date_created": "2022-01-16T22:20:08Z", "date_published": "2022-01-17T05:03:20Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/50708573", "tag": "v0.1-rsb-weights", "name": "v0.1-rsb-weights", "author": {"name": "rwightman", "type": "User"}, "description": "# Weights for ResNet Strikes Back\r\nPaper: https://arxiv.org/abs/2110.00476\r\n\r\nMore details on weights and hparams to come...\r\n\r\n", "tarball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/tarball/v0.1-rsb-weights", "zipball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/zipball/v0.1-rsb-weights", "html_url": "https://github.com/huggingface/pytorch-image-models/releases/tag/v0.1-rsb-weights", "url": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/50708573", "release_id": 50708573, "date_created": "2021-09-30T20:32:15Z", "date_published": "2021-10-04T00:02:48Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/48999067", "tag": "v0.1-attn-weights", "name": "v0.1-attn-weights", "author": {"name": "rwightman", "type": "User"}, "description": "A collection of weights I've trained comparing various types of SE-like (SE, ECA, GC, etc), self-attention (bottleneck, halo, lambda) blocks, and related non-attn baselines.\r\n\r\n# ResNet-26-T series\r\n* [2, 2, 2, 2] repeat Bottlneck block ResNet architecture\r\n* ReLU activations\r\n* 3 layer stem with 24, 32, 64 chs, max-pool\r\n* avg pool in shortcut downsample\r\n* self-attn blocks replace 3x3 in both blocks for last stage, and second block of penultimate stage\r\n\r\n|model         |top1  |top1_err|top5  |top5_err|param_count|img_size|cropt_pct|interpolation|\r\n|--------------|------|--------|------|--------|-----------|--------|---------|-------------|\r\n|botnet26t_256 |79.246|20.754  |94.53 |5.47    |12.49      |256     |0.95     |bicubic      |\r\n|halonet26t    |79.13 |20.87   |94.314|5.686   |12.48      |256     |0.95     |bicubic      |\r\n|lambda_resnet26t|79.112|20.888  |94.59 |5.41    |10.96      |256     |0.94     |bicubic      |\r\n|lambda_resnet26rpt_256|78.964|21.036  |94.428|5.572   |10.99      |256     |0.94     |bicubic      |\r\n|resnet26t     |77.872|22.128  |93.834|6.166   |16.01      |256     |0.94     |bicubic      |\r\n\r\nDetails:\r\n* HaloNet - 8 pixel block size, 2 pixel halo (overlap), relative position embedding\r\n* BotNet - relative position embedding\r\n* Lambda-ResNet-26-T - 3d lambda conv, kernel = 9\r\n* Lambda-ResNet-26-RPT - relative position embedding\r\n\r\n## Benchmark - RTX 3090  - AMP - NCHW - NGC 21.09\r\n|model         |infer_samples_per_sec|infer_step_time|infer_batch_size|infer_img_size|train_samples_per_sec|train_step_time|train_batch_size|train_img_size|param_count|\r\n|--------------|---------------------|---------------|----------------|--------------|---------------------|---------------|----------------|--------------|-----------|\r\n|resnet26t     |2967.55              |86.252         |256             |256           |857.62               |297.984        |256             |256           |16.01      |\r\n|botnet26t_256 |2642.08              |96.879         |256             |256           |809.41               |315.706        |256             |256           |12.49      |\r\n|halonet26t    |2601.91              |98.375         |256             |256           |783.92               |325.976        |256             |256           |12.48      |\r\n|lambda_resnet26t|2354.1               |108.732        |256             |256           |697.28               |366.521        |256             |256           |10.96      |\r\n|lambda_resnet26rpt_256|1847.34              |138.563        |256             |256           |644.84               |197.892        |128             |256           |10.99      |\r\n\r\n## Benchmark - RTX 3090 - AMP - NHWC - NGC 21.09\r\n|model                 |infer_samples_per_sec|infer_step_time|infer_batch_size|infer_img_size|train_samples_per_sec|train_step_time|train_batch_size|train_img_size|param_count|\r\n|----------------------|---------------------|---------------|----------------|--------------|---------------------|---------------|----------------|--------------|-----------|\r\n|resnet26t             |3691.94              |69.327         |256             |256           |1188.17              |214.96         |256             |256           |16.01      |\r\n|botnet26t_256         |3291.63              |77.76          |256             |256           |1126.68              |226.653        |256             |256           |12.49      |\r\n|halonet26t            |3230.5               |79.232         |256             |256           |1077.82              |236.934        |256             |256           |12.48      |\r\n|lambda_resnet26rpt_256|2324.15              |110.133        |256             |256           |864.42               |147.485        |128             |256           |10.99      |\r\n|lambda_resnet26t|Not Supported             |       |            |          |              |        | \r\n\r\n# ResNeXT-26-T series\r\n* [2, 2, 2, 2] repeat Bottlneck block ResNeXt architectures\r\n* SiLU activations\r\n* grouped 3x3 convolutions in bottleneck, 32 channels per group\r\n* 3 layer stem with 24, 32, 64 chs, max-pool\r\n* avg pool in shortcut downsample\r\n* channel attn (active in non self-attn blocks) between 3x3 and last 1x1 conv\r\n* when active, self-attn blocks replace 3x3 conv in both blocks for last stage, and second block of penultimate stage\r\n\r\n|model         |top1  |top1_err|top5  |top5_err|param_count|img_size|cropt_pct|interpolation|\r\n|--------------|------|--------|------|--------|-----------|--------|---------|-------------|\r\n|eca_halonext26ts|79.484  |20.516  |94.600  |5.400  |10.76      |256     |0.94     |bicubic      |\r\n|eca_botnext26ts_256|79.270  |20.730  |94.594  |5.406  |10.59   |256     |0.95      |bicubic      |\r\n|bat_resnext26ts|78.268|21.732  |94.1  |5.9     |10.73      |256     |0.9      |bicubic      |\r\n|seresnext26ts |77.852|22.148  |93.784|6.216   |10.39      |256     |0.9      |bicubic      |\r\n|gcresnext26ts |77.804|22.196  |93.824|6.176   |10.48      |256     |0.9      |bicubic      |\r\n|eca_resnext26ts|77.446|22.554  |93.57 |6.43    |10.3       |256     |0.9      |bicubic      |\r\n|resnext26ts   |76.764|23.236  |93.136|6.864   |10.3       |256     |0.9      |bicubic      |\r\n\r\n\r\n## Benchmark - RTX 3090 - AMP - NCHW - NGC 21.09\r\n|model                 |infer_samples_per_sec|infer_step_time|infer_batch_size|infer_img_size|train_samples_per_sec|train_step_time|train_batch_size|train_img_size|param_count|\r\n|----------------------|---------------------|---------------|----------------|--------------|---------------------|---------------|----------------|--------------|-----------|\r\n|resnext26ts           |3006.57              |85.134         |256             |256           |864.4                |295.646        |256             |256           |10.3       |\r\n|seresnext26ts         |2931.27              |87.321         |256             |256           |836.92               |305.193        |256             |256           |10.39      |\r\n|eca_resnext26ts       |2925.47              |87.495         |256             |256           |837.78               |305.003        |256             |256           |10.3       |\r\n|gcresnext26ts         |2870.01              |89.186         |256             |256           |818.35               |311.97         |256             |256           |10.48      |\r\n|eca_botnext26ts_256   |2652.03              |96.513         |256             |256           |790.43               |323.257        |256             |256           |10.59      |\r\n|eca_halonext26ts      |2593.03              |98.705         |256             |256           |766.07               |333.541        |256             |256           |10.76      |\r\n|bat_resnext26ts       |2469.78              |103.64         |256             |256           |697.21               |365.964        |256             |256           |10.73      |\r\n\r\n## Benchmark - RTX 3090 - AMP - NHWC - NGC 21.09\r\nNOTE: there are performance issues with certain grouped conv configs with channels last layout, backwards pass in particular is really slow. Also causing issues for RegNet and NFNet networks.\r\n|model                 |infer_samples_per_sec|infer_step_time|infer_batch_size|infer_img_size|train_samples_per_sec|train_step_time|train_batch_size|train_img_size|param_count|\r\n|----------------------|---------------------|---------------|----------------|--------------|---------------------|---------------|----------------|--------------|-----------|\r\n|resnext26ts           |3952.37              |64.755         |256             |256           |608.67               |420.049        |256             |256           |10.3       |\r\n|eca_resnext26ts       |3815.77              |67.074         |256             |256           |594.35               |430.146        |256             |256           |10.3       |\r\n|seresnext26ts         |3802.75              |67.304         |256             |256           |592.82               |431.14         |256             |256           |10.39      |\r\n|gcresnext26ts         |3626.97              |70.57          |256             |256           |581.83               |439.119        |256             |256           |10.48      |\r\n|eca_botnext26ts_256   |3515.84              |72.8           |256             |256           |611.71               |417.862        |256             |256           |10.59      |\r\n|eca_halonext26ts      |3410.12              |75.057         |256             |256           |597.52               |427.789        |256             |256           |10.76      |\r\n|bat_resnext26ts       |3053.83              |83.811         |256             |256           |533.23               |478.839        |256             |256           |10.73      |\r\n\r\n# ResNet-33-T series.\r\n* [2, 3, 3, 2] repeat Bottlneck block  ResNet architecture\r\n* SiLU activations\r\n* 3 layer stem with 24, 32, 64 chs, no max-pool, 1st and 3rd conv stride 2\r\n* avg pool in shortcut downsample\r\n* channel attn (active in non self-attn blocks) between 3x3 and last 1x1 conv\r\n* when active, self-attn blocks replace 3x3 conv last block of stage 2 and 3, and both blocks of final stage\r\n* FC 1x1 conv between last block and classifier\r\n\r\nThe 33-layer models have an extra 1x1 FC layer between last conv block and classifier. There is both a non-attenion 33 layer baseline and a 32 layer without the extra FC.\r\n\r\n|model         |top1  |top1_err|top5  |top5_err|param_count|img_size|cropt_pct|interpolation|\r\n|--------------|------|--------|------|--------|-----------|--------|---------|-------------|\r\n|sehalonet33ts  |80.986|19.014  |95.272|4.728   |13.69      |256     |0.94     |bicubic      |\r\n|seresnet33ts  |80.388|19.612  |95.108|4.892   |19.78      |256     |0.94     |bicubic      |\r\n|eca_resnet33ts|80.132|19.868  |95.054|4.946   |19.68      |256     |0.94     |bicubic      |\r\n|gcresnet33ts  |79.99 |20.01   |94.988|5.012   |19.88      |256     |0.94     |bicubic      |\r\n|resnet33ts    |79.352|20.648  |94.596|5.404   |19.68      |256     |0.94     |bicubic      |\r\n|resnet32ts    |79.028|20.972  |94.444|5.556   |17.96      |256     |0.94     |bicubic      |\r\n\r\n## Benchmark - RTX 3090 - AMP - NCHW - NGC 21.09\r\n|model                 |infer_samples_per_sec|infer_step_time|infer_batch_size|infer_img_size|train_samples_per_sec|train_step_time|train_batch_size|train_img_size|param_count|\r\n|----------------------|---------------------|---------------|----------------|--------------|---------------------|---------------|----------------|--------------|-----------|\r\n|resnet32ts            |2502.96              |102.266        |256             |256           |733.27               |348.507        |256             |256           |17.96      |\r\n|resnet33ts            |2473.92              |103.466        |256             |256           |725.34               |352.309        |256             |256           |19.68      |\r\n|seresnet33ts          |2400.18              |106.646        |256             |256           |695.19               |367.413        |256             |256           |19.78      |\r\n|eca_resnet33ts        |2394.77              |106.886        |256             |256           |696.93               |366.637        |256             |256           |19.68      |\r\n|gcresnet33ts          |2342.81              |109.257        |256             |256           |678.22               |376.404        |256             |256           |19.88      |\r\n|sehalonet33ts         |1857.65              |137.794        |256             |256           |577.34               |442.545        |256             |256           |13.69      |\r\n\r\n## Benchmark - RTX 3090 - AMP - NHWC - NGC 21.09\r\n|model                 |infer_samples_per_sec|infer_step_time|infer_batch_size|infer_img_size|train_samples_per_sec|train_step_time|train_batch_size|train_img_size|param_count|\r\n|----------------------|---------------------|---------------|----------------|--------------|---------------------|---------------|----------------|--------------|-----------|\r\n|resnet32ts            |3306.22              |77.416         |256             |256           |1012.82              |252.158        |256             |256           |17.96      |\r\n|resnet33ts            |3257.59              |78.573         |256             |256           |1002.38              |254.778        |256             |256           |19.68      |\r\n|seresnet33ts          |3128.08              |81.826         |256             |256           |950.27               |268.581        |256             |256           |19.78      |\r\n|eca_resnet33ts        |3127.11              |81.852         |256             |256           |948.84               |269.123        |256             |256           |19.68      |\r\n|gcresnet33ts          |2984.87              |85.753         |256             |256           |916.98               |278.169        |256             |256           |19.88      |\r\n|sehalonet33ts         |2188.23              |116.975        |256             |256           |711.63               |179.03         |128    |256   |13.69   |\r\n\r\n# ResNet-50(ish) models\r\nIn Progress\r\n\r\n# RegNet\"Z\" series\r\n* RegNetZ inspired architecture, inverted bottleneck, SE attention, pre-classifier FC, essentially an EfficientNet w/ grouped conv instead of depthwise\r\n* b, c, and d are three different sizes I put together to cover differing flop ranges, not based on the paper (https://arxiv.org/abs/2103.06877) or a search process\r\n* for comparison to RegNetY and paper RegNetZ models, at 224x224 b,c, and d models are 1.45, 1.92, and 4.58 GMACs respectively, b, and c are trained at 256 here so higher than that (see tables)\r\n* `haloregnetz_c` uses halo attention for all of last stage, and interleaved every 3 (for 4) of penultimate stage\r\n* b, c variants use a stem / 1st stage like the paper, d uses a 3-deep tiered stem with 2-1-2 striding\r\n\r\n## ImageNet-1k validation at train resolution\r\n|model        |top1  |top1_err|top5  |top5_err|param_count|img_size|cropt_pct|interpolation|\r\n|-------------|------|--------|------|--------|-----------|--------|---------|-------------|\r\n|regnetz_d    |83.422|16.578  |96.636|3.364   |27.58      |256     |0.95     |bicubic      |\r\n|regnetz_c    |82.164|17.836  |96.058|3.942   |13.46      |256     |0.94     |bicubic      |\r\n|haloregnetz_b|81.058|18.942  |95.2  |4.8     |11.68      |224     |0.94     |bicubic      |\r\n|regnetz_b    |79.868|20.132  |94.988|5.012   |9.72       |224     |0.94     |bicubic      |\r\n\r\n## ImageNet-1k validation at optimal test res\r\n|model        |top1  |top1_err|top5  |top5_err|param_count|img_size|cropt_pct|interpolation|\r\n|-------------|------|--------|------|--------|-----------|--------|---------|-------------|\r\n|regnetz_d    |84.04 |15.96   |96.87 |3.13    |27.58      |320     |0.95     |bicubic      |\r\n|regnetz_c    |82.516|17.484  |96.356|3.644   |13.46      |320     |0.94     |bicubic      |\r\n|haloregnetz_b|81.058|18.942  |95.2  |4.8     |11.68      |224     |0.94     |bicubic      |\r\n|regnetz_b    |80.728|19.272  |95.47 |4.53    |9.72       |288     |0.94     |bicubic      |\r\n\r\n## Benchmark - RTX 3090 - AMP - NCHW - NGC 21.09\r\n|model        |infer_samples_per_sec|infer_step_time|infer_batch_size|infer_img_size|infer_GMACs|train_samples_per_sec|train_step_time|train_batch_size|train_img_size|param_count|\r\n|-------------|---------------------|---------------|----------------|--------------|-----------|---------------------|---------------|----------------|--------------|-----------|\r\n|regnetz_b    |2703.42              |94.68          |256             |224           |1.45       |764.85               |333.348        |256             |224           |9.72       |\r\n|haloregnetz_b|2086.22              |122.695        |256             |224           |1.88       |620.1                |411.415        |256             |224           |11.68      |\r\n|regnetz_c    |1653.19              |154.836        |256             |256           |2.51       |459.41               |277.268        |128             |256           |13.46      |\r\n|regnetz_d    |1060.91              |241.284        |256             |256           |5.98       |296.51               |430.143        |128             |256           |27.58      |\r\n\r\n## Benchmark - RTX 3090 - AMP - NHWC - NGC 21.09\r\nNOTE: channels last layout is painfully slow for backward pass here due to some sort of cuDNN issue\r\n|model        |infer_samples_per_sec|infer_step_time|infer_batch_size|infer_img_size|infer_GMACs|train_samples_per_sec|train_step_time|train_batch_size|train_img_size|param_count|\r\n|-------------|---------------------|---------------|----------------|--------------|-----------|---------------------|---------------|----------------|--------------|-----------|\r\n|regnetz_b    |4152.59              |61.634         |256             |224           |1.45       |399.37               |639.572        |256             |224           |9.72       |\r\n|haloregnetz_b|2770.78              |92.378         |256             |224           |1.88       |364.22               |701.386        |256             |224           |11.68      |\r\n|regnetz_c    |2512.4               |101.878        |256             |256           |2.51       |376.72               |338.372        |128             |256           |13.46      |\r\n|regnetz_d    |1456.05              |175.8          |256             |256           |5.98       |111.32               |1148.279       |128             |256           |27.58      |\r\n\r\n\r\n", "tarball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/tarball/v0.1-attn-weights", "zipball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/zipball/v0.1-attn-weights", "html_url": "https://github.com/huggingface/pytorch-image-models/releases/tag/v0.1-attn-weights", "url": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/48999067", "release_id": 48999067, "date_created": "2021-08-27T16:22:20Z", "date_published": "2021-09-04T00:25:26Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/45505300", "tag": "v0.4.12", "name": "v0.4.12. Vision Transformer AugReg support and more", "author": {"name": "rwightman", "type": "User"}, "description": "* Vision Transformer AugReg weights and model defs (https://arxiv.org/abs/2106.10270)\r\n* ResMLP official weights\r\n* ECA-NFNet-L2 weights\r\n* gMLP-S weights\r\n* ResNet51-Q\r\n* Visformer, LeViT, ConViT, Twins\r\n* Many fixes, improvements, better test coverage", "tarball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/tarball/v0.4.12", "zipball_url": "https://api.github.com/repos/huggingface/pytorch-image-models/zipball/v0.4.12", "html_url": "https://github.com/huggingface/pytorch-image-models/releases/tag/v0.4.12", "url": "https://api.github.com/repos/huggingface/pytorch-image-models/releases/45505300", "release_id": 45505300, "date_created": "2021-06-30T16:25:58Z", "date_published": "2021-06-30T16:35:55Z"}, "confidence": 1, "technique": "GitHub_API"}], "has_script_file": [{"result": {"value": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/distributed_train.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}], "readme_url": [{"result": {"value": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}], "contributing_guidelines": [{"result": {"value": "*This guideline is very much a work-in-progress.*\n\nContributions to `timm` for code, documentation, tests are more than welcome!\n\nThere haven't been any formal guidelines to date so please bear with me, and feel free to add to this guide.\n\n# Coding style\n\nCode linting and auto-format (black) are not currently in place but open to consideration. In the meantime, the style to follow is (mostly) aligned with Google's guide: https://google.github.io/styleguide/pyguide.html. \n\nA few specific differences from Google style (or black)\n1. Line length is 120 char. Going over is okay in some cases (e.g. I prefer not to break URL across lines).\n2. Hanging indents are always prefered, please avoid aligning arguments with closing brackets or braces.\n\nExample, from Google guide, but this is a NO here:\n```\n   # Aligned with opening delimiter.\n   foo = long_function_name(var_one, var_two,\n                            var_three, var_four)\n   meal = (spam,\n           beans)\n\n   # Aligned with opening delimiter in a dictionary.\n   foo = {\n       'long_dictionary_key': value1 +\n                              value2,\n       ...\n   }\n```\nThis is YES:\n\n```\n   # 4-space hanging indent; nothing on first line,\n   # closing parenthesis on a new line.\n   foo = long_function_name(\n       var_one, var_two, var_three,\n       var_four\n   )\n   meal = (\n       spam,\n       beans,\n   )\n\n   # 4-space hanging indent in a dictionary.\n   foo = {\n       'long_dictionary_key':\n           long_dictionary_value,\n       ...\n   }\n```\n\nWhen there is discrepancy in a given source file (there are many origins for various bits of code and not all have been updated to what I consider current goal), please follow the style in a given file.\n\nIn general, if you add new code, formatting it with black using the following options should result in a style that is compatible with the rest of the code base:\n\n```\nblack --skip-string-normalization --line-length 120 <path-to-file>\n```\n\nAvoid formatting code that is unrelated to your PR though.\n\nPR with pure formatting / style fixes will be accepted but only in isolation from functional changes, best to ask before starting such a change.\n\n# Documentation\n\nAs with code style, docstrings style based on the Google guide: guide: https://google.github.io/styleguide/pyguide.html\n\nThe goal for the code is to eventually move to have all major functions and `__init__` methods use PEP484 type annotations.\n\nWhen type annotations are used for a function, as per the Google pyguide, they should **NOT** be duplicated in the docstrings, please leave annotations as the one source of truth re typing.\n\nThere are a LOT of gaps in current documentation relative to the functionality in timm, please, document away!\n\n# Installation\n\nCreate a Python virtual environment using Python 3.10. Inside the environment, install torch` and `torchvision` using the instructions matching your system as listed on the [PyTorch website](https://pytorch.org/).\n\nThen install the remaining dependencies:\n\n```\npython -m pip install -r requirements.txt\npython -m pip install -r requirements-dev.txt  # for testing\npython -m pip install -e .\n```\n\n## Unit tests\n\nRun the tests using:\n\n```\npytest tests/\n```\n\nSince the whole test suite takes a lot of time to run locally (a few hours), you may want to select a subset of tests relating to the changes you made by using the `-k` option of [`pytest`](https://docs.pytest.org/en/7.1.x/example/markers.html#using-k-expr-to-select-tests-based-on-their-name). Moreover, running tests in parallel (in this example 4 processes) with the `-n` option may help:\n\n```\npytest -k \"substring-to-match\" -n 4 tests/\n```\n\n## Building documentation\n\nPlease refer to [this document](https://github.com/huggingface/pytorch-image-models/tree/main/hfdocs).\n\n# Questions\n\nIf you have any questions about contribution, where / how to contribute, please ask in the [Discussions](https://github.com/huggingface/pytorch-image-models/discussions/categories/contributing) (there is a `Contributing` topic).\n", "type": "File_dump"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/CONTRIBUTING.md"}], "documentation": [{"result": {"value": "https://github.com/rwightman/pytorch-image-models/tree/main/docs", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "The official documentation can be found at https://huggingface.co/docs/hub/timm. Documentation contributions are welcome.\n\n[Getting Started with PyTorch Image Models (timm): A Practitioner\u2019s Guide](https://towardsdatascience.com/getting-started-with-pytorch-image-models-timm-a-practitioners-guide-4e77b4bf9055) by [Chris Hughes](https://github.com/Chris-hughes10) is an extensive blog post covering many aspects of `timm` in detail.\n\n[timmdocs](http://timm.fast.ai/) is an alternate set of documentation for `timm`. A big thanks to [Aman Arora](https://github.com/amaarora) for his efforts creating timmdocs.\n\n[paperswithcode](https://paperswithcode.com/lib/timm) is a good resource for browsing the models within `timm`.\n", "type": "Text_excerpt", "original_header": "Getting Started (Documentation)", "parent_header": ["PyTorch Image Models"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}], "usage": [{"result": {"value": "The official documentation can be found at https://huggingface.co/docs/hub/timm. Documentation contributions are welcome.\n\n[Getting Started with PyTorch Image Models (timm): A Practitioner\u2019s Guide](https://towardsdatascience.com/getting-started-with-pytorch-image-models-timm-a-practitioners-guide-4e77b4bf9055) by [Chris Hughes](https://github.com/Chris-hughes10) is an extensive blog post covering many aspects of `timm` in detail.\n\n[timmdocs](http://timm.fast.ai/) is an alternate set of documentation for `timm`. A big thanks to [Aman Arora](https://github.com/amaarora) for his efforts creating timmdocs.\n\n[paperswithcode](https://paperswithcode.com/lib/timm) is a good resource for browsing the models within `timm`.\n", "type": "Text_excerpt", "original_header": "Getting Started (Documentation)", "parent_header": ["PyTorch Image Models"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"value": "* Detectron2 - https://github.com/facebookresearch/detectron2\n* Segmentation Models (Semantic) - https://github.com/qubvel/segmentation_models.pytorch\n* EfficientDet (Obj Det, Semantic soon) - https://github.com/rwightman/efficientdet-pytorch\n", "type": "Text_excerpt", "original_header": "Object Detection, Instance and Semantic Segmentation", "parent_header": ["PyTorch Image Models", "Awesome PyTorch Resources"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}], "citation": [{"result": {"value": "```bibtex\n@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/rwightman/pytorch-image-models}}\n}\n```\n", "type": "Text_excerpt", "original_header": "BibTeX", "parent_header": ["PyTorch Image Models", "Citing"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"value": "[![DOI](https://zenodo.org/badge/168799526.svg)](https://zenodo.org/badge/latestdoi/168799526)\n", "type": "Text_excerpt", "original_header": "Latest DOI", "parent_header": ["PyTorch Image Models", "Citing"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"value": "@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/rwightman/pytorch-image-models}}\n}", "type": "Text_excerpt", "format": "bibtex", "doi": "https://doi.org/10.5281/zenodo.4414861"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}], "application_domain": [{"result": {"type": "String", "value": "Computer Vision"}, "confidence": 0.9929019848027437, "technique": "supervised_classification"}], "installation": [{"result": {"type": "Text_excerpt", "value": "* Albumentations - https://github.com/albumentations-team/albumentations\n* Kornia - https://github.com/kornia/kornia\n \n", "original_header": "Computer Vision / Image Augmentation"}, "confidence": 0.8181539988117291, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "* RepDistiller - https://github.com/HobbitLong/RepDistiller\n* torchdistill - https://github.com/yoshitomo-matsubara/torchdistill\n \n", "original_header": "Knowledge Distillation"}, "confidence": 0.8918974083095406, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "* PyTorch Metric Learning - https://github.com/KevinMusgrave/pytorch-metric-learning\n \n", "original_header": "Metric Learning"}, "confidence": 0.875936585602119, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "* fastai - https://github.com/fastai/fastai\n \n", "original_header": "Training / Frameworks"}, "confidence": 0.8918974083095406, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}], "invocation": [{"result": {"type": "Text_excerpt", "value": "* Add dynamic img size support to models in `vision_transformer.py`, `vision_transformer_hybrid.py`, `deit.py`, and `eva.py` w/o breaking backward compat.\n  * Add `dynamic_img_size=True` to args at model creation time to allow changing the grid size (interpolate abs and/or ROPE pos embed each forward pass).\n  * Add `dynamic_img_pad=True` to allow image sizes that aren't divisible by patch size (pad bottom right to patch size each forward pass).\n  * Enabling either dynamic mode will break FX tracing unless PatchEmbed module added as leaf.\n  * Existing method of resizing position embedding by passing different `img_size` (interpolate pretrained embed weights once) on creation still works.\n  * Existing method of changing `patch_size` (resize pretrained patch_embed weights once) on creation still works.\n  * Example validation cmd `python validate.py /imagenet --model vit_base_patch16_224 --amp --amp-dtype bfloat16 --img-size 255 --crop-pct 1.0 --model-kwargs dynamic_img_size=True dyamic_img_pad=True`\n \n", "original_header": "Aug 28, 2023"}, "confidence": 0.894571940487905, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "* Swin, MaxViT, CoAtNet, and BEiT models support resizing of image/window size on creation with adaptation of pretrained weights\n* Example validation cmd to test w/ non-square resize `python validate.py /imagenet --model swin_base_patch4_window7_224.ms_in22k_ft_in1k --amp --amp-dtype bfloat16 --input-size 3 256 320 --model-kwargs window_size=8,10 img_size=256,320`\n  \n", "original_header": "Aug 11, 2023"}, "confidence": 0.895237058334607, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "| model                                              |top1  |top5  |param_count|img_size|\n|----------------------------------------------------|------|------|-----------|--------|\n| [eva02_large_patch14_448.mim_m38m_ft_in22k_in1k](https://huggingface.co/timm/eva02_large_patch14_448.mim_m38m_ft_in1k) |90.054|99.042|305.08     |448     |\n| eva02_large_patch14_448.mim_in22k_ft_in22k_in1k    |89.946|99.01 |305.08     |448     |\n| eva_giant_patch14_560.m30m_ft_in22k_in1k           |89.792|98.992|1014.45    |560     |\n| eva02_large_patch14_448.mim_in22k_ft_in1k          |89.626|98.954|305.08     |448     |\n| eva02_large_patch14_448.mim_m38m_ft_in1k           |89.57 |98.918|305.08     |448     |\n| eva_giant_patch14_336.m30m_ft_in22k_in1k           |89.56 |98.956|1013.01    |336     |\n| eva_giant_patch14_336.clip_ft_in1k                 |89.466|98.82 |1013.01    |336     |\n| eva_large_patch14_336.in22k_ft_in22k_in1k          |89.214|98.854|304.53     |336     |\n| eva_giant_patch14_224.clip_ft_in1k                 |88.882|98.678|1012.56    |224     |\n| eva02_base_patch14_448.mim_in22k_ft_in22k_in1k     |88.692|98.722|87.12      |448     |\n| eva_large_patch14_336.in22k_ft_in1k                |88.652|98.722|304.53     |336     |\n| eva_large_patch14_196.in22k_ft_in22k_in1k          |88.592|98.656|304.14     |196     |\n| eva02_base_patch14_448.mim_in22k_ft_in1k           |88.23 |98.564|87.12      |448     |\n| eva_large_patch14_196.in22k_ft_in1k                |87.934|98.504|304.14     |196     |\n| eva02_small_patch14_336.mim_in22k_ft_in1k          |85.74 |97.614|22.13      |336     |\n| eva02_tiny_patch14_336.mim_in22k_ft_in1k           |80.658|95.524|5.76       |336     | \n", "original_header": "March 31, 2023"}, "confidence": 0.8241800495680058, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "|model                                                                                                                   |top1 |top5 |samples / sec  |Params (M)     |GMAC  |Act (M)|\n|------------------------------------------------------------------------------------------------------------------------|----:|----:|--------------:|--------------:|-----:|------:|\n|[maxvit_xlarge_tf_512.in21k_ft_in1k](https://huggingface.co/timm/maxvit_xlarge_tf_512.in21k_ft_in1k)                    |88.53|98.64|          21.76|         475.77|534.14|1413.22|\n|[maxvit_xlarge_tf_384.in21k_ft_in1k](https://huggingface.co/timm/maxvit_xlarge_tf_384.in21k_ft_in1k)                    |88.32|98.54|          42.53|         475.32|292.78| 668.76|\n|[maxvit_base_tf_512.in21k_ft_in1k](https://huggingface.co/timm/maxvit_base_tf_512.in21k_ft_in1k)                        |88.20|98.53|          50.87|         119.88|138.02| 703.99|\n|[maxvit_large_tf_512.in21k_ft_in1k](https://huggingface.co/timm/maxvit_large_tf_512.in21k_ft_in1k)                      |88.04|98.40|          36.42|         212.33|244.75| 942.15|\n|[maxvit_large_tf_384.in21k_ft_in1k](https://huggingface.co/timm/maxvit_large_tf_384.in21k_ft_in1k)                      |87.98|98.56|          71.75|         212.03|132.55| 445.84|\n|[maxvit_base_tf_384.in21k_ft_in1k](https://huggingface.co/timm/maxvit_base_tf_384.in21k_ft_in1k)                        |87.92|98.54|         104.71|         119.65| 73.80| 332.90|\n|[maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k](https://huggingface.co/timm/maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k)        |87.81|98.37|         106.55|         116.14| 70.97| 318.95|\n|[maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k](https://huggingface.co/timm/maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k)  |87.47|98.37|         149.49|         116.09| 72.98| 213.74|\n|[coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k](https://huggingface.co/timm/coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k)            |87.39|98.31|         160.80|          73.88| 47.69| 209.43|\n|[maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k](https://huggingface.co/timm/maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k)        |86.89|98.02|         375.86|         116.14| 23.15|  92.64|\n|[maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k](https://huggingface.co/timm/maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k)  |86.64|98.02|         501.03|         116.09| 24.20|  62.77|\n|[maxvit_base_tf_512.in1k](https://huggingface.co/timm/maxvit_base_tf_512.in1k)                                          |86.60|97.92|          50.75|         119.88|138.02| 703.99|\n|[coatnet_2_rw_224.sw_in12k_ft_in1k](https://huggingface.co/timm/coatnet_2_rw_224.sw_in12k_ft_in1k)                      |86.57|97.89|         631.88|          73.87| 15.09|  49.22|\n|[maxvit_large_tf_512.in1k](https://huggingface.co/timm/maxvit_large_tf_512.in1k)                                        |86.52|97.88|          36.04|         212.33|244.75| 942.15|\n|[coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k](https://huggingface.co/timm/coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k)            |86.49|97.90|         620.58|          73.88| 15.18|  54.78|\n|[maxvit_base_tf_384.in1k](https://huggingface.co/timm/maxvit_base_tf_384.in1k)                                          |86.29|97.80|         101.09|         119.65| 73.80| 332.90|\n|[maxvit_large_tf_384.in1k](https://huggingface.co/timm/maxvit_large_tf_384.in1k)                                        |86.23|97.69|          70.56|         212.03|132.55| 445.84|\n|[maxvit_small_tf_512.in1k](https://huggingface.co/timm/maxvit_small_tf_512.in1k)                                        |86.10|97.76|          88.63|          69.13| 67.26| 383.77|\n|[maxvit_tiny_tf_512.in1k](https://huggingface.co/timm/maxvit_tiny_tf_512.in1k)                                          |85.67|97.58|         144.25|          31.05| 33.49| 257.59|\n|[maxvit_small_tf_384.in1k](https://huggingface.co/timm/maxvit_small_tf_384.in1k)                                        |85.54|97.46|         188.35|          69.02| 35.87| 183.65|\n|[maxvit_tiny_tf_384.in1k](https://huggingface.co/timm/maxvit_tiny_tf_384.in1k)                                          |85.11|97.38|         293.46|          30.98| 17.53| 123.42|\n|[maxvit_large_tf_224.in1k](https://huggingface.co/timm/maxvit_large_tf_224.in1k)                                        |84.93|96.97|         247.71|         211.79| 43.68| 127.35|\n|[coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k](https://huggingface.co/timm/coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k)          |84.90|96.96|        1025.45|          41.72|  8.11|  40.13|\n|[maxvit_base_tf_224.in1k](https://huggingface.co/timm/maxvit_base_tf_224.in1k)                                          |84.85|96.99|         358.25|         119.47| 24.04|  95.01|\n|[maxxvit_rmlp_small_rw_256.sw_in1k](https://huggingface.co/timm/maxxvit_rmlp_small_rw_256.sw_in1k)                      |84.63|97.06|         575.53|          66.01| 14.67|  58.38|\n|[coatnet_rmlp_2_rw_224.sw_in1k](https://huggingface.co/timm/coatnet_rmlp_2_rw_224.sw_in1k)                              |84.61|96.74|         625.81|          73.88| 15.18|  54.78|\n|[maxvit_rmlp_small_rw_224.sw_in1k](https://huggingface.co/timm/maxvit_rmlp_small_rw_224.sw_in1k)                        |84.49|96.76|         693.82|          64.90| 10.75|  49.30|\n|[maxvit_small_tf_224.in1k](https://huggingface.co/timm/maxvit_small_tf_224.in1k)                                        |84.43|96.83|         647.96|          68.93| 11.66|  53.17|\n|[maxvit_rmlp_tiny_rw_256.sw_in1k](https://huggingface.co/timm/maxvit_rmlp_tiny_rw_256.sw_in1k)                          |84.23|96.78|         807.21|          29.15|  6.77|  46.92|\n|[coatnet_1_rw_224.sw_in1k](https://huggingface.co/timm/coatnet_1_rw_224.sw_in1k)                                        |83.62|96.38|         989.59|          41.72|  8.04|  34.60|\n|[maxvit_tiny_rw_224.sw_in1k](https://huggingface.co/timm/maxvit_tiny_rw_224.sw_in1k)                                    |83.50|96.50|        1100.53|          29.06|  5.11|  33.11|\n|[maxvit_tiny_tf_224.in1k](https://huggingface.co/timm/maxvit_tiny_tf_224.in1k)                                          |83.41|96.59|        1004.94|          30.92|  5.60|  35.78|\n|[coatnet_rmlp_1_rw_224.sw_in1k](https://huggingface.co/timm/coatnet_rmlp_1_rw_224.sw_in1k)                              |83.36|96.45|        1093.03|          41.69|  7.85|  35.47|\n|[maxxvitv2_nano_rw_256.sw_in1k](https://huggingface.co/timm/maxxvitv2_nano_rw_256.sw_in1k)                              |83.11|96.33|        1276.88|          23.70|  6.26|  23.05|\n|[maxxvit_rmlp_nano_rw_256.sw_in1k](https://huggingface.co/timm/maxxvit_rmlp_nano_rw_256.sw_in1k)                        |83.03|96.34|        1341.24|          16.78|  4.37|  26.05|\n|[maxvit_rmlp_nano_rw_256.sw_in1k](https://huggingface.co/timm/maxvit_rmlp_nano_rw_256.sw_in1k)                          |82.96|96.26|        1283.24|          15.50|  4.47|  31.92|\n|[maxvit_nano_rw_256.sw_in1k](https://huggingface.co/timm/maxvit_nano_rw_256.sw_in1k)                                    |82.93|96.23|        1218.17|          15.45|  4.46|  30.28|\n|[coatnet_bn_0_rw_224.sw_in1k](https://huggingface.co/timm/coatnet_bn_0_rw_224.sw_in1k)                                  |82.39|96.19|        1600.14|          27.44|  4.67|  22.04|\n|[coatnet_0_rw_224.sw_in1k](https://huggingface.co/timm/coatnet_0_rw_224.sw_in1k)                                        |82.39|95.84|        1831.21|          27.44|  4.43|  18.73|\n|[coatnet_rmlp_nano_rw_224.sw_in1k](https://huggingface.co/timm/coatnet_rmlp_nano_rw_224.sw_in1k)                        |82.05|95.87|        2109.09|          15.15|  2.62|  20.34|\n|[coatnext_nano_rw_224.sw_in1k](https://huggingface.co/timm/coatnext_nano_rw_224.sw_in1k)                                |81.95|95.92|        2525.52|          14.70|  2.47|  12.80|\n|[coatnet_nano_rw_224.sw_in1k](https://huggingface.co/timm/coatnet_nano_rw_224.sw_in1k)                                  |81.70|95.64|        2344.52|          15.14|  2.41|  15.41|\n|[maxvit_rmlp_pico_rw_256.sw_in1k](https://huggingface.co/timm/maxvit_rmlp_pico_rw_256.sw_in1k)                          |80.53|95.21|        1594.71|           7.52|  1.85|  24.86|\n \n", "original_header": "Jan 20, 2023"}, "confidence": 0.8531908535418569, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "* Finally got around to adding `--model-kwargs` and `--opt-kwargs` to scripts to pass through rare args directly to model classes from cmd line\n  * `train.py /imagenet --model resnet50 --amp --model-kwargs output_stride=16 act_layer=silu`\n  * `train.py /imagenet --model vit_base_patch16_clip_224 --img-size 240 --amp --model-kwargs img_size=240 patch_size=12`\n* Cleanup some popular models to better support arg passthrough / merge with model configs, more to go.\n \n", "original_header": "Jan 6, 2023"}, "confidence": 0.8490783970335116, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "| model                              |   top1 |   param_count |   gmac |   macts | hub                                                                    |\n|:-----------------------------------|-------:|--------------:|-------:|--------:|:-----------------------------------------------------------------------|\n| maxvit_xlarge_tf_512.in21k_ft_in1k |   88.5 |         475.8 |  534.1 |  1413.2 | [link](https://huggingface.co/timm/maxvit_xlarge_tf_512.in21k_ft_in1k) |\n| maxvit_xlarge_tf_384.in21k_ft_in1k |   88.3 |         475.3 |  292.8 |   668.8 | [link](https://huggingface.co/timm/maxvit_xlarge_tf_384.in21k_ft_in1k) |\n| maxvit_base_tf_512.in21k_ft_in1k   |   88.2 |         119.9 |  138   |   704   | [link](https://huggingface.co/timm/maxvit_base_tf_512.in21k_ft_in1k)   |\n| maxvit_large_tf_512.in21k_ft_in1k  |   88   |         212.3 |  244.8 |   942.2 | [link](https://huggingface.co/timm/maxvit_large_tf_512.in21k_ft_in1k)  |\n| maxvit_large_tf_384.in21k_ft_in1k  |   88   |         212   |  132.6 |   445.8 | [link](https://huggingface.co/timm/maxvit_large_tf_384.in21k_ft_in1k)  |\n| maxvit_base_tf_384.in21k_ft_in1k   |   87.9 |         119.6 |   73.8 |   332.9 | [link](https://huggingface.co/timm/maxvit_base_tf_384.in21k_ft_in1k)   |\n| maxvit_base_tf_512.in1k            |   86.6 |         119.9 |  138   |   704   | [link](https://huggingface.co/timm/maxvit_base_tf_512.in1k)            |\n| maxvit_large_tf_512.in1k           |   86.5 |         212.3 |  244.8 |   942.2 | [link](https://huggingface.co/timm/maxvit_large_tf_512.in1k)           |\n| maxvit_base_tf_384.in1k            |   86.3 |         119.6 |   73.8 |   332.9 | [link](https://huggingface.co/timm/maxvit_base_tf_384.in1k)            |\n| maxvit_large_tf_384.in1k           |   86.2 |         212   |  132.6 |   445.8 | [link](https://huggingface.co/timm/maxvit_large_tf_384.in1k)           |\n| maxvit_small_tf_512.in1k           |   86.1 |          69.1 |   67.3 |   383.8 | [link](https://huggingface.co/timm/maxvit_small_tf_512.in1k)           |\n| maxvit_tiny_tf_512.in1k            |   85.7 |          31   |   33.5 |   257.6 | [link](https://huggingface.co/timm/maxvit_tiny_tf_512.in1k)            |\n| maxvit_small_tf_384.in1k           |   85.5 |          69   |   35.9 |   183.6 | [link](https://huggingface.co/timm/maxvit_small_tf_384.in1k)           |\n| maxvit_tiny_tf_384.in1k            |   85.1 |          31   |   17.5 |   123.4 | [link](https://huggingface.co/timm/maxvit_tiny_tf_384.in1k)            |\n| maxvit_large_tf_224.in1k           |   84.9 |         211.8 |   43.7 |   127.4 | [link](https://huggingface.co/timm/maxvit_large_tf_224.in1k)           |\n| maxvit_base_tf_224.in1k            |   84.9 |         119.5 |   24   |    95   | [link](https://huggingface.co/timm/maxvit_base_tf_224.in1k)            |\n| maxvit_small_tf_224.in1k           |   84.4 |          68.9 |   11.7 |    53.2 | [link](https://huggingface.co/timm/maxvit_small_tf_224.in1k)           |\n| maxvit_tiny_tf_224.in1k            |   83.4 |          30.9 |    5.6 |    35.8 | [link](https://huggingface.co/timm/maxvit_tiny_tf_224.in1k)            |\n \n", "original_header": "Dec 5, 2022"}, "confidence": 0.8019504627612403, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}], "identifier": [{"result": {"type": "Url", "value": "https://zenodo.org/badge/latestdoi/168799526"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}], "full_title": [{"result": {"type": "String", "value": "PyTorch Image Models"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}], "related_papers": [{"result": {"type": "Url", "value": "https://arxiv.org/abs/2103.10697\n* CspNet (Cross-Stage Partial Networks"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2211.07636\n\n| model                                    |   top1 |   param_count |   gmac |   macts | hub                                     |\n|:-----------------------------------------|-------:|--------------:|-------:|--------:|:----------------------------------------|\n| eva_giant_patch14_560.m30m_ft_in22k_in1k |   89.8 |        1014.4 | 1906.8 |  2577.2 | [link](https://huggingface.co/BAAI/EVA"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2111.14725\n* Swin Transformer - https://arxiv.org/abs/2103.14030\n* Swin Transformer V2 - https://arxiv.org/abs/2111.09883\n* Transformer-iN-Transformer (TNT"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1802.02611\n* XCiT (Cross-Covariance Image Transformers"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1905.02244\n  * FBNet-V3 - https://arxiv.org/abs/2006.02049\n  * HardCoRe-NAS - https://arxiv.org/abs/2102.11646\n  * LCNet - https://arxiv.org/abs/2109.15099\n* MobileOne - https://arxiv.org/abs/2206.04040\n* MobileViT - https://arxiv.org/abs/2110.02178\n* MobileViT-V2 - https://arxiv.org/abs/2206.02680\n* MViT-V2 (Improved Multiscale Vision Transformer"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1903.06586\n    * Split (SPLAT"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2206.01191\n* EfficientNet (MBConvNet Family"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2206.09959\n* GhostNet - https://arxiv.org/abs/1911.11907\n* GhostNet-V2 - https://arxiv.org/abs/2211.12905\n* gMLP - https://arxiv.org/abs/2105.08050\n* GPU-Efficient Networks - https://arxiv.org/abs/2006.14090\n* Halo Nets - https://arxiv.org/abs/2103.12731\n* HRNet - https://arxiv.org/abs/1908.07919\n* InceptionNeXt - https://arxiv.org/abs/2303.16900\n* Inception-V3 - https://arxiv.org/abs/1512.00567\n* Inception-ResNet-V2 and Inception-V4 - https://arxiv.org/abs/1602.07261\n* Lambda Networks - https://arxiv.org/abs/2102.08602\n* LeViT (Vision Transformer in ConvNet's Clothing"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2206.04040\n* MobileViT - https://arxiv.org/abs/2110.02178\n* MobileViT-V2 - https://arxiv.org/abs/2206.02680\n* MViT-V2 (Improved Multiscale Vision Transformer"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2103.12731\n* HRNet - https://arxiv.org/abs/1908.07919\n* InceptionNeXt - https://arxiv.org/abs/2303.16900\n* Inception-V3 - https://arxiv.org/abs/1512.00567\n* Inception-ResNet-V2 and Inception-V4 - https://arxiv.org/abs/1602.07261\n* Lambda Networks - https://arxiv.org/abs/2102.08602\n* LeViT (Vision Transformer in ConvNet's Clothing"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2004.08955\n* ReXNet - https://arxiv.org/abs/2007.00992\n* SelecSLS - https://arxiv.org/abs/1907.00837\n* Selective Kernel Networks - https://arxiv.org/abs/1903.06586\n* Sequencer2D - https://arxiv.org/abs/2205.01972\n* Swin S3 (AutoFormerV2"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2102.08602\n* LeViT (Vision Transformer in ConvNet's Clothing"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2105.08050\n* GPU-Efficient Networks - https://arxiv.org/abs/2006.14090\n* Halo Nets - https://arxiv.org/abs/2103.12731\n* HRNet - https://arxiv.org/abs/1908.07919\n* InceptionNeXt - https://arxiv.org/abs/2303.16900\n* Inception-V3 - https://arxiv.org/abs/1512.00567\n* Inception-ResNet-V2 and Inception-V4 - https://arxiv.org/abs/1602.07261\n* Lambda Networks - https://arxiv.org/abs/2102.08602\n* LeViT (Vision Transformer in ConvNet's Clothing"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2104.01136\n* MaxViT (Multi-Axis Vision Transformer"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2205.01972\n* Swin S3 (AutoFormerV2"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1911.06667\n* Xception - https://arxiv.org/abs/1610.02357\n* Xception (Modified Aligned, Gluon"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1904.11486"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1905.00546\n    * ECA-Net (ECAResNet"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2203.11926\n* GCViT (Global Context Vision Transformer"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2106.09681\n\n## Features\n\nSeveral (less common"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2212.08013\n* FocalNet (Focal Modulation Networks"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1810.12348\n    * Global Context (GC"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1611.05431\n    * 'Bag of Tricks' / Gluon C, D, E, S variations - https://arxiv.org/abs/1812.01187\n    * Weakly-supervised (WSL"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1810.12890"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2210.13452\n* MLP-Mixer - https://arxiv.org/abs/2105.01601\n* MobileNet-V3 (MBConvNet w/ Efficient Head"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1908.07919\n* InceptionNeXt - https://arxiv.org/abs/2303.16900\n* Inception-V3 - https://arxiv.org/abs/1512.00567\n* Inception-ResNet-V2 and Inception-V4 - https://arxiv.org/abs/1602.07261\n* Lambda Networks - https://arxiv.org/abs/2102.08602\n* LeViT (Vision Transformer in ConvNet's Clothing"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1912.11370\n* Bottleneck Transformers - https://arxiv.org/abs/2101.11605\n* CaiT (Class-Attention in Image Transformers"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2206.10589\n* EfficientFormer - https://arxiv.org/abs/2206.01191\n* EfficientNet (MBConvNet Family"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2303.11331\n* FastViT - https://arxiv.org/abs/2303.14189\n* FlexiViT - https://arxiv.org/abs/2212.08013\n* FocalNet (Focal Modulation Networks"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2303.14189\n* FlexiViT - https://arxiv.org/abs/2212.08013\n* FocalNet (Focal Modulation Networks"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2003.13678\n* RegNetZ - https://arxiv.org/abs/2103.06877\n* RepVGG - https://arxiv.org/abs/2101.03697\n* RepGhostNet - https://arxiv.org/abs/2211.06088\n* RepViT - https://arxiv.org/abs/2307.09283\n* ResMLP - https://arxiv.org/abs/2105.03404\n* ResNet/ResNeXt\n    * ResNet (v1b/v1.5"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2104.00298\n    * FBNet-C - https://arxiv.org/abs/1812.03443\n    * MixNet - https://arxiv.org/abs/1907.09595\n    * MNASNet B1, A1 (Squeeze-Excite"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2006.02049\n  * HardCoRe-NAS - https://arxiv.org/abs/2102.11646\n  * LCNet - https://arxiv.org/abs/2109.15099\n* MobileOne - https://arxiv.org/abs/2206.04040\n* MobileViT - https://arxiv.org/abs/2110.02178\n* MobileViT-V2 - https://arxiv.org/abs/2206.02680\n* MViT-V2 (Improved Multiscale Vision Transformer"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2305.07027\n* EVA - https://arxiv.org/abs/2211.07636\n* EVA-02 - https://arxiv.org/abs/2303.11331\n* FastViT - https://arxiv.org/abs/2303.14189\n* FlexiViT - https://arxiv.org/abs/2212.08013\n* FocalNet (Focal Modulation Networks"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1903.06586\n* Sequencer2D - https://arxiv.org/abs/2205.01972\n* Swin S3 (AutoFormerV2"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1707.07012\n* NesT - https://arxiv.org/abs/2105.12723\n* NFNet-F - https://arxiv.org/abs/2102.06171\n* NF-RegNet / NF-ResNet - https://arxiv.org/abs/2101.08692\n* PNasNet - https://arxiv.org/abs/1712.00559\n* PoolFormer (MetaFormer"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2204.01697\n* MetaFormer (PoolFormer-v2, ConvFormer, CAFormer"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2101.03697\n* RepGhostNet - https://arxiv.org/abs/2211.06088\n* RepViT - https://arxiv.org/abs/2307.09283\n* ResMLP - https://arxiv.org/abs/2105.03404\n* ResNet/ResNeXt\n    * ResNet (v1b/v1.5"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2010.11929\n* VOLO (Vision Outlooker"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1912.02781"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2305.07027 (thanks https://github.com/seefun"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/pdf/2104.13840.pdf\n* Visformer - https://arxiv.org/abs/2104.12533\n* Vision Transformer - https://arxiv.org/abs/2010.11929\n* VOLO (Vision Outlooker"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2307.09283\n* ResMLP - https://arxiv.org/abs/2105.03404\n* ResNet/ResNeXt\n    * ResNet (v1b/v1.5"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2111.09883\n* Transformer-iN-Transformer (TNT"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2106.04803\n* ConvNeXt - https://arxiv.org/abs/2201.03545\n* ConvNeXt-V2 - http://arxiv.org/abs/2301.00808\n* ConViT (Soft Convolutional Inductive Biases Vision Transformers"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2007.00992\n* SelecSLS - https://arxiv.org/abs/1907.00837\n* Selective Kernel Networks - https://arxiv.org/abs/1903.06586\n* Sequencer2D - https://arxiv.org/abs/2205.01972\n* Swin S3 (AutoFormerV2"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1707.06484\n* DPN (Dual-Path Network"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2102.06171\n* NF-RegNet / NF-ResNet - https://arxiv.org/abs/2101.08692\n* PNasNet - https://arxiv.org/abs/1712.00559\n* PoolFormer (MetaFormer"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1907.08610"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2111.11418\n* Pooling-based Vision Transformer (PiT"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2211.07636\n* EVA-02 - https://arxiv.org/abs/2303.11331\n* FastViT - https://arxiv.org/abs/2303.14189\n* FlexiViT - https://arxiv.org/abs/2212.08013\n* FocalNet (Focal Modulation Networks"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2206.02680\n* MViT-V2 (Improved Multiscale Vision Transformer"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2105.12723\n* NFNet-F - https://arxiv.org/abs/2102.06171\n* NF-RegNet / NF-ResNet - https://arxiv.org/abs/2101.08692\n* PNasNet - https://arxiv.org/abs/1712.00559\n* PoolFormer (MetaFormer"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2211.06088 (thanks https://github.com/ChengpengChen"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1905.11286"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2206.04040\n  * InceptionNeXt - https://arxiv.org/abs/2303.16900\n  * RepGhostNet - https://arxiv.org/abs/2211.06088 (thanks https://github.com/ChengpengChen"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1904.02877\n    * TinyNet - https://arxiv.org/abs/2010.14819\n* EfficientViT (MIT"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/pdf/2204.07118.pdf\n* DenseNet - https://arxiv.org/abs/1608.06993\n* DLA - https://arxiv.org/abs/1707.06484\n* DPN (Dual-Path Network"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1608.03983"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2205.14756\n* EfficientViT (MSRA"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2101.08692\n* PNasNet - https://arxiv.org/abs/1712.00559\n* PoolFormer (MetaFormer"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2205.14756 (thanks https://github.com/seefun"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2303.16900\n  * RepGhostNet - https://arxiv.org/abs/2211.06088 (thanks https://github.com/ChengpengChen"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1911.09665\n    * EfficientNet (B0-B7"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2103.14030\n* Swin Transformer V2 - https://arxiv.org/abs/2111.09883\n* Transformer-iN-Transformer (TNT"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1904.01169\n* ResNeSt - https://arxiv.org/abs/2004.08955\n* ReXNet - https://arxiv.org/abs/2007.00992\n* SelecSLS - https://arxiv.org/abs/1907.00837\n* Selective Kernel Networks - https://arxiv.org/abs/1903.06586\n* Sequencer2D - https://arxiv.org/abs/2205.01972\n* Swin S3 (AutoFormerV2"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1807.06521\n    * Effective Squeeze-Excitation (ESE"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1709.01507\n    * ResNet-RS - https://arxiv.org/abs/2103.07579\n* Res2Net - https://arxiv.org/abs/1904.01169\n* ResNeSt - https://arxiv.org/abs/2004.08955\n* ReXNet - https://arxiv.org/abs/2007.00992\n* SelecSLS - https://arxiv.org/abs/1907.00837\n* Selective Kernel Networks - https://arxiv.org/abs/1903.06586\n* Sequencer2D - https://arxiv.org/abs/2205.01972\n* Swin S3 (AutoFormerV2"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1904.11492\n    * Halo - https://arxiv.org/abs/2103.12731\n    * Involution - https://arxiv.org/abs/2103.06255\n    * Lambda Layer - https://arxiv.org/abs/2102.08602\n    * Non-Local (NL"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2106.08254\n* Big Transfer ResNetV2 (BiT"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1603.09382"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1910.03151\n    * Gather-Excite (GE"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2003.13630\n* Twins (Spatial Attention in Vision Transformers"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2303.16900\n* Inception-V3 - https://arxiv.org/abs/1512.00567\n* Inception-ResNet-V2 and Inception-V4 - https://arxiv.org/abs/1602.07261\n* Lambda Networks - https://arxiv.org/abs/2102.08602\n* LeViT (Vision Transformer in ConvNet's Clothing"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2109.15099\n* MobileOne - https://arxiv.org/abs/2206.04040\n* MobileViT - https://arxiv.org/abs/2110.02178\n* MobileViT-V2 - https://arxiv.org/abs/2206.02680\n* MViT-V2 (Improved Multiscale Vision Transformer"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1512.03385\n    * ResNeXt - https://arxiv.org/abs/1611.05431\n    * 'Bag of Tricks' / Gluon C, D, E, S variations - https://arxiv.org/abs/1812.01187\n    * Weakly-supervised (WSL"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1805.00932\n    * Semi-supervised (SSL"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2006.14090\n* Halo Nets - https://arxiv.org/abs/2103.12731\n* HRNet - https://arxiv.org/abs/1908.07919\n* InceptionNeXt - https://arxiv.org/abs/2303.16900\n* Inception-V3 - https://arxiv.org/abs/1512.00567\n* Inception-ResNet-V2 and Inception-V4 - https://arxiv.org/abs/1602.07261\n* Lambda Networks - https://arxiv.org/abs/2102.08602\n* LeViT (Vision Transformer in ConvNet's Clothing"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1608.06993\n* DLA - https://arxiv.org/abs/1707.06484\n* DPN (Dual-Path Network"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2006.08217"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1712.00559\n* PoolFormer (MetaFormer"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1908.03265"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2102.11646\n  * LCNet - https://arxiv.org/abs/2109.15099\n* MobileOne - https://arxiv.org/abs/2206.04040\n* MobileViT - https://arxiv.org/abs/2110.02178\n* MobileViT-V2 - https://arxiv.org/abs/2206.02680\n* MViT-V2 (Improved Multiscale Vision Transformer"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2211.12905\n* gMLP - https://arxiv.org/abs/2105.08050\n* GPU-Efficient Networks - https://arxiv.org/abs/2006.14090\n* Halo Nets - https://arxiv.org/abs/2103.12731\n* HRNet - https://arxiv.org/abs/1908.07919\n* InceptionNeXt - https://arxiv.org/abs/2303.16900\n* Inception-V3 - https://arxiv.org/abs/1512.00567\n* Inception-ResNet-V2 and Inception-V4 - https://arxiv.org/abs/1602.07261\n* Lambda Networks - https://arxiv.org/abs/2102.08602\n* LeViT (Vision Transformer in ConvNet's Clothing"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2302.06675"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2103.00112\n* TResNet - https://arxiv.org/abs/2003.13630\n* Twins (Spatial Attention in Vision Transformers"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2105.03404\n* ResNet/ResNeXt\n    * ResNet (v1b/v1.5"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2103.17239\n* CoaT (Co-Scale Conv-Attentional Image Transformers"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1911.11907\n* GhostNet-V2 - https://arxiv.org/abs/2211.12905\n* gMLP - https://arxiv.org/abs/2105.08050\n* GPU-Efficient Networks - https://arxiv.org/abs/2006.14090\n* Halo Nets - https://arxiv.org/abs/2103.12731\n* HRNet - https://arxiv.org/abs/1908.07919\n* InceptionNeXt - https://arxiv.org/abs/2303.16900\n* Inception-V3 - https://arxiv.org/abs/1512.00567\n* Inception-ResNet-V2 and Inception-V4 - https://arxiv.org/abs/1602.07261\n* Lambda Networks - https://arxiv.org/abs/2102.08602\n* LeViT (Vision Transformer in ConvNet's Clothing"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1709.01507\n    * Selective Kernel (SK"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1801.04590"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2106.13797\n* RegNet - https://arxiv.org/abs/2003.13678\n* RegNetZ - https://arxiv.org/abs/2103.06877\n* RepVGG - https://arxiv.org/abs/2101.03697\n* RepGhostNet - https://arxiv.org/abs/2211.06088\n* RepViT - https://arxiv.org/abs/2307.09283\n* ResMLP - https://arxiv.org/abs/2105.03404\n* ResNet/ResNeXt\n    * ResNet (v1b/v1.5"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2012.12877\n* DeiT-III - https://arxiv.org/pdf/2204.07118.pdf\n* DenseNet - https://arxiv.org/abs/1608.06993\n* DLA - https://arxiv.org/abs/1707.06484\n* DPN (Dual-Path Network"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1708.04896"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2101.11605\n    * CBAM - https://arxiv.org/abs/1807.06521\n    * Effective Squeeze-Excitation (ESE"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2106.13112\n* VovNet V2 and V1 - https://arxiv.org/abs/1911.06667\n* Xception - https://arxiv.org/abs/1610.02357\n* Xception (Modified Aligned, Gluon"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1807.11626\n    * MobileNet-V2 - https://arxiv.org/abs/1801.04381\n    * Single-Path NAS - https://arxiv.org/abs/1904.02877\n    * TinyNet - https://arxiv.org/abs/2010.14819\n* EfficientViT (MIT"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2201.03545\n* ConvNeXt-V2 - http://arxiv.org/abs/2301.00808\n* ConViT (Soft Convolutional Inductive Biases Vision Transformers"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1802.02611\n* Xception (Modified Aligned, TF"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1805.09501"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2211.12905 (thanks https://github.com/yehuitang"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2302.05442"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2105.01601\n* MobileNet-V3 (MBConvNet w/ Efficient Head"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1710.09412"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2104.12533\n* Vision Transformer - https://arxiv.org/abs/2010.11929\n* VOLO (Vision Outlooker"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2103.07579\n* Res2Net - https://arxiv.org/abs/1904.01169\n* ResNeSt - https://arxiv.org/abs/2004.08955\n* ReXNet - https://arxiv.org/abs/2007.00992\n* SelecSLS - https://arxiv.org/abs/1907.00837\n* Selective Kernel Networks - https://arxiv.org/abs/1903.06586\n* Sequencer2D - https://arxiv.org/abs/2205.01972\n* Swin S3 (AutoFormerV2"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1512.00567\n* Inception-ResNet-V2 and Inception-V4 - https://arxiv.org/abs/1602.07261\n* Lambda Networks - https://arxiv.org/abs/2102.08602\n* LeViT (Vision Transformer in ConvNet's Clothing"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1905.11946\n    * EfficientNet-EdgeTPU (S, M, L"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2307.09283"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1602.07261\n* Lambda Networks - https://arxiv.org/abs/2102.08602\n* LeViT (Vision Transformer in ConvNet's Clothing"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2102.06171, https://github.com/deepmind/deepmind-research/tree/master/nfnets"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2211.06088\n* RepViT - https://arxiv.org/abs/2307.09283\n* ResMLP - https://arxiv.org/abs/2105.03404\n* ResNet/ResNeXt\n    * ResNet (v1b/v1.5"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2103.16302\n* PVT-V2 (Improved Pyramid Vision Transformer"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2212.08013"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1804.04235"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1911.11929\n* DeiT - https://arxiv.org/abs/2012.12877\n* DeiT-III - https://arxiv.org/pdf/2204.07118.pdf\n* DenseNet - https://arxiv.org/abs/1608.06993\n* DLA - https://arxiv.org/abs/1707.06484\n* DPN (Dual-Path Network"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1711.07971\n    * Squeeze-and-Excitation (SE"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2103.12731\n    * Involution - https://arxiv.org/abs/2103.06255\n    * Lambda Layer - https://arxiv.org/abs/2102.08602\n    * Non-Local (NL"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2004.08955\n    * Shifted Window (SWIN"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1910.03151v4\n    * Squeeze-and-Excitation Networks (SEResNet"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1909.13719"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2103.06877\n* RepVGG - https://arxiv.org/abs/2101.03697\n* RepGhostNet - https://arxiv.org/abs/2211.06088\n* RepViT - https://arxiv.org/abs/2307.09283\n* ResMLP - https://arxiv.org/abs/2105.03404\n* ResNet/ResNeXt\n    * ResNet (v1b/v1.5"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2006.00719"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1812.03443\n    * MixNet - https://arxiv.org/abs/1907.09595\n    * MNASNet B1, A1 (Squeeze-Excite"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1707.01629\n* EdgeNeXt - https://arxiv.org/abs/2206.10589\n* EfficientFormer - https://arxiv.org/abs/2206.01191\n* EfficientNet (MBConvNet Family"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1911.06667\n    * Efficient Channel Attention (ECA"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2103.14030\n\n## Results\n\nModel validation results can be found in the [results tables](results/README.md"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2105.12723\n* BEiT - https://arxiv.org/abs/2106.08254\n* Big Transfer ResNetV2 (BiT"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2102.08602\n    * Non-Local (NL"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1907.09595\n    * MNASNet B1, A1 (Squeeze-Excite"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2103.06255\n    * Lambda Layer - https://arxiv.org/abs/2102.08602\n    * Non-Local (NL"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2010.14819\n* EfficientViT (MIT"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1610.02357\n* Xception (Modified Aligned, Gluon"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1812.01187\n    * Weakly-supervised (WSL"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1905.04899"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2110.02178\n* MobileViT-V2 - https://arxiv.org/abs/2206.02680\n* MViT-V2 (Improved Multiscale Vision Transformer"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2104.06399\n* CoAtNet (Convolution and Attention"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2101.11605\n* CaiT (Class-Attention in Image Transformers"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1907.00837\n* Selective Kernel Networks - https://arxiv.org/abs/1903.06586\n* Sequencer2D - https://arxiv.org/abs/2205.01972\n* Swin S3 (AutoFormerV2"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2112.01526\n* NASNet-A - https://arxiv.org/abs/1707.07012\n* NesT - https://arxiv.org/abs/2105.12723\n* NFNet-F - https://arxiv.org/abs/2102.06171\n* NF-RegNet / NF-ResNet - https://arxiv.org/abs/2101.08692\n* PNasNet - https://arxiv.org/abs/1712.00559\n* PoolFormer (MetaFormer"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1911.04252\n    * EfficientNet AdvProp (B0-B8"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2303.14189\n  * MobileOne - https://arxiv.org/abs/2206.04040\n  * InceptionNeXt - https://arxiv.org/abs/2303.16900\n  * RepGhostNet - https://arxiv.org/abs/2211.06088 (thanks https://github.com/ChengpengChen"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1801.04381\n    * Single-Path NAS - https://arxiv.org/abs/1904.02877\n    * TinyNet - https://arxiv.org/abs/2010.14819\n* EfficientViT (MIT"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/README.md"}]}