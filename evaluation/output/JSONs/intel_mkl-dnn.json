{"somef_provenance": {"somef_version": "0.9.4", "somef_schema_version": "1.0.0", "date": "2023-12-21 18:55:47"}, "code_repository": [{"result": {"value": "https://github.com/oneapi-src/oneDNN", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "owner": [{"result": {"value": "oneapi-src", "type": "Organization"}, "confidence": 1, "technique": "GitHub_API"}], "date_created": [{"result": {"value": "2016-05-09T23:26:42Z", "type": "Date"}, "confidence": 1, "technique": "GitHub_API"}], "date_updated": [{"result": {"value": "2023-12-21T01:42:39Z", "type": "Date"}, "confidence": 1, "technique": "GitHub_API"}], "license": [{"result": {"value": "https://api.github.com/licenses/apache-2.0", "type": "License", "name": "Apache License 2.0", "url": "https://api.github.com/licenses/apache-2.0", "spdx_id": "Apache-2.0"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   ============================================================================\n\n   Copyright 2016-2023 Intel Corporation\n   Copyright 2018 YANDEX LLC\n   Copyright 2019-2023 FUJITSU LIMITED\n   Copyright 2020-2023 Arm Ltd. and affiliates\n   Copyright 2020-2022 Codeplay Software Limited\n   Copyright 2021 Alanna Tempest\n   Copyright 2022-2023 IBM Corporation\n   Copyright 2023 KNS Group LLC (YADRO)\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n   This distribution includes third party software (\"third party programs\").\n   This third party software, even if included with the distribution of\n   the Intel software, may be governed by separate license terms, including\n   without limitation, third party license terms, other Intel software license\n   terms, and open source software license terms. These separate license terms\n   govern your use of the third party programs as set forth in the\n   \"THIRD-PARTY-PROGRAMS\" file.\n", "type": "File_dump"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/intel/mkl-dnn/main/LICENSE"}, {"result": {"value": "Copyright 2008, Google Inc.\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n    * Redistributions of source code must retain the above copyright\nnotice, this list of conditions and the following disclaimer.\n    * Redistributions in binary form must reproduce the above\ncopyright notice, this list of conditions and the following disclaimer\nin the documentation and/or other materials provided with the\ndistribution.\n    * Neither the name of Google Inc. nor the names of its\ncontributors may be used to endorse or promote products derived from\nthis software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n", "type": "File_dump"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/intel/mkl-dnn/main/tests/gtests/gtest/LICENSE"}, {"result": {"value": "oneDNN is licensed under [Apache License Version 2.0](LICENSE). Refer to the\n\"[LICENSE](LICENSE)\" file for the full license text and copyright notice.\n\nThis distribution includes third party software governed by separate license\nterms.\n\n3-clause BSD license:\n* [Xbyak](https://github.com/herumi/xbyak)\n* [gtest](https://github.com/google/googletest)\n* [Instrumentation and Tracing Technology API (ITT API)](https://github.com/intel/ittapi)\n* [CMake](https://github.com/Kitware/CMake)\n\n2-clause BSD license:\n* [Sphinx](https://www.sphinx-doc.org/)\n\nApache License Version 2.0:\n* [Xbyak_aarch64](https://github.com/fujitsu/xbyak_aarch64)\n* [LLVM](https://llvm.org)\n\nBoost Software License, Version 1.0:\n* [Boost C++ Libraries](https://www.boost.org/)\n\nMIT License:\n* [Intel Graphics Compute Runtime for oneAPI Level Zero and OpenCL Driver](https://github.com/intel/compute-runtime)\n* [Intel Graphics Compiler](https://github.com/intel/intel-graphics-compiler)\n* [oneAPI Level Zero](https://github.com/oneapi-src/level-zero)\n* [Doxyrest](https://github.com/vovkos/doxyrest)\n* [Intel Metrics Discovery Application Programming Interface](https://github.com/intel/metrics-discovery)\n\nThis third party software, even if included with the distribution of\nthe Intel software, may be governed by separate license terms, including\nwithout limitation, third party license terms, other Intel software license\nterms, and open source software license terms. These separate license terms\ngovern your use of the third party programs as set forth in the\n\"[THIRD-PARTY-PROGRAMS](THIRD-PARTY-PROGRAMS)\" file.\n", "type": "Text_excerpt", "original_header": "License"}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/intel/mkl-dnn/main/README.md"}], "description": [{"result": {"value": "oneAPI Deep Neural Network Library (oneDNN)", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Text_excerpt", "value": "oneAPI Deep Neural Network Library (oneDNN) is an open-source cross-platform\nperformance library of basic building blocks for deep learning applications.\noneDNN is part of [oneAPI](https://oneapi.io).\nThe library is optimized for Intel(R) Architecture Processors, Intel Graphics,\nand Arm\\* 64-bit Architecture (AArch64)-based processors. oneDNN has\nexperimental support for the following architectures: NVIDIA\\* GPU,\nAMD\\* GPU, OpenPOWER\\* Power ISA (PPC64), IBMz\\* (s390x), and RISC-V. \noneDNN is intended for deep learning applications and framework\ndevelopers interested in improving application performance on CPUs and GPUs.\nDeep learning practitioners should use one of the\n[applications enabled with oneDNN](#applications-enabled-with-onednn).\n \n", "original_header": "oneAPI Deep Neural Network Library (oneDNN)"}, "confidence": 0.9704388929863106, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/intel/mkl-dnn/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "See Intel's [Security Center](https://www.intel.com/content/www/us/en/security-center/default.html)\nfor information on how to report a potential security issue or vulnerability. \nSee also: [Security Policy](SECURITY.md)\n \n", "original_header": "Security"}, "confidence": 0.9234524176499583, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/intel/mkl-dnn/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "Intel, the Intel logo, Arc, Intel Atom, Intel Core, Iris,\nOpenVINO, the OpenVINO logo, Pentium, VTune, and Xeon are trademarks\nof Intel Corporation or its subsidiaries. \nOpenCL and the OpenCL logo are trademarks of Apple Inc. used by permission\nby Khronos. \n", "original_header": "Trademark Information"}, "confidence": 0.9676197780221755, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/intel/mkl-dnn/main/README.md"}], "name": [{"result": {"value": "oneDNN", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}], "full_name": [{"result": {"value": "oneapi-src/oneDNN", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}], "issue_tracker": [{"result": {"value": "https://api.github.com/repos/oneapi-src/oneDNN/issues", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "forks_url": [{"result": {"value": "https://api.github.com/repos/oneapi-src/oneDNN/forks", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "stargazers_count": [{"result": {"value": 3317, "type": "Number"}, "confidence": 1, "technique": "GitHub_API"}], "keywords": [{"result": {"value": "aarch64, amx, avx512, bfloat16, cpp, deep-learning, deep-neural-networks, library, oneapi, onednn, openmp, performance, sycl, tbb, vnni, x64, x86-64, xe-architecture", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}], "forks_count": [{"result": {"value": 927, "type": "Number"}, "confidence": 1, "technique": "GitHub_API"}], "download_url": [{"result": {"value": "https://github.com/intel/mkl-dnn/releases", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "programming_languages": [{"result": {"value": "C++", "name": "C++", "type": "Programming_language", "size": 47717200}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "C", "name": "C", "type": "Programming_language", "size": 3674589}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "Python", "name": "Python", "type": "Programming_language", "size": 584881}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "CMake", "name": "CMake", "type": "Programming_language", "size": 327218}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "Assembly", "name": "Assembly", "type": "Programming_language", "size": 22181}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "Shell", "name": "Shell", "type": "Programming_language", "size": 1241}, "confidence": 1, "technique": "GitHub_API"}], "releases": [{"result": {"type": "Release", "value": "https://api.github.com/repos/oneapi-src/oneDNN/releases/133999570", "tag": "v3.3.3", "name": "v3.3.3", "author": {"name": "vpirogov", "type": "User"}, "description": "This is a patch release containing the following changes to v3.3.2:\r\n* Fixed performance regression in int8 convolutions on processors with Intel AVX-512 and Intel DL Boost support (a00661ff735e5448ef3a80e4e2df7a1556f8a84f)\r\n* Fixed race condition during library initialization on Intel Data Center GPU Max Series (7dfcd116e245e4a167a64bd39a24e957d2b939de)\r\n* Fixed accuracy issue in experimental Graph Compiler with LLVM code generator (8892e7efadeaf42d75f75e64d095635458836cd7)\r\n* Disabled int8 RNN implementation for cases with non-trivial strides (2195e4b23d57c38a439c50232783f654b96f575c)\r\n* Fixed incorrect results in bfloat16 convolution implementation on processors with Intel AMX support (9f00af9312a9b76a1880e1aaac513188793ecaa7)\r\n* Fixed incorrect results in fp16 and int8 convolution on Intel Core Ultra integrated GPUs (69cef84c4f09398858393035eafa2bd4a29ec0b0, 79bc6cc0477db1ce7e732f20d005ff2b9e88390e, c9c0b09c5e64114eada1b6beb7f6db36331e0fac)\r\n", "tarball_url": "https://api.github.com/repos/oneapi-src/oneDNN/tarball/v3.3.3", "zipball_url": "https://api.github.com/repos/oneapi-src/oneDNN/zipball/v3.3.3", "html_url": "https://github.com/oneapi-src/oneDNN/releases/tag/v3.3.3", "url": "https://api.github.com/repos/oneapi-src/oneDNN/releases/133999570", "release_id": 133999570, "date_created": "2023-12-14T19:36:58Z", "date_published": "2023-12-14T19:40:23Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/oneapi-src/oneDNN/releases/132132792", "tag": "v3.3.2", "name": "v3.3.2", "author": {"name": "vpirogov", "type": "User"}, "description": "This is a patch release containing the following changes to v3.3.1:\r\n* Fixed incorrect results in bfloat16 reorder on Intel Core Ultra integrates GPUs (9025980286c506908f98819e068a047a1d268842, ed9de2afd1fede32a317cbc5df953dfe997e78ea, 0c6bda10b3ea760205d4707a554b76045ef6f964)\r\n* Fixed incorrect results in matmul, inner product, and RNN primitives on Intel Core Ultra integrated GPUs (6edab9f01ec5cf8b30ee0b474aa25417f0493897)\r\n* Updated compiler optimization flags for AArch64 processors to make build portable (8829c249b713dddc87c2669120a9798e202ac633)\r\n* Fixed segmentation fault during library initialization on AArch64 processors (3e15c6113ffeff3545775cbcca9bd84911856cb9)\r\n", "tarball_url": "https://api.github.com/repos/oneapi-src/oneDNN/tarball/v3.3.2", "zipball_url": "https://api.github.com/repos/oneapi-src/oneDNN/zipball/v3.3.2", "html_url": "https://github.com/oneapi-src/oneDNN/releases/tag/v3.3.2", "url": "https://api.github.com/repos/oneapi-src/oneDNN/releases/132132792", "release_id": 132132792, "date_created": "2023-11-30T15:53:57Z", "date_published": "2023-11-30T15:55:14Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/oneapi-src/oneDNN/releases/130002569", "tag": "v3.3.1", "name": "v3.3.1", "author": {"name": "vpirogov", "type": "User"}, "description": "This is a patch release containing the following changes to v3.3:\r\n* Fixed int8 convolution accuracy issue on Intel GPUs (09c87c79bccbad8fa451b224a0f07f87095e3907)\r\n* Switched internal stream to in-order mode for NVIDIA and AMD GPUs to avoid synchronization issues (db01d62b3fc80897d88dc42f4dcdfcb0d90c131a)\r\n* Fixed runtime error for `avgpool_bwd` operation in Graph API (d025ef6620b131f3487bb748866ddd9d7225c09f, 9e0602ad37afa18d46f407cb52577f1afead238b, e0dc1b3d070313052f5fd6ac739778d45b57859c)\r\n* Fixed benchdnn error reporting for some Graph API cases (98dc9dbecb3f36234474c9d6e96ab6571497633b)\r\n* Fixed accuracy issue in experimental Graph Compiler for int8 MHA variant from StarCoder model (5476ef7c165d943fbce94ca0f44a13d6868e65f3)\r\n* Fixed incorrect results for layer normalization with trivial dimensions on Intel GPUs (a2ec0a0c5805314220db925e1323e4675e3ca379)\r\n* Removed redundant synchronization for out-of-order SYCL queues (a96e9b1a6769171e74b0b8e031489303438906e5)\r\n* Fixed runtime error in experimental Graph Compiler for int8 MLP subgraph from LLAMA model (595543dd093df3e92621c253d6da3f9092ec7ff8)\r\n* Fixed `SEGFAULT` in experimental Graph Compiler for fp32 MLP subgraph (42071057abb2fcbbca6ed67117bdb1a5ee3dc0cd)\r\n* Fixed incorrect results in experimental Graph Compiler for MLP subgraph (57e14b56d4e6fab2ab49dbd47fd579482d79535a)\r\n* Fixed the issue with f16 inner product primitive with s8 output returning `unimplemented` on Intel GPUs (bf12207b0312c0174f0c47ae0d3abd70edc31957, 800b5e9613bd0994af82706ef024ad2b453be2b6, ec7054a2c79ae33d3db4ff04ce11360c2c896d56)\r\n* Fixed incorrect results for int8 deconvolution with zero-points on processors with Intel AMX instructions support (55d2cecd698f865efac2e1dbf2f701b4b8095df1)\r\n", "tarball_url": "https://api.github.com/repos/oneapi-src/oneDNN/tarball/v3.3.1", "zipball_url": "https://api.github.com/repos/oneapi-src/oneDNN/zipball/v3.3.1", "html_url": "https://github.com/oneapi-src/oneDNN/releases/tag/v3.3.1", "url": "https://api.github.com/repos/oneapi-src/oneDNN/releases/130002569", "release_id": 130002569, "date_created": "2023-11-17T18:00:43Z", "date_published": "2023-11-17T18:02:32Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/oneapi-src/oneDNN/releases/124109849", "tag": "v3.3", "name": "v3.3", "author": {"name": "harrymao2022", "type": "User"}, "description": "# Performance Optimizations\r\n* Intel Architecture Processors:\r\n  * Improved performance for 4th generation Intel Xeon Scalable processors (formerly Sapphire Rapids).\r\n  * Improved int8 convolution performance with zero points on processors with Intel AMX instruction set support.\r\n  * Improved performance for the future Intel Xeon Scalable processors (code-named Sierra Forest and Granite Rapids). This functionality is disabled by default and can be enabled via [CPU dispatcher control](https://oneapi-src.github.io/oneDNN/dev_guide_cpu_dispatcher_control.html).\r\n  * Improved fp32 and int8 convolution performance for cases with small numbers of input channels for processors with Intel AVX-512 and/or Intel AMX instruction set support.\r\n  * Improved s32 binary primitive performance.\r\n  * Improved fp16, fp32, and int8 convolution performance for processors with Intel AVX2 instructions support.\r\n  * Improved performance of subgraphs with convolution, matmul, avgpool, maxpool, and softmax operations followed by unary or binary operations with Graph API.\r\n  * Improved performance of convolution for depthwise cases with Graph API.\r\n  * **[experimental]** Improved performance of LLAMA2 MLP block with Graph Compiler.\r\n* Intel Graphics Products:\r\n  * Improved performance for the Intel Data Center GPU Max Series (formerly Ponte Vecchio).\r\n  * Improved performance for Intel Arc graphics (formerly Alchemist and DG2) and the Intel Data Center GPU Flex Series (formerly Arctic Sound-M).\r\n  * Reduced RNN primitive initialization time on Intel GPUs.\r\n* AArch64-based Processors:\r\n  * Improved fp32 to bf16 reorder performance.\r\n  * Improved max pooling performance with Arm Compute Library (ACL).\r\n  * Improved dilated convolution performance for depthwise cases with ACL.\r\n \r\n# Functionality\r\n  * Introduced group normalization primitive support. The functionality is currently available on CPUs.\r\n* Intel CPUs:\r\n  * Introduced support for zero points in int8 convolution with groups and 3D spatial.\r\n# Usability\r\n  * Extended [verbose mode](https://oneapi-src.github.io/oneDNN/dev_guide_verbose.html) output:\r\n    * Improved diagnostics on engine creation errors.\r\n    * Added information on Graph API calls.\r\n    * Added information on strides for non-dense memory objects.\r\n    * Added values of runtime dimension.\r\n    * Added indication that primitive descriptor was created with `any` memory format tag.\r\n  * Introduced [examples for Graph API](https://github.com/oneapi-src/oneDNN/tree/master/examples/graph).\r\n  * Graph API constant tensor cache is now disabled by default and requires opt-in with [`dnnl::graph::set_constant_tensor_cache()`](https://oneapi-src.github.io/oneDNN/group_dnnl_graph_api_constant_tensor_cache.html#doxid-group-dnnl-graph-api-constant-tensor-cache-1ga9e37974d35ff5aafe1cbae2f69a2ab00) call.\r\n  * Reduced oneDNN Graph API memory consumption in certain scenarios.\r\n \r\n# Validation \r\n  * Extended benchdnn performance reporting with primitive creation time.  \r\n  * Introduced [cold cache mode](https://github.com/oneapi-src/oneDNN/blob/master/tests/benchdnn/doc/cold_cache.md) in benchdnn. \r\n \r\n# Known Limitations \r\n  * Current GPU OpenCL runtime for Linux has an issue resulting in convolution producing incorrect results on integrated GPUs based on Xe architecture. SYCL configuration is not affected. \r\n  * Pooling, resampling, prelu, batch normalization, layer normalization, and eltwise primitives may sporadically produce incorrect results on Intel Arc GPUs on Windows. \r\n  * Current GPU driver for Linux has an issue resulting in program hangs or crashes when oneDNN primitives are executed concurrently on Intel Datacenter GPU Max Series. \r\n  * Extensive use of RNN primitive on Intel GPUs with default primitive cache setting may lead to a device reboot. Workaround: consider reducing primitive cache size to 100. \r\n  * Int8 deconvolution with signed weights and activations may produce incorrect results of processors with Intel AMX support. \r\n  * Int8 softmax may fail crash on Windows in SYCL debug configuration. \r\n \r\n# Thanks to these Contributors \r\nThis release contains contributions from the project core team as well as Amy Wignall @AmyWignall-arm, @baibeta, Benjamin Taylor @bentaylorhk-arm, Ilya Lavrenov @ilya-lavrenov, Kentaro Kawakami @kawakami-k, Milos Puzovic @milpuz01, Renato Barros Arantes @renato-arantes, @snadampal, @sparkyrider, and Thomas K\u00f6ppe @tkoeppe. We would also like to thank everyone who asked questions and reported issues.\r\n", "tarball_url": "https://api.github.com/repos/oneapi-src/oneDNN/tarball/v3.3", "zipball_url": "https://api.github.com/repos/oneapi-src/oneDNN/zipball/v3.3", "html_url": "https://github.com/oneapi-src/oneDNN/releases/tag/v3.3", "url": "https://api.github.com/repos/oneapi-src/oneDNN/releases/124109849", "release_id": 124109849, "date_created": "2023-10-04T20:03:56Z", "date_published": "2023-10-06T20:48:14Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/oneapi-src/oneDNN/releases/121748940", "tag": "v3.3-rc", "name": "v3.3-rc", "author": {"name": "harrymao2022", "type": "User"}, "description": "# Performance Optimizations\r\n* Intel Architecture Processors:\r\n  * Improved performance for 4th generation Intel Xeon Scalable processors (formerly Sapphire Rapids).\r\n  * Improved int8 convolution performance with zero points on processors with Intel AMX instruction set support.\r\n  * Improved performance for the future Intel Xeon Scalable processors (code-named Sierra Forest and Granite Rapids). This functionality is disabled by default and can be enabled via [CPU dispatcher control](https://oneapi-src.github.io/oneDNN/dev_guide_cpu_dispatcher_control.html).\r\n  * Improved fp32 and int8 convolution performance for cases with small numbers of input channels for processors with Intel AVX-512 and/or Intel AMX instruction set support.\r\n  * Improved s32 binary primitive performance.\r\n  * Improved fp16, fp32, and int8 convolution performance for processors with Intel AVX2 instructions support.\r\n  * Improved performance of subgraphs with convolution, matmul, avgpool, maxpool, and softmax operations followed by unary or binary operations with Graph API.\r\n  * Improved performance of convolution for depthwise cases with Graph API.\r\n  * **[experimental]** Improved performance of LLAMA2 MLP block with Graph Compiler.\r\n* Intel Graphics Products:\r\n  * Improved performance for the Intel Data Center GPU Max Series (formerly Ponte Vecchio).\r\n  * Improved performance for Intel Arc graphics (formerly Alchemist and DG2) and the Intel Data Center GPU Flex Series (formerly Arctic Sound-M).\r\n  * Reduced RNN primitive initialization time on Intel GPUs.\r\n* AArch64-based Processors:\r\n  * Improved fp32 to bf16 reorder performance.\r\n  * Improved max pooling performance with Arm Compute Library (ACL).\r\n  * Improved dilated convolution performance for depthwise cases with ACL.\r\n \r\n# Functionality\r\n  * Introduced group normalization primitive support. The functionality is currently available on CPUs.\r\n* Intel CPUs:\r\n  * Introduced support for zero points in int8 convolution with groups and 3D spatial.\r\n# Usability\r\n  * Extended [verbose mode](https://oneapi-src.github.io/oneDNN/dev_guide_verbose.html) output:\r\n    * Improved diagnostics on engine creation errors.\r\n    * Added information on Graph API calls.\r\n    * Added information on strides for non-dense memory objects.\r\n    * Added values of runtime dimension.\r\n    * Added indication that primitive descriptor was created with `any` memory format tag.\r\n  * Introduced [examples for Graph API](https://github.com/oneapi-src/oneDNN/tree/master/examples/graph).\r\n  * Graph API constant tensor cache is now disabled by default and requires opt-in with [`dnnl::graph::set_constant_tensor_cache()`](https://oneapi-src.github.io/oneDNN/group_dnnl_graph_api_constant_tensor_cache.html#doxid-group-dnnl-graph-api-constant-tensor-cache-1ga9e37974d35ff5aafe1cbae2f69a2ab00) call.\r\n  * Reduced oneDNN Graph API memory consumption in certain scenarios.\r\n \r\n# Validation\r\n  * Extended benchdnn performance reporting with primitive creation time.\r\n  * Introduced [cold cache mode](https://github.com/oneapi-src/oneDNN/blob/master/tests/benchdnn/doc/cold_cache.md) in benchdnn.\r\n \r\n# Thanks to these Contributors \r\nThis release contains contributions from the project core team as well as Amy Wignall @AmyWignall-arm, @baibeta, Benjamin Taylor @bentaylorhk-arm, Ilya Lavrenov @ilya-lavrenov, Kentaro Kawakami @kawakami-k, Milos Puzovic @milpuz01, Renato Barros Arantes @renato-arantes, @snadampal, @sparkyrider, and Thomas K\u00f6ppe @tkoeppe. We would also like to thank everyone who asked questions and reported issues.\r\n", "tarball_url": "https://api.github.com/repos/oneapi-src/oneDNN/tarball/v3.3-rc", "zipball_url": "https://api.github.com/repos/oneapi-src/oneDNN/zipball/v3.3-rc", "html_url": "https://github.com/oneapi-src/oneDNN/releases/tag/v3.3-rc", "url": "https://api.github.com/repos/oneapi-src/oneDNN/releases/121748940", "release_id": 121748940, "date_created": "2023-09-19T17:10:47Z", "date_published": "2023-09-19T18:21:15Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/oneapi-src/oneDNN/releases/116095117", "tag": "v2.7.5", "name": "v2.7.5", "author": {"name": "vpirogov", "type": "User"}, "description": "This is a patch release containing the following changes to v2.7.4:\r\n* Fixed a correctness issue in fp32 batched matmul with transposed source tensor on processors with Intel AVX-512 instruction set support (1a9b80d7ad6856437c3e0b504bb53dca772eb0fe)\r\n* Improved batched matmul performance on processors with Intel AMX instructions support (8c20f62dbcd4d622c8a279b7c81dacb629f1de41, acb8e12b0f3e70d2e80543e31a91362c8852bbaf)\r\n* Fixed a correctness issue in int8 convolution primitive with zero points on processors with Intel AVX2 and Intel DL Boost support (0abbf225ce906987bc3728252b5842fb0239daab, d3a9f02e50334a0ebe102dd8bdb7887deeb12ec5)\r\n* Improved convolution performance with small number of input channels on processors with Intel AVX-512 instruction set support (fc7fced9988124027220fb53dfb16022c9be35c0)\r\n", "tarball_url": "https://api.github.com/repos/oneapi-src/oneDNN/tarball/v2.7.5", "zipball_url": "https://api.github.com/repos/oneapi-src/oneDNN/zipball/v2.7.5", "html_url": "https://github.com/oneapi-src/oneDNN/releases/tag/v2.7.5", "url": "https://api.github.com/repos/oneapi-src/oneDNN/releases/116095117", "release_id": 116095117, "date_created": "2023-08-11T16:36:07Z", "date_published": "2023-08-11T16:38:55Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/oneapi-src/oneDNN/releases/115048527", "tag": "v3.2.1", "name": "v3.2.1", "author": {"name": "vpirogov", "type": "User"}, "description": "This is a patch release containing the following changes to v3.2:\r\n* Fixed a potential issue `SEGFAULT` when oneDNN primitives created in parallel (0a6202f5000cf347995ab744c25aa26cabf2482d)\r\n* Replaced deprecated SYCL API `get_pointer` with `get_multi_ptr` (fdbff4591f952d02a0c934f854a9b225a7097a21, 51ed43bb5cb08f38b0b652255a13bb4072b2ee57)\r\n* Fixed an error in device indices detection for persistent cache (25575c2d20a9885640c89771c99a0d27b5444b4d)\r\n* Improved benchdnn performance results accuracy for Graph API (9dfe343992209ecc6eb1265a140b6f0db228d90a)\r\n* Fixed an issue with profiling API not respecting `ONEDNN_EXPERIMENTAL_PROFILING` build knob. This behavior manifests in apparent memory leak when oneDNN primitives are executed on a queue with enabled profiling (8d796efb609c33ecdd31e3e7b26d94d959dd83b9, 51a8f7ad892b1174d32cba8358804fad09b58f76, 2ca29381eeb5dde64d90468e440f87b6f9ad01d9)\r\n* Fixed a correctness issue in resampling primitive with binary and/or sum post-op on Intel CPUs (65ccd2506eeafb44822c682acfef97ef18bea09f, 4a0e087b405f4ebc682cf82c4a5bb96e9b9976d4, f333bb8c191fbfab368645aeac1c3a0d1892eda4)\r\n* Fixed a correctness issue in int8 matmul with zero-points for processors with Intel AVX2 and Intel DL Boost instructions support (ec0b2ee85fc2a2dbdeec10035c5ef5813d8fb5ea, 6d2e567c9361992adf235545c9fc2047184ed6e6)\r\n* Fixed a correctness issue in fp32 batched matmul with transposed source tensor on processors with Intel AVX-512 instruction set support (36f355e0773f79cca5a639a5a3558f45da57c35d)\r\n* Fixed a correctness issue in matmul and inner product with post-ops on processors with Intel AVX2 and Intel DL Boost with fp16 and bfloat16 instruction set support (b76d4cae333fc4e015d47eb737e10551daf30334)\r\n* Fixed a potential out of bounds issue during GPU kernel creation (190a9b28170f5326241c9c4ab6bc7964877e953d)\r\n* Updated build system to use TBB-provided CMake config file when available (40112196287e8866a7259df35f817229454d0c96)\r\n", "tarball_url": "https://api.github.com/repos/oneapi-src/oneDNN/tarball/v3.2.1", "zipball_url": "https://api.github.com/repos/oneapi-src/oneDNN/zipball/v3.2.1", "html_url": "https://github.com/oneapi-src/oneDNN/releases/tag/v3.2.1", "url": "https://api.github.com/repos/oneapi-src/oneDNN/releases/115048527", "release_id": 115048527, "date_created": "2023-08-04T00:53:31Z", "date_published": "2023-08-04T00:54:49Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/oneapi-src/oneDNN/releases/109732540", "tag": "v3.2", "name": "v3.2", "author": {"name": "harrymao2022", "type": "User"}, "description": "# Performance Optimizations\r\n* Intel Architecture Processors:\r\n  * Improved performance for 4th generation Intel Xeon Scalable Processor (formerly Sapphire Rapids).\r\n  * Improved performance for future Intel Xeon Scalable Processor (code-named Sierra Forest). The functionality is disabled by default and can be enabled via [CPU dispatcher control](https://oneapi-src.github.io/oneDNN/dev_guide_cpu_dispatcher_control.html).\r\n  * Improved fp32 inner product performance for processors with Intel AVX-512 instructions support.\r\n  * Improved bf16 and int8 matmul performance with runtime dimensions for processors with Intel AMX instructions support.\r\n\r\n* Intel Graphics Products:\r\n  * Improved performance for Intel Data Center GPU Max Series (formerly Ponte Vecchio).\r\n  * Improved performance for Intel Arc Graphics (formerly Alchemist and DG2) and Intel Data Center GPU Flex Series (formerly Arctic Sound-M).\r\n  * Reduced creation time for matmul, inner product, and RNN primitives.\r\n\r\n* AArch64-based Processors:\r\n  * Improved convolution performance with post-ops on processors with SVE support.\r\n  * Improved fp32 and fp16 depth-wise convolution performance with Arm Compute Library (ACL).\r\n  * Improved fp32 deconvolution performance for math mode `bf16` or `any` with ACL.\r\n\r\n* IBM Z Platform:\r\n  * Improved int8 matmul, inner product, and RNN performance for s390 z15 systems.\r\n\r\n# Functionality\r\n* **[experimental]** Introduced [Graph Compiler backend](https://oneapi-src.github.io/oneDNN/dev_guide_graph_compiler.html) for Graph API. Graph Compiler improves performance of composite operations like multi-head attention (MHA), multi-level perceptron (MLP), and convolution residual blocks for processors with Intel AVX-512 and Intel AMX instructions support.\r\n* Extended Graph API with boolean data type, [select](https://oneapi-src.github.io/oneDNN/dev_guide_op_select.html), and [pow](https://oneapi-src.github.io/oneDNN/dev_guide_op_pow.html) operations.\r\n* Introduced support for binary and eltwise post-ops in softmax primitives.\r\n* Introduced reference SYCL implementations of batch normalization, layer normalization, linear response normalization (LRN), binary, softmax, eltwise, pooling, PReLU, shuffle, and resampling primitives. These implementations address functional gaps on NVIDIA and AMD GPUs where support is missing in native libraries.\r\n\r\n* Intel Graphics Products:\r\n  * Introduced mixed precision support for binary primitives.\r\n\r\n* NVIDIA GPUs:\r\n  * Introduced bfloat16 support for deconvolution and softmax primitives.\r\n\r\n* AMD GPUs:\r\n  * Introduced support for inner product, convolution, deconvolution, batch normalization, and reorder primitives support.\r\n\r\n# Usability\r\n* Extended [verbose mode](https://oneapi-src.github.io/oneDNN/dev_guide_verbose.html) with additional capabilities, including information about implementation dispatching decisions and reasons for primitive creation errors.\r\n* Reduced stack consumption to less than 20 KB across implementations.\r\n* **[experimental]** Introduced [profiling API](https://oneapi-src.github.io/oneDNN/dev_guide_experimental.html#onednn-experimental-profiling) for SYCL and OpenCL applications.\r\n\r\n# Validation\r\n* Introduced fast performance validation mode (`--mode=F`) in [benchdnn](https://github.com/oneapi-src/oneDNN/tree/master/tests/benchdnn#readme). Testing speed is improved by initializing oneDNN objects in parallel and avoiding use of host memory when benchmarking GPU primitives.\r\n* Reduced benchdnn memory consumption in performance validation mode.\r\n* Introduced [smoke test set](https://github.com/oneapi-src/oneDNN/blob/rls-v3.2/cmake/options.cmake#L75) for benchdnn. This test set provides basic validation for all primitives.\r\n\r\n# Known Limitations\r\n* fp32 matmul with bfloat16 binary post-op may produce incorrect results on processors with Intel AVX2 and Intel DL Boost support.\r\n* fp32 convolution forward propagation with strides has performance regression on processors with Intel AVX-512 instructions support.\r\n* Resampling primitive with binary post-op may produce incorrect results on CPUs.\r\n* Extensive use of the RNN primitive on Intel GPUs with default primitive cache settings may lead to a device reboot. Workaround: consider reducing primitive cache size to 100.\r\n* Convolution and deconvolution primitives on Intel Arc GPUs on Windows may cause memory corruption under heavy repeated use.  \r\n* bfloat16 matmul primitive may crash on Intel Arc GPUs on Windows.\r\n* Pooling, resampling, PRelu, batch normalization, and layer normalization may sporadically produce incorrect results on Intel Arc GPUs on Windows.\r\n* oneDNN Graph partitions containing `ConvTransposeBackwardWeights` or int8 `matmul` operations may produce incorrect results on Intel Processor Graphics on Windows.\r\n* bfloat16 matmul primitive has performance regression with shapes 14x128:128x200:14x200 and 200x128:128x200:200x200 on the Intel Data Center GPU MAX Series.\r\n* oneDNN primitives may crash or produce incorrect results with tensors exceeding 4 Gb in size on Intel GPUs.\r\n* Softmax primitive with a NHWC memory format may produce incorrect results on the Intel Data Center GPU Max Series.\r\n* Inner product weight gradient may produce incorrect results on Intel Processor Graphics on Windows.\r\n\r\n# Thanks to the Contributors\r\nThis release contains contributions from the project core team as well as Abdelrauf @quickwritereader, Alexey Vishnyakov @SweetVishnya, Annop Wongwathanarat @annop-w, Anthony Roberts @anthony-linaro, Crefeda Rodrigues @cfRod, David Svantesson @davsva01, Fadi Arafeh @fadara01, Ilya Lavrenov @ilya-lavrenov, Jonathan Deakin @jondea, Kentaro Kawakami @kawakami-k, Milos Puzovic @milpuz01, RambabuSwargam @RambabuSwargam, Sai Teja @saiteja13427, Taiju Tsuiki @tzik. We would also like to thank everyone who asked questions and reported issues.\r\n", "tarball_url": "https://api.github.com/repos/oneapi-src/oneDNN/tarball/v3.2", "zipball_url": "https://api.github.com/repos/oneapi-src/oneDNN/zipball/v3.2", "html_url": "https://github.com/oneapi-src/oneDNN/releases/tag/v3.2", "url": "https://api.github.com/repos/oneapi-src/oneDNN/releases/109732540", "release_id": 109732540, "date_created": "2023-06-23T01:17:29Z", "date_published": "2023-06-23T18:31:10Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/oneapi-src/oneDNN/releases/107918805", "tag": "v3.1.1", "name": "v3.1.1", "author": {"name": "vpirogov", "type": "User"}, "description": "This is a patch release containing the following changes to v3.1:\r\n* Fixed correctness issue in pooling primitive with post-ops on Intel GPUs (4b7bc1a7bf16909003f63bf66d3d730cee00e5db)\r\n* Fixed segfault in `bfloat16` convolution on processors with Intel AMX support (461d55e65f2bc0f45fcdfc3405493226218d22ee)\r\n* Fixed correctness issue in deconvolution primitive with post-ops on Intel GPUs based on Xe-LP architecture (c8943f588e99f6251a443ee4eb5c274e9c942947, ad3c62f104b07d30cc0f5cf34ca7bf127041e4dc)\r\n* Fixed performance regression in `int8` convolution primitive with scales (7fa3b6f335893270cdd079f4f8aadd36cf8f490b, bb3ecc460605eae3ca8a8ee79a8d9122f195730b)\r\n* Fixed correctness issue in `int8` convolution primitive with zero points on processors with Intel AVX2 and Intel DL Boost support (d721767a554f9a4da70bd6bc1c27c00b1ea80cc2, f6365b1b2c6e6d79e59207dad090b9643224f147)\r\n* Fixed performance regression in `int8` inner product on processors with Intel AVX-512 and Intel DL Boost or Intel AMX support (2ede31e834a25ca14c648e8617b972148c94554c)\r\n* Fixed segfault in pooling primitive with post-ops on processors with Intel SSE4.1 support (d712173a5b9df2bdefd12cc94be2e83e64cfb433, e4085a706dd0b41c3d8171193b816a3c4e52c01d)\r\n* Fixed integer overflow in eltwise primitive on Intel GPUs (1932b3d04e574745d54802ee19e18bcbe8887e2d, be05c3392eaf86f2d897c5ec42a8860361c290b8, 148006b86f66e4af8f3ebd7db94980de487b9287, 2e643692480be21019b2b71db69e07729bfbf26c, b4423fbc11e574697d97eda18d4b0d8d7b1f60f3, 87fd48f48847463cbd1c42a39c9aa092158dbf2f, 9a66ac6f394071b05285b063a393acd297e3c662, 6ce52eb340486373670a9975c54449cf14a73d4f, 36bf079e7e99e0408ec11fe94cd64439f30b4014, 161d2b6416f4e9c17eabd1d45b8a3aeb2d4e9dd0, a5ef0788afcb719d22a311f91b31f3afca392a7c, d058bd8898b92330546d3f8d52335631fda5051a)\r\n* Fixed primitive creation error in large 3D convolutions on Intel GPUs (7c23d9e85ef328081f7d9836ebfffda755f4b496)\r\n* Fixed performance regression in `fp32` convolution primitive weight gradient on Intel GPUs (ff209f967c2bdfa1139779cf59dced374e2064c5, 87108392da71b06594356a18232ac1378e28adfc)\r\n* Fixed primitive creation error in `int8` convolution with zero points on Intel GPUs (cb9169397ceee206fece71f73b5d627ee9eea33f, 85e58af6b5cb1a9cd42cd602832c035a3b3a660f)\r\n* Fixed correctness issue in `fp32` convolution with Winograd algorithm on Intel GPUs (97ac88509bf8799fd03eb768faec302d44ce38dc)\r\n* Fixed primitive creation error in depthwise convolution on Intel GPUs based on Xe-LP architecture (51d608d24f09d6b0ad2d60008f09646dbf79ee60)\r\n* Fixed segfault during Graph partition compilation (a5d35682307ec81107f603b66c5f4ca95f421fbb)\r\n* Fixed crashes in inner product with unsupported weight formats on Intel64 CPUs (c0f4e93903f1c32bef8378d58177ef971c400e90)\r\n* Fixed an issue with compilation of Graph partitions containing matmul and using destination tensor layout `any` on Intel GPUs (ab2041d39862de747535037eb5a73c675d93d323, f2c457d72896d6c86245a6c6e80539b842aec369)\r\n* Improved accuracy of eltwise primitive with `gelu_erf` algorithm on Intel64 CPUs (e67abefadbb4fd73ea6a4d3981965bc56eb77b97)\r\n* Fixed correctness issue in `int8` matmul and inner product primitives on Intel GPUs based on Xe-HPG and Xe-HPC architecture (36aa6224ebae1413a6badd43ffc96d3412c8f8ec)\r\n* Fixed potential correctness issue in `bfloat16` convolution weight gradient on processors with Intel AMX support (c93e673bba299fdc62733f22d65d91f4dbc300dd, 8da108375bc02b08a385b167a49aa8d1189b66d6, f7acf9877b368a5f704dcc9efcb913345b477bbc)\r\n* Fixed memory corruption in inner product weight gradient on processors with Intel AMX support (b56a89e1b977d793f2de89dc95bb7f07f2449cd8)\r\n* Fixed integer overflow issue in convolution primitive on Intel GPUs (774deabcbb9dc3e452bdafcde5e92a55c3701309, 663c2e44272c57a97e5f20e3a7a28cb9ac91ae01, 12d57430c66eb4d83532a2338443faae7be8ea5c, 31ac0e045981b03434c7592fe84af97a79a3d4a8, e3cb07d60473c23829db987384e5366b924e22c4)\r\n* Fixed correctness issue in matmul primitive with broadcasted bias on Intel GPUs (3ba7e8b9c14948da35c86d4d74725f0d23511fc8)\r\n* Fixed correctness issue in inner product primitive with post-ops on processors with Intel AVX2 support (69260f661030f66b34fefeab97044c81769462a9)\r\n* Fixed out of bounds prefetching in matmul and inner product primitives on Intel GPUs (2b8f6b16dd894f7c13c33a9fd5c497cff10d66c2)\r\n* Fixed dispatching issues for `fp32` inner product implementation on processors with Intel AVX2 and Intel DL Boost supprt (f27dedbfc093f51032a4580198bb80579440dc15, f8d7c2e40a965fc52521d4ba9c793d8adc2be4e1)\r\n* Fixed division by zero issue in eltwise and eltwise post-op on Intel GPUs (f5654f55582f003c22aee23e5a91acfead8d1e1b, a18c19e654483b547bbe791d0640eceef4ef2e79, a7c8cbc428ad361e2f290605be1280268eb8ea56, 44355a60e31fd20bf6fa029af5bf3eebc533ec2c)\r\n* Fixed correctness issue for 3D convolution primitive with post-ops (e6b93af5bdb32691ad90d3f537158649b61a6fc4)\r\n", "tarball_url": "https://api.github.com/repos/oneapi-src/oneDNN/tarball/v3.1.1", "zipball_url": "https://api.github.com/repos/oneapi-src/oneDNN/zipball/v3.1.1", "html_url": "https://github.com/oneapi-src/oneDNN/releases/tag/v3.1.1", "url": "https://api.github.com/repos/oneapi-src/oneDNN/releases/107918805", "release_id": 107918805, "date_created": "2023-06-09T00:49:54Z", "date_published": "2023-06-09T00:52:08Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/oneapi-src/oneDNN/releases/107398937", "tag": "v3.2-rc", "name": "v3.2-rc", "author": {"name": "harrymao2022", "type": "User"}, "description": "# Performance Optimizations\r\n* Intel Architecture Processors:\r\n  * Improved performance for 4th generation Intel Xeon Scalable Processor (formerly Sapphire Rapids).\r\n  * Improved performance for future Intel Xeon Scalable Processor (code-named Sierra Forest). The functionality is disabled by default and can be enabled via [CPU dispatcher control](https://oneapi-src.github.io/oneDNN/dev_guide_cpu_dispatcher_control.html).\r\n  * Improved fp32 inner product performance for processors with Intel AVX-512 instructions support.\r\n  * Improved bf16 and int8 matmul performance with runtime dimensions for processors with Intel AMX instructions support.\r\n\r\n* Intel Graphics Products:\r\n  * Improved performance for Intel Data Center GPU Max Series (formerly Ponte Vecchio).\r\n  * Improved performance for Intel Arc Graphics (formerly Alchemist and DG2) and Intel Data Center GPU Flex Series (formerly Arctic Sound-M).\r\n  * Reduced creation time for matmul, inner product, and RNN primitives.\r\n\r\n* AArch64-based Processors:\r\n  * Improved convolution performance with post-ops on processors with SVE support.\r\n  * Improved fp32 and fp16 depth-wise convolution performance with Arm Compute Library (ACL).\r\n  * Improved fp32 deconvolution performance for math mode `bf16` or `any` with ACL.\r\n\r\n* IBM Z Platform:\r\n  * Improved int8 matmul, inner product, and RNN performance for s390 z15 systems.\r\n\r\n# Functionality\r\n* *[experimental]* Introduced [Graph Compiler backend](https://oneapi-src.github.io/oneDNN/dev_guide_graph_compiler.html) for Graph API. Graph Compiler improves performance of composite operations like multi-head attention (MHA), multi-level perceptron (MLP), and convolution residual blocks for processors with Intel AVX-512 and Intel AMX instructions support.\r\n* Extended Graph API with boolean data type, [select](https://oneapi-src.github.io/oneDNN/dev_guide_op_select.html), and [pow](https://oneapi-src.github.io/oneDNN/dev_guide_op_pow.html) operations.\r\n* Introduced support for binary and eltwise post-ops in softmax primitives.\r\n* Introduced reference SYCL implementations of batch normalization, layer normalization, linear response normalization (LRN), binary, softmax, eltwise, pooling, PReLU, shuffle, and resampling primitives. These implementations address functional gaps on NVIDIA and AMD GPUs where support is missing in native libraries.\r\n\r\n* Intel Graphics Products:\r\n  * Introduced mixed precision support for binary primitives.\r\n\r\n* NVIDIA GPUs:\r\n  * Introduced bfloat16 support for deconvolution and softmax primitives.\r\n\r\n* AMD GPUs:\r\n  * Introduced support for inner product, convolution, deconvolution, batch normalization, and reorder primitives support.\r\n\r\n# Usability\r\n* Extended [verbose mode](https://oneapi-src.github.io/oneDNN/dev_guide_verbose.html) with additional capabilities, including information about implementation dispatching decisions and reasons for primitive creation errors.\r\n* Reduced stack consumption to less than 20 KB across implementations.\r\n* *[experimental]* Introduced [profiling API](https://oneapi-src.github.io/oneDNN/dev_guide_experimental.html#onednn-experimental-profiling) for SYCL and OpenCL applications.\r\n\r\n# Validation\r\n* Introduced fast performance validation mode (`--mode=F`) in [benchdnn](https://github.com/oneapi-src/oneDNN/tree/master/tests/benchdnn#readme). Testing speed is improved by initializing oneDNN objects in parallel and avoiding use of host memory when benchmarking GPU primitives.\r\n* Reduced benchdnn memory consumption in performance validation mode.\r\n* Introduced [smoke test set](https://github.com/oneapi-src/oneDNN/blob/rls-v3.2/cmake/options.cmake#L75) for benchdnn. This test set provides basic validation for all primitives.\r\n\r\n# Thanks to the Contributors\r\nThis release contains contributions from the project core team as well as Abdelrauf @quickwritereader, Alexey Vishnyakov @SweetVishnya, Annop Wongwathanarat @annop-w, Anthony Roberts @anthony-linaro, Crefeda Rodrigues @cfRod, David Svantesson @davsva01, Fadi Arafeh @fadara01, Ilya Lavrenov @ilya-lavrenov, Jonathan Deakin @jondea, Kentaro Kawakami @kawakami-k, Milos Puzovic @milpuz01, RambabuSwargam @RambabuSwargam, Sai Teja @saiteja13427, Taiju Tsuiki @tzik. We would also like to thank everyone who asked questions and reported issues.\r\n", "tarball_url": "https://api.github.com/repos/oneapi-src/oneDNN/tarball/v3.2-rc", "zipball_url": "https://api.github.com/repos/oneapi-src/oneDNN/zipball/v3.2-rc", "html_url": "https://github.com/oneapi-src/oneDNN/releases/tag/v3.2-rc", "url": "https://api.github.com/repos/oneapi-src/oneDNN/releases/107398937", "release_id": 107398937, "date_created": "2023-06-05T16:14:52Z", "date_published": "2023-06-05T17:45:06Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/oneapi-src/oneDNN/releases/101023074", "tag": "v2.7.4", "name": "v2.7.4", "author": {"name": "vpirogov", "type": "User"}, "description": "This is a patch release containing the following changes to v2.7.3:\r\n* Fixed potential `NaN` issue in convolution weight gradient on Intel CPUs (6d80bb48714f8f8d030f055435f5bfde3a382f15, 4c34f89653259b2e15e277ff0663d6705f093e1b, 017950a16168640764d17558e41010d0ae038377, 796a600c3de2993b5d5819995ad13eb70d097496)\r\n* Improved bfloat16 convolution weight gradient performance for processors with Intel AMX support (21bdc21f37ff835b9ce54d4b713d7bfd65060e30, 82cb7d37f861a471215b242e8df0330523cdf223, b2e948f931367c81a6887d4e0e544a9f50dcd673, 0a33f70c1b283d18631d299d3c907743d215e80d, ff05d0e8c2db056b0857bcbed22c5097f76529da)\r\n* Fixed out of bounds writes in bfloat16 inner product weight gradient for processors with Intel AMX support (caead724fc6d309c7706760a520908e28b8f8b0b)\r\n* Fixed illegal instruction in matmul for processors with Intel AMX support (be942a240e775dfda47bfff5622106851df218e5, 28ddb5bc91b01e266575047a676569c4af35a5eb, d264ba494a9f6b15d3eb21ec26e4606dd8d458c8)\r\n* Fixed segfault in convolution with depthwise post-op for processors with Intel SSE4.1 support (f7081009737b836f23ef8adce70994815acfa842)\r\n* Worked around segfaults for builds with Intel C/C++ Compiler 2021 for macOS (1382605c20bcdac9aa17c62cc38924138bc57db1)\r\n* Fixed segfault in bfloat16 convolution with strides for processors with Intel AMX support (c3b1dcd2605cae5609d7175fcf5223da16e03fb9)\r\n* Fixed correctness issue in int8 convolution with zero points for processors with Intel AMX support (5e76d8b07a431051b7d6a612c4933e36621fbc39)\r\n* Fixed assertion fail in int8 convolution for processors with Intel AMX support (05629a5ccfae9250e6495ffc7d51152025fcfee1)\r\n* Fixed incorrect results in vanilla GRU for Intel CPUs (2089770c4818be8933c5e9d1dd3cbaeba1457667)\r\n* Improved bfloat16 convolution performance for cases with large number of channels and spatial dimensions (c67f46b0df29c3a7c6cbd0a9f1ebbc9adf4457e8, c9cb51d6bfb68aee8377e7781a5c4512f6aa4bea, 4e2c5730426422fc362c02a963b66072c083acaf, 474527f47acb1aeff2bf52efd64e09ac95d8ef5b, 87e8ea9d8e0499b19149c69748ef8503ad2fb75b)\r\n* Fixed an issue with incorrect header files location when using oneDNN as subproject (be6abca883303e0cb4d2edac28da929a21d5d2a2)\r\n", "tarball_url": "https://api.github.com/repos/oneapi-src/oneDNN/tarball/v2.7.4", "zipball_url": "https://api.github.com/repos/oneapi-src/oneDNN/zipball/v2.7.4", "html_url": "https://github.com/oneapi-src/oneDNN/releases/tag/v2.7.4", "url": "https://api.github.com/repos/oneapi-src/oneDNN/releases/101023074", "release_id": 101023074, "date_created": "2023-04-26T23:02:11Z", "date_published": "2023-04-26T23:04:04Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/oneapi-src/oneDNN/releases/97682011", "tag": "v3.1", "name": "v3.1", "author": {"name": "harrymao2022", "type": "User"}, "description": "# Performance Optimizations  \r\n* Intel Architecture Processors: \r\n  * Improved performance for 4th generation Intel Xeon Scalable processor (formerly Sapphire Rapids). \r\n  * Introduced initial optimizations for future Intel Xeon Scalable processor (code name Sierra Forest). The functionality is disabled by default and should be enabled via [CPU dispatcher control](https://oneapi-src.github.io/oneDNN/dev_guide_cpu_dispatcher_control.html). \r\n \r\n* Intel Graphics Products: \r\n  * Improved performance for Intel Data Center GPU Max Series (formerly Ponte Vecchio). \r\n  * Improved performance for Intel Arc graphics (formerly Alchemist and DG2) and Intel Data Center GPU Flex Series (formerly Arctic Sound-M). \r\n  * Improved concat primitive performance with per-argument scales on Intel GPUs. \r\n \r\n* AArch64-based Processors: \r\n  * Improved layer normalization primitive performance with Compute Library for the Arm Architecture (ACL). \r\n \r\n* AMD GPUs: \r\n  * Introduced optimized matmul implementation. \r\n \r\n* RISC-V-based Processors: \r\n  * Improved pooling primitive performance for processors with RISC-V vector extension (RVV) support. \r\n \r\n# Functionality  \r\n  * Enabled [Graph API](https://oneapi-src.github.io/oneDNN/graph_extension.html) as a production feature. Graph API is intended to simplify oneDNN integration into frameworks. \r\n  * Added an option to zero-out weight gradient in RNN primitive. See details in corresponding [RFC](https://github.com/oneapi-src/oneDNN/tree/rfcs/rfcs/20221229-rnn-bwd-w-accumulation). \r\n  * **[experimental]** Added support for [sparse memory](https://oneapi-src.github.io/oneDNN/dev_guide_experimental.html#onednn-experimental-sparse) and dense by sparse matrix-matrix multiplication support in the matmul primitive. The functionality is supported on processors with Intel AVX2 and Intel AVX-512 instruction support. \r\n  * Introduced out-of-order queues support for OpenCL runtime. See the [OpenCL Interoperability](https://oneapi-src.github.io/oneDNN/dev_guide_opencl_interoperability.html) section in the Developer Guide for more details. \r\n  * Added support for the non-zero alpha parameter in the batch normalization ReLU post-op on Intel GPUs. \r\n  * Enabled the layer normalization primitive with f64 datatype support on Intel GPUs. \r\n  * Added support of per-argument scales in matmul, convolution, inner product, and reorder primitives on NVIDIA GPUs. \r\n \r\n# Validation \r\n* Extended benchdnn with functional and performance validation for [Graph API](https://oneapi-src.github.io/oneDNN/graph_extension.html). \r\n \r\n# Breaking Changes  \r\n* Builds with OpenCL runtime will fail unless Graph API is disabled with `ONEDNN_BUILD_GRAPH=OFF`. \r\n \r\n# Known Issues and Limitations \r\n* Graph API constant cache feature is disabled with SYCL CPU runtime due to an issue with the oneAPI DPC++ Compiler runtime. This will result in lower performance for some scenarios. \r\n \r\n# Thanks to the Contributors  \r\nThis release contains contributions from the project core team as well as Amy Wignall @AmyWignall-arm, Annop Wongwathanarat @annop-w, @arlesniak, @bdmoore1, Crefeda Rodrigues @cfRod, David Svantesson @davsva01, Fadi Arafeh @fadara01, Jonathan Deakin @jondea, Kentaro Kawakami @kawakami-k, Pavel Zamelin @pazamelin, Pawel Piotrowicz @pawelpiotrowicz, Peter Caday @petercad, @ranzhejiang, and Sanchit Grover @sanchit-grover-intel. We would also like to thank everyone who asked questions and reported issues. \r\n", "tarball_url": "https://api.github.com/repos/oneapi-src/oneDNN/tarball/v3.1", "zipball_url": "https://api.github.com/repos/oneapi-src/oneDNN/zipball/v3.1", "html_url": "https://github.com/oneapi-src/oneDNN/releases/tag/v3.1", "url": "https://api.github.com/repos/oneapi-src/oneDNN/releases/97682011", "release_id": 97682011, "date_created": "2023-03-30T23:32:55Z", "date_published": "2023-03-31T19:32:56Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/oneapi-src/oneDNN/releases/96044204", "tag": "graph-v0.9", "name": "graph-v0.9", "author": {"name": "vpirogov", "type": "User"}, "description": "This is the Beta Update 3 release of oneDNN Graph API based on oneDNN [v3.0.1](https://github.com/oneapi-src/oneDNN/releases/tag/v3.0.1).\r\n\r\n# Performance Optimizations\r\n* Improved multi-level perceptron (MLP) and residual block subgraphs performance with oneDNN Graph Compiler backend on 4th generation Intel Xeon Scalable processors (formerly Sapphire Rapids).\r\n* Improved dynamic shape performance for MLP and multi-head attention (MHA) patterns with oneDNN Graph Compiler backend.\r\n* Improved performance of oneDNN Graph Compiler built-in code generator.\r\n \r\n# Functionality\r\n* Extended the set of multi-head attention (MHA) variants supported by oneDNN Graph Compiler.\r\n\r\n# Known Issues and Limitations\r\n* The weight\u2019s opaque layout can be queried only from a compiled partition.\r\n\r\n# Thanks to the Contributors\r\nThis release contains contributions from the project core teams as well as Jiong Gong, Chunyuan Wu, Sanchit Jain, Yiqiang Li, Yunfei Mao, Kiefer Kuah and others.\r\n", "tarball_url": "https://api.github.com/repos/oneapi-src/oneDNN/tarball/graph-v0.9", "zipball_url": "https://api.github.com/repos/oneapi-src/oneDNN/zipball/graph-v0.9", "html_url": "https://github.com/oneapi-src/oneDNN/releases/tag/graph-v0.9", "url": "https://api.github.com/repos/oneapi-src/oneDNN/releases/96044204", "release_id": 96044204, "date_created": "2023-03-15T07:28:59Z", "date_published": "2023-03-17T22:12:19Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/oneapi-src/oneDNN/releases/94485610", "tag": "v3.1-rc", "name": "v3.1-rc", "author": {"name": "harrymao2022", "type": "User"}, "description": "This is a release candidate for oneDNN v3.1. Please provide feedback and submit defect reports via [Github issues](https://github.com/oneapi-src/oneDNN/issues/new/choose).\r\n\r\n# Performance Optimizations\r\n* Intel Architecture Processors:\r\n  * Improved performance for 4th generation Intel Xeon Scalable processor (formerly Sapphire Rapids).\r\n  * Introduced initial optimizations for future Intel Xeon Scalable processor (code name Sierra Forest). The functionality is disabled by default and should be enabled via [CPU dispatcher control](https://oneapi-src.github.io/oneDNN/dev_guide_cpu_dispatcher_control.html).\r\n\r\n* Intel Graphics Products:\r\n  * Improved performance for Intel Data Center GPU Max Series (formerly Ponte Vecchio).\r\n  * Improved performance for Intel Arc graphics (formerly Alchemist and DG2) and Intel Data Center GPU Flex Series (formerly Arctic Sound-M).\r\n  * Improved concat primitive performance with per-argument scales on Intel GPUs.\r\n\r\n* AArch64-based Processors:\r\n  * Improved layer normalization primitive performance with Compute Library for the Arm Architecture (ACL).\r\n\r\n* AMD GPUs:\r\n  * Introduced optimized matmul implementation.\r\n\r\n* RISC-V-based Processors:\r\n  * Improved pooling primitive performance for processors with RISC-V vector extension (RVV) support.\r\n\r\n# Functionality\r\n  * Enabled [Graph API](https://oneapi-src.github.io/oneDNN/graph_extension.html) as a production feature. Graph API is intended to simplify oneDNN integration into frameworks.\r\n  * Added an option to zero-out weight gradient in RNN primitive. See details in corresponding [RFC](https://github.com/oneapi-src/oneDNN/tree/rfcs/rfcs/20221229-rnn-bwd-w-accumulation).\r\n  * [experimental] Added support for [sparse memory](https://oneapi-src.github.io/oneDNN/dev_guide_experimental.html#onednn-experimental-sparse) and dense by sparse matrix-matrix multiplication support in the matmul primitive. The functionality is supported on processors with Intel AVX2 and Intel AVX-512 instruction support.\r\n  * Introduced out-of-order queues support for OpenCL runtime. See the [OpenCL Interoperability](https://oneapi-src.github.io/oneDNN/dev_guide_opencl_interoperability.html) section in the Developer Guide for more details.\r\n  * Added support for the non-zero alpha parameter in the batch normalization ReLU post-op on Intel GPUs.\r\n  * Enabled the layer normalization primitive with f64 datatype support on Intel GPUs.\r\n  * Added support of per-argument scales in matmul, convolution, inner product, and reorder primitives on NVIDIA GPUs.\r\n\r\n# Validation\r\n* Extended benchdnn with functional and performance validation for [Graph API](https://oneapi-src.github.io/oneDNN/graph_extension.html).\r\n\r\n# Breaking Changes\r\n* Builds with OpenCL runtime will fail unless Graph API is disabled with `ONEDNN_BUILD_GRAPH=OFF`.\r\n\r\n# Known Issues and Limitations\r\n* Graph API constant cache feature is disabled with SYCL CPU runtime due to an issue with the oneAPI DPC++ Compiler runtime. This will result in lower performance for some scenarios.\r\n\r\n# Thanks to the Contributors\r\nThis release contains contributions from the project core team as well as Amy Wignall @AmyWignall-arm, Annop Wongwathanarat @annop-w, @arlesniak, @bdmoore1, Crefeda Rodrigues @cfRod, David Svantesson @davsva01, Fadi Arafeh @fadara01, Jonathan Deakin @jondea, Kentaro Kawakami @kawakami-k, Pavel Zamelin @pazamelin, Pawel Piotrowicz @pawelpiotrowicz, Peter Caday @petercad, @ranzhejiang, and Sanchit Grover @sanchit-grover-intel. We would also like to thank everyone who asked questions and reported issues.\r\n", "tarball_url": "https://api.github.com/repos/oneapi-src/oneDNN/tarball/v3.1-rc", "zipball_url": "https://api.github.com/repos/oneapi-src/oneDNN/zipball/v3.1-rc", "html_url": "https://github.com/oneapi-src/oneDNN/releases/tag/v3.1-rc", "url": "https://api.github.com/repos/oneapi-src/oneDNN/releases/94485610", "release_id": 94485610, "date_created": "2023-03-04T00:39:37Z", "date_published": "2023-03-04T02:10:06Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/oneapi-src/oneDNN/releases/93523115", "tag": "v3.0.1", "name": "v3.0.1", "author": {"name": "vpirogov", "type": "User"}, "description": "This is a patch release containing the following changes to v3.0:\r\n* Fixed potential correctness issue in convolution weight gradient with 1x1 filter and strides (e58996692802f4a94651f6baa6e3f0debf93b537)\r\n* Improved convolution, deconvolution, inner product, and matmul primitives performance with scales on Intel CPUs (38319f1f822387bd755183bcac2ec3d0745a88b4, 18de927dc205543701942f0f26d61f72c51f5f0b, b6170d1b79332d8ba0f72227cb5edd2aced837c0, 85171b0cc057d5ba682dee582cd72c48543389db)\r\n* Reverted MEMFD allocator in Xbyak to avoid fails in high load scenarios (eaaa41b8a30101640094e46af7f27969ed105ee2)\r\n* Fixed array out of bounds issue in `bfloat16` convolution weight gradient on Intel CPUs (a17a64c330d1153fdea3d81f1420fb38c50248bd)\r\n* Improved compatibility with future versions of Intel GPU driver (eb7a0a07df12874a40c0f135d8bf16116594e0e8)\r\n* Fixed segfault in `fp16` and `bfloat16` convolution backward propagation on systems with Intel AMX support (293561b6a2644ef05d8d664cd81c1bcde876b481)\r\n* Fixed build issue with GCC 13 (1d7971ce488da657e23f08488cdb6ef8e484c5e8)\r\n* Fixed correctness issue in `int8` RNN primitive Vanilla GRU flavor on Intel CPUs (f4a149c16faff0fb51fb292d12a7b51f6fac53bf, fbf8dca1ba9b565ddedd1cb291d3b466d0a5a45b)\r\n* Added check for unsupported arguments in binary primitive implementation for AArch64-based processors (5bb907077cd7b4c3983f7215d5509b17f3da67e2)\r\n* Fixed correctness issue in `int8` convolution with zero-points on Intel Data Center GPU Max Series (96e868c473bb0e2a9b1a42b51e8f91997b52b471)\r\n* Fixed runtime error in convolution primitive with small number of channels on Xe-based graphics (068893e1c792c8e9ad5b17bc6e494359b32f910f)\r\n* Removed use of OpenCL C variable length arrays in reduction primitive implementation for Intel GPUs (41e8612f212d939643932ef309cd78bd4194f42d)\r\n* Fixed correctness issue in matmul and inner product primitives on Intel Data Center GPU Max Series (a1e6bc57b233d85a6f382db611879614236d9b05, dbb7c284e0834cd0fe84c8311484880802fa9af0)\r\n* Fixed segfault in `fp16` and `bfloat16` convolution backward propagation on future Intel Xeon processors (code name Sierra Forest) (399b7c5af4c5238f9956d71270adbd44f3cb25a3)\r\n* Fixed runtime error in Graph API for partitions with quantized matmul and add operations (f881da5be31abc71f90a1a750c50ec2ea5dbc516, 699ba755fde86aea3714bbce75d5b0b274302545, b8d21a58d8247097ed26816b730e3cd4c19f61c, 9421fb2a453aee957a0c1dc10be5675e5f916c2e)\r\n* Fixed convolution performance regression on Xe-based graphics (1869bf26a92f8d8f36853e537f9727412a4d1f94)\r\n* Improved convolution performance with `OHWI` and `OIHW` weight formats on Intel Data Center GPU Max Series (2d0b31ee82dc681b829f67100c05ae4e689633e6, 5bd5d52e7ee832fb0d5ece6d42a6b230023c9dd0)\r\n* Fixed include files handling in build system affecting CMake projects relying on oneDNN (c61645392fde55ac361c95a752df0cfa7ef24345)\r\n* Added `tbb::finalize` to tests and examples to address intermittent test crashes with TBB runtime (891a41560382cc0f991c428392078d13ccb76129, c79e54322f251aa70783ca1b837ce0d558bf3396, 8312c3addc597e6565cf1233801234c2ffafd092, 1a32b95a2c61d094206ed49d69843fdcdeb2ffcd, bd0389d81509baf6696d3927d0da4cce4c06d2d4, f05013d0e419df22ec2755dc5d74f5974871cf9e, ab7938f1b889aa43f155216f774297e8c765cd97, 31c9e7b3c1a7e262cecafe98bed128843f1c2969, f3261e4556935424946697be4b336020653b41a5, d58ac41a12179f8cca48962c4b5a44940bea97d7, f8c67b9026dc2945ed66a8f1c276611c063dae4d, 258849b71c24a89b08ac12972ec1fcaa72a9da39, b20a8c786c5a2cb676a2a8b599edf5cfd7ee0c3a)\r\n* Fixed segfault in `fp16` convolution primitive on future Intel Xeon processors (code name Granite Rapids) (a574ffff870318cc104d8af4a2368d47b433b27f)\r\n* Fixed correctness issue in `fp16` convolution primitive on future Intel Xeon processors (code name Sierra Forest) (f165ed8a8872e72a7d9651c3dd38bd6c2909fdce)\r\n* Fixed correctness issue in `int8` convolution primitive on Intel CPUs (ca1592237b87cae5e4a55fb464ad90fb9f91957d, 27845b8e66d354549ac6c6fceeb92c267a9e910f)\r\n* Fixed correctness issue in `int8` convolution primitive on Intel Data Center GPU Max Series (8bb651cb99e2875aea44b907bdc54418b2d4932a)\r\n* Fixed correctness issue in resampling primitive with post-ops on Intel CPUs (aa52a5128d44c6d745b89beabcd47f428665843e)\r\n* Addressed excessive memory consumption in 3D convolution on Intel CPUs (3d6412af5cb99863ede8753238533dcabcd3c5d9, 097acb5e108eb57b38a8a2409b083a1819b9f962, fd696639c70c4cd92e2aaf871bc4165c269d29f7)\r\n* Fixed segfault in convolution with `sum` and `relu` post-ops on Intel CPUs (63ad769939dd8307935caac67c0fc7c9bc9206de, 1b1303748b80360e5f93740d6ea03063132fd8f8, 0a8116b3de98243a234680d8cda869d2f20dd178, 9972cb80a29da9f14efbe8518bc10a21f7ae6e36)\r\n* Addressed convolution performance regression with small number of channels on Intel GPUs (d3af87710fcae9561ae22017d45bd670f8858272)\r\n* Worked around MSVS 2019 bug resulting in build fails on Windows (40247753290e3e886b9235c5f80a2997eb85372a)\r\n* Updated code base formatting to clang-format 11 (23576f935fcef245b26cc78ef74935ea6bb7e6b7, 0b1bf845e05da75e4d994e01a0d7996b64787ece)\r\n", "tarball_url": "https://api.github.com/repos/oneapi-src/oneDNN/tarball/v3.0.1", "zipball_url": "https://api.github.com/repos/oneapi-src/oneDNN/zipball/v3.0.1", "html_url": "https://github.com/oneapi-src/oneDNN/releases/tag/v3.0.1", "url": "https://api.github.com/repos/oneapi-src/oneDNN/releases/93523115", "release_id": 93523115, "date_created": "2023-02-24T02:36:27Z", "date_published": "2023-02-24T03:20:54Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/oneapi-src/oneDNN/releases/91023491", "tag": "graph-v0.8.1", "name": "graph-v0.8.1", "author": {"name": "vpirogov", "type": "User"}, "description": "This is a patch release containing the following changes to [graph-v0.8](https://github.com/oneapi-src/oneDNN/releases/tag/graph-v0.8):\r\n\r\n* Upgraded oneDNN dependency from v2.7.2 to v2.7.3 (93237aa, 260bdb5)\r\n* Fixed a correctness issue of quantized Convolution + Add fusion (26a9a5b, beba352)\r\n* Fixed `query_dynamic_outputs()` interface implementation in graph compiler backend (8dbca04)\r\n", "tarball_url": "https://api.github.com/repos/oneapi-src/oneDNN/tarball/graph-v0.8.1", "zipball_url": "https://api.github.com/repos/oneapi-src/oneDNN/zipball/graph-v0.8.1", "html_url": "https://github.com/oneapi-src/oneDNN/releases/tag/graph-v0.8.1", "url": "https://api.github.com/repos/oneapi-src/oneDNN/releases/91023491", "release_id": 91023491, "date_created": "2023-02-01T12:03:25Z", "date_published": "2023-02-01T18:55:27Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/oneapi-src/oneDNN/releases/88797648", "tag": "v2.7.3", "name": "v2.7.3", "author": {"name": "tprimak", "type": "User"}, "description": "This is a patch release containing the following changes to v2.7.2:\r\n* Fixed segfault in int8 convolution with binary post-ops on Intel CPUs (c8d40c0719f9d9cffa1c5eb04f3f40fa1f9546b8)\r\n* Applied workaround for tanh post-op on some Xe architecture based GPUs (3eb3267dc3bcfe64a081731ac9d08c84bc6827f7)\r\n* Disabled fp16 post-ops with Compute Library for Arm Architecture (ACL) (f7b7dc0a8b3125602295047cdd7feb3cbb8d9a06)\r\n* Fixed incorrect results for sequence of eltwise post-op with same algorithm but different parameters (02c26781171f6350634b41d80cbff7ae5092c1a1, 1c36e27520617e23b74ed32e675804ac7806576e, 81ba0fe626c93e51935d5e8776dd7e8bf4105487)\r\n* Fixed issue in convolution with groups and plain activation layout on Intel GPUs (df6f2e34bfb1e3d6bcd5498a4febb149b2be8b2b, d0c14c204782945b3732bd83b7329c314c3339c1)\r\n* Fixed reorder failures on Xe HPC architecture based GPUs (c3cb1d5fa7e2e41c7059fa7e5ebcee34aa3e5242)\r\n* Fixed thread safety issue in convolution primitive (2955c9d5d5f97f03c4068af37f6783f0be256695)\r\n* Fixed scratchpad allocation issue in matmul (989acd3b0dbd304fe47ac7837bb33e73a4ca7cd6)\r\n* Disabled concat batching with scales on Intel GPUs since implementation doesn't support it yet (8aab73fe1897542c5ec740ac718b00e7d72edd92, 1eac450ca742cd9905addf36ee038a8e17e03474, 82838de623057ffd1dfc0f879afcd02e72f9538f)\r\n* Fixed segfault and correctness issues in convolution primitive with sum and relu post-ops on Intel CPUs (fc335be0d1376f1dca527bd543f929739dffd55f, 0f4697a87c0f550339598c1918d5479801337426, 60f1727fcaf06416c5464b44c177ec16829bd2c1, d28f2c1757e2cc6b792e4fd5de40987e811d086d, 4761ee91b3729d124135273a7450d3d2cf0dce53, f674fbf917e92b2623184ad8c603f20ae4fe0ad7)", "tarball_url": "https://api.github.com/repos/oneapi-src/oneDNN/tarball/v2.7.3", "zipball_url": "https://api.github.com/repos/oneapi-src/oneDNN/zipball/v2.7.3", "html_url": "https://github.com/oneapi-src/oneDNN/releases/tag/v2.7.3", "url": "https://api.github.com/repos/oneapi-src/oneDNN/releases/88797648", "release_id": 88797648, "date_created": "2023-01-13T00:55:31Z", "date_published": "2023-01-13T01:03:41Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/oneapi-src/oneDNN/releases/86596145", "tag": "v3.0", "name": "v3.0", "author": {"name": "harrymao2022", "type": "User"}, "description": "# Performance Optimizations  \r\n* Intel Architecture Processors: \r\n  * Improved performance for 4th generation Intel Xeon Scalable processor (formerly Sapphire Rapids). \r\n  * Introduced FP16 support and initial optimizations for future Intel Xeon Scalable processor (code name Granite Rapids). The functionality is disabled by default and should be enabled via\u202f[CPU dispatcher control](https://oneapi-src.github.io/oneDNN/dev_guide_cpu_dispatcher_control.html). \r\n* Intel Graphics Products: \r\n  * Improved performance for Intel Data Center GPU Max Series (formerly Ponte Vecchio). \r\n  * Improved performance for Intel Arc graphics (formerly Alchemist and DG2) and Intel Data Center GPU Flex Series (formerly Arctic Sound-M). \r\n* AArch64-based Processors: \r\n  * Improved reorder performance for processors with Scalable Vector Extensions (SVE) support. \r\n  * Improved pooling performance with post-ops for processors with SVE 512 support. \r\n  * Improved batch normalization performance with non-default flags for processors with SVE 512 support. \r\n  * Improved performance of FP16 functionality with Compute Library for Arm Architecture (ACL). \r\n  * Improved deconvolution performance with ACL. \r\n* PowerPC64-based Processors: \r\n  * Improved int8 GEMM performance. \r\n# Functionality  \r\n  * Introduced [new quantization scheme](https://github.com/oneapi-src/oneDNN/blob/rfcs/rfcs/20220201-quantization-scaling). Major changes include support for per-argument runtime scales in all primitives and unquantized bias. \r\n  * [experimental] Introduced [Graph API support](https://oneapi-src.github.io/oneDNN/graph_extension.html) that simplifies oneDNN integration into applications. The functionality is disabled by default and can be enabled at build time with `ONEDNN_BUILD_GRAPH=ON` flag. \r\n  * Introduced support for Intel DPC++/C++ Compiler 2023.0, including new features from the SYCL 2020 standard. \r\n  * Extended [persistent cache](https://oneapi-src.github.io/oneDNN/dev_guide_persistent_cache.html) to cover GPU engine object. This improvement allows applications to further reduce oneDNN initialization time. \r\n  * Extended [threadpool API](https://oneapi-src.github.io/oneDNN/dev_guide_threadpool.html) with a function to indicate maximum available concurrency. \r\n  * Extended binary primitive implementation on GPU with bfloat16 source and int8 destination support. \r\n  * Introduced pooling and reduction primitives support on AMD GPUs. \r\n  * Introduced reduction primitive support on NVIDIA GPUs. \r\n \r\n# Usability   \r\n  * Extended the set of supported format tags to cover formats used in applications. \r\n \r\n# Validation \r\n  * Extended the GoogleTest (gtest) suite with support for Parametric Rectified Linear Unit (PReLU) primitive. \r\n \r\n# Breaking Changes  \r\n  * Removed [deprecated APIs](https://github.com/oneapi-src/oneDNN/tree/rfcs/rfcs/20220815-v3.0-API-cleanup). \r\n  * Removed operation descriptor object and made memory descriptor object opaque. See details in [operation and memory descriptors RFC](https://github.com/oneapi-src/oneDNN/tree/rfcs/rfcs/20220608-make-opdesc-and-md-opaque). \r\n  * Removed creation time primitive scales support and primitive output scales support. See details in [quantization scaling RFC](https://github.com/oneapi-src/oneDNN/blob/rfcs/rfcs/20220201-quantization-scaling). \r\n  * Removed support for Intel DPC++/C++ Compiler 2022 and SYCL 1.2.1 (aka SYCL 2017) standard support. Use Intel DPC++/C++ Compiler and SYCL 2020 standard instead.\r\n  * Removed Winograd convolution implementation for int8 data type. \r\n  * Updated minimal supported ACL version to 22.08 (was 22.05). \r\n \r\n# Thanks to the Contributors  \r\nThis release contains contributions from the project core team as well as @akshatasangelkar, Aryan Karumuri @AryanKarumuri, Crefeda Rodrigues @cfRod, Divakar Mariyanna @bmdivakar, Gordon Fossum @austinpagan, Jonathan Deakin @jondea, Kentaro Kawakami @kawakami-k, lilianhuang @lilh9598, Milos Puzovic @milpuz01, Mona Minakshi @monaminakshi, Nathan John Sircombe @nSircombe, Peter Caday @petercad, and Sreekanth Yalachigere @sreekanth-yalachigere. We would also like to thank everyone who asked questions and reported issues. \r\n", "tarball_url": "https://api.github.com/repos/oneapi-src/oneDNN/tarball/v3.0", "zipball_url": "https://api.github.com/repos/oneapi-src/oneDNN/zipball/v3.0", "html_url": "https://github.com/oneapi-src/oneDNN/releases/tag/v3.0", "url": "https://api.github.com/repos/oneapi-src/oneDNN/releases/86596145", "release_id": 86596145, "date_created": "2022-12-19T17:12:17Z", "date_published": "2022-12-20T00:15:51Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/oneapi-src/oneDNN/releases/86964236", "tag": "graph-v0.8", "name": "graph-v0.8", "author": {"name": "vpirogov", "type": "User"}, "description": "This is the Beta Update 2 release of oneDNN Graph API based on [oneDNN v2.7.2](https://github.com/oneapi-src/oneDNN/releases/tag/v2.7.2).\r\n\r\n# Functionality\r\n* Added `HardSigmoid` operation.\r\n* Added block tensor layout support to improve performance on Xe architecture-based GPUs.\r\n* Added support of `IOX` and `XOI` weight formats for `ConvTranspose` operation.\r\n* Added `query_dynamic_outputs` API to support dynamic shapes in the graph. This functionality allows Graph API to infer output tensors shapes based on input tensors.\r\n* **Experimental**: Introduced dynamic shapes support for MHA via oneDNN Graph Compiler.\r\n\r\n# Known Issues and Limitations\r\n* The weight\u2019s opaque layout can be queried only from a compiled partition, which requires that input tensor shapes must be known at compilation time.\r\n* MHA and MLP fusion are not activated on machines without Intel AVX-512 support.\r\n\r\n# Thanks to the Contributors\r\nThis release contains contributions from the project core teams as well as Jiong Gong, Chunyuan Wu, Sanchit Jain, Yiqiang Li, Yunfei Mao, Kiefer Kuah and others.", "tarball_url": "https://api.github.com/repos/oneapi-src/oneDNN/tarball/graph-v0.8", "zipball_url": "https://api.github.com/repos/oneapi-src/oneDNN/zipball/graph-v0.8", "html_url": "https://github.com/oneapi-src/oneDNN/releases/tag/graph-v0.8", "url": "https://api.github.com/repos/oneapi-src/oneDNN/releases/86964236", "release_id": 86964236, "date_created": "2022-12-15T07:22:52Z", "date_published": "2022-12-22T17:19:24Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/oneapi-src/oneDNN/releases/84866102", "tag": "v3.0-rc", "name": "v3.0-rc", "author": {"name": "harrymao2022", "type": "User"}, "description": "This is a release candidate for oneDNN v3.0. Please provide feedback and submit defect reports via [Github issues](https://github.com/oneapi-src/oneDNN/issues/new/choose).\r\n\r\n# Performance Optimizations  \r\n* Intel Architecture Processors: \r\n  * Improved performance for 4th generation Intel Xeon Scalable processor (formerly Sapphire Rapids). \r\n  * Introduced FP16 support and initial optimizations for future Intel Xeon Scalable processor (code name Granite Rapids). \r\n* Intel Graphics Products: \r\n  * Improved performance for Intel Data Center GPU Max Series (formerly Ponte Vecchio). \r\n  * Improved performance for Intel Arc graphics (formerly Alchemist and DG2) and Intel Data Center GPU Flex Series (formerly Arctic Sound-M). \r\n* AArch64-based Processors: \r\n  * Improved reorder performance for processors with Scalable Vector Extensions (SVE) support. \r\n  * Improved pooling performance with post-ops for processors with SVE 512 support. \r\n  * Improved batch normalization performance with non-default flags for processors with SVE 512 support. \r\n  * Improved performance of FP16 functionality with Compute Library for Arm Architecture (ACL). \r\n  * Improved deconvolution performance with ACL. \r\n* PowerPC64-based Processors: \r\n  * Improved int8 GEMM performance. \r\n# Functionality  \r\n  * Introduced [new quantization scheme](https://github.com/oneapi-src/oneDNN/blob/rfcs/rfcs/20220201-quantization-scaling). Major changes include support for per-argument runtime scales in all primitives and unquantized bias. \r\n  * [experimental] Introduced [Graph API support](https://oneapi-src.github.io/oneDNN/graph_extension.html) that simplifies oneDNN integration into applications. The functionality is disabled by default and can be enabled at build time with `ONEDNN_BUILD_GRAPH=ON` flag. \r\n  * Introduced support for Intel DPC++/C++ Compiler 2023.0, including new features from the SYCL 2020 standard. \r\n  * Extended [persistent cache](https://oneapi-src.github.io/oneDNN/dev_guide_persistent_cache.html) to cover GPU engine object. This improvement allows applications to further reduce oneDNN initialization time. \r\n  * Extended [threadpool API](https://oneapi-src.github.io/oneDNN/dev_guide_threadpool.html) with a function to indicate maximum available concurrency. \r\n  * Extended binary primitive implementation on GPU with bfloat16 source and int8 destination support. \r\n  * Introduced pooling and reduction primitives support on AMD GPUs. \r\n  * Introduced reduction primitive support on NVIDIA GPUs. \r\n \r\n# Usability   \r\n  * Extended the set of supported format tags to cover formats used in applications. \r\n \r\n# Validation \r\n  * Extended the GoogleTest (gtest) suite with support for Parametric Rectified Linear Unit (PReLU) primitive. \r\n \r\n# Breaking Changes  \r\n  * Removed [deprecated APIs](https://github.com/oneapi-src/oneDNN/tree/rfcs/rfcs/20220815-v3.0-API-cleanup). \r\n  * Removed operation descriptor object and made memory descriptor object opaque. See details in [operation and memory descriptors RFC](https://github.com/oneapi-src/oneDNN/tree/rfcs/rfcs/20220608-make-opdesc-and-md-opaque). \r\n  * Removed creation time primitive scales support and primitive output scales support. See details in [quantization scaling RFC](https://github.com/oneapi-src/oneDNN/blob/rfcs/rfcs/20220201-quantization-scaling). \r\n  * Removed support for Intel DPC++/C++ Compiler with SYCL 1.2.1 (aka SYCL 2017) standard. \r\n  * Removed Winograd convolution implementation for int8 data type. \r\n  * Updated minimal supported ACL version to 22.08 (was 22.05). \r\n \r\n# Thanks to the Contributors  \r\nThis release contains contributions from the project core team as well as @akshatasangelkar, Aryan Karumuri @AryanKarumuri, Crefeda Rodrigues @cfRod, Divakar Mariyanna @bmdivakar, Gordon Fossum @austinpagan, Jonathan Deakin @jondea, Kentaro Kawakami @kawakami-k, lilianhuang @lilh9598, Milos Puzovic @milpuz01, Mona Minakshi @monaminakshi, Nathan John Sircombe @nSircombe, Peter Caday @petercad, and Sreekanth Yalachigere @sreekanth-yalachigere. We would also like to thank everyone who asked questions and reported issues. \r\n\r\n\r\n", "tarball_url": "https://api.github.com/repos/oneapi-src/oneDNN/tarball/v3.0-rc", "zipball_url": "https://api.github.com/repos/oneapi-src/oneDNN/zipball/v3.0-rc", "html_url": "https://github.com/oneapi-src/oneDNN/releases/tag/v3.0-rc", "url": "https://api.github.com/repos/oneapi-src/oneDNN/releases/84866102", "release_id": 84866102, "date_created": "2022-12-02T21:09:40Z", "date_published": "2022-12-02T22:54:48Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/oneapi-src/oneDNN/releases/84749608", "tag": "graph-v0.7.2", "name": "graph-v0.7.2", "author": {"name": "vpirogov", "type": "User"}, "description": "This is a patch release containing the following changes to [graph-v0.7.1](https://github.com/oneapi-src/oneDNN/releases/tag/graph-v0.7.1):\r\n* Upgraded oneDNN dependency to [v2.7.2](https://github.com/oneapi-src/oneDNN/releases/tag/v2.7.2) (dec9f8cc6)\r\n", "tarball_url": "https://api.github.com/repos/oneapi-src/oneDNN/tarball/graph-v0.7.2", "zipball_url": "https://api.github.com/repos/oneapi-src/oneDNN/zipball/graph-v0.7.2", "html_url": "https://github.com/oneapi-src/oneDNN/releases/tag/graph-v0.7.2", "url": "https://api.github.com/repos/oneapi-src/oneDNN/releases/84749608", "release_id": 84749608, "date_created": "2022-12-01T02:23:13Z", "date_published": "2022-12-01T20:28:43Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/oneapi-src/oneDNN/releases/83624072", "tag": "v2.7.2", "name": "v2.7.2", "author": {"name": "vpirogov", "type": "User"}, "description": "This is a patch release containing the following changes to v2.7.1:\r\n* Fixed segfaults in deconvolution backpropagation with ACL on AArch64-based processors (f02e6f3f262813b8d0b6cb1f7b55fcc08b4b5bac)\r\n* Fixed code generation issues in Intel AVX2 convolution implementation (2ba25236bc417c4d5fe1729ddf9e01f1d1d25fb3, b60633f79947199a1f0cfce7aa42b0ae14690401, 844326b853ba9ca9b7a34ec08ca6e2e28d7332e8, 2009164c2ae90e1e938ab8823c817a6c95fccc11)\r\n* Fixed correcteness issues and runtime errors in deconvolution with binary post-ops on Intel GPUs (dd54d3906c9613a967b709907306b946cfe32cac)\r\n* Improved performance of convolutions with small number of channels and large spatial sizes on systems with Intel AMX (26f97dc7a47aa2c0f0e13e6ff61dd3fc28fa077b, 4cb648d9e3620876fa7d7dca38a902643cd97dbc)\r\n* Fixed runtime error in int8 convolutions with groups on Xe architecture based GPUs (e5a70f43639ba968869a99931d77116791ace355)\r\n* Improved inner product weight gradient performance on Xe architecture based GPUs (9e9b859fddc6f813f9b9cac093d7d131c84054ab, 12ec4e3a51ddc105e86e9d29661690750560cd1c)\r\n* Improved batch normalization performance with threadpool threading (4fd5ab2dd312b2b79e8f2f1b18b39a94fee39e84)\r\n* Improved inner product performance with binary post-ops in broadcast mode on Intel CPUs (d43c70d4aafd58c241d456453994f4c7fe6aefff, 49ca4e17e7fd889c6c153f52dffa6f4d4a10e7c9)\r\n* Fixed segfaults and correctness issues in sum primitive with threadpool  threading (ee7a3219db8bcdb7870b65b6ee0aadfba2275513)\r\n* Extended persistent cache API to cover engine objects (58481d606c19f4e46c1cd7dbfd4aba819ae024d3, 5f69dade29e317eab37455d477892996e80aea75, 16c0a95180a362c079fb2d3f01a4cea084b99628, 068071b326f253791ae767cae25258e6d47426ad)\r\n* Added support for newer versions of Intel GPU drivers (71443935355ef4fc52b510be761c487de8677386)\r\n* Updated ITT API version to 3.23.0 (d23cc9503f94ea9267bc8b6e654a912caa70e333)\r\n* Fixed convolution correctness issue on Intel Data Center GPU Flex Series (365ac202ca2f58078549116a0650a91566a256b6)\r\n* Fixed fp64 convolution correctness issue on Intel Data Center GPU MAX Series (9d4bf94d89b945cb703a7b4d04d539daf7aab8b5, 67054032e4b1b4eae11f006e3857fe20a0d7b16a)\r\n* Fixed correctness issues in reduction primitive with binary post-op on Intel GPUs (ae9d075dbba068287b6cb280f0f22d3cdcbfcb36, e3b80c58f493e7972eb4d0317518534c1d8412e9)\r\n* Improved convolution performance on on Intel Data Center GPU MAX Series (90be8d501f3b35e88f997bf9e0fd139a740f72f7, caf4863f40dd06b807d2bb1abb487aad21d586a6)\r\n* Fixed build errors with ONEDNN_ENABLE_PRIMITIVE_GPU_ISA build option (de2db042bbb733de7c925224934ded766de74d68)\r\n* Fixed correctness issues in convolution with per-tensor binary post-ops on Intel CPUs (9cf9c189f6f674bba38ea11217f4b06acab87194)\r\n* Improved convolution performance on Intel Data Center GPU Flex Series (8b08a07574888bc265818a751eab82aa28115d72)\r\n", "tarball_url": "https://api.github.com/repos/oneapi-src/oneDNN/tarball/v2.7.2", "zipball_url": "https://api.github.com/repos/oneapi-src/oneDNN/zipball/v2.7.2", "html_url": "https://github.com/oneapi-src/oneDNN/releases/tag/v2.7.2", "url": "https://api.github.com/repos/oneapi-src/oneDNN/releases/83624072", "release_id": 83624072, "date_created": "2022-11-18T22:53:29Z", "date_published": "2022-11-19T00:47:58Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/oneapi-src/oneDNN/releases/82601942", "tag": "graph-v0.7.1", "name": "graph-v0.7.1", "author": {"name": "vpirogov", "type": "User"}, "description": "This is a patch release containing the following changes to [graph-v0.7](https://github.com/oneapi-src/oneDNN/releases/tag/graph-v0.7):\r\n \r\n* Fixed a build issue in compiler backend (70258d306)\r\n* Optimized for zero points folding (d6f12b50c)\r\n* Fixed a primitive descriptor cache issue in reorder fusion (08876524d)\r\n", "tarball_url": "https://api.github.com/repos/oneapi-src/oneDNN/tarball/graph-v0.7.1", "zipball_url": "https://api.github.com/repos/oneapi-src/oneDNN/zipball/graph-v0.7.1", "html_url": "https://github.com/oneapi-src/oneDNN/releases/tag/graph-v0.7.1", "url": "https://api.github.com/repos/oneapi-src/oneDNN/releases/82601942", "release_id": 82601942, "date_created": "2022-11-04T09:34:50Z", "date_published": "2022-11-09T20:34:27Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/oneapi-src/oneDNN/releases/80680110", "tag": "v2.7.1", "name": "v2.7.1", "author": {"name": "vpirogov", "type": "User"}, "description": "This is a patch release containing the following changes to v2.7:\r\n* Fixed performance regression for batch normalization primitive in TBB and threadpool configurations (cd953e4ca7390387b53fba7105f81a6fc1fc0382)\r\n* Improved grouped convolution performance on Xe Architecture GPUs (d7a781e166ef3206d9b0ab79a69d76034d663c20, cb1f3fe27f466a26b484ed063546bd0b6c4cd306, 4e844740d6b26709c0aa3c2604ed52130560208a, 7ba3c40f65425c4bc2b922ae7b2cdd8cb8e5181c)\r\n* Fixed runtime error in int8 reorder on Intel GPUs (53532a9944b2e4694d4c0135f0a1a5102ca97613)\r\n* Reverted MEMFD allocator in Xbyak to avoid segfaults in high load scenarios (3e29ae26dba137a6232669bd1c5d42ad4449b794)\r\n* Fixed a defect with incorrect caching of BRGEMM-based matmul primitive implementations with trivial dimensions (87cd9796a98497ab9a3ff5250ad3a396199590fb)\r\n* Improved depthwise convolution performance with per-tensor binary post-ops for Intel CPUs (f430a5a4c883ef846f938f571020565d41719e9c)\r\n* Extended threadpool API to [manage maximum concurrency](https://github.com/oneapi-src/oneDNN/blob/rfcs/rfcs/20220929-threadpool_max_concurrency/README.md) (8a1e9595f131e1303887fba407a03dbd64ac301e, 64e559454787651186ed6a32e4eef2a17132b9b6)\r\n* Fixed potential integer overflow in BRGEMM-based convolution implementation (25ccee38b97e935e6c3c729b9134804c6a2ea6a7)\r\n* Fixed performance regression in concat primitive with any format on Intel CPUs (2a60adec0e73895caefb3dc7d1de74b5eac8c6da, feb614d5fef07fb2a188ceef15ebeaf9f9f45acf)\r\n* Fixed compile-time warnings in `matmul_perf` example (b5faa77a4a651f1e44fa77348eded54ea3ec3eef)\r\n* Fixed 'insufficient registers in requested bundle' runtime error in convolution primitive on Xe Architecture GPUs (4c9d46acc35126fec2b59125403566a90b6bed36)\r\n* Addressed performance regression for certain convolution cases on Xe Architecture GPUs (f28b58aec55c5087127702f7c0a38d21b3006d35, 18764fbef1f1f90bc696fe35d059685b2b37f149)\r\n* Added support for Intel DPC++/C++ Compiler 2023 (c3781c671dcc23c0fa16eb648c98ef33b79c737b, a1a8952656b2e84a4124cc0d2f8c7aae10e62a46, 9bc87e635dbeffd77808c70fbd51ac5dc834b582, e3b19871cab6c9b5c317cddb18f4264575868ed7)\r\n* Fixed int8 matmul and inner product performance regression on Xe Architecture GPUs (3693fbf0e8b0cd3bcc2308a4504772c0af2eaf88, c8adc179133f7212523f4ecb1cdab648b0cec796)\r\n* Fixed accuracy issue for convolution, inner product and matmul primitives with `tanh` post-op on Xe Architecture GPUs (88b4e57718014bd50f78461a5c80dc680074f9b6, 83ce6d27a8699d7ab0d1ee450e2e7e9ec87a6e13, 6224dc6b3e2073c98f4b8278bf7e87769dd85a55, 10f0d0ade797a90c93b7450c1e0b151dc415dab3)\r\n* Suppressed spurious build warnings with GCC 11 (44255a8a57dc40ccc8f7b464e5638d6715216756)\r\n", "tarball_url": "https://api.github.com/repos/oneapi-src/oneDNN/tarball/v2.7.1", "zipball_url": "https://api.github.com/repos/oneapi-src/oneDNN/zipball/v2.7.1", "html_url": "https://github.com/oneapi-src/oneDNN/releases/tag/v2.7.1", "url": "https://api.github.com/repos/oneapi-src/oneDNN/releases/80680110", "release_id": 80680110, "date_created": "2022-10-21T22:44:23Z", "date_published": "2022-10-21T22:48:44Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/oneapi-src/oneDNN/releases/80658712", "tag": "v2.6.3", "name": "v2.6.3", "author": {"name": "tprimak", "type": "User"}, "description": "This is a patch release containing the following changes to v2.6.2:\r\n* Fixed potential integer overflow in BRGEMM-based convolution implementation (deb5595a0f96b54f9106cb846e6fc4e0af49aadf)\r\n* Fixed a defect with incorrect caching of BRGEMM-based matmul primitive implementations with trivial dimensions (305bed526492f2400a1a7fdfcb54b0ee41adc67e)\r\n* Extended benchdnn performance benchmarking capabilities on GPU with device-side performance measurement mode (ba8632592018070a46e4d349bbe3628756022c15)\r\n* Fixed segfault in pooling primitive on CPUs (689d874bbf0a3e1bdc75e99ad2453e6aac9cfe84)", "tarball_url": "https://api.github.com/repos/oneapi-src/oneDNN/tarball/v2.6.3", "zipball_url": "https://api.github.com/repos/oneapi-src/oneDNN/zipball/v2.6.3", "html_url": "https://github.com/oneapi-src/oneDNN/releases/tag/v2.6.3", "url": "https://api.github.com/repos/oneapi-src/oneDNN/releases/80658712", "release_id": 80658712, "date_created": "2022-10-21T19:08:27Z", "date_published": "2022-10-21T19:37:38Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/oneapi-src/oneDNN/releases/79959629", "tag": "graph-v0.7", "name": "graph-v0.7", "author": {"name": "vpirogov", "type": "User"}, "description": "This is the Beta Update release for oneDNN Graph API based on [oneDNN v2.7 release](https://github.com/oneapi-src/oneDNN/releases/tag/v2.7).\r\n\r\n# Functionality\r\n* Added operations `Select`, `LogicalAnd`, `LogicalOr`, `LogicalXor`, `LogicalNot`, `Greater`, `GreaterEqual`, `Equal`, `NoeEqual`, `Less`, and `LessEqual`.\r\n* Added `boolean` data type to support logical operations.\r\n* Added support for passing compilation context to the compile API. This feature allows passing additional information, like tensor shape context, for the backend to generate better kernel code.\r\n* Introduced convolution block fusion via oneDNN Graph Compiler.\r\n* **Experimental**: Introduced dynamic shapes support for multi-level perceptron (MLP) block via oneDNN Graph Compiler.\r\n\r\n# Known Issues and Limitations \r\n* The weight\u2019s opaque layout can be queried only from a compiled partition, which requires that input tensor shapes must be known at compilation time.\r\n* MHA and MLP fusion are not activated on machines without Intel AVX-512 support.\r\n\r\n# Thanks to the Contributors\r\nThis release contains contributions from the project core teams as well as Jiong Gong, Chunyuan Wu, Sanchit Jain, Yiqiang Li, Yunfei Mao, Kiefer Kuah and others.\r\n", "tarball_url": "https://api.github.com/repos/oneapi-src/oneDNN/tarball/graph-v0.7", "zipball_url": "https://api.github.com/repos/oneapi-src/oneDNN/zipball/graph-v0.7", "html_url": "https://github.com/oneapi-src/oneDNN/releases/tag/graph-v0.7", "url": "https://api.github.com/repos/oneapi-src/oneDNN/releases/79959629", "release_id": 79959629, "date_created": "2022-10-14T05:35:19Z", "date_published": "2022-10-14T20:21:16Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/oneapi-src/oneDNN/releases/79931978", "tag": "graph-v0.6", "name": "graph-v0.6", "author": {"name": "vpirogov", "type": "User"}, "description": "This is the Beta release for oneDNN Graph based on [oneDNN v2.7 release](https://github.com/oneapi-src/oneDNN/releases/tag/v2.7).\r\n\r\n# Functionality\r\n* Introduced FP32, BF16, FP16, and INT8 inference support on GPU.\r\n* Introduced FP32 and BF16 training support on GPU.\r\n* Introduced support for floating point math mode at graph construction phase. The mode allows the implementation to use low precision datatype for computations when possible.\r\n* Added `graph::finalize()` function to indicate that the user has finished adding operations into the graph and the graph is ready for partitioning.\r\n* Added operations `AbsBackprop`, `Mish`, `MishBackprop`, and `LeakyReLU`.\r\n* Updated API and operation definitions to comply with [oneDNN Graph Specification 1.0-beta](https://spec.oneapi.io/onednn-graph/v1.0-beta/index.html).\r\n\r\n# Usability\r\n* Integrated Graph component headers, source and build system into oneDNN:\r\n  * Headers moved to `include/oneapi/dnnl`.\r\n  * Source moved to `src/graph`.\r\n  * Graph functionality is included into single shared object or dynamic library produced by the build system.\r\n* Aligned API with oneDNN:\r\n  * Shared common `dnnl::engine` and `dnnl::stream`. The original `dnnl::graph::engine` and `dnnl::graph::stream` API were removed.\r\n  * Added a new `make_engine_with_allocator()` API to create `dnnl::engine` with `dnnl::graph::allocator`.\r\n  * A few common basic types were shared between oneDNN and oneDNN Graph, including `dnnl_status_t`, `dnnl_data_type_t`, and `dnnl_dims_t`, etc.\r\n* Introduced `ONEDNN_BUILD_GRAPH` build option to manage Graph component build.\r\n\r\n# Validation\r\n* Introduced `ONEDNN_GRAPH_DUMP` environment variable that serialized library graph and subgraph into JSON files.\r\n* Added the initial version of benchdnn graph driver which can be used to benchmark the performance with a dumped graph JSON file.\r\n\r\n# Breaking changes\r\n* Removed operations `HardTanh`, `Index`, `Pow`, etc. Please check the operation kind list for details.\r\n\r\n# Known Issues and Limitations\r\n* Graph Compiler component is not included with this release. It will be reinstated in oneDNN Graph Beta Update release.\r\n* The weight\u2019s opaque layout can be queried only from a compiled partition, which requires that input tensor shapes must be known at compilation time.\r\n* Build option `ONEDNN_BUILD_GRAPH` is not compatible with some of the build options supported by the build system including `ONEDNN_GPU_RUNTIME=OCL`,  `ONEDNN_ENABLE_WORKLOAD=INFERENCE`, `ONEDNN_ENABLE_PRIMITIVE`, and others.\r\n\r\n# Thanks to the Contributors\r\nThis release contains contributions from the project core teams as well as Jiong Gong, Chunyuan Wu, Sanchit Jain, Yiqiang Li, Yunfei Mao, Kiefer Kuah and others.\r\n", "tarball_url": "https://api.github.com/repos/oneapi-src/oneDNN/tarball/graph-v0.6", "zipball_url": "https://api.github.com/repos/oneapi-src/oneDNN/zipball/graph-v0.6", "html_url": "https://github.com/oneapi-src/oneDNN/releases/tag/graph-v0.6", "url": "https://api.github.com/repos/oneapi-src/oneDNN/releases/79931978", "release_id": 79931978, "date_created": "2022-10-10T03:27:41Z", "date_published": "2022-10-14T15:05:28Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/oneapi-src/oneDNN/releases/78372665", "tag": "v2.7", "name": "v2.7", "author": {"name": "harrymao2022", "type": "User"}, "description": "# Performance Optimizations  \r\n* Intel Architecture Processors \r\n  * Improved performance for future Intel Xeon Scalable processors (code name Sapphire Rapids).  \r\n  * Introduced performance optimizations for [bf16 floating point math mode](http://oneapi-src.github.io/oneDNN/group_dnnl_api_mathmode.html) on Intel Xeon Scalable processors (code name Sapphire Rapids). The bf16 math mode allows oneDNN to use bf16 arithmetic and Intel AMX instructions in computations on fp32 data. \r\n* Intel Graphics Products \r\n  * Improved performance for future Xe Architecture graphics (code name Ponte Vecchio). \r\n  * Introduced performance optimizations for [tf32 floating point math mode](http://oneapi-src.github.io/oneDNN/group_dnnl_api_mathmode.html) on future Xe Architecture graphics (code name Ponte Vecchio). The tf32 math mode allows oneDNN to use tf32 arithmetic in computations on fp32 data. \r\n  * Improved performance for Intel Arc graphics (formerly Alchemist and DG2) and Intel Data Center GPU Flex Series (formerly Arctic Sound-M) \r\n* AArch64-based Processors \r\n  * Improved convolution and binary primitive performance for processors with SVE 512 support. \r\n  * Improved shuffle and eltwise primitives performance for processors with SVE 256 and SVE 128 support. \r\n  * Improved PReLU, batch normalization, and pooling primitives performance via Compute Library for the Arm Architecture (ACL). \r\n  * Improved performance of inner product, matmul, convolution, and batch norm primitives with post-ops via ACL. \r\n* PowerPC64-based Processors \r\n  * Introduced performance optimizations for int8 and bfloat16 GEMM. \r\n# Functionality  \r\n  * Introduced runtime output scales support in all primitives. \r\n  * Introduced scales support in concat primitive. \r\n  * Extended [floating point math mode API](http://oneapi-src.github.io/oneDNN/group_dnnl_api_mathmode.html) with tf32 data type option. \r\n  * Extended eltwise primitive with support for `hardsigmoid` algorithm. \r\n  * Extended layer normalization primitive with support for mixed source and destination data types. \r\n  * Extended depthwise post-op with support for arbitrary padding size. The implementation is available only on Intel processors. \r\n  * Added limited fp64 data type support in convolution primitive. Optimized implementation is available for future Xe Architecture graphics (code name Ponte Vecchio). \r\n  * Extended int8 convolution and deconvolution implementations on GPUs with arbitrary destination data type support.   \r\n  * Extended batch normalization primitive with `dnnl_fuse_norm_add_relu` flag that allows to fuse sum and relu operations. The implementation is available for Intel GPUs. \r\n  * Extended GPU deconvolution primitive implementation with support for output scales and zero points. \r\n  * Introduced threadpool threading support for AArch64-based processors. \r\n  * Introduced Unified Shared Memory (USM) support for SYCL backend on NVIDIA GPUs. \r\n  * Introduced initial support for AMD GPUs via MIOpen library. Supported primitives include Local Response Normalization (LRN), softmax, and eltwise.  \r\n# Usability   \r\n  * Added `matmul_perf` example that benchmarks matmul primitive for all supported data types. \r\n  * Introduced annotations for JIT kernels to allow profilers like Linux perf to correctly label JIT code.  \r\n  * Extended verbose logs converter with RNN primitive support. \r\n  * Added verbose output for `dnnl_*gemm*` calls. \r\n  * Removed Level Zero headers from the list of build time dependencies.  \r\n  * Adjusted NVIDIA GPU implementation to comply with oneDNN numerical behavior. Implicit downconvert to fp16 and tf32 are now managed via [math mode API](http://oneapi-src.github.io/oneDNN/group_dnnl_api_mathmode.html). \r\n \r\n# Validation \r\n  * Added benchdnn driver for validation of internal BRGEMM implementation. \r\n  * Improved benchdnn reference implementation performance with threadpool threading model. \r\n  * Extended benchdnn performance benchmarking capabilities on GPU with device-side performance measurement mode (`mode=po`). \r\n \r\n# Deprecated Functionality \r\n  * Support for SYCL 1.2.1 (aka SYCL 2017 standard) is deprecated and will be removed in the future releases. \r\n  * Static output scales are deprecated and will be removed in the next release. \r\n  * Convolution Winograd algorithm implementation for int8 data type is deprecated and will be removed in the next release. \r\n \r\n# Breaking Changes  \r\n  * Changed formula for AUGRU RNN cell to align with Tensorflow. See [proposal](https://github.com/oneapi-src/oneDNN/blob/rfcs/rfcs/20211025-augru/augru-v2.md) for details. \r\n \r\n# Thanks to the Contributors  \r\nThis release contains contributions from the project core team as well as Aidan Belton @AidanBeltonS, @akshatasangelkar, Alex Bojan @lb991, Crefeda Rodrigues @cfRod, Damian Szwichtenberg @dszwicht, Diana Bite @diaena, Divakar Mariyanna @bmdivakar, Emilio Cota @cota, Gordon Fossum @austinpagan, Hugh Delaney @hdelan, Jacek Czaja @jczaja, @jakpiase, Jonathan Deakin @jondea, Kentaro Kawakami @kawakami-k, Kotha Sowmya @Sowmyakotha1999, Louie Tsai @louie-tsai, Mark Ryan @markdryan, MITSUNARI Shigeo @herumi, Mona Minakshi @monaminakshi, @NaNAGISaSA, Nathan John Sircombe @nSircombe, Peter Caday @petercad,  @pgorlani, Sreekanth Yalachigere @sreekanth-yalachigere, Tadej Ciglari\u010d @t4c1, and Thiago Macieira @thiagomacieira. We would also like to thank everyone who asked questions and reported issues. \r\n", "tarball_url": "https://api.github.com/repos/oneapi-src/oneDNN/tarball/v2.7", "zipball_url": "https://api.github.com/repos/oneapi-src/oneDNN/zipball/v2.7", "html_url": "https://github.com/oneapi-src/oneDNN/releases/tag/v2.7", "url": "https://api.github.com/repos/oneapi-src/oneDNN/releases/78372665", "release_id": 78372665, "date_created": "2022-09-26T22:40:04Z", "date_published": "2022-09-27T22:17:15Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/oneapi-src/oneDNN/releases/77418068", "tag": "v2.7-rc", "name": "v2.7-rc", "author": {"name": "harrymao2022", "type": "User"}, "description": "This is a release candidate for oneDNN v2.7. Please provide feedback and submit defect reports via [Github issues](https://github.com/oneapi-src/oneDNN/issues/new/choose).\r\n\r\n# Performance Optimizations  \r\n* Intel Architecture Processors \r\n  * Improved performance for future Intel Xeon Scalable processors (code name Sapphire Rapids).  \r\n  * Introduced performance optimizations for [bf16 floating point math mode](http://oneapi-src.github.io/oneDNN/group_dnnl_api_mathmode.html) on Intel Xeon Scalable processors (code name Sapphire Rapids). The bf16 math mode allows oneDNN to use bf16 arithmetic and Intel AMX instructions in computations on fp32 data. \r\n* Intel Graphics Products \r\n  * Improved performance for future Xe Architecture graphics (code name Ponte Vecchio). \r\n  * Introduced performance optimizations for [tf32 floating point math mode](http://oneapi-src.github.io/oneDNN/group_dnnl_api_mathmode.html) on future Xe Architecture graphics (code name Ponte Vecchio). The tf32 math mode allows oneDNN to use tf32 arithmetic in computations on fp32 data. \r\n  * Improved performance for Intel Arc graphics (formerly Alchemist and DG2) and Intel Data Center GPU Flex Series (formerly Arctic Sound-M) \r\n* AArch64-based Processors \r\n  * Improved convolution and binary primitive performance for processors with SVE 512 support. \r\n  * Improved eltwise and shuffle primitives performance for processors with SVE 256 and SVE 128 support. \r\n  * Improved PReLU, batch normalization, and pooling primitives performance via Compute Library for the Arm Architecture (ACL).\r\n  * Improved performance of inner product, matmul, convolution, and batch norm primitives with post-ops via ACL. \r\n* PowerPC64-based Processors \r\n  * Introduced performance optimizations for int8 and bfloat16 GEMM. \r\n# Functionality  \r\n  * Introduced runtime output scales support in all primitives. \r\n  * Introduced scales support in concat primitive. \r\n  * Extended [floating point math mode API](http://oneapi-src.github.io/oneDNN/group_dnnl_api_mathmode.html) with tf32 data type option. \r\n  * Extended eltwise primitive with support for `hardsigmoid` algorithm. \r\n  * Extended layer normalization primitive with support for mixed source and destination data types. \r\n  * Extended depthwise post-op with support for arbitrary padding size. The implementation is available only on Intel processors. \r\n  * Added limited fp64 data type support in convolution primitive. Optimized implementation is available for future Xe Architecture graphics (code name Ponte Vecchio). \r\n  * Extended int8 convolution and deconvolution implementations on GPUs with arbitrary destination data type support.   \r\n  * Extended batch normalization primitive with `dnnl_fuse_norm_add_relu` flag that allows to fuse sum and relu operations. The implementation is available for Intel GPUs. \r\n  * Extended GPU deconvolution primitive implementation with support for output scales and zero points. \r\n  * Introduced threadpool threading support for AArch64-based processors. \r\n  * Introduced Unified Shared Memory (USM) support for SYCL backend on NVIDIA GPUs. \r\n  * Introduced initial support for AMD GPUs via MIOpen library. Supported primitives include Local Response Normalization (LRN), softmax, and eltwise.  \r\n# Usability   \r\n  * Introduced annotations for JIT kernels to allow profilers like Linux perf to correctly label JIT code.  \r\n  * Extended verbose logs converter with RNN primitive support. \r\n  * Added verbose output for `dnnl_*gemm*` calls. \r\n  * Removed Level Zero headers from the list of build time dependencies.  \r\n  * Adjusted NVIDIA GPU implementation to comply with oneDNN numerical behavior. Implicit downconvert to fp16 and tf32 are now managed via [math mode API](http://oneapi-src.github.io/oneDNN/group_dnnl_api_mathmode.html). \r\n \r\n# Validation \r\n  * Added benchdnn driver for validation of internal BRGEMM implementation. \r\n  * Improved benchdnn reference implementation performance with threadpool threading model. \r\n  * Extended benchdnn performance benchmarking capabilities on GPU with device-side performance measurement mode (`mode=po`). \r\n \r\n# Deprecated Functionality \r\n  * Support for SYCL 1.2.1 (aka SYCL 2017 standard) is deprecated and will be removed in the future releases. \r\n  * Static output scales are deprecated and will be removed in the next release. \r\n  * Convolution Winograd algorithm implementation for int8 data type is deprecated and will be removed in the next release. \r\n \r\n# Breaking Changes  \r\n  * Changed formula for AUGRU RNN cell to align with Tensorflow. See [proposal](https://github.com/oneapi-src/oneDNN/blob/rfcs/rfcs/20211025-augru/augru-v2.md) for details. \r\n \r\n# Thanks to the Contributors  \r\nThis release contains contributions from the project core team as well as Aidan Belton @AidanBeltonS, @akshatasangelkar, Alex Bojan @lb991, Crefeda Rodrigues @cfRod, Damian Szwichtenberg @dszwicht, Diana Bite @diaena, Divakar Mariyanna @bmdivakar, Emilio Cota @cota, Gordon Fossum @austinpagan, Hugh Delaney @hdelan, Jacek Czaja @jczaja, @jakpiase, Jonathan Deakin @jondea, Kentaro Kawakami @kawakami-k, Kotha Sowmya @Sowmyakotha1999, Louie Tsai @louie-tsai, Mark Ryan @markdryan, MITSUNARI Shigeo @herumi, Mona Minakshi @monaminakshi, @NaNAGISaSA, Nathan John Sircombe @nSircombe, Peter Caday @petercad,  @pgorlani, Sreekanth Yalachigere @sreekanth-yalachigere, Tadej Ciglari\u010d @t4c1, and Thiago Macieira @thiagomacieira. We would also like to thank everyone who asked questions and reported issues. \r\n", "tarball_url": "https://api.github.com/repos/oneapi-src/oneDNN/tarball/v2.7-rc", "zipball_url": "https://api.github.com/repos/oneapi-src/oneDNN/zipball/v2.7-rc", "html_url": "https://github.com/oneapi-src/oneDNN/releases/tag/v2.7-rc", "url": "https://api.github.com/repos/oneapi-src/oneDNN/releases/77418068", "release_id": 77418068, "date_created": "2022-09-15T23:23:39Z", "date_published": "2022-09-16T17:31:31Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/oneapi-src/oneDNN/releases/76770325", "tag": "v2.6.2", "name": "v2.6.2", "author": {"name": "vpirogov", "type": "User"}, "description": "This is a patch release containing the following changes to v2.6.1:\r\n* Removed unused variables (2500b0f6c1931f4b0b22b5fc92fcc87c6b875a3f, b4e00322c93984082b987408af8a2e341c7fd6c2)\r\n* Fixed correctness issue in fp32 convolution implementation for cases with large spatial size (207af06637ccf36fb08c5fd93b55d52a578cfa5a)\r\n* Fixed correctness issue in bfloat16 matmul implementation for processors with Intel AMX support (404b762f27350d5ad59225d966310b481951451e)\r\n* Fixed correctness issue in int8 reorder implementation with zero points (b340cba1cadc8fc6424945b5b2a09960bd8d47ec)\r\n* Improved int8 matmul and inner product primitives performance with small matrices for processors with Intel AMX support (73b75723921e9881b88b027a8f1b2d42251f6403, 58b386a21cfc9dbb7c331626e9e4752751cdf415)\r\n* Improved int8 convolution performance for processors with Intel DL Boost support (f35a62f9b3c1db5ce8a2704e530e050b2f4b1807)\r\n* Aligned AUGRU formula with Tensorflow definition (e47c6c570d97545b56f3afef77ce9fbd63ea320b, 4ba0a577947733690cdd0f9ecf269121148a28e1, b311e24ac3b669d6200b595201107601b6ce1f58)\r\n* Suppressed 'unvectorized loop' warning for Intel C/C++ Compiler (3932d0493586963df3cefb3c8f35cb6503cd444e)\r\n", "tarball_url": "https://api.github.com/repos/oneapi-src/oneDNN/tarball/v2.6.2", "zipball_url": "https://api.github.com/repos/oneapi-src/oneDNN/zipball/v2.6.2", "html_url": "https://github.com/oneapi-src/oneDNN/releases/tag/v2.6.2", "url": "https://api.github.com/repos/oneapi-src/oneDNN/releases/76770325", "release_id": 76770325, "date_created": "2022-09-09T22:35:06Z", "date_published": "2022-09-09T22:36:17Z"}, "confidence": 1, "technique": "GitHub_API"}], "code_of_conduct": [{"result": {"value": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\n advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\n address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies both within project spaces and in public spaces\nwhen an individual is representing the project or its community. Examples of\nrepresenting a project or community include using an official project e-mail\naddress, posting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event. Representation of a project may be\nfurther defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project maintainers at dnnl.maintainers@intel.com. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq\n", "type": "File_dump"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/intel/mkl-dnn/main/CODE_OF_CONDUCT.md"}], "readme_url": [{"result": {"value": "https://raw.githubusercontent.com/intel/mkl-dnn/main/README.md", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}], "contributing_guidelines": [{"result": {"value": "# Contributing guidelines\n\nIf you have improvements to the oneDNN code, please send us your pull\nrequests! To get started, see the GitHub\n[howto](https://help.github.com/en/articles/about-pull-requests).\n\nYou can:\n\n- Submit your changes directly with a\n  [pull request](https://github.com/oneapi-src/oneDNN/pulls)\n- Log a bug or feedback with an [issue](https://github.com/oneapi-src/oneDNN/issues)\n\n**See also:** [Contributor Covenant](CODE_OF_CONDUCT.md) code of conduct.\n\n## Pull request checklist\n\nBefore sending your pull requests, make sure that you have followed this list:\n\n* Check the [library functionality guidelines](CONTRIBUTING.md#library-functionality-guidelines).\n  If you are contributing a new compute primitive or propose changes to the\n  external API, it is strongly advised to first open an [RFC pull request](CONTRIBUTING.md#RFC-pull-requests)\n  with a detailed explanation of expected use cases and performance benefits.\n\n* Ensure that the changes are consistent with the\n  [code contribution guidelines](CONTRIBUTING.md#code-contribution-guidelines)\n  and [coding standards](CONTRIBUTING.md#coding-standards).\n\n* Check that [unit tests](CONTRIBUTING.md#unit-tests) pass.\n\n## Library functionality guidelines\n\noneDNN focuses on functionality that satisfies all of the following\ncriteria:\n\n1. *Performance*: the functionality has material impact on a workload level.\n   In other words, this means that for a new primitive it should be\n   demonstrated that it brings visible performance improvement to some\n   workload.\n\n2. *Generality*: the functionality is useful in a wide range of deep learning\n   applications. This implies that when introducing a new primitive, its API\n   needs to be general enough to be integrated into multiple deep learning\n   frameworks that have similar functionality.\n\n3. *Complexity*: it is not trivial to implement the functionality directly in\n   a deep learning application.\n\n### RFC pull requests\n\nSignificant library changes (new primitives, library architecture changes,\nAPI modifications, etc) require approval from oneDNN maintainers before\nopening a pull request with such implementation. For that we use the Request\nFor Comments (RFC) process, which consists of opening, discussing, and\naccepting (promoting) RFC pull requests.\n\nMore information about the process can be found in the dedicated\n[`rfcs`](https://github.com/oneapi-src/oneDNN/tree/rfcs) branch.\n\n## Code contribution guidelines\n\nWhen submitting your contribution, please make sure that it is:\n\n* *Tested*: oneDNN uses gtests for lightweight functional testing and\n  benchdnn for functionality that requires both performance and functional\n  testing.\n\n* *Documented*: oneDNN uses Doxygen for inline comments in public header\n  files that is used to build reference manual and markdown (also processed by\n  Doxygen) for user guide.\n\n* *Portable*: oneDNN supports different operating systems, CPU and GPU\n  architectures, compilers, and run-times. The new code should be compliant\n  with the [System Requirements](README.md#system-requirements).\n\nAll code in oneDNN gets promoted to product branches (`master`, `rls-`, and\n`mnt-`) only through GitHub pull requests. Requirements for promotion:\n\n- The request is reviewed and approved by maintainers for all affected\n  components.\n- All discussions in the pull request are resolved.\n- Continuous integration pipeline passed without errors.\n- Promotion to release (`rls-`) branches can be done only by maintainers\n  (enforced by GitHub)\n- The pull request author is responsible for collecting all the necessary\n  approvals, rebasing of the changes, and resolving the discussions.\n\nTo simplify the work of reviewers, make sure that the commits in the pull\nrequest adhere to the following requirements:\n\n- Commit message should be fit into 50 (at most 72) characters and have the\n  imperative mood.\n- Commit message should follow the format:\n  `<scope>:[scope: ..] <short description>`\n  Scope examples:\n  * Top level: `build`, `api`, `doc`, `tests`, `common`, `cpu`, `gpu`\n  * Second level: `convolution`, `pooling`, `utils`, `verbose`\n  * Example commit message:\n~~~git\ncommon: verbose: fix crash when prim_iface_t is empty\n~~~\n\n- Commit body should also fit 72 characters. Think of it as a standard e-mail\n  body or a markdown document in terms of styling - write sentences from the\n  very left border keeping capital letters and punctuation in place.\n- oneDNN branches maintain linear history. Rebase the changes on top of target\n  branch before creating a pull request. Rebase again after resolving all the\n  discussions, as well as in case of merge conflicts.\n- Use `git add -p`  and `git rebase -i` liberally to split unrelated changes\n  into multiple self-contained commits. This is a courtesy to reviewers: smaller\n  commits are easier to comprehend. It also helps with bisecting in the future.\n  Of course judgement needs to be applied whether to split changes or not. For\n  example, split code cleanup and the actual fix into two separate patches.\n\n## Coding Standards\n\nContributions to oneDNN must follow the [Coding Standards](CODING_STANDARDS.md)\nin order to simplify development and review processes. The general principle is\nto follow the style of existing/surrounding code.\n\nThe Coding Standards are subject to change and contributions to the Coding\nStandards are welcome.\n\nIf you wish to propose changes to the Coding Standards (including `clang-tidy`\nchecks and `clang-format` options), please submit the proposal via an [RFC pull\nrequest](CONTRIBUTING.md#RFC-pull-requests). The proposal should contain the\nfollowing information:\n* *Motivation*: Why should the proposed standard be introduced and applied?\n* *Enforcement*: Can the proposed standard be applied via an automated process\n  or other practical means?\n* *Example*: What does the code base look like with the proposed standard\n  applied?\n  * For instance, in case of a `clang-tidy` check, please open a separate PR\n    with the check applied to the code base alongside the RFC PR.\n\n## Unit tests\n\noneDNN uses gtests for lightweight functional testing and benchdnn for\nperformance and functional testing.\n\nBe sure to extend the existing tests when fixing an issue.\n\nDeveloping new benchdnn tests can be hard, so it is a good idea to start with\ngtests first.\n", "type": "File_dump"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/intel/mkl-dnn/main/CONTRIBUTING.md"}, {"result": {"value": "We welcome community contributions to oneDNN. If you have an idea on how\nto improve the library:\n\n* For changes impacting the public API or library overall, such as adding new\n  primitives or changes to the architecture, submit an\n  [RFC pull request](https://github.com/oneapi-src/oneDNN/tree/rfcs).\n* Ensure that the changes are consistent with the\n  [code contribution guidelines](CONTRIBUTING.md#code-contribution-guidelines)\n  and [coding standards](CONTRIBUTING.md#coding-standards).\n* Ensure that you can build the product and run all the examples with your\n  patch.\n* Submit a [pull request](https://github.com/oneapi-src/oneDNN/pulls).\n\nFor additional details, see [contribution guidelines](CONTRIBUTING.md).\n\nThis project is intended to be a safe, welcoming space for collaboration, and\ncontributors are expected to adhere to the\n[Contributor Covenant](CODE_OF_CONDUCT.md) code of conduct.\n", "type": "Text_excerpt", "original_header": "Contributing"}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/intel/mkl-dnn/main/README.md"}], "has_script_file": [{"result": {"value": "https://raw.githubusercontent.com/intel/mkl-dnn/main/tests/generate_c_symbols_refs.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/intel/mkl-dnn/main/.github/automation/build.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/intel/mkl-dnn/main/.github/automation/clang-format.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/intel/mkl-dnn/main/.github/automation/build_acl.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/intel/mkl-dnn/main/.github/automation/test.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/intel/mkl-dnn/main/.github/automation/env/clang.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/intel/mkl-dnn/main/.github/automation/env/qemu.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}], "documentation": [{"result": {"value": "* [Developer guide](https://oneapi-src.github.io/oneDNN) explains programming\n  model, supported functionality, and implementation details, and\n  includes annotated examples.\n* [API reference](https://oneapi-src.github.io/oneDNN/group_dnnl_api.html) provides\n  a comprehensive reference of the library API.\n", "type": "Text_excerpt", "original_header": "Documentation"}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/intel/mkl-dnn/main/README.md"}], "installation": [{"result": {"value": "Binary distribution of this software is available in:\n* [Anaconda](https://anaconda.org/conda-forge/onednn)\n* [Intel oneAPI](https://www.intel.com/content/www/us/en/developer/tools/oneapi/onednn.html)\n\nThe packages do not include library dependencies and these need to be resolved\nin the application at build time. See the\n[System Requirements](#system-requirements) section below and the\n[Build Options](https://oneapi-src.github.io/oneDNN/dev_guide_build_options.html)\nsection in the [developer guide](https://oneapi-src.github.io/oneDNN) for more\ndetails on CPU and GPU runtimes.\n\nIf the configuration you need is not available, you can\n[build the library from source](https://oneapi-src.github.io/oneDNN/dev_guide_build.html).\n", "type": "Text_excerpt", "original_header": "Installation"}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/intel/mkl-dnn/main/README.md"}], "requirements": [{"result": {"value": "oneDNN supports platforms based on the following architectures:\n- [Intel 64 or AMD64](https://en.wikipedia.org/wiki/X86-64),\n- [Arm 64-bit Architecture (AArch64)](https://developer.arm.com/architectures/cpu-architecture/a-profile).\n- [OpenPOWER](https://openpowerfoundation.org/) / [IBM Power ISA](https://en.wikipedia.org/wiki/Power_ISA).\n- [IBMz z/Architecture (s390x)](https://en.wikipedia.org/wiki/Z/Architecture).\n- [RISC-V 64-bit (RV64)](https://en.wikipedia.org/wiki/RISC-V).\n\n> **WARNING**\n>\n> Power ISA (PPC64), IBMz (s390x), and RISC-V (RV64) support is\n> **experimental** with limited testing validation.\n\nThe library is optimized for the following CPUs:\n* Intel Atom(R) processor (at least Intel SSE4.1 support is required)\n* Intel Core(TM) processor (at least Intel SSE4.1 support is required)\n* Intel Xeon(R) processor E3, E5, and E7 family (formerly Sandy Bridge,\n  Ivy Bridge, Haswell, and Broadwell)\n* Intel Xeon Scalable processor (formerly Skylake, Cascade Lake, Cooper\n  Lake, Ice Lake, and Sapphire Rapids)\n* Intel Xeon CPU Max Series (formerly Sapphire Rapids HBM)\n* future Intel Xeon Scalable processor (code name Granite Rapids)\n\nOn a CPU based on Intel 64 or on AMD64 architecture, oneDNN detects\nthe instruction set architecture (ISA) at runtime and uses just-in-time (JIT)\ncode generation to deploy the code optimized for the latest supported ISA.\nFuture ISAs may have initial support in the library disabled by default and\nrequire the use of run-time controls to enable them. See\n[CPU dispatcher control](https://oneapi-src.github.io/oneDNN/dev_guide_cpu_dispatcher_control.html)\nfor more details.\n\nOn a CPU based on Arm AArch64 architecture, oneDNN can be built with Arm Compute Library\nintegration. Compute Library is an open-source library for machine learning applications\nand provides AArch64 optimized implementations of core functions. This functionality currently\nrequires that Compute Library is downloaded and built separately, see\n[Build from Source](https://oneapi-src.github.io/oneDNN/dev_guide_build.html).\noneDNN only supports Compute Library versions 23.02.1 or later.\n\n> **WARNING**\n>\n> On macOS, applications that use oneDNN may need to request special\n> entitlements if they use the hardened runtime. See the\n> [linking guide](https://oneapi-src.github.io/oneDNN/dev_guide_link.html)\n> for more details.\n\nThe library is optimized for the following GPUs:\n* Intel UHD Graphics for 11th generation Intel processors or newer\n* Intel Iris(R) Xe graphics (formerly DG1)\n* Intel Arc(TM) graphics (formerly Alchemist and DG2)\n* Intel Data Center GPU Flex Series (formerly Arctic Sound-M)\n* Intel Data Center GPU Max Series (formerly Ponte Vecchio)\n", "type": "Text_excerpt", "original_header": "System Requirements"}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/intel/mkl-dnn/main/README.md"}, {"result": {"value": "oneDNN supports systems meeting the following requirements:\n* Operating system with Intel 64 / Arm 64 / Power / IBMz architecture support\n* C++ compiler with C++11 standard support\n* [CMake](https://cmake.org/download/) 2.8.12 or later\n* [Arm Compute Library](https://github.com/arm-software/ComputeLibrary)\n  for builds using Compute Library on AArch64.\n\nThe following tools are required to build oneDNN documentation:\n* [Doxygen](http://www.doxygen.nl/download.html#srcbin) 1.8.5 or later\n* [Doxyrest](https://github.com/vovkos/doxyrest) 2.1.2 or later\n* [Sphinx](https://www.sphinx-doc.org/en/master/usage/installation.html) 4.0.2 or later\n* [sphinx-book-theme](https://sphinx-book-theme.readthedocs.io/en/latest) 0.0.41 or later\n\nConfigurations of CPU and GPU engines may introduce additional build time\ndependencies.\n", "type": "Text_excerpt", "original_header": "Requirements for Building from Source", "parent_header": ["System Requirements"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/intel/mkl-dnn/main/README.md"}, {"result": {"value": "oneDNN CPU engine is used to execute primitives on Intel Architecture\nProcessors, 64-bit Arm Architecture (AArch64) processors,\n64-bit Power ISA (PPC64) processors, IBMz (s390x), and compatible devices.\n\nThe CPU engine is built by default but can be disabled at build time by setting\n`DNNL_CPU_RUNTIME` to `NONE`. In this case, GPU engine must be enabled.\nThe CPU engine can be configured to use the OpenMP, TBB or SYCL runtime.\nThe following additional requirements apply:\n* OpenMP runtime requires C++ compiler with OpenMP 2.0 or later\n  standard support\n* TBB runtime requires\n[Threading Building Blocks (TBB)](https://www.threadingbuildingblocks.org/)\n2017 or later.\n* SYCL runtime requires\n  * [Intel oneAPI DPC++/C++ Compiler](https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html)\n  * [Threading Building Blocks (TBB)](https://www.intel.com/content/www/us/en/developer/tools/oneapi/onetbb.html)\n\nSome implementations rely on OpenMP 4.0 SIMD extensions. For the best\nperformance results on Intel Architecture Processors we recommend using the\nIntel C++ Compiler.\n", "type": "Text_excerpt", "original_header": "CPU Engine", "parent_header": ["System Requirements", "Requirements for Building from Source"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/intel/mkl-dnn/main/README.md"}, {"result": {"value": "Intel Processor Graphics and Xe Architecture graphics are supported by\nthe oneDNN GPU engine. The GPU engine is disabled in the default build\nconfiguration. The following additional requirements apply when GPU engine\nis enabled:\n* OpenCL runtime requires\n    * OpenCL\\* runtime library (OpenCL version 1.2 or later)\n    * OpenCL driver (with kernel language support for OpenCL C 2.0 or later)\n      with Intel subgroups and USM extensions support\n* SYCL runtime requires\n    * [Intel oneAPI DPC++/C++ Compiler](https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html)\n    * OpenCL runtime library (OpenCL version 1.2 or later)\n    * [oneAPI Level Zero](https://github.com/oneapi-src/level-zero)\n* SYCL runtime with NVIDIA GPU support requires\n    * [oneAPI DPC++ Compiler with support for CUDA](https://github.com/intel/llvm/blob/sycl/sycl/doc/GetStartedGuide.md#build-dpc-toolchain-with-support-for-nvidia-cuda)\n      or [Intel oneAPI DPC++/C++ Compiler](https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html) with [NVIDIA plugin](https://developer.codeplay.com/products/oneapi/nvidia/home)\n    * NVIDIA CUDA\\* driver\n    * cuBLAS 10.1 or later\n    * cuDNN 7.6 or later\n* SYCL runtime with AMD GPU support requires\n    * [oneAPI DPC++ Compiler with support for HIP AMD](https://github.com/intel/llvm/blob/sycl/sycl/doc/GetStartedGuide.md#build-dpc-toolchain-with-support-for-hip-amd)\n    * [AMD ROCm](https://github.com/RadeonOpenCompute/ROCm), version 5.3 or later\n    * [MIOpen](https://github.com/ROCmSoftwarePlatform/MIOpen), version 2.18 or later (optional if AMD ROCm includes the required version of MIOpen)\n    * [rocBLAS](https://github.com/ROCmSoftwarePlatform/rocBLAS), version 2.45.0 or later (optional if AMD ROCm includes the required version of rocBLAS)\n\n> **WARNING**\n>\n> Linux will reset GPU when kernel runtime exceeds several seconds. The user\n> can prevent this behavior by following [instructions](https://www.intel.com/content/www/us/en/docs/oneapi/installation-guide-linux/2023-0/gpu-disable-hangcheck.html)\n> on disabling hangcheck for Intel GPU driver. Same behavior [applies](https://learn.microsoft.com/en-us/windows-hardware/drivers/display/timeout-detection-and-recovery)\n> to Windows. The user can prevent this behavior by increasing the [TdrDelay](https://learn.microsoft.com/en-us/windows-hardware/drivers/display/tdr-registry-keys#tdrdelay)\n> setting value.\n\n> **WARNING**\n>\n> NVIDIA GPU support is experimental. General information, build instructions,\n> and implementation limitations are available in the\n> [NVIDIA backend readme](https://github.com/oneapi-src/oneDNN/blob/master/src/gpu/nvidia/README.md).\n\n> **WARNING**\n>\n> AMD GPU support is experimental. General information, build instructions,\n> and implementation limitations are available in the\n> [AMD backend readme](https://github.com/oneapi-src/oneDNN/blob/master/src/gpu/amd/README.md).\n", "type": "Text_excerpt", "original_header": "GPU Engine", "parent_header": ["System Requirements", "Requirements for Building from Source"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/intel/mkl-dnn/main/README.md"}, {"result": {"value": "When oneDNN is built from source, the library runtime dependencies\nand specific versions are defined by the build environment.\n", "type": "Text_excerpt", "original_header": "Runtime Dependencies", "parent_header": ["System Requirements", "Requirements for Building from Source"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/intel/mkl-dnn/main/README.md"}, {"result": {"value": "Common dependencies:\n* GNU C Library (`libc.so`)\n* GNU Standard C++ Library v3 (`libstdc++.so`)\n* Dynamic Linking Library (`libdl.so`)\n* C Math Library (`libm.so`)\n* POSIX Threads Library (`libpthread.so`)\n\nRuntime-specific dependencies:\n\n| Runtime configuration    | Compiler                      | Dependency\n| :----------------------- | :---------------------------- | :---------\n| `DNNL_CPU_RUNTIME=OMP`   | GCC                           | GNU OpenMP runtime (`libgomp.so`)\n| `DNNL_CPU_RUNTIME=OMP`   | Intel C/C++ Compiler          | Intel OpenMP runtime (`libiomp5.so`)\n| `DNNL_CPU_RUNTIME=OMP`   | Clang                         | Intel OpenMP runtime (`libiomp5.so`)\n| `DNNL_CPU_RUNTIME=TBB`   | any                           | TBB (`libtbb.so`)\n| `DNNL_CPU_RUNTIME=SYCL`  | Intel oneAPI DPC++ Compiler   | Intel oneAPI DPC++ Compiler runtime (`libsycl.so`), TBB (`libtbb.so`), OpenCL loader (`libOpenCL.so`)\n| `DNNL_GPU_RUNTIME=OCL`   | any                           | OpenCL loader (`libOpenCL.so`)\n| `DNNL_GPU_RUNTIME=SYCL`  | Intel oneAPI DPC++ Compiler   | Intel oneAPI DPC++ Compiler runtime (`libsycl.so`), OpenCL loader (`libOpenCL.so`), oneAPI Level Zero loader (`libze_loader.so`)\n", "type": "Text_excerpt", "original_header": "Linux", "parent_header": ["System Requirements", "Requirements for Building from Source", "Runtime Dependencies"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/intel/mkl-dnn/main/README.md"}, {"result": {"value": "Common dependencies:\n* Microsoft Visual C++ Redistributable (`msvcrt.dll`)\n\nRuntime-specific dependencies:\n\n| Runtime configuration    | Compiler                      | Dependency\n| :----------------------- | :---------------------------- | :---------\n| `DNNL_CPU_RUNTIME=OMP`   | Microsoft Visual C++ Compiler | No additional requirements\n| `DNNL_CPU_RUNTIME=OMP`   | Intel C/C++ Compiler          | Intel OpenMP runtime (`iomp5.dll`)\n| `DNNL_CPU_RUNTIME=TBB`   | any                           | TBB (`tbb.dll`)\n| `DNNL_CPU_RUNTIME=SYCL`  | Intel oneAPI DPC++ Compiler   | Intel oneAPI DPC++ Compiler runtime (`sycl.dll`), TBB (`tbb.dll`), OpenCL loader (`OpenCL.dll`)\n| `DNNL_GPU_RUNTIME=OCL`   | any                           | OpenCL loader (`OpenCL.dll`)\n| `DNNL_GPU_RUNTIME=SYCL`  | Intel oneAPI DPC++ Compiler   | Intel oneAPI DPC++ Compiler runtime (`sycl.dll`), OpenCL loader (`OpenCL.dll`), oneAPI Level Zero loader (`ze_loader.dll`)\n", "type": "Text_excerpt", "original_header": "Windows", "parent_header": ["System Requirements", "Requirements for Building from Source", "Runtime Dependencies"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/intel/mkl-dnn/main/README.md"}, {"result": {"value": "Common dependencies:\n* System C/C++ runtime (`libc++.dylib`, `libSystem.dylib`)\n\nRuntime-specific dependencies:\n\n| Runtime configuration  | Compiler                      | Dependency\n| :--------------------- | :---------------------------- | :---------\n| `DNNL_CPU_RUNTIME=OMP` | Intel C/C++ Compiler          | Intel OpenMP runtime (`libiomp5.dylib`)\n| `DNNL_CPU_RUNTIME=TBB` | any                           | TBB (`libtbb.dylib`)\n", "type": "Text_excerpt", "original_header": "macOS", "parent_header": ["System Requirements", "Requirements for Building from Source", "Runtime Dependencies"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/intel/mkl-dnn/main/README.md"}, {"result": {"value": "CPU engine was validated on RedHat\\* Enterprise Linux 7 with\n* GNU Compiler Collection 4.8, 5.4, 6.1, 7.2, 8.1, and 9.1\n* Clang\\* 3.8.1, 7.1, 8.0, and 9.0\n* [Intel oneAPI DPC++/C++ Compiler](https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html) 2023.1\n\non Windows Server\\* 2016 with\n* Microsoft Visual Studio 2019 and 2022\n* [Intel oneAPI DPC++/C++ Compiler](https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html) 2023.1\n\non macOS 11 (Big Sur) with\n* Apple LLVM version 13.0\n* [Intel oneAPI DPC++/C++ Compiler](https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html) 2023.1\n\nGPU engine was validated on Ubuntu\\* 20.04 with\n* GNU Compiler Collection 7.2, 8.1, and 9.1\n* Clang 3.8.1, 7.1, 8.0, and 9.0\n* [Intel oneAPI DPC++/C++ Compiler](https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html) 2023.1\n* [Intel Software for General Purpose GPU capabilities](https://dgpu-docs.intel.com/index.html)\nlatest stable version available at the time of release\n\non Windows Server 2019 with\n* Microsoft Visual Studio 2019 and 2022\n* [Intel oneAPI DPC++/C++ Compiler](https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html) 2023.1\n* [Intel Graphics - Windows 10 DCH Drivers](https://www.intel.com/content/www/us/en/download/19344/intel-graphics-windows-dch-drivers.html)\n* [Intel Arc Graphics Windows DCH Driver](https://www.intel.com/content/www/us/en/download/726609/intel-arc-graphics-windows-dch-driver.html)\nlatest stable version available at the time of release\n", "type": "Text_excerpt", "original_header": "Validated Configurations", "parent_header": ["System Requirements", "Requirements for Building from Source"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/intel/mkl-dnn/main/README.md"}, {"result": {"value": "See the README included in the corresponding binary package.\n", "type": "Text_excerpt", "original_header": "Requirements for Pre-built Binaries", "parent_header": ["System Requirements"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/intel/mkl-dnn/main/README.md"}], "support": [{"result": {"value": "Please submit your questions, feature requests, and bug reports on the\n[GitHub issues](https://github.com/oneapi-src/oneDNN/issues) page.\n\nYou may reach out to project maintainers privately\nat dnnl.maintainers@intel.com.\n", "type": "Text_excerpt", "original_header": "Support"}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/intel/mkl-dnn/main/README.md"}], "invocation": [{"result": {"type": "Text_excerpt", "value": "<img align=\"left\" src=\"https://spec.oneapi.io/oneapi-logo-white-scaled.jpg\" alt=\"oneAPI logo\"> \n", "original_header": "oneAPI Deep Neural Network Library (oneDNN)"}, "confidence": 0.815286729484345, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/intel/mkl-dnn/main/README.md"}], "full_title": [{"result": {"type": "String", "value": "oneAPI Deep Neural Network Library (oneDNN)"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/intel/mkl-dnn/main/README.md"}], "related_documentation": [{"result": {"type": "Url", "value": "https://sphinx-book-theme.readthedocs.io/", "format": "readthedocs"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/intel/mkl-dnn/main/README.md"}], "logo": [{"result": {"type": "Url", "value": "https://spec.oneapi.io/oneapi-logo-white-scaled.jpg"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/intel/mkl-dnn/main/README.md"}]}