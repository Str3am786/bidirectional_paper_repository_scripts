{"somef_provenance": {"somef_version": "0.9.4", "somef_schema_version": "1.0.0", "date": "2023-12-21 19:11:55"}, "code_repository": [{"result": {"value": "https://github.com/facebookresearch/fairseq", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "owner": [{"result": {"value": "facebookresearch", "type": "Organization"}, "confidence": 1, "technique": "GitHub_API"}], "date_created": [{"result": {"value": "2017-08-29T16:26:12Z", "type": "Date"}, "confidence": 1, "technique": "GitHub_API"}], "date_updated": [{"result": {"value": "2023-12-21T14:38:45Z", "type": "Date"}, "confidence": 1, "technique": "GitHub_API"}], "license": [{"result": {"value": "https://api.github.com/licenses/mit", "type": "License", "name": "MIT License", "url": "https://api.github.com/licenses/mit", "spdx_id": "MIT"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "MIT License\n\nCopyright (c) Facebook, Inc. and its affiliates.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n", "type": "File_dump"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/fairseq/main/LICENSE"}, {"result": {"value": "fairseq(-py) is MIT-licensed.\nThe license applies to the pre-trained models as well.\n", "type": "Text_excerpt", "original_header": "License"}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/pytorch/fairseq/main/README.md"}], "description": [{"result": {"value": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python.", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Text_excerpt", "value": "* multi-GPU training on one machine or across multiple machines (data and model parallel)\n* fast generation on both CPU and GPU with multiple search algorithms implemented:\n  + beam search\n  + Diverse Beam Search ([Vijayakumar et al., 2016](https://arxiv.org/abs/1610.02424))\n  + sampling (unconstrained, top-k and top-p/nucleus)\n  + [lexically constrained decoding](examples/constrained_decoding/README.md) (Post & Vilar, 2018)\n* [gradient accumulation](https://fairseq.readthedocs.io/en/latest/getting_started.html#large-mini-batch-training-with-delayed-updates) enables training with large mini-batches even on a single GPU\n* [mixed precision training](https://fairseq.readthedocs.io/en/latest/getting_started.html#training-with-half-precision-floating-point-fp16) (trains faster with less GPU memory on [NVIDIA tensor cores](https://developer.nvidia.com/tensor-cores))\n* [extensible](https://fairseq.readthedocs.io/en/latest/overview.html): easily register new models, criterions, tasks, optimizers and learning rate schedulers\n* [flexible configuration](docs/hydra_integration.md) based on [Hydra](https://github.com/facebookresearch/hydra) allowing a combination of code, command-line and file based configuration\n* [full parameter and optimizer state sharding](examples/fully_sharded_data_parallel/README.md)\n* [offloading parameters to CPU](examples/fully_sharded_data_parallel/README.md) \nSee the PyTorch Hub tutorials for [translation](https://pytorch.org/hub/pytorch_fairseq_translation/)\nand [RoBERTa](https://pytorch.org/hub/pytorch_fairseq_roberta/) for more examples.\n \n", "original_header": "Features:"}, "confidence": 0.916107734594172, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/pytorch/fairseq/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "Fairseq(-py) is a sequence modeling toolkit that allows researchers and\ndevelopers to train custom models for translation, summarization, language\nmodeling and other text generation tasks. \nList of implemented papers \n"}, "confidence": 0.858168413793117, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/pytorch/fairseq/main/README.md"}], "name": [{"result": {"value": "fairseq", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}], "full_name": [{"result": {"value": "facebookresearch/fairseq", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}], "issue_tracker": [{"result": {"value": "https://api.github.com/repos/facebookresearch/fairseq/issues", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "forks_url": [{"result": {"value": "https://api.github.com/repos/facebookresearch/fairseq/forks", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "stargazers_count": [{"result": {"value": 28307, "type": "Number"}, "confidence": 1, "technique": "GitHub_API"}], "keywords": [{"result": {"value": "artificial-intelligence, python, pytorch", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}], "forks_count": [{"result": {"value": 6268, "type": "Number"}, "confidence": 1, "technique": "GitHub_API"}], "download_url": [{"result": {"value": "https://github.com/pytorch/fairseq/releases", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "programming_languages": [{"result": {"value": "Python", "name": "Python", "type": "Programming_language", "size": 4228380}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "Cuda", "name": "Cuda", "type": "Programming_language", "size": 38178}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "C++", "name": "C++", "type": "Programming_language", "size": 21106}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "Cython", "name": "Cython", "type": "Programming_language", "size": 13294}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "Lua", "name": "Lua", "type": "Programming_language", "size": 4210}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "Shell", "name": "Shell", "type": "Programming_language", "size": 2182}, "confidence": 1, "technique": "GitHub_API"}], "releases": [{"result": {"type": "Release", "value": "https://api.github.com/repos/facebookresearch/fairseq/releases/70676490", "tag": "v0.12.2", "name": "v0.12.2", "author": {"name": "github-actions[bot]", "type": "Bot"}, "tarball_url": "https://api.github.com/repos/facebookresearch/fairseq/tarball/v0.12.2", "zipball_url": "https://api.github.com/repos/facebookresearch/fairseq/zipball/v0.12.2", "html_url": "https://github.com/facebookresearch/fairseq/releases/tag/v0.12.2", "url": "https://api.github.com/repos/facebookresearch/fairseq/releases/70676490", "release_id": 70676490, "date_created": "2022-06-27T19:12:01Z", "date_published": "2022-06-27T19:32:58Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/facebookresearch/fairseq/releases/69311025", "tag": "v0.12.1", "name": "v0.12.1", "author": {"name": "github-actions[bot]", "type": "Bot"}, "tarball_url": "https://api.github.com/repos/facebookresearch/fairseq/tarball/v0.12.1", "zipball_url": "https://api.github.com/repos/facebookresearch/fairseq/zipball/v0.12.1", "html_url": "https://github.com/facebookresearch/fairseq/releases/tag/v0.12.1", "url": "https://api.github.com/repos/facebookresearch/fairseq/releases/69311025", "release_id": 69311025, "date_created": "2022-06-13T14:44:14Z", "date_published": "2022-06-13T15:07:55Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/facebookresearch/fairseq/releases/69163668", "tag": "v0.12.0", "name": "v0.12.0", "author": {"name": "github-actions[bot]", "type": "Bot"}, "tarball_url": "https://api.github.com/repos/facebookresearch/fairseq/tarball/v0.12.0", "zipball_url": "https://api.github.com/repos/facebookresearch/fairseq/zipball/v0.12.0", "html_url": "https://github.com/facebookresearch/fairseq/releases/tag/v0.12.0", "url": "https://api.github.com/repos/facebookresearch/fairseq/releases/69163668", "release_id": 69163668, "date_created": "2022-06-10T14:45:26Z", "date_published": "2022-06-10T14:45:28Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/facebookresearch/fairseq/releases/36014635", "tag": "v0.10.2", "name": "v0.10.2", "author": {"name": "myleott", "type": "User"}, "description": "Bug fixes:\r\n- fix register_model_architecture for Transformer language model (#3097)\r\n- fix logging to use stdout instead of stderr (#3052)", "tarball_url": "https://api.github.com/repos/facebookresearch/fairseq/tarball/v0.10.2", "zipball_url": "https://api.github.com/repos/facebookresearch/fairseq/zipball/v0.10.2", "html_url": "https://github.com/facebookresearch/fairseq/releases/tag/v0.10.2", "url": "https://api.github.com/repos/facebookresearch/fairseq/releases/36014635", "release_id": 36014635, "date_created": "2021-01-05T20:02:23Z", "date_published": "2021-01-05T20:26:03Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/facebookresearch/fairseq/releases/34270311", "tag": "v0.10.1", "name": "v0.10.1", "author": {"name": "myleott", "type": "User"}, "description": "This minor release includes fixes for torch.distributed.launch, `--user-dir` and a few smaller bugs. We also include prebuilt wheels for common platforms.", "tarball_url": "https://api.github.com/repos/facebookresearch/fairseq/tarball/v0.10.1", "zipball_url": "https://api.github.com/repos/facebookresearch/fairseq/zipball/v0.10.1", "html_url": "https://github.com/facebookresearch/fairseq/releases/tag/v0.10.1", "url": "https://api.github.com/repos/facebookresearch/fairseq/releases/34270311", "release_id": 34270311, "date_created": "2020-11-21T17:39:09Z", "date_published": "2020-11-21T20:52:27Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/facebookresearch/fairseq/releases/33822685", "tag": "v0.10.0", "name": "v0.10.0", "author": {"name": "myleott", "type": "User"}, "description": "It's been a long time since our last release (0.9.0) nearly a year ago! There have been numerous changes and new features added since then, which we've tried to summarize below. While this release carries the same major version as our previous release (0.x.x), if you have code that relies on 0.9.0, it is likely you'll need to adapt it before updating to 0.10.0.\r\n\r\nLooking forward, this will also be the last significant release with the 0.x.x numbering. The next release will be 1.0.0 and will include a major migration to the [Hydra configuration system](https://github.com/facebookresearch/hydra), with an eye towards modularizing fairseq to be more usable as a library.\r\n\r\n### Changelog:\r\n\r\n#### New papers:\r\n- [Reducing Transformer Depth on Demand with Structured Dropout (Fan et al., 2019)](https://github.com/pytorch/fairseq/tree/master/examples/layerdrop/README.md)\r\n- [MBART: Multilingual Denoising Pre-training for Neural Machine Translation ({Liu*,Gu*,Goyal*} et al., 2020)](https://github.com/pytorch/fairseq/blob/master/examples/mbart/README.md)\r\n- [Neural Machine Translation with Byte-Level Subwords (Wang et al., 2019)](https://github.com/pytorch/fairseq/blob/master/examples/byte_level_bpe/README.md)\r\n- [Training with Quantization Noise for Extreme Model Compression ({Fan*,Stock*} et al., 2019)](https://github.com/pytorch/fairseq/blob/master/examples/quant_noise/README.md)\r\n- [Monotonic Multihead Attention (Ma et al., 2020)](https://github.com/pytorch/fairseq/blob/master/examples/simultaneous_translation/README.md)\r\n- [Unsupervised Quality Estimation for Neural Machine Translation (Fomicheva et al., 2020)](examples/unsupervised_quality_estimation/README.md)\r\n- [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations (Baevski et al., 2020)](https://github.com/pytorch/fairseq/blob/master/examples/wav2vec/README.md)\r\n- [Lexically constrained decoding with dynamic beam allocation](examples/constrained_decoding/README.md)\r\n- [Generating Medical Reports from Patient-Doctor Conversations Using Sequence-to-Sequence Models (Enarvi et al., 2020)](examples/pointer_generator/README.md)\r\n- [Linformer: Self-Attention with Linear Complexity (Wang et al., 2020)](examples/linformer/README.md)\r\n- [Cross-lingual Retrieval for Iterative Self-Supervised Training (Tran et al., 2020)](examples/criss/README.md)\r\n- [Deep Transformers with Latent Depth (Li et al., 2020)](examples/latent_depth/README.md)\r\n- [Better Fine-Tuning by Reducing Representational Collapse (Aghajanyan et al. 2020)](examples/rxf/README.md)\r\n\r\n#### Major new features:\r\n- TorchScript support for Transformer and SequenceGenerator (PyTorch 1.6+ only)\r\n- Model parallel training support (see [Megatron-11b](https://github.com/pytorch/fairseq/tree/master/examples/megatron_11b))\r\n- TPU support via `--tpu` and `--bf16` options (775122950d145382146e9120308432a9faf9a9b8)\r\n- Added [VizSeq (a visual analysis toolkit for evaluating fairseq models)](https://facebookresearch.github.io/vizseq/docs/getting_started/fairseq_example)\r\n- Migrated to Python logging (fb76dac1c4e314db75f9d7a03cb4871c532000cb)\r\n- Added \u201cSlowMo\u201d distributed training backend (0dac0ff3b1d18db4b6bb01eb0ea2822118c9dd13)\r\n- Added Optimizer State Sharding (ZeRO) (5d7ed6ab4f92d20ad10f8f792b8703e260a938ac)\r\n- Added several features to improve speech recognition support in fairseq: CTC criterion, external ASR decoder support (currently only wav2letter decoder) with KenLM and fairseq language model fusion\r\n\r\n#### Minor features:\r\n- Added `--patience` for early stopping\r\n- Added `--shorten-method=[none|truncate|random_crop]` to language modeling (and other) tasks\r\n- Added `--eval-bleu` for computing BLEU scores during training (60fbf64f302a825eee77637a0b7de54fde38fb2c)\r\n- Added support for training huggingface models (e.g. `hf_gpt2`) (2728f9b06d9a3808cc7ebc2afa1401eddef35e35)\r\n- Added FusedLAMB optimizer (`--optimizer=lamb`) (f75411af2690a54a5155871f3cf7ca1f6fa15391)\r\n- Added LSTM-based language model (`lstm_lm`) (9f4256edf60554afbcaadfa114525978c141f2bd)\r\n- Added dummy tasks and models for benchmarking (91f05347906e80e6705c141d4c9eb7398969a709; a541b19d853cf4a5209d3b8f77d5d1261554a1d9)\r\n- Added tutorial and pretrained models for paraphrasing (630701eaa750efda4f7aeb1a6d693eb5e690cab1)\r\n- Support quantization for Transformer (6379573c9e56620b6b4ddeb114b030a0568ce7fe)\r\n- Support multi-GPU validation in fairseq-validate (2f7e3f33235b787de2e34123d25f659e34a21558)\r\n- Support batched inference in hub interface (3b53962cd7a42d08bcc7c07f4f858b55bf9bbdad)\r\n- Support for language model fusion in standard beam search (5379461e613263911050a860b79accdf4d75fd37)\r\n\r\n#### Breaking changes:\r\n- Updated requirements to Python 3.6+ and PyTorch 1.5+\r\n- `--max-sentences` renamed to `--batch-size`\r\n- Main entry point scripts (eval_lm.py, generate.py, etc.) removed from root directory into `fairseq_cli`\r\n- Changed format for generation output; `H-` now corresponds to tokenized system outputs and newly added `D-` lines correspond to detokenized outputs (f353913420b6ef8a31ecc55d2ec0c988178698e0)\r\n- We now log the stats from the log-interval (displayed as `train_inner`) instead of a rolling average over each epoch.\r\n- SequenceGenerator/Scorer does not print alignment by default, re-enable with `--print-alignment`\r\n- Print base 2 scores in generation scripts (660d69fd2bdc4c3468df7eb26b3bbd293c793f94)\r\n- Incremental decoding interface changed to use `FairseqIncrementalState` (4e48c4ae5da48a5f70c969c16793e55e12db3c81; 88185fcc3f32bd24f65875bd841166daa66ed301)\r\n- Refactor namespaces in Criterions to support library usage (introduce `LegacyFairseqCriterion` for BC) (46b773a393c423f653887c382e4d55e69627454d)\r\n- Deprecate `FairseqCriterion::aggregate_logging_outputs` interface, use `FairseqCriterion::reduce_metrics` instead (86793391e38bf88c119699bfb1993cb0a7a33968)\r\n- Moved `fairseq.meters` to `fairseq.logging.meters` and added new metrics aggregation module (`fairseq.logging.metrics`) (1e324a5bbe4b1f68f9dadf3592dab58a54a800a8; f8b795f427a39c19a6b7245be240680617156948)\r\n- Reset mid-epoch stats every log-interval steps (244835d811c2c66b1de2c5e86532bac41b154c1a)\r\n- Ignore duplicate entries in dictionary files (dict.txt) and support manual overwrite with `#fairseq:overwrite` option (dd1298e15fdbfc0c3639906eee9934968d63fc29; 937535dba036dc3759a5334ab5b8110febbe8e6e)\r\n- Use 1-based indexing for epochs everywhere (aa79bb9c37b27e3f84e7a4e182175d3b50a79041)\r\n\r\n#### Minor interface changes:\r\n- Added `FairseqTask::begin_epoch` hook (122fc1db49534a5ca295fcae1b362bbd6308c32f)\r\n- `FairseqTask::build_generator` interface changed (cd2555a429b5f17bc47260ac1aa61068d9a43db8)\r\n- Change `RobertaModel` base class to `FairseqEncoder` (307df5604131dc2b93cc0a08f7c98adbfae9d268)\r\n- Expose `FairseqOptimizer.param_groups` property (8340b2d78f2b40bc365862b24477a0190ad2e2c2)\r\n- Deprecate `--fast-stat-sync` and replace with `FairseqCriterion::logging_outputs_can_be_summed` interface (fe6c2edad0c1f9130847b9a19fbbef169529b500)\r\n- `--raw-text` and `--lazy-load` are fully deprecated; use `--dataset-impl` instead\r\n- Mixture of expert tasks moved to `examples/` (8845dcf5ff43ca4d3e733ade62ceca52f1f1d634)\r\n\r\n#### Performance improvements:\r\n- Use cross entropy from apex for improved memory efficiency (5065077dfc1ec4da5246a6103858641bfe3c39eb)\r\n- Added buffered dataloading (`--data-buffer-size`) (411531734df8c7294e82c68e9d42177382f362ef)", "tarball_url": "https://api.github.com/repos/facebookresearch/fairseq/tarball/v0.10.0", "zipball_url": "https://api.github.com/repos/facebookresearch/fairseq/zipball/v0.10.0", "html_url": "https://github.com/facebookresearch/fairseq/releases/tag/v0.10.0", "url": "https://api.github.com/repos/facebookresearch/fairseq/releases/33822685", "release_id": 33822685, "date_created": "2020-11-12T14:18:37Z", "date_published": "2020-11-12T14:22:05Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/facebookresearch/fairseq/releases/21964561", "tag": "v0.9.0", "name": "v0.9.0", "author": {"name": "myleott", "type": "User"}, "description": "Possibly breaking changes:\r\n- Set global numpy seed (4a7cd58)\r\n- Split `in_proj_weight` into separate k, v, q projections in MultiheadAttention (fdf4c3e)\r\n- TransformerEncoder returns namedtuples instead of dict (27568a7)\r\n\r\nNew features:\r\n- Add `--fast-stat-sync` option (e1ba32a)\r\n- Add `--empty-cache-freq` option (315c463)\r\n- Support criterions with parameters (ba5f829)\r\n\r\nNew papers:\r\n- Simple and Effective Noisy Channel Modeling for Neural Machine Translation (49177c9)\r\n- Levenshtein Transformer (86857a5, ...)\r\n- Cross+Self-Attention for Transformer Models (4ac2c5f)\r\n- Jointly Learning to Align and Translate with Transformer Models (1c66792)\r\n- Reducing Transformer Depth on Demand with Structured Dropout (dabbef4)\r\n- Unsupervised Cross-lingual Representation Learning at Scale (XLM-RoBERTa) (e23e5ea)\r\n- BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (a92bcda)\r\n- CamemBERT: a French BERT (b31849a)\r\n\r\nSpeed improvements:\r\n- Add CUDA kernels for LightConv and DynamicConv (f840564)\r\n- Cythonization of various dataloading components (4fc3953, ...)\r\n- Don't project mask tokens for MLM training (718677e)", "tarball_url": "https://api.github.com/repos/facebookresearch/fairseq/tarball/v0.9.0", "zipball_url": "https://api.github.com/repos/facebookresearch/fairseq/zipball/v0.9.0", "html_url": "https://github.com/facebookresearch/fairseq/releases/tag/v0.9.0", "url": "https://api.github.com/repos/facebookresearch/fairseq/releases/21964561", "release_id": 21964561, "date_created": "2019-12-03T23:19:33Z", "date_published": "2019-12-04T14:31:56Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/facebookresearch/fairseq/releases/19284231", "tag": "v0.8.0", "name": "v0.8.0", "author": {"name": "myleott", "type": "User"}, "description": "Changelog:\r\n- Relicensed under MIT license\r\n- Add RoBERTa\r\n- Add wav2vec\r\n- Add WMT'19 models\r\n- Add initial ASR code\r\n- Changed torch.hub interface (`generate` renamed to `translate`)\r\n- Add `--tokenizer` and `--bpe`\r\n- f812e52: Renamed data.transforms -> data.encoders\r\n- 654affc: New Dataset API (optional)\r\n- `47fd985`: Deprecate old Masked LM components\r\n- `5f78106`: Set mmap as default dataset format and infer format automatically\r\n- Misc fixes for sampling\r\n- Misc fixes to support PyTorch 1.2", "tarball_url": "https://api.github.com/repos/facebookresearch/fairseq/tarball/v0.8.0", "zipball_url": "https://api.github.com/repos/facebookresearch/fairseq/zipball/v0.8.0", "html_url": "https://github.com/facebookresearch/fairseq/releases/tag/v0.8.0", "url": "https://api.github.com/repos/facebookresearch/fairseq/releases/19284231", "release_id": 19284231, "date_created": "2019-08-14T12:02:45Z", "date_published": "2019-08-14T12:16:38Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/facebookresearch/fairseq/releases/18733528", "tag": "v0.7.2", "name": "v0.7.2", "author": {"name": "myleott", "type": "User"}, "description": "No major API changes since the last release. Cutting a new release since we'll be merging significant (possibly breaking) changes to logging, data loading and the masked LM implementation soon.", "tarball_url": "https://api.github.com/repos/facebookresearch/fairseq/tarball/v0.7.2", "zipball_url": "https://api.github.com/repos/facebookresearch/fairseq/zipball/v0.7.2", "html_url": "https://github.com/facebookresearch/fairseq/releases/tag/v0.7.2", "url": "https://api.github.com/repos/facebookresearch/fairseq/releases/18733528", "release_id": 18733528, "date_created": "2019-07-19T13:33:40Z", "date_published": "2019-07-19T13:41:44Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/facebookresearch/fairseq/releases/18123007", "tag": "v0.7.1", "name": "v0.7.1", "author": {"name": "myleott", "type": "User"}, "description": "Changelog:\r\n- 9462a81: Enhanced MMapIndexedDataset: less memory, higher speed\r\n- 392fce8: Add code for wav2vec paper", "tarball_url": "https://api.github.com/repos/facebookresearch/fairseq/tarball/v0.7.1", "zipball_url": "https://api.github.com/repos/facebookresearch/fairseq/zipball/v0.7.1", "html_url": "https://github.com/facebookresearch/fairseq/releases/tag/v0.7.1", "url": "https://api.github.com/repos/facebookresearch/fairseq/releases/18123007", "release_id": 18123007, "date_created": "2019-06-20T13:28:37Z", "date_published": "2019-06-20T15:24:10Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/facebookresearch/fairseq/releases/18108958", "tag": "v0.7.0", "name": "v0.7.0", "author": {"name": "myleott", "type": "User"}, "description": "Notable (possibly breaking) changes:\r\n- d45db80: Remove checkpoint utility functions from utils.py into checkpoint_utils.py\r\n- f2563c2: Move LM definitions into separate files\r\n- dffb167: Updates to model API:\r\n  - `FairseqModel` -> `FairseqEncoderDecoderModel`\r\n  - add `FairseqDecoder.extract_features` and `FairseqDecoder.output_layer`\r\n  - `encoder_out_dict` -> `encoder_out`\r\n  - rm unused `remove_head` functions\r\n- 34726d5: Move `distributed_init` into `DistributedFairseqModel`\r\n- cf17068: Simplify distributed launch by automatically launching multiprocessing on each node for all visible GPUs (allows launching just one job per node instead of one per GPU)\r\n- d45db80: Change default LR scheduler from `reduce_lr_on_plateau` to `fixed`\r\n- 96ac28d: Rename `--sampling-temperature` -> `--temperature`\r\n- fc1a19a: Deprecate dummy batches\r\n- a1c997b: Add memory mapped datasets\r\n- 0add50c: Allow cycling over multiple datasets, where each one becomes an \"epoch\"\r\n\r\nPlus many additional features and bugfixes", "tarball_url": "https://api.github.com/repos/facebookresearch/fairseq/tarball/v0.7.0", "zipball_url": "https://api.github.com/repos/facebookresearch/fairseq/zipball/v0.7.0", "html_url": "https://github.com/facebookresearch/fairseq/releases/tag/v0.7.0", "url": "https://api.github.com/repos/facebookresearch/fairseq/releases/18108958", "release_id": 18108958, "date_created": "2019-06-20T02:08:50Z", "date_published": "2019-06-20T03:03:21Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/facebookresearch/fairseq/releases/16146534", "tag": "v0.6.2", "name": "v0.6.2", "author": {"name": "myleott", "type": "User"}, "description": "Changelog:\r\n- 998ba4f: Add language models from Baevski & Auli (2018)\r\n- 4294c4f: Add mixture of experts code from Shen et al. (2019)\r\n- 0049349: Add example for multilingual training\r\n- 48d9afb: Speed improvements, including fused operators from apex\r\n- 44d27e6: Add Tensorboard support\r\n- d17fa85: Add Adadelta optimizer\r\n- 9e1c880: Add `FairseqEncoderModel`\r\n- b65c579: Add `FairseqTask.inference_step` to modularize generate.py\r\n- 2ad1178: Add back `--curriculum`\r\n- Misc bug fixes and other features", "tarball_url": "https://api.github.com/repos/facebookresearch/fairseq/tarball/v0.6.2", "zipball_url": "https://api.github.com/repos/facebookresearch/fairseq/zipball/v0.6.2", "html_url": "https://github.com/facebookresearch/fairseq/releases/tag/v0.6.2", "url": "https://api.github.com/repos/facebookresearch/fairseq/releases/16146534", "release_id": 16146534, "date_created": "2019-03-15T17:27:01Z", "date_published": "2019-03-15T17:28:36Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/facebookresearch/fairseq/releases/15464922", "tag": "v0.6.1", "name": "v0.6.1", "author": {"name": "myleott", "type": "User"}, "description": "Bumping version number for PyPI release.", "tarball_url": "https://api.github.com/repos/facebookresearch/fairseq/tarball/v0.6.1", "zipball_url": "https://api.github.com/repos/facebookresearch/fairseq/zipball/v0.6.1", "html_url": "https://github.com/facebookresearch/fairseq/releases/tag/v0.6.1", "url": "https://api.github.com/repos/facebookresearch/fairseq/releases/15464922", "release_id": 15464922, "date_created": "2019-02-09T06:03:29Z", "date_published": "2019-02-09T18:37:36Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/facebookresearch/fairseq/releases/13109945", "tag": "v0.6.0", "name": "v0.6.0", "author": {"name": "myleott", "type": "User"}, "description": "Changelog:\r\n- 4908863: Switch to DistributedDataParallelC10d and bump version 0.5.0 -> 0.6.0\r\n  - no more FP16Trainer, we just have an FP16Optimizer wrapper\r\n  - most of the distributed code is moved to a new wrapper class called DistributedFairseqModel, which behaves like DistributedDataParallel and a FairseqModel at the same time\r\n  - Trainer now requires an extra dummy_batch argument at initialization, which we do fwd/bwd on when there's an uneven number of batches per worker. We hide the gradients from these dummy batches by multiplying the loss by 0\r\n  - Trainer.train_step now takes a list of samples, which will allow cleaner --update-freq\r\n- 1c56b58: parallelize preprocessing\r\n- Misc bug fixes and features", "tarball_url": "https://api.github.com/repos/facebookresearch/fairseq/tarball/v0.6.0", "zipball_url": "https://api.github.com/repos/facebookresearch/fairseq/zipball/v0.6.0", "html_url": "https://github.com/facebookresearch/fairseq/releases/tag/v0.6.0", "url": "https://api.github.com/repos/facebookresearch/fairseq/releases/13109945", "release_id": 13109945, "date_created": "2018-09-25T21:36:43Z", "date_published": "2018-09-26T17:16:09Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/facebookresearch/fairseq/releases/11508039", "tag": "v0.5.0", "author": {"name": "myleott", "type": "User"}, "tarball_url": "https://api.github.com/repos/facebookresearch/fairseq/tarball/v0.5.0", "zipball_url": "https://api.github.com/repos/facebookresearch/fairseq/zipball/v0.5.0", "html_url": "https://github.com/facebookresearch/fairseq/releases/tag/v0.5.0", "url": "https://api.github.com/repos/facebookresearch/fairseq/releases/11508039", "release_id": 11508039, "date_created": "2018-06-15T19:21:18Z", "date_published": "2018-06-15T20:38:58Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/facebookresearch/fairseq/releases/11506688", "tag": "v0.4.0", "author": {"name": "myleott", "type": "User"}, "tarball_url": "https://api.github.com/repos/facebookresearch/fairseq/tarball/v0.4.0", "zipball_url": "https://api.github.com/repos/facebookresearch/fairseq/zipball/v0.4.0", "html_url": "https://github.com/facebookresearch/fairseq/releases/tag/v0.4.0", "url": "https://api.github.com/repos/facebookresearch/fairseq/releases/11506688", "release_id": 11506688, "date_created": "2018-05-24T17:38:12Z", "date_published": "2018-06-15T19:07:22Z"}, "confidence": 1, "technique": "GitHub_API"}], "code_of_conduct": [{"result": {"value": "# Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to make participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\n  advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\n  address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies within all project spaces, and it also applies when\nan individual is representing the project or its community in public spaces.\nExamples of representing a project or community include using an official\nproject e-mail address, posting via an official social media account, or acting\nas an appointed representative at an online or offline event. Representation of\na project may be further defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at <conduct@pytorch.org>. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq\n\n", "type": "File_dump"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/fairseq/main/CODE_OF_CONDUCT.md"}], "readme_url": [{"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/README.md", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}], "contributing_guidelines": [{"result": {"value": "# Contributing to Facebook AI Research Sequence-to-Sequence Toolkit (fairseq)\nWe want to make contributing to this project as easy and transparent as\npossible.\n\n## Pull Requests\nWe actively welcome your pull requests.\n\n1. Fork the repo and create your branch from `main`.\n2. If you've added code that should be tested, add tests.\n3. If you've changed APIs, update the documentation.\n4. Ensure the test suite passes.\n5. Make sure your code lints.\n6. If you haven't already, complete the Contributor License Agreement (\"CLA\").\n\n## Contributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\n\nComplete your CLA here: <https://code.facebook.com/cla>\n\n## Issues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\n\n## License\nBy contributing to Facebook AI Research Sequence-to-Sequence Toolkit (fairseq),\nyou agree that your contributions will be licensed under the LICENSE file in\nthe root directory of this source tree.\n\n## Pre-commit hooks\nIn order to ensure your code lints, there are pre-commit hooks configured in the repository which you can install.\nAfter installation, they will automatically run each time you commit.\nAn abbreviated guide is given below; for more information, refer to [the offical pre-commit documentation](https://pre-commit.com/).\n\n### Installation\n```\npip install pre-commit\npre-commit install\n```\n\n### Usage\nJust commit your changes:\n```\ngit commit -m \"My informative commit message\"\n```\n\nIf there was a failure, you will get feedback\n```\n[INFO] Initializing environment for https://github.com/PyCQA/flake8.\n[INFO] Installing environment for https://github.com/pre-commit/pre-commit-hooks.\n[INFO] Once installed this environment will be reused.\n[INFO] This may take a few minutes...\n[INFO] Installing environment for https://github.com/PyCQA/flake8.\n[INFO] Once installed this environment will be reused.\n[INFO] This may take a few minutes...\nTrim Trailing Whitespace.................................................Failed\n- hook id: trailing-whitespace\n- exit code: 1\n- files were modified by this hook\nFixing examples/nllb/modeling/wmt15_benchmark/eval_langs2.sh\nFix End of Files.........................................................Failed\n- hook id: end-of-file-fixer\n- exit code: 1\n- files were modified by this hook\nFixing examples/few_shot/scripts/schedule_jobs_few_shot.py\nflake8...................................................................Passed\n```\n\nCertain hooks modify your files to comply.\nTo include these modifications, you will need to add them (i.e. `git add ...`) and commit again.\n\nIf all is well, you should see something like:\n```\nTrim Trailing Whitespace.................................................Passed\nFix End of Files.........................................................Passed\nflake8...................................................................Passed\n[gshard-fix-ci 8698644e1] Fix lint, add pre-commit hooks\n 10 files changed, 148 insertions(+), 110 deletions(-)\n create mode 100644 .flake8\n create mode 100644 .pre-commit-config.yaml\n rename examples/nllb/modeling/wmt15_benchmark/{eval_langs2.py => eval_langs2.sh} (99%)\n ```\n", "type": "File_dump"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/fairseq/main/CONTRIBUTING.md"}], "documentation": [{"result": {"value": "https://github.com/pytorch/fairseq/tree/main/docs", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://github.com/pytorch/fairseq/tree/main/examples/speech_to_speech/docs", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://github.com/pytorch/fairseq/tree/main/examples/speech_to_text/docs", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://github.com/pytorch/fairseq/tree/main/examples/simultaneous_translation/docs", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://github.com/pytorch/fairseq/tree/main/examples/speech_text_joint_to_text/docs", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://github.com/pytorch/fairseq/tree/main/examples/speech_synthesis/docs", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"type": "Url", "value": "https://fairseq.readthedocs.io/", "format": "readthedocs"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/pytorch/fairseq/main/README.md"}], "has_script_file": [{"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/backtranslation/prepare-de-monolingual.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/backtranslation/sacrebleu.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/backtranslation/tokenized_bleu.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/backtranslation/prepare-wmt18en2de.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/m2m_100/tok.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/m2m_100/install_dependecies.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/m2m_100/tokenizers/tokenizer_ar.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/m2m_100/tokenizers/seg_ja.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/m2m_100/tokenizers/seg_ko.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/wmt21/eval.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/translation/prepare-wmt14en2de.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/translation/prepare-wmt14en2fr.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/translation/prepare-iwslt14.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/translation/prepare-iwslt17-multilingual.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/hubert/tests/test_finetuned_asr.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/hubert/tests/test_feature_and_unit.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/textless_nlp/pgslm/scripts/prepare_data.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/textless_nlp/pgslm/scripts/prepare_f0_quantization.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/data2vec/scripts/multi/finetune_all_fair_aws_local_lr.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/data2vec/scripts/multi/finetune_all_fair_local_lr.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/data2vec/scripts/multi/finetune_all_fair_aws_local_lr_nodep.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/data2vec/scripts/text/finetune_all_fair_nodep_aws.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/data2vec/scripts/text/finetune_all_fair_aws.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/data2vec/scripts/text/finetune_sst2_qnli_sweep_fair_nodep.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/data2vec/scripts/text/finetune_all_fair_aws_local_lr.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/data2vec/scripts/text/finetune_all_fair_local_lr.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/data2vec/scripts/text/finetune_all_fair_nodep_aws_local_lr.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/data2vec/scripts/text/finetune_all_fair_nodep.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/data2vec/scripts/text/finetune_all_large_fair_local_lr.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/data2vec/scripts/text/finetune_all_fair.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/data2vec/scripts/text/finetune_all_large_fair_aws_local_lr.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/data2vec/scripts/text/finetune_all_large_fair_nodep_aws_local_lr.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/data2vec/scripts/text/finetune_all_char_fair_aws_local_lr.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/data2vec/scripts/text/finetune_all_fair_nodep_aws_lr.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/data2vec/scripts/text/finetune_all_fair_nodep_aws_lr_nopos.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/speech_recognition/datasets/prepare-librispeech.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/mms/asr/infer/example_infer_adapter.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/audio_nlp/nlu/create_dict_stop.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/multilingual/train_multilingual_model.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/multilingual/finetune_multilingual_model.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/multilingual/multilingual_fairseq_gen.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/multilingual/data_scripts/download_flores_data.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/multilingual/data_scripts/download_wat19_my.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/multilingual/data_scripts/download_ML50_v1.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/multilingual/data_scripts/download_af_xh.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/multilingual/data_scripts/preprocess_ML50_v1.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/multilingual/data_scripts/download_iwslt_and_extract.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/multilingual/data_scripts/download_iitb.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/multilingual/data_scripts/download_lotus.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/multilingual/data_scripts/download_wmt20.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/multilingual/data_scripts/utils/strip_sgm.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/joint_alignment_translation/prepare-wmt18en2de_no_norm_no_escape_no_agressive.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/MMPT/scripts/video_feature_extractor/how2/s3d.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/normformer/train_lm.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/language_model/prepare-wikitext-103.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/wav2vec/unsupervised/kaldi_self_train/st/decode_phone.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/wav2vec/unsupervised/kaldi_self_train/st/train.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/wav2vec/unsupervised/kaldi_self_train/st/decode_word_step2.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/wav2vec/unsupervised/kaldi_self_train/st/path.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/wav2vec/unsupervised/kaldi_self_train/st/cmd.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/wav2vec/unsupervised/kaldi_self_train/st/decode_word_step1.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/wav2vec/unsupervised/kaldi_self_train/st/local/unsup_select_decode.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/wav2vec/unsupervised/kaldi_self_train/st/local/decode.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/wav2vec/unsupervised/kaldi_self_train/st/local/prepare_lang.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/wav2vec/unsupervised/kaldi_self_train/st/local/score.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/wav2vec/unsupervised/kaldi_self_train/st/local/train_subset_lgbeam.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/wav2vec/unsupervised/kaldi_self_train/st/local/unsup_select_decode_word.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/wav2vec/unsupervised/kaldi_self_train/st/local/prepare_lang_word.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/wav2vec/unsupervised/kaldi_self_train/st/local/prepare_lm.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/wav2vec/unsupervised/kaldi_self_train/st/local/show_wer.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/wav2vec/unsupervised/kaldi_self_train/st/steps_gan/train_deltas.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/wav2vec/unsupervised/kaldi_self_train/st/steps_gan/train_lda_mllt.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/wav2vec/unsupervised/kaldi_self_train/st/steps_gan/train_sat.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/wav2vec/unsupervised/scripts/prepare_audio.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/wav2vec/unsupervised/scripts/prepare_timit.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/wav2vec/unsupervised/scripts/prepare_audio_v2.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/wav2vec/unsupervised/scripts/prepare_text.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/wav2vec/scripts/binarize_manifest.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/roberta/preprocess_RACE.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/roberta/preprocess_GLUE_tasks.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/roberta/commonsense_qa/download_cqa_data.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/criss/download_and_preprocess_flores_test.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/criss/download_and_preprocess_tatoeba.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/criss/mining/mine_example.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/criss/sentence_retrieval/sentence_retrieval_tatoeba.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/criss/unsupervised_mt/eval.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/byte_level_bpe/get_data.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/scripts/compound_split_bleu.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/scripts/sacrebleu.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/scripts/test_fsdp.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}], "executable_example": [{"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/mms/lid/tutorial/MMS_LID_Inference_Colab.ipynb", "type": "Url", "format": "jupyter_notebook"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/mms/lid/tutorial/MMS_LID_Inference_Colab.ipynb"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/mms/asr/tutorial/MMS_ASR_Inference_Colab.ipynb", "type": "Url", "format": "jupyter_notebook"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/mms/asr/tutorial/MMS_ASR_Inference_Colab.ipynb"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/mms/tts/tutorial/MMS_TTS_Inference_Colab.ipynb", "type": "Url", "format": "jupyter_notebook"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/fairseq/main/examples/mms/tts/tutorial/MMS_TTS_Inference_Colab.ipynb"}], "requirements": [{"result": {"value": "* [PyTorch](http://pytorch.org/) version >= 1.10.0\n* Python version >= 3.8\n* For training new models, you'll also need an NVIDIA GPU and [NCCL](https://github.com/NVIDIA/nccl)\n* **To install fairseq** and develop locally:\n\n``` bash\ngit clone https://github.com/pytorch/fairseq\ncd fairseq\npip install --editable ./\n\n# on MacOS:\n# CFLAGS=\"-stdlib=libc++\" pip install --editable ./\n\n# to install the latest stable release (0.10.x)\n# pip install fairseq\n```\n\n* **For faster training** install NVIDIA's [apex](https://github.com/NVIDIA/apex) library:\n\n``` bash\ngit clone https://github.com/NVIDIA/apex\ncd apex\npip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" \\\n  --global-option=\"--deprecated_fused_adam\" --global-option=\"--xentropy\" \\\n  --global-option=\"--fast_multihead_attn\" ./\n```\n\n* **For large datasets** install [PyArrow](https://arrow.apache.org/docs/python/install.html#using-pip): `pip install pyarrow`\n* If you use Docker make sure to increase the shared memory size either with `--ipc=host` or `--shm-size`\n as command line options to `nvidia-docker run` .\n", "type": "Text_excerpt", "original_header": "Requirements and Installation"}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/pytorch/fairseq/main/README.md"}], "installation": [{"result": {"value": "* [PyTorch](http://pytorch.org/) version >= 1.10.0\n* Python version >= 3.8\n* For training new models, you'll also need an NVIDIA GPU and [NCCL](https://github.com/NVIDIA/nccl)\n* **To install fairseq** and develop locally:\n\n``` bash\ngit clone https://github.com/pytorch/fairseq\ncd fairseq\npip install --editable ./\n\n# on MacOS:\n# CFLAGS=\"-stdlib=libc++\" pip install --editable ./\n\n# to install the latest stable release (0.10.x)\n# pip install fairseq\n```\n\n* **For faster training** install NVIDIA's [apex](https://github.com/NVIDIA/apex) library:\n\n``` bash\ngit clone https://github.com/NVIDIA/apex\ncd apex\npip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" \\\n  --global-option=\"--deprecated_fused_adam\" --global-option=\"--xentropy\" \\\n  --global-option=\"--fast_multihead_attn\" ./\n```\n\n* **For large datasets** install [PyArrow](https://arrow.apache.org/docs/python/install.html#using-pip): `pip install pyarrow`\n* If you use Docker make sure to increase the shared memory size either with `--ipc=host` or `--shm-size`\n as command line options to `nvidia-docker run` .\n", "type": "Text_excerpt", "original_header": "Requirements and Installation"}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/pytorch/fairseq/main/README.md"}], "usage": [{"result": {"value": "The [full documentation](https://fairseq.readthedocs.io/) contains instructions\nfor getting started, training new models and extending fairseq with new model\ntypes and tasks.\n", "type": "Text_excerpt", "original_header": "Getting Started"}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/pytorch/fairseq/main/README.md"}, {"result": {"value": "We provide pre-trained models and pre-processed, binarized test sets for several tasks listed below,\nas well as example training and evaluation commands.\n\n* [Translation](examples/translation/README.md): convolutional and transformer models are available\n* [Language Modeling](examples/language_model/README.md): convolutional and transformer models are available\n\nWe also have more detailed READMEs to reproduce results from specific papers:\n\n* [XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale (Babu et al., 2021)](examples/wav2vec/xlsr/README.md)\n* [Cross-lingual Retrieval for Iterative Self-Supervised Training (Tran et al., 2020)](examples/criss/README.md)\n* [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations (Baevski et al., 2020)](examples/wav2vec/README.md)\n* [Unsupervised Quality Estimation for Neural Machine Translation (Fomicheva et al., 2020)](examples/unsupervised_quality_estimation/README.md)\n* [Training with Quantization Noise for Extreme Model Compression ({Fan*, Stock*} et al., 2020)](examples/quant_noise/README.md)\n* [Neural Machine Translation with Byte-Level Subwords (Wang et al., 2020)](examples/byte_level_bpe/README.md)\n* [Multilingual Denoising Pre-training for Neural Machine Translation (Liu et at., 2020)](examples/mbart/README.md)\n* [Reducing Transformer Depth on Demand with Structured Dropout (Fan et al., 2019)](examples/layerdrop/README.md)\n* [Jointly Learning to Align and Translate with Transformer Models (Garg et al., 2019)](examples/joint_alignment_translation/README.md)\n* [Levenshtein Transformer (Gu et al., 2019)](examples/nonautoregressive_translation/README.md)\n* [Facebook FAIR's WMT19 News Translation Task Submission (Ng et al., 2019)](examples/wmt19/README.md)\n* [RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al., 2019)](examples/roberta/README.md)\n* [wav2vec: Unsupervised Pre-training for Speech Recognition (Schneider et al., 2019)](examples/wav2vec/README.md)\n* [Mixture Models for Diverse Machine Translation: Tricks of the Trade (Shen et al., 2019)](examples/translation_moe/README.md)\n* [Pay Less Attention with Lightweight and Dynamic Convolutions (Wu et al., 2019)](examples/pay_less_attention_paper/README.md)\n* [Understanding Back-Translation at Scale (Edunov et al., 2018)](examples/backtranslation/README.md)\n* [Classical Structured Prediction Losses for Sequence to Sequence Learning (Edunov et al., 2018)](https://github.com/pytorch/fairseq/tree/classic_seqlevel)\n* [Hierarchical Neural Story Generation (Fan et al., 2018)](examples/stories/README.md)\n* [Scaling Neural Machine Translation (Ott et al., 2018)](examples/scaling_nmt/README.md)\n* [Convolutional Sequence to Sequence Learning (Gehring et al., 2017)](examples/conv_seq2seq/README.md)\n* [Language Modeling with Gated Convolutional Networks (Dauphin et al., 2017)](examples/language_model/README.conv.md)\n", "type": "Text_excerpt", "original_header": "Pre-trained models and examples"}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/pytorch/fairseq/main/README.md"}], "citation": [{"result": {"value": "Please cite as:\n\n``` bibtex\n@inproceedings{ott2019fairseq,\n  title = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},\n  author = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},\n  booktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},\n  year = {2019},\n}\n```\n", "type": "Text_excerpt", "original_header": "Citation"}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/pytorch/fairseq/main/README.md"}, {"result": {"value": "@inproceedings{ott2019fairseq,\n  title = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},\n  author = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},\n  booktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},\n  year = {2019},\n}", "type": "Text_excerpt", "format": "bibtex"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/pytorch/fairseq/main/README.md"}], "application_domain": [{"result": {"type": "String", "value": "Natural Language Processing"}, "confidence": 0.9193779755350222, "technique": "supervised_classification"}], "full_title": [{"result": {"type": "String", "value": ""}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/pytorch/fairseq/main/README.md"}], "logo": [{"result": {"type": "Url", "value": "https://raw.githubusercontent.com/pytorch/fairseq/main/docs/fairseq_logo.png"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/pytorch/fairseq/main/README.md"}], "image": [{"result": {"type": "Url", "value": "https://circleci.com/gh/facebookresearch/fairseq.svg?style=shield"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/pytorch/fairseq/main/README.md"}], "related_papers": [{"result": {"type": "Url", "value": "https://arxiv.org/abs/2104.01027"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/pytorch/fairseq/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2006.13979"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/pytorch/fairseq/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1610.02424"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/pytorch/fairseq/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2010.11430"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/pytorch/fairseq/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2105.11084"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/pytorch/fairseq/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2109.11680"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/pytorch/fairseq/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/pdf/2109.14084.pdf"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/pytorch/fairseq/main/README.md"}]}