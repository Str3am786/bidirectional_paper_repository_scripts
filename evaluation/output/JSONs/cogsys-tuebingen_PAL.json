{"somef_provenance": {"somef_version": "0.9.4", "somef_schema_version": "1.0.0", "date": "2023-12-21 18:55:35"}, "code_repository": [{"result": {"value": "https://github.com/cogsys-tuebingen/PAL", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "owner": [{"result": {"value": "cogsys-tuebingen", "type": "Organization"}, "confidence": 1, "technique": "GitHub_API"}], "date_created": [{"result": {"value": "2019-03-26T15:04:32Z", "type": "Date"}, "confidence": 1, "technique": "GitHub_API"}], "date_updated": [{"result": {"value": "2023-12-20T03:39:53Z", "type": "Date"}, "confidence": 1, "technique": "GitHub_API"}], "description": [{"result": {"value": "Reference implementation of the PAL optimizer", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "<img src=\"/Images/explanation.png \" title=\"PAL' basic idea\" alt=\"PAL' basic idea\" width=\"400\"/> \n<!-- d align=\"right\"> Fig1: PAL' basic idea </a> -->\n\n***Fig1: PAL's basic idea*** \n\nPAL is based on the empirical observarion that the loss function can be approximated by a one-dimensional parabola in negative gradient/line direction.\nTo do this, one additional point has to be meashured on the line.  \nPAL performs a variable update step by jumping into the minimum of the approximated parabola.   \nPAL surpasses SLS, ALIG, SGD-HD and COCOB and competes against ADAM, SLS, SGD and RMSProp on ResNet-32, MobilenetV2, DenseNet-40 and EfficientNet architectures trained on CIFAR-10 and CIFAR-100.  \nHowever, the latter are tuned by piecewise constant step sizes, whereas PAL does derive its own learning rate schedule.  \nPAL surpasses all those optimizers when they are trained without a schedule.  \nTherefore we PAL could be used in scenarios where default schedules fail.  \nFor a detailed explanation, please refer to our paper.: https://arxiv.org/abs/1903.11991\n\n<p float=\"left\">\n<img src=\"/Images/MobileNetV2_CIFAR-100_train_loss.png\" title=\"Exemplary performance of PAL with data augmentations\" alt=\"Exemplary Performance of PAL with data augmentation\" width=\"380\" />\n<img src=\"/Images/MobileNetV2_CIFAR-100_eval_acc.png\" title=\"Exemplary performance of PAL with data augmentations\" alt=\"Exemplary Performance of PAL with data augmentation\" width=\"380\" />\n<img src=\"/Images/MobileNetV2_CIFAR-100_step_sizes.png\" title=\"Exemplary performance of PAL with data augmentations\" alt=\"Exemplary Performance of PAL with data augmentation\" width=\"380\" />\n</p>\n\n***Fig2: Exemplary performance of PAL with data augmentation***\n\n<img src=\"/Images/ResNetCifarMin30.png\" title=\"Exemplary performance of PAL without data augmentation\" alt=\"Exemplary Performance of PAL of PAL without data augmentation\" width=\"420\" />\n\n***Fig3: Exemplary performance of PAL without data augmentation, however this leads to severe overfitting***\n", "type": "Text_excerpt", "original_header": "Short introduction to PAL:", "parent_header": ["PAL - Parabolic Approximation Line Search for DNNs:"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/cogsys-tuebingen/PAL/master/README.md"}, {"result": {"type": "Text_excerpt", "value": "This repository provides Tensorflow and Pytorch reference implementations for PAL.\nPAL is an An efficient and effective line search approach for DNNs which exploits the almost parabolic \nshape of the loss in negative gradient direction to automatically estimate good step sizes. \n", "original_header": "PAL - Parabolic Approximation Line Search for DNNs:"}, "confidence": 0.9852752888318683, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/cogsys-tuebingen/PAL/master/README.md"}, {"result": {"type": "Text_excerpt", "value": "For a detailed explanation, please refer to our paper.\nThe introduced hyperparameters lead to good training and test errors:   \n Usually only the measuring step size has to be adapted slightly.\nIts sensitivity is not as high as the one of of the learning rate of SGD.   \n", "original_header": "The hyperparameters:"}, "confidence": 0.9884291052073464, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/cogsys-tuebingen/PAL/master/README.md"}, {"result": {"type": "Text_excerpt", "value": "- No limitations. Can be used in the same way as any other PyTorch optimizer.\n- Runs with PyTorch 1.4\n- Uses tensorboardX for plotting\n- Parabola approximations and loss lines can be plotted\n \n", "original_header": "PyTorch Implementation:"}, "confidence": 0.8007138283077868, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/cogsys-tuebingen/PAL/master/README.md"}, {"result": {"type": "Text_excerpt", "value": "- limitations:\n    - The DNN must not contain any random components such as Dropout or ShakeDrop. This is because PALS requires two loss values of the same deterministic function (= two network inferences) to determine an update step. Otherwise the function would not be continuous and a parabolic approximation is not be possible. However, if these random component implementations could be changed so that drawn random numbers can be reused for at least two inferences, PAL would also support these operations. \n    - If using Dropout this has to be replaced with the adapted implementation we provide which works with PAL.\n    - With Tensorflow 1.15 and 2.0 is was not possible for us to write a completely graph-based optimizer. Therefore it has to be used slightly different as other optimizers. Have a look into the example code! This is not the case with Pytorch.\n    - The Tensorflow implementation does not support Keras and Estimator API.\n- Runs with Tensorflow 1.15\n- Uses tensorboard for plotting\n- Parabola approximations and loss lines can be plotted\n \n", "original_header": "Tensorflow Implementation:"}, "confidence": 0.9740175617363199, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/cogsys-tuebingen/PAL/master/README.md"}], "name": [{"result": {"value": "PAL", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}], "full_name": [{"result": {"value": "cogsys-tuebingen/PAL", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}], "issue_tracker": [{"result": {"value": "https://api.github.com/repos/cogsys-tuebingen/PAL/issues", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "forks_url": [{"result": {"value": "https://api.github.com/repos/cogsys-tuebingen/PAL/forks", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "stargazers_count": [{"result": {"value": 20, "type": "Number"}, "confidence": 1, "technique": "GitHub_API"}], "keywords": [{"result": {"value": "", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}], "forks_count": [{"result": {"value": 3, "type": "Number"}, "confidence": 1, "technique": "GitHub_API"}], "download_url": [{"result": {"value": "https://github.com/cogsys-tuebingen/PAL/releases", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "programming_languages": [{"result": {"value": "Python", "name": "Python", "type": "Programming_language", "size": 529959}, "confidence": 1, "technique": "GitHub_API"}], "readme_url": [{"result": {"value": "https://raw.githubusercontent.com/cogsys-tuebingen/PAL/master/README.md", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}], "installation": [{"result": {"type": "Text_excerpt", "value": "A virtual environment capable of executing the provided code can be created with the provided python_virtual_env_requirements.txt \n", "original_header": "Virtual Environment"}, "confidence": 0.819824113725427, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/cogsys-tuebingen/PAL/master/README.md"}], "full_title": [{"result": {"type": "String", "value": "PAL - Parabolic Approximation Line Search for DNNs:"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/cogsys-tuebingen/PAL/master/README.md"}], "image": [{"result": {"type": "Url", "value": "https://raw.githubusercontent.com/cogsys-tuebingen/PAL/master//Images/explanation.png "}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/cogsys-tuebingen/PAL/master/README.md"}, {"result": {"type": "Url", "value": "https://raw.githubusercontent.com/cogsys-tuebingen/PAL/master//Images/MobileNetV2_CIFAR-100_train_loss.png"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/cogsys-tuebingen/PAL/master/README.md"}, {"result": {"type": "Url", "value": "https://raw.githubusercontent.com/cogsys-tuebingen/PAL/master//Images/MobileNetV2_CIFAR-100_eval_acc.png"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/cogsys-tuebingen/PAL/master/README.md"}, {"result": {"type": "Url", "value": "https://raw.githubusercontent.com/cogsys-tuebingen/PAL/master//Images/MobileNetV2_CIFAR-100_step_sizes.png"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/cogsys-tuebingen/PAL/master/README.md"}, {"result": {"type": "Url", "value": "https://raw.githubusercontent.com/cogsys-tuebingen/PAL/master//Images/ResNetCifarMin30.png"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/cogsys-tuebingen/PAL/master/README.md"}], "related_papers": [{"result": {"type": "Url", "value": "https://arxiv.org/abs/1903.11991\n\n<p float=\"left\">\n<img src=\"/Images/MobileNetV2_CIFAR-100_train_loss.png\" title=\"Exemplary performance of PAL with data augmentations\" alt=\"Exemplary Performance of PAL with data augmentation\" width=\"380\" />\n<img src=\"/Images/MobileNetV2_CIFAR-100_eval_acc.png\" title=\"Exemplary performance of PAL with data augmentations\" alt=\"Exemplary Performance of PAL with data augmentation\" width=\"380\" />\n<img src=\"/Images/MobileNetV2_CIFAR-100_step_sizes.png\" title=\"Exemplary performance of PAL with data augmentations\" alt=\"Exemplary Performance of PAL with data augmentation\" width=\"380\" />\n</p>\n\n***Fig2: Exemplary performance of PAL with data augmentation***\n\n<img src=\"/Images/ResNetCifarMin30.png\" title=\"Exemplary performance of PAL without data augmentation\" alt=\"Exemplary Performance of PAL of PAL without data augmentation\" width=\"420\" />\n\n***Fig3: Exemplary performance of PAL without data augmentation, however this leads to severe overfitting***\n\n## The hyperparameters:\n\nFor a detailed explanation, please refer to our paper.\nThe introduced hyperparameters lead to good training and test errors:   \n Usually only the measuring step size has to be adapted slightly.\nIts sensitivity is not as high as the one of of the learning rate of SGD.  \n\n\n <table style=\"width:100%\">\n    <tr>\n    <th>Abbreviation  </th>\n    <th>Name</th>\n    <th>Description   </th>\n    <th>Default parameter intervalls   </th>\n    <th>Sensitivity compared to SGD leaning rate</th>\n  </tr>\n  <tr>\n    <td>&mu; </th>\n    <td>measuring step size</th>\n    <td>distance to the second sampled training loss value   </th>\n    <td>[0.1,1]   </th>\n    <td> medium </th>\n  </tr>\n  <tr>\n    <td> &alpha; </td>\n    <td>update step adaptation </td>\n    <td>Multiplier to the update step </td>\n    <td>[1.0,1.2,1.7]   </td>\n    <td> low </th>\n  </tr>\n  <tr>\n    <td>&beta;</td>\n    <td>direciton adaptation factor </td>\n    <td>Adapts the line direction depending on previous line directions </td>\n    <td>[0.0.4] </td>\n    <td> low </th>\n  </tr>\n    <tr>\n    <td>s<sub>max</sub> </td>\n    <td>maximum step size  </td>\n    <td>maximum step size  on line.</td>\n     <td>[3.6] </td>\n     <td> low </th>\n  </tr>\n</table> \n\n## PyTorch Implementation:\n- No limitations. Can be used in the same way as any other PyTorch optimizer.\n- Runs with PyTorch 1.4\n- Uses tensorboardX for plotting\n- Parabola approximations and loss lines can be plotted\n\n## Tensorflow Implementation:\n- limitations:\n    - The DNN must not contain any random components such as Dropout or ShakeDrop. This is because PALS requires two loss values of the same deterministic function (= two network inferences"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/cogsys-tuebingen/PAL/master/README.md"}]}