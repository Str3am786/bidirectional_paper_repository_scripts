{"somef_provenance": {"somef_version": "0.9.4", "somef_schema_version": "1.0.0", "date": "2023-12-21 18:48:09"}, "code_repository": [{"result": {"value": "https://github.com/awslabs/sockeye", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "owner": [{"result": {"value": "awslabs", "type": "Organization"}, "confidence": 1, "technique": "GitHub_API"}], "date_created": [{"result": {"value": "2017-06-08T07:44:30Z", "type": "Date"}, "confidence": 1, "technique": "GitHub_API"}], "date_updated": [{"result": {"value": "2023-12-20T03:23:55Z", "type": "Date"}, "confidence": 1, "technique": "GitHub_API"}], "license": [{"result": {"value": "https://api.github.com/licenses/apache-2.0", "type": "License", "name": "Apache License 2.0", "url": "https://api.github.com/licenses/apache-2.0", "spdx_id": "Apache-2.0"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"{}\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright {yyyy} {name of copyright owner}\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n", "type": "File_dump"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/LICENSE"}], "description": [{"result": {"value": "Sequence-to-sequence framework with a focus on Neural Machine Translation based on PyTorch", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Text_excerpt", "value": "Sockeye is an open-source sequence-to-sequence framework for Neural Machine Translation built on [PyTorch](https://pytorch.org/). It implements distributed training and optimized inference for state-of-the-art models, powering [Amazon Translate](https://aws.amazon.com/translate/) and other MT applications. Recent developments and changes are tracked in our [CHANGELOG](https://github.com/awslabs/sockeye/blob/master/CHANGELOG.md). \nFor a quickstart guide to training a standard NMT model on any size of data, see the [WMT 2014 English-German tutorial](docs/tutorials/wmt_large.md). \nFor questions and issue reports, please [file an issue](https://github.com/awslabs/sockeye/issues/new) on GitHub.\n \n", "original_header": "Sockeye"}, "confidence": 0.9409444995783244, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "With version 3.1.x, we remove support for MXNet 2.x. Models trained with PyTorch and Sockeye 3.0.x remain compatible\nwith Sockeye 3.1.x. Models trained with 2.3.x (using MXNet) and converted to PyTorch with Sockeye 3.0.x's conversion\ntool can NOT be used with Sockeye 3.1.x.\n \n", "original_header": "Version 3.1.x: PyTorch only"}, "confidence": 0.9411219155514703, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "* Currey, Anna, Maria N\u0103dejde, Raghavendra Pappagari, Mia Mayer, Stanislas Lauly, Xing Niu, Benjamin Hsu, Georgiana Dinu. \"MT-GenEval: A Counterfactual and Contextual Dataset for Evaluating Gender Accuracy in Machine Translation\". Proceedings of EMNLP (2022).\n* Domhan, Tobias, Eva Hasler, Ke Tran, Sony Trenous, Bill Byrne and Felix Hieber. \"The Devil is in the Details: On the Pitfalls of Vocabulary Selection in Neural Machine Translation\". Proceedings of NAACL-HLT (2022)\n* Fischer, Lukas, Patricia Scheurer, Raphael Schwitter, Martin Volk. \"Machine Translation of 16th Century Letters from Latin to German\". Workshop on Language Technologies for Historical and Ancient Languages (2022).\n* Knowles, Rebecca, Patrick Littell. \"Translation Memories as Baselines for Low-Resource Machine Translation\". Proceedings of LREC (2022)\n* McNamee, Paul, Kevin Duh. \"The Multilingual Microblog Translation Corpus: Improving and Evaluating Translation of User-Generated Text\". Proceedings of LREC (2022)\n* Nadejde Maria, Anna Currey, Benjamin Hsu, Xing Niu, Marcello Federico, Georgiana Dinu. \"CoCoA-MT: A Dataset and Benchmark for Contrastive Controlled MT with Application to Formality\". Proceedings of NAACL (2022).\n* Weller-Di Marco, Marion, Matthias Huck, Alexander Fraser. \"Modeling Target-Side Morphology in Neural Machine Translation: A Comparison of Strategies\n\". arXiv preprint arXiv:2203.13550 (2022) \n", "original_header": "2022"}, "confidence": 0.960949375298981, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "* Bergmanis, Toms, M\u0101rcis Pinnis. \"Facilitating Terminology Translation with Target Lemma Annotations\". arXiv preprint arXiv:2101.10035 (2021)\n* Briakou, Eleftheria, Marine Carpuat. \"Beyond Noise: Mitigating the Impact of Fine-grained Semantic Divergences on Neural Machine Translation\". arXiv preprint arXiv:2105.15087 (2021)\n* Hasler, Eva, Tobias Domhan, Sony Trenous, Ke Tran, Bill Byrne, Felix Hieber. \"Improving the Quality Trade-Off for Neural Machine Translation Multi-Domain Adaptation\". Proceedings of EMNLP (2021)\n* Tang, Gongbo, Philipp R\u00f6nchen, Rico Sennrich, Joakim Nivre. \"Revisiting Negation in Neural Machine Translation\". Transactions of the Association for Computation Linguistics 9 (2021)\n* Vu, Thuy, Alessandro Moschitti. \"Machine Translation Customization via Automatic Training Data Selection from the Web\". arXiv preprint arXiv:2102.1024 (2021)\n* Xu, Weijia, Marine Carpuat. \"EDITOR: An Edit-Based Transformer with Repositioning for Neural Machine Translation with Soft Lexical Constraints.\" Transactions of the Association for Computation Linguistics 9 (2021)\n* M\u00fcller, Mathias, Rico Sennrich. \"Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation\". Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) (2021)\n* Popovi\u0107, Maja, Alberto Poncelas. \"On Machine Translation of User Reviews.\" Proceedings of RANLP (2021)\n* Popovi\u0107, Maja. \"On nature and causes of observed MT errors.\" Proceedings of the 18th MT Summit (Volume 1: Research Track) (2021)\n* Jain, Nishtha, Maja Popovi\u0107, Declan Groves, Eva Vanmassenhove. \"Generating Gender Augmented Data for NLP.\" Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing (2021)\n* Vilar, David, Marcello Federico. \"A Statistical Extension of Byte-Pair Encoding.\" Proceedings of IWSLT (2021)\n \n", "original_header": "2021"}, "confidence": 0.9570115638440125, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "* Dinu, Georgiana, Prashant Mathur, Marcello Federico, Stanislas Lauly, Yaser Al-Onaizan. \"Joint translation and unit conversion for end-to-end localization.\" Proceedings of IWSLT (2020)\n* Exel, Miriam, Bianka Buschbeck, Lauritz Brandt, Simona Doneva. \"Terminology-Constrained Neural Machine Translation at SAP\". Proceedings of EAMT (2020).\n* Hisamoto, Sorami, Matt Post, Kevin Duh. \"Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data In Your Machine Translation System?\" Transactions of the Association for Computational Linguistics, Volume 8 (2020)\n* Naradowsky, Jason, Xuan Zhan, Kevin Duh. \"Machine Translation System Selection from Bandit Feedback.\" arXiv preprint arXiv:2002.09646 (2020)\n* Niu, Xing, Prashant Mathur, Georgiana Dinu, Yaser Al-Onaizan. \"Evaluating Robustness to Input Perturbations for Neural Machine Translation\". arXiv preprint \tarXiv:2005.00580 (2020)\n* Niu, Xing, Marine Carpuat. \"Controlling Neural Machine Translation Formality with Synthetic Supervision.\" Proceedings of AAAI (2020)\n* Keung, Phillip, Julian Salazar, Yichao Liu, Noah A. Smith. \"Unsupervised Bitext Mining and Translation\nvia Self-Trained Contextual Embeddings.\" arXiv preprint arXiv:2010.07761 (2020).\n* Sokolov, Alex, Tracy Rohlin, Ariya Rastrow. \"Neural Machine Translation for Multilingual Grapheme-to-Phoneme Conversion.\" arXiv preprint arXiv:2006.14194 (2020)\n* Stafanovi\u010ds, Art\u016brs, Toms Bergmanis, M\u0101rcis Pinnis. \"Mitigating Gender Bias in Machine Translation with Target Gender\nAnnotations.\" arXiv preprint arXiv:2010.06203 (2020)\n* Stojanovski, Dario, Alexander Fraser. \"Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation.\" arXiv preprint arXiv preprint arXiv:2004.14927 (2020)\n* Stojanovski, Dario, Benno Krojer, Denis Peskov, Alexander Fraser. \"ContraCAT: Contrastive Coreference Analytical Templates for Machine Translation\". Proceedings of COLING (2020)\n* Zhang, Xuan, Kevin Duh. \"Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems.\" Transactions of the Association for Computational Linguistics, Volume 8 (2020)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant, Nandar Win Min, and Thepchai Supnithi, \"Unsupervised Neural Machine Translation between Myanmar Sign Language and Myanmar Language\", Journal of Intelligent Informatics and Smart Technology, April 1st Issue, 2020, pp. 53-61. (Submitted December 21, 2019; accepted March 6, 2020; revised March 16, 2020; published online April 30, 2020)\n* Thazin Myint Oo, Ye Kyaw Thu, Khin Mar Soe and Thepchai Supnithi, \"Neural Machine Translation between Myanmar (Burmese) and Dawei (Tavoyan)\", In Proceedings of the 18th International Conference on Computer Applications (ICCA 2020), Feb 27-28, 2020, Yangon, Myanmar, pp. 219-227\n* M\u00fcller, Mathias, Annette Rios, Rico Sennrich. \"Domain Robustness in Neural Machine Translation.\" Proceedings of AMTA (2020)\n* Rios, Annette, Mathias M\u00fcller, Rico Sennrich. \"Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Neural Machine Translation between similar South-Slavic languages.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Extracting correctly aligned segments from unclean parallel data using character n-gram matching.\" Proceedings of Conference on Language Technologies & Digital Humanities (JTDH 2020).\n* Popovi\u0107, Maja, Alberto Poncelas, Marija Brkic, Andy Way. \"Neural Machine Translation for translating into Croatian and Serbian.\" Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects (2020)\n \n", "original_header": "2020"}, "confidence": 0.8409154200759298, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017).\n \n", "original_header": "2017"}, "confidence": 0.8455522619627996, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}], "name": [{"result": {"value": "sockeye", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}], "full_name": [{"result": {"value": "awslabs/sockeye", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}], "issue_tracker": [{"result": {"value": "https://api.github.com/repos/awslabs/sockeye/issues", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "forks_url": [{"result": {"value": "https://api.github.com/repos/awslabs/sockeye/forks", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "stargazers_count": [{"result": {"value": 1194, "type": "Number"}, "confidence": 1, "technique": "GitHub_API"}], "keywords": [{"result": {"value": "attention-is-all-you-need, attention-mechanism, attention-model, deep-learning, deep-neural-networks, encoder-decoder, machine-learning, machine-translation, neural-machine-translation, pytorch, seq2seq, sequence-to-sequence, sequence-to-sequence-models, sockeye, transformer, transformer-architecture, transformer-network, translation", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}], "forks_count": [{"result": {"value": 331, "type": "Number"}, "confidence": 1, "technique": "GitHub_API"}], "download_url": [{"result": {"value": "https://github.com/awslabs/sockeye/releases", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "programming_languages": [{"result": {"value": "Python", "name": "Python", "type": "Programming_language", "size": 1125928}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "JavaScript", "name": "JavaScript", "type": "Programming_language", "size": 4196}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "Shell", "name": "Shell", "type": "Programming_language", "size": 2912}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "TeX", "name": "TeX", "type": "Programming_language", "size": 2847}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "Dockerfile", "name": "Dockerfile", "type": "Programming_language", "size": 1028}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "CSS", "name": "CSS", "type": "Programming_language", "size": 824}, "confidence": 1, "technique": "GitHub_API"}], "releases": [{"result": {"type": "Release", "value": "https://api.github.com/repos/awslabs/sockeye/releases/94378712", "tag": "3.1.34", "name": "3.1.34", "author": {"name": "fhieber", "type": "User"}, "description": "## [3.1.34]\r\n\r\n### Fixed\r\n- Do not mask prepended tokens by default (for self-attention).\r\n- Do not require specifying `--end-of-prepending-tag` if it is already done when preparing the data.", "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/3.1.34", "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/3.1.34", "html_url": "https://github.com/awslabs/sockeye/releases/tag/3.1.34", "url": "https://api.github.com/repos/awslabs/sockeye/releases/94378712", "release_id": 94378712, "date_created": "2023-03-02T23:03:48Z", "date_published": "2023-03-03T07:44:13Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/awslabs/sockeye/releases/94049185", "tag": "3.1.33", "name": "3.1.33", "author": {"name": "fhieber", "type": "User"}, "description": "## [3.1.33]\r\n\r\n### Fixed \r\n- Two small fixes to SampleK. Before the device was not set correctly leading to issues when running sampling on GPUs. Furthermore, SampleK did not return the top-k values correctly.\r\n\r\n## [3.1.32]\r\n\r\n### Added\r\n\r\n- Sockeye now supports blocking cross-attention between decoder and encoded prepended tokens.\r\n  - If the source contains prepended text and a tag indicating the end of prepended text,\r\n    Sockeye supports blocking the cross-attention between decoder and encoded prepended tokens (including the tag).\r\n    To enable this operation, specify `--end-of-prepending-tag` for training or data preparation,\r\n    and `--transformer-block-prepended-cross-attention` for training.\r\n\r\n### Changed\r\n\r\n- Sockeye uses a new dictionary-based prepared data format that supports storing length of prepended source tokens\r\n  (version 7). The previous format (version 6) is still supported.", "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/3.1.33", "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/3.1.33", "html_url": "https://github.com/awslabs/sockeye/releases/tag/3.1.33", "url": "https://api.github.com/repos/awslabs/sockeye/releases/94049185", "release_id": 94049185, "date_created": "2023-03-01T08:57:49Z", "date_published": "2023-03-01T09:01:57Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/awslabs/sockeye/releases/90952690", "tag": "3.1.31", "name": "3.1.31", "author": {"name": "fhieber", "type": "User"}, "description": "## [3.1.31]\r\n\r\n### Fixed\r\n\r\n- Fixed sequence copying integration tests to correctly specify that scoring/translation outputs should not be checked.\r\n- Enabled `bfloat16` integration and system testing on all platforms.\r\n\r\n## [3.1.30]\r\n\r\n### Added\r\n\r\n- Added support for `--dtype bfloat16` to `sockeye-translate`, `sockeye-score`, and `sockeye-quantize`.\r\n\r\n### Fixed\r\n\r\n- Fixed compatibility issue with `numpy==1.24.0` by using `pickle` instead of `numpy` to save/load `ParallelSampleIter` data permutations.", "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/3.1.31", "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/3.1.31", "html_url": "https://github.com/awslabs/sockeye/releases/tag/3.1.31", "url": "https://api.github.com/repos/awslabs/sockeye/releases/90952690", "release_id": 90952690, "date_created": "2023-01-03T17:45:03Z", "date_published": "2023-02-01T09:12:16Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/awslabs/sockeye/releases/85702177", "tag": "3.1.29", "name": "3.1.29", "author": {"name": "fhieber", "type": "User"}, "description": "## [3.1.29]\r\n\r\n### Changed\r\n\r\n- Running `sockeye-evaluate` no longer applies text tokenization for TER (same behavior as other metrics).\r\n- Turned on type checking for all `sockeye` modules except `test_utils` and addressed resulting type issues.\r\n- Refactored code in various modules without changing user-level behavior.\r\n\r\n## [3.1.28]\r\n\r\n### Added\r\n\r\n- Added kNN-MT model from [Khandelwal et al., 2021](https://arxiv.org/abs/2010.00710).\r\n  - Installation: see [faiss document](https://github.com/facebookresearch/faiss/blob/main/INSTALL.md) -- installation via conda is recommended.\r\n  - Building a faiss index from a sockeye model takes two steps:\r\n    - Generate decoder states: `sockeye-generate-decoder-states -m [model] --source [src] --target [tgt] --output-dir [output dir]`\r\n    - Build index: `sockeye-knn -i [input_dir] -o [output_dir] -t [faiss_index_signature]` where `input_dir` is the same as `output_dir` from the `sockeye-generate-decoder-states` command.\r\n    - Faiss index signature reference: [see here](https://github.com/facebookresearch/faiss/wiki/The-index-factory)\r\n  - Running inference using the built index: `sockeye-translate ... --knn-index [index_dir] --knn-lambda [interpolation_weight]` where `index_dir` is the same as `output_dir` from the `sockeye-knn` command.", "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/3.1.29", "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/3.1.29", "html_url": "https://github.com/awslabs/sockeye/releases/tag/3.1.29", "url": "https://api.github.com/repos/awslabs/sockeye/releases/85702177", "release_id": 85702177, "date_created": "2022-12-11T11:11:32Z", "date_published": "2022-12-12T08:29:00Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/awslabs/sockeye/releases/82195737", "tag": "3.1.27", "name": "3.1.27", "author": {"name": "fhieber", "type": "User"}, "description": "## [3.1.27]\r\n\r\n### Changed\r\n\r\n- allow torch 1.13 in requirements.txt\r\n- Replaced deprecated `torch.testing.assert_allclose` with `torch.testing.close` for PyTorch 1.14 compatibility.\r\n\r\n## [3.1.26]\r\n\r\n### Added\r\n\r\n- `--tf32 0|1` bool device (`torch.backends.cuda.matmul.allow_tf32`)\r\n enabling 10-bit precision (19 bit total) transparent float32\r\n acceleration. default true for backward compat with torch < 1.12.\r\n allow different `--tf32` training continuation\r\n\r\n### Changed\r\n\r\n- `device.init_device()` called by train, translate, and score\r\n- allow torch 1.12 in requirements.txt\r\n\r\n## [3.1.25]\r\n\r\n## Changed\r\n- Updated to sacrebleu==2.3.1. Changed default BLEU floor smoothing offset from 0.01 to 0.1.\r\n\r\n## [3.1.24]\r\n\r\n### Fixed\r\n\r\n- Updated DeepSpeed checkpoint conversion to support newer versions of DeepSpeed.\r\n\r\n## [3.1.23]\r\n\r\n### Changed\r\n\r\n- Change decoder softmax size logging level from info to debug.\r\n\r\n## [3.1.22]\r\n\r\n### Added\r\n\r\n- log beam search avg output vocab size\r\n\r\n### Changed\r\n\r\n- common base Search for GreedySearch and BeamSearch\r\n- .pylintrc: suppress warnings about deprecated pylint warning suppressions\r\n\r\n## [3.1.21]\r\n\r\n### Fixed\r\n\r\n- Send skip_nvs and nvs_thresh args now to Translator constructor in sockeye-translate instead of ignoring them.\r\n\r\n## [3.1.20]\r\n\r\n### Added\r\n\r\n- Added training support for [DeepSpeed](https://www.deepspeed.ai/).\r\n  - Installation: `pip install deepspeed`\r\n  - Usage: `deepspeed --no_python ... sockeye-train ...`\r\n  - DeepSpeed mode uses Zero Redundancy Optimizer (ZeRO) stage 1 ([Rajbhandari et al., 2019](https://arxiv.org/abs/1910.02054v3)).\r\n  - Run in FP16 mode with `--deepspeed-fp16` or BF16 mode with `--deepspeed-bf16`.\r\n\r\n## [3.1.19]\r\n\r\n### Added\r\n\r\n- Clean up GPU and CPU memory used during training initialization before starting the main training loop.\r\n\r\n### Changed\r\n\r\n- Refactored training code in advance of adding DeepSpeed support:\r\n  - Moved logic for flagging interleaved key-value parameters from layers.py to model.py.\r\n  - Refactored LearningRateScheduler API to be compatible with PyTorch/DeepSpeed.\r\n  - Refactored optimizer and learning rate scheduler creation to be modular.\r\n  - Migrated to ModelWithLoss API, which wraps a Sockeye model and its losses in a single module.\r\n  - Refactored primary and secondary worker logic to reduce redundant calculations.\r\n  - Refactored code for saving/loading training states.\r\n  - Added utility code for managing model/training configurations.\r\n\r\n### Removed\r\n\r\n- Removed unused training option `--learning-rate-t-scale`.\r\n\r\n## [3.1.18]\r\n\r\n### Added\r\n\r\n- Added `sockeye-train` and `sockeye-translate` option `--clamp-to-dtype` that clamps outputs of transformer attention, feed-forward networks, and process blocks to the min/max finite values for the current dtype. This can prevent inf/nan values from overflow when running large models in float16 mode. See: https://discuss.huggingface.co/t/t5-fp16-issue-is-fixed/3139\r\n\r\n## [3.1.17]\r\n\r\n### Added\r\n\r\n- Added support for offline model quantization with `sockeye-quantize`.\r\n  - Pre-quantizing a model avoids the load-time memory spike of runtime quantization. For example, a float16 model loads directly as float16 instead of loading as float32 then casting to float16.\r\n\r\n## [3.1.16]\r\n\r\n### Added\r\n- Added nbest list reranking options using isometric translation criteria as proposed in an ICASSP 2021 paper https://arxiv.org/abs/2110.03847.\r\nTo use this feature pass a criterion (`isometric-ratio, isometric-diff, isometric-lc`) when specifying `--metric`.\r\n- Added `--output-best-non-blank` to output non-blank best hypothesis from the nbest list.\r\n\r\n## [3.1.15]\r\n\r\n### Fixed\r\n\r\n- Fix type of valid_length to be pt.Tensor instead of Optional[pt.Tensor] = None for jit tracing", "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/3.1.27", "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/3.1.27", "html_url": "https://github.com/awslabs/sockeye/releases/tag/3.1.27", "url": "https://api.github.com/repos/awslabs/sockeye/releases/82195737", "release_id": 82195737, "date_created": "2022-11-06T14:01:10Z", "date_published": "2022-11-06T14:13:40Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/awslabs/sockeye/releases/66092463", "tag": "3.1.14", "name": "3.1.14", "author": {"name": "fhieber", "type": "User"}, "description": "## [3.1.14]\r\n\r\n### Added\r\n- Added the implementation of Neural vocabulary selection to Sockeye as presented in our NAACL 2022 paper \"The Devil is in the Details: On the Pitfalls of Vocabulary Selection in Neural Machine Translation\" (Tobias Domhan, Eva Hasler, Ke Tran, Sony Trenous, Bill Byrne and Felix Hieber).\r\n  - To use NVS simply specify `--neural-vocab-selection` to `sockeye-train`. This will train a model with Neural Vocabulary Selection that is automatically used by `sockeye-translate`. If you want look at translations without vocabulary selection specify `--skip-nvs` as an argument to `sockeye-translate`.\r\n\r\n## [3.1.13]\r\n\r\n### Added\r\n\r\n- Added `sockeye-train` argument `--no-reload-on-learning-rate-reduce` that disables reloading the best training checkpoint when reducing the learning rate. This currently only applies to the `plateau-reduce` learning rate scheduler since other schedulers do not reload checkpoints.", "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/3.1.14", "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/3.1.14", "html_url": "https://github.com/awslabs/sockeye/releases/tag/3.1.14", "url": "https://api.github.com/repos/awslabs/sockeye/releases/66092463", "release_id": 66092463, "date_created": "2022-05-04T14:38:57Z", "date_published": "2022-05-05T08:42:03Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/awslabs/sockeye/releases/65318126", "tag": "3.1.12", "name": "3.1.12", "author": {"name": "fhieber", "type": "User"}, "description": "## [3.1.12]\r\n\r\n### Fixed\r\n\r\n- Fix scoring with batches of size 1 (whic may occur when `|data| % batch_size == 1`.\r\n\r\n## [3.1.11]\r\n\r\n### Fixed\r\n\r\n- When resuming training with a fully trained model, `sockeye-train` will correctly exit without creating a duplicate (but separately numbered) checkpoint.\r\n", "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/3.1.12", "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/3.1.12", "html_url": "https://github.com/awslabs/sockeye/releases/tag/3.1.12", "url": "https://api.github.com/repos/awslabs/sockeye/releases/65318126", "release_id": 65318126, "date_created": "2022-04-21T10:38:32Z", "date_published": "2022-04-26T08:56:30Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/awslabs/sockeye/releases/64206059", "tag": "3.1.10", "name": "3.1.10", "author": {"name": "fhieber", "type": "User"}, "description": "## [3.1.10]\r\n\r\n### Fixed\r\n\r\n- When loading parameters, SockeyeModel now ignores false positive missing parameters for traced modules. These modules use the same parameters as their original non-traced versions.", "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/3.1.10", "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/3.1.10", "html_url": "https://github.com/awslabs/sockeye/releases/tag/3.1.10", "url": "https://api.github.com/repos/awslabs/sockeye/releases/64206059", "release_id": 64206059, "date_created": "2022-04-12T07:22:38Z", "date_published": "2022-04-12T07:23:27Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/awslabs/sockeye/releases/64113863", "tag": "3.1.9", "name": "3.1.9", "author": {"name": "fhieber", "type": "User"}, "description": "## [3.1.9]\r\n\r\n### Changed\r\n\r\n- Clarified usage of `batch_size` in Translator code.\r\n\r\n## [3.1.8]\r\n\r\n### Fixed\r\n\r\n- When saving parameters, SockeyeModel now skips parameters for traced modules because these modules are created at runtime and use the same parameters as non-traced versions. When loading parameters, SockeyeModel ignores parameters for traced modules that may have been saved by earlier versions.", "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/3.1.9", "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/3.1.9", "html_url": "https://github.com/awslabs/sockeye/releases/tag/3.1.9", "url": "https://api.github.com/repos/awslabs/sockeye/releases/64113863", "release_id": 64113863, "date_created": "2022-04-05T15:38:13Z", "date_published": "2022-04-11T14:12:57Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/awslabs/sockeye/releases/62553767", "tag": "3.1.7", "name": "3.1.7", "author": {"name": "fhieber", "type": "User"}, "description": "## [3.1.7]\r\n\r\n### Changed\r\n\r\n- SockeyeModel components are now traced regardless of whether `inference_only` is set, including for the CheckpointDecoder during training.\r\n\r\n## [3.1.6]\r\n\r\n### Changed\r\n\r\n- Moved offsetting of topk scores out of the (traced) TopK module. This allows sending requests of variable\r\n  batch size to the same Translator/Model/BeamSearch instance.\r\n\r\n## [3.1.5]\r\n\r\n### Changed\r\n- Allow PyTorch 1.11 in requirements", "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/3.1.7", "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/3.1.7", "html_url": "https://github.com/awslabs/sockeye/releases/tag/3.1.7", "url": "https://api.github.com/repos/awslabs/sockeye/releases/62553767", "release_id": 62553767, "date_created": "2022-03-23T08:35:17Z", "date_published": "2022-03-23T09:21:54Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/awslabs/sockeye/releases/61473688", "tag": "3.1.4", "name": "3.1.4", "author": {"name": "fhieber", "type": "User"}, "description": "## [3.1.4]\r\n\r\n### Added\r\n- Added support for the use of adding target prefix and target prefix factors to the input in JSON format during inference.", "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/3.1.4", "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/3.1.4", "html_url": "https://github.com/awslabs/sockeye/releases/tag/3.1.4", "url": "https://api.github.com/repos/awslabs/sockeye/releases/61473688", "release_id": 61473688, "date_created": "2022-03-10T09:10:48Z", "date_published": "2022-03-10T09:14:37Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/awslabs/sockeye/releases/60591739", "tag": "3.1.3", "name": "3.1.3", "author": {"name": "fhieber", "type": "User"}, "description": "## [3.1.3]\r\n\r\n### Added\r\n- Added support for the use of adding source prefixes to the input in JSON format during inference.\r\n\r\n## [3.1.2]\r\n\r\n### Changed\r\n- Optimized creation of source length mask by using `expand` instead of `repeat_interleave`.\r\n\r\n## [3.1.1]\r\n\r\n### Changed\r\n- Updated torch dependency to 1.10.x (`torch>=1.10.0,<1.11.0`)", "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/3.1.3", "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/3.1.3", "html_url": "https://github.com/awslabs/sockeye/releases/tag/3.1.3", "url": "https://api.github.com/repos/awslabs/sockeye/releases/60591739", "release_id": 60591739, "date_created": "2022-02-22T07:20:52Z", "date_published": "2022-02-28T09:43:29Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/awslabs/sockeye/releases/59315894", "tag": "3.1.0", "name": "3.1.0", "author": {"name": "fhieber", "type": "User"}, "description": "## [3.1.0]\r\nSockeye is now exclusively based on Pytorch.\r\n\r\n### Changed\r\n- Renamed `x_pt` modules to `x`. Updated entry points in `setup.py`.\r\n\r\n### Removed\r\n- Removed MXNet from the codebase\r\n- Removed device locking / GPU acquisition logic. Removed dependency on `portalocker`.\r\n- Removed arguments `--softmax-temperature`, `--weight-init-*`, `--mc-dropout`, `--horovod`, `--device-ids`\r\n- Removed all MXNet-related tests", "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/3.1.0", "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/3.1.0", "html_url": "https://github.com/awslabs/sockeye/releases/tag/3.1.0", "url": "https://api.github.com/repos/awslabs/sockeye/releases/59315894", "release_id": 59315894, "date_created": "2022-02-09T19:36:56Z", "date_published": "2022-02-11T09:25:54Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/awslabs/sockeye/releases/59164442", "tag": "3.0.15", "name": "3.0.15", "author": {"name": "fhieber", "type": "User"}, "description": "## [3.0.15]\r\n\r\n### Fixed\r\n- Fixed GPU-based scoring by copying to cpu tensor first before converting to numpy.\r\n\r\n## [3.0.14]\r\n\r\n### Added\r\n- Added support for Translation Error Rate (TER) metric as implemented in sacrebleu==1.4.14.\r\n  Checkpoint decoder metrics will now include TER scores and early stopping can be determined\r\n  via TER improvements (`--optimized-metric ter`)", "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/3.0.15", "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/3.0.15", "html_url": "https://github.com/awslabs/sockeye/releases/tag/3.0.15", "url": "https://api.github.com/repos/awslabs/sockeye/releases/59164442", "release_id": 59164442, "date_created": "2022-02-08T10:29:35Z", "date_published": "2022-02-09T19:13:09Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/awslabs/sockeye/releases/58592534", "tag": "3.0.13", "name": "3.0.13", "author": {"name": "fhieber", "type": "User"}, "description": "## [3.0.13]\r\n\r\n### Changed\r\n- use `expand` instead of `repeat` for attention masks to not allocate additional memory\r\n- avoid repeated `transpose` for initializing cached encoder-attention states in the decoder.\r\n\r\n## [3.0.12]\r\n\r\n### Removed\r\n- Removed unused code for Weight Normalization. Minor code cleanups.\r\n\r\n## [3.0.11]\r\n\r\n### Fixed\r\n\r\n- Fixed training with a single, fixed learning rate instead of a rate scheduler (`--learning-rate-scheduler none --initial-learning-rate ...`).", "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/3.0.13", "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/3.0.13", "html_url": "https://github.com/awslabs/sockeye/releases/tag/3.0.13", "url": "https://api.github.com/repos/awslabs/sockeye/releases/58592534", "release_id": 58592534, "date_created": "2022-02-02T17:50:58Z", "date_published": "2022-02-03T12:08:07Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/awslabs/sockeye/releases/57434782", "tag": "3.0.10", "name": "3.0.10", "author": {"name": "fhieber", "type": "User"}, "description": "## [3.0.10]\r\n\r\n### Changed\r\n\r\n- End-to-end trace decode_step of the Sockeye model. Creates less overhead during decoding and a small speedup.\r\n\r\n## [3.0.9]\r\n\r\n### Fixed\r\n\r\n- Fixed not calling the traced target embedding module during inference.\r\n\r\n## [3.0.8]\r\n\r\n### Changed\r\n\r\n- Add support for JIT tracing source/target embeddings and JIT scripting the output layer during inference. \r\n", "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/3.0.10", "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/3.0.10", "html_url": "https://github.com/awslabs/sockeye/releases/tag/3.0.10", "url": "https://api.github.com/repos/awslabs/sockeye/releases/57434782", "release_id": 57434782, "date_created": "2022-01-18T18:49:22Z", "date_published": "2022-01-19T07:19:21Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/awslabs/sockeye/releases/55656008", "tag": "3.0.7", "name": "3.0.7", "author": {"name": "fhieber", "type": "User"}, "description": "## [3.0.7]\r\n\r\n## Changed\r\n\r\n- Improve training speed by using`torch.nn.functional.multi_head_attention_forward` for self- and encoder-attention\r\n  during training. Requires reorganization of the parameter layout of the key-value input projections,\r\n  as the current Sockeye attention interleaves for faster inference.\r\n  Attention masks (both for source masking and autoregressive masks need some shape adjustments as requirements\r\n  for the fused MHA op differ slightly).\r\n  - Non-interleaved format for joint key-value input projection parameters:\r\n    `in_features=hidden, out_features=2*hidden -> Shape: (2*hidden, hidden)`\r\n  - Interleaved format for joint-key-value input projection stores key and value parameters, grouped by heads:\r\n    `Shape: ((num_heads * 2 * hidden_per_head), hidden)`\r\n  - Models save and load key-value projection parameters in interleaved format.\r\n  - When `model.training == True` key-value projection parameters are put into\r\n    non-interleaved format for `torch.nn.functional.multi_head_attention_forward`\r\n  - When `model.training == False`, i.e. model.eval() is called, key-value projection\r\n    parameters are again converted into interleaved format in place.\r\n\r\n## [3.0.6]\r\n\r\n### Fixed\r\n\r\n- Fixed checkpoint decoder issue that prevented using `bleu` as `--optimized-metric` for distributed training ([#995](https://github.com/awslabs/sockeye/issues/995)).\r\n\r\n## [3.0.5]\r\n\r\n### Fixed\r\n\r\n- Fixed data download in multilingual tutorial.", "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/3.0.7", "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/3.0.7", "html_url": "https://github.com/awslabs/sockeye/releases/tag/3.0.7", "url": "https://api.github.com/repos/awslabs/sockeye/releases/55656008", "release_id": 55656008, "date_created": "2021-12-19T16:49:18Z", "date_published": "2021-12-20T09:59:18Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/awslabs/sockeye/releases/55207161", "tag": "3.0.4", "name": "3.0.4", "author": {"name": "fhieber", "type": "User"}, "description": "## [3.0.4]\r\n\r\n###\r\n\r\n- Make sure data permutation indices are in int64 format (doesn't seem to be the case by default on all platforms).\r\n\r\n## [3.0.3]\r\n\r\n### Fixed\r\n\r\n- Fixed ensemble decoding for models without target factors.\r\n\r\n## [3.0.2]\r\n\r\n### Changed\r\n\r\n- `sockeye-translate`: Beam search now computes and returns secondary target factor scores. Secondary target factors\r\n  do not participate in beam search, but are greedily chosen at every time step. Accumulated scores for secondary factors\r\n  are not normalized by length. Factor scores are included in JSON output (``--output-type json``).\r\n- `sockeye-score` now returns tab-separated scores for each target factor. Users can decide how to combine factor scores\r\n  depending on the downstream application. Score for the first, primary factor (i.e. output words) are normalized,\r\n  other factors are not.\r\n\r\n## [3.0.1]\r\n\r\n### Fixed\r\n\r\n- Parameter averaging (`sockeye-average`) now always uses the CPU, which enables averaging parameters from GPU-trained models on CPU-only hosts.", "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/3.0.4", "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/3.0.4", "html_url": "https://github.com/awslabs/sockeye/releases/tag/3.0.4", "url": "https://api.github.com/repos/awslabs/sockeye/releases/55207161", "release_id": 55207161, "date_created": "2021-12-13T17:22:22Z", "date_published": "2021-12-13T17:39:31Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/awslabs/sockeye/releases/54306072", "tag": "3.0.0", "name": "3.0.0", "author": {"name": "fhieber", "type": "User"}, "description": "## [3.0.0] Sockeye 3: Fast Neural Machine Translation with PyTorch\r\n\r\nSockeye is now based on PyTorch.\r\nWe maintain backwards compatibility with MXNet models in version 2.3.x until 3.1.0.\r\nIf MXNet 2.x is installed, Sockeye can run both with PyTorch or MXNet but MXNet is no longer strictly required.\r\n\r\n### Added\r\n\r\n- Added model converter CLI `sockeye.mx_to_pt` that converts MXNet models to PyTorch models.\r\n- Added `--apex-amp` training argument that runs entire model in FP16 mode, replaces `--dtype float16` (requires [Apex](https://github.com/NVIDIA/apex)).\r\n- Training automatically uses Apex fused optimizers if available (requires [Apex](https://github.com/NVIDIA/apex)).\r\n- Added training argument `--label-smoothing-impl` to choose label smoothing implementation (default of `mxnet` uses the same logic as MXNet Sockeye 2).\r\n\r\n### Changed\r\n\r\n- CLI names point to the PyTorch code base (e.g. `sockeye-train` etc.).\r\n- MXNet-based CLIs are now accessible via `sockeye-<name>-mx`.\r\n- MXNet code requires MXNet >= 2.0 since we adopted the new numpy interface.\r\n- `sockeye-train` now uses PyTorch's distributed data-parallel mode for multi-process (multi-GPU) training. Launch with: `torchrun --no_python --nproc_per_node N sockeye-train --dist ...`\r\n- Updated the [quickstart tutorial](docs/tutorials/wmt_large.md) to cover multi-device training with PyTorch Sockeye.\r\n- Changed `--device-ids` argument (plural) to `--device-id` (singular). For multi-GPU training, see distributed mode noted above.\r\n- Updated default value: `--pad-vocab-to-multiple-of 8`\r\n- Removed `--horovod` argument used with `horovodrun` (use `--dist` with `torchrun`).\r\n- Removed `--optimizer-params` argument (use `--optimizer-betas`, `--optimizer-eps`).\r\n- Removed `--no-hybridization` argument (use `PYTORCH_JIT=0`, see [Disable JIT for Debugging](https://pytorch.org/docs/stable/jit.html#disable-jit-for-debugging)).\r\n- Removed `--omp-num-threads` argument (use `--env=OMP_NUM_THREADS=N`).\r\n\r\n### Removed\r\n\r\n- Removed support for constrained decoding (both positive and negative lexical constraints)\r\n- Removed support for beam histories\r\n- Removed `--amp-scale-interval` argument.\r\n- Removed `--kvstore` argument.\r\n- Removed arguments: `--weight-init`, `--weight-init-scale` `--weight-init-xavier-factor-type`, `--weight-init-xavier-rand-type`\r\n- Removed `--decode-and-evaluate-device-id` argument.\r\n- Removed arguments: `--monitor-pattern'`, `--monitor-stat-func`\r\n- Removed CUDA-specific requirements files in `requirements/`", "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/3.0.0", "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/3.0.0", "html_url": "https://github.com/awslabs/sockeye/releases/tag/3.0.0", "url": "https://api.github.com/repos/awslabs/sockeye/releases/54306072", "release_id": 54306072, "date_created": "2021-11-30T09:42:15Z", "date_published": "2021-11-30T09:48:34Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/awslabs/sockeye/releases/52769304", "tag": "2.3.24", "name": "2.3.24", "author": {"name": "fhieber", "type": "User"}, "description": "## [2.3.24]\r\n### Added\r\n\r\n- Use of the safe yaml loader for the model configuration files.\r\n\r\n## [2.3.23]\r\n### Changed\r\n\r\n- Do not sort BIAS_STATE in beam search. It is constant across decoder steps.", "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/2.3.24", "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/2.3.24", "html_url": "https://github.com/awslabs/sockeye/releases/tag/2.3.24", "url": "https://api.github.com/repos/awslabs/sockeye/releases/52769304", "release_id": 52769304, "date_created": "2021-10-21T15:53:54Z", "date_published": "2021-11-05T09:28:33Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/awslabs/sockeye/releases/50535944", "tag": "2.3.22", "name": "2.3.22", "author": {"name": "fhieber", "type": "User"}, "description": "## [2.3.22]\r\n### Fixed\r\n\r\n- The previous commit introduced a regression for vocab creation. The results was that the vocabulary was created on the input characters rather than on tokens.\r\n\r\n\r\n## [2.3.21]\r\n### Added\r\n\r\n- Extended parallelization of data preparation to vocabulary and statistics creation while minimizing the overhead of sharding.\r\n\r\n## [2.3.20]\r\n### Added\r\n\r\n- Added debug logging for restrict_lexicon lookups\r\n\r\n## [2.3.19]\r\n### Changed\r\n\r\n- When training only the decoder (`--fixed-param-strategy all_except_decoder`), disable autograd for the encoder and embeddings to save memory.\r\n\r\n## [2.3.18]\r\n### Changed\r\n\r\n- Updated Docker builds and documentation.  See [sockeye_contrib/docker](sockeye_contrib/docker).", "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/2.3.22", "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/2.3.22", "html_url": "https://github.com/awslabs/sockeye/releases/tag/2.3.22", "url": "https://api.github.com/repos/awslabs/sockeye/releases/50535944", "release_id": 50535944, "date_created": "2021-09-23T12:50:13Z", "date_published": "2021-09-30T09:41:22Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/awslabs/sockeye/releases/44786693", "tag": "2.3.17", "name": "2.3.17", "author": {"name": "fhieber", "type": "User"}, "description": "## [2.3.17]\r\n### Added\r\n- Added an alternative, faster implementation of greedy search. The '--greedy' flag to `sockeye.translate` will enable it. This implementation does not support hypothesis scores, batch decoding, or lexical constraints.\"\r\n\r\n## [2.3.16]\r\n\r\n### Added\r\n- Added option `--transformer-feed-forward-use-glu` to use Gated Linear Units in transformer feed forward networks ([Dauphin et al., 2016](https://arxiv.org/abs/1612.08083); [Shazeer, 2020](https://arxiv.org/abs/2002.05202)).\r\n\r\n## [2.3.15]\r\n\r\n### Changed\r\n- Optimization: Decoder class is now a complete HybridBlock (no forward method).", "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/2.3.17", "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/2.3.17", "html_url": "https://github.com/awslabs/sockeye/releases/tag/2.3.17", "url": "https://api.github.com/repos/awslabs/sockeye/releases/44786693", "release_id": 44786693, "date_created": "2021-05-30T20:04:06Z", "date_published": "2021-06-17T10:50:20Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/awslabs/sockeye/releases/41053485", "tag": "2.3.14", "name": "2.3.14", "author": {"name": "fhieber", "type": "User"}, "description": "## [2.3.14]\r\n\r\n### Changed\r\n- Updated to [MXNet 1.8.0](https://github.com/apache/incubator-mxnet/tree/1.8.0)\r\n- Removed dependency support for Cuda 9.2 (no longer supported by MXNet 1.8).\r\n- Added dependency support for Cuda 11.0 and 11.2.\r\n- Updated Python requirement to 3.7 and later. (Removed backporting `dataclasses` requirement)\r\n\r\n## [2.3.13]\r\n\r\n### Added\r\n- Target factors are now also collected for nbest translations (and stored in the JSON output handler).\r\n\r\n## [2.3.12]\r\n\r\n### Added\r\n- Added `--config` option to `prepare_data` CLI to allow setting commandline flags via a yaml config. \r\n- Flags for the `prepare_data` CLI are now stored in the output folder under `args.yaml`\r\n  (equivalent to the behavior of `sockeye_train`)\r\n\r\n## [2.3.11]\r\n\r\n### Added\r\n- Added option `prevent_unk` to avoid generating `<unk>` token in beam search.", "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/2.3.14", "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/2.3.14", "html_url": "https://github.com/awslabs/sockeye/releases/tag/2.3.14", "url": "https://api.github.com/repos/awslabs/sockeye/releases/41053485", "release_id": 41053485, "date_created": "2021-04-06T12:21:35Z", "date_published": "2021-04-07T11:50:08Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/awslabs/sockeye/releases/37730556", "tag": "2.3.10", "name": "2.3.10", "author": {"name": "fhieber", "type": "User"}, "description": "## [2.3.10]\r\n\r\n### Changed\r\n\r\n- Make sure that the top N best params files retained, even if N > --keep-last-params. This ensures that model\r\n  averaging will not be crippled when keeping only a few params files during training. This can result in a\r\n  significant savings of disk space during training.\r\n\r\n## [2.3.9]\r\n\r\n### Added\r\n\r\n- Added scripts for processing Sockeye benchmark output (`--output-type benchmark`):\r\n  - [benchmark_to_output.py](sockeye_contrib/benchmark/benchmark_to_output.py) extracts translations\r\n  - [benchmark_to_percentiles.py](sockeye_contrib/benchmark/benchmark_to_percentiles.py) computes percentiles", "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/2.3.10", "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/2.3.10", "html_url": "https://github.com/awslabs/sockeye/releases/tag/2.3.10", "url": "https://api.github.com/repos/awslabs/sockeye/releases/37730556", "release_id": 37730556, "date_created": "2021-02-05T10:25:14Z", "date_published": "2021-02-08T10:00:44Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/awslabs/sockeye/releases/36130087", "tag": "2.3.8", "name": "2.3.8", "author": {"name": "fhieber", "type": "User"}, "description": "## [2.3.8]\r\n\r\n### Fixed\r\n\r\n- Fix problem identified in issue #925 that caused learning rate\r\n  warmup to fail in some instances when doing continued training\r\n\r\n## [2.3.7]\r\n\r\n### Changed\r\n\r\n- Use dataclass module to simplify Config classes. No functional change.\r\n\r\n## [2.3.6]\r\n\r\n### Fixed\r\n\r\n- Fixes the problem identified in issue #890, where the lr_scheduler\r\n  does not behave as expected when continuing training. The problem is\r\n  that the lr_scheduler is kept as part of the optimizer, but the\r\n  optimizer is not saved when saving state. Therefore, every time\r\n  training is restarted, a new lr_scheduler is created with initial\r\n  parameter settings. Fix by saving and restoring the lr_scheduling\r\n  separately.\r\n\r\n## [2.3.5]\r\n\r\n### Fixed\r\n\r\n- Fixed issue with LearningRateSchedulerPlateauReduce.__repr__ printing\r\n    out num_not_improved instead of reduce_num_not_improved.\r\n\r\n## [2.3.4]\r\n\r\n### Fixed\r\n\r\n- Fixed issue with dtype mismatch in beam search when translating with `--dtype float16`.\r\n\r\n## [2.3.3]\r\n\r\n### Changed\r\n\r\n- Upgraded `SacreBLEU` dependency of Sockeye to a newer version (`1.4.14`).\r\n", "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/2.3.8", "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/2.3.8", "html_url": "https://github.com/awslabs/sockeye/releases/tag/2.3.8", "url": "https://api.github.com/repos/awslabs/sockeye/releases/36130087", "release_id": 36130087, "date_created": "2021-01-08T08:07:29Z", "date_published": "2021-01-08T08:10:37Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/awslabs/sockeye/releases/34117464", "tag": "2.3.2", "name": "2.3.2", "author": {"name": "fhieber", "type": "User"}, "description": "## [2.3.2]\r\n### Fixed\r\n\r\n- Fixed edge case that unintentionally skips softmax for sampling if beam size is 1.\r\n\r\n## [2.3.1]\r\n### Fixed\r\n\r\n- Optimizing for BLEU/CHRF with horovod required the secondary workers to also create checkpoint decoders.\r\n\r\n## [2.3.0]\r\n\r\n### Added\r\n\r\n- Added support for target factors.\r\n  If provided with additional target-side tokens/features (token-parallel to the regular target-side) at training time,\r\n  the model can now learn to predict these in a multi-task setting. You can provide target factor data similar to source\r\n  factors: `--target-factors <factor_file1> [<factor_fileN>]`. During training, Sockeye optimizes one loss per factor\r\n  in a multi-task setting. The weight of the losses can be controlled by `--target-factors-weight`.\r\n  At inference, target factors are decoded greedily, they do not participate in beam search.\r\n  The predicted factor at each time step is the argmax over its separate output\r\n  layer distribution. To receive the target factor predictions at inference time, use\r\n  `--output-type translation_with_factors`. \r\n  \r\n### Changed\r\n\r\n- `load_model(s)` now returns a list of target vocabs.\r\n- Default source factor combination changed to `sum` (was `concat` before).\r\n- `SockeyeModel` class has three new properties: `num_target_factors`, `target_factor_configs`,\r\n  and `factor_output_layers`.", "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/2.3.2", "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/2.3.2", "html_url": "https://github.com/awslabs/sockeye/releases/tag/2.3.2", "url": "https://api.github.com/repos/awslabs/sockeye/releases/34117464", "release_id": 34117464, "date_created": "2020-11-10T16:02:33Z", "date_published": "2020-11-18T13:41:36Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/awslabs/sockeye/releases/33504760", "tag": "2.2.8", "name": "2.2.8", "author": {"name": "fhieber", "type": "User"}, "description": "## [2.2.8]\r\n\r\n### Changed\r\n- Make source/target data parameters required for the scoring CLI to avoid cryptic error messages.\r\n\r\n\r\n## [2.2.7]\r\n\r\n### Added\r\n- Added an argument to specify the log level of secondary workers. Defaults to ERROR to hide any logs except for exceptions.\r\n\r\n\r\n## [2.2.6]\r\n\r\n### Fixed\r\n- Avoid a crash due to an edge case when no model improvement has been observed by the time the learning rate gets reduced for the first time.\r\n\r\n## [2.2.5]\r\n\r\n### Fixed\r\n- Enforce sentence batching for sockeye score tool, set default batch size to 56\r\n\r\n## [2.2.4]\r\n\r\n### Changed\r\n- Use softmax with length in DotAttentionCell.\r\n- Use `contrib.arange_like` in AutoRegressiveBias block to reduce number of ops.\r\n\r\n## [2.2.3]\r\n\r\n### Added\r\n\r\n- Log the absolute number of `<unk>` tokens in source and target data\r\n\r\n## [2.2.2]\r\n\r\n### Fixed\r\n\r\n- Fix: Guard against null division for small batch sizes.\r\n\r\n## [2.2.1]\r\n\r\n## Fixed\r\n\r\n- Fixes a corner case bug by which the beam decoder can wrongly return a best hypothesis with -infinite score.\r\n", "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/2.2.8", "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/2.2.8", "html_url": "https://github.com/awslabs/sockeye/releases/tag/2.2.8", "url": "https://api.github.com/repos/awslabs/sockeye/releases/33504760", "release_id": 33504760, "date_created": "2020-11-03T16:25:59Z", "date_published": "2020-11-05T14:06:49Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/awslabs/sockeye/releases/32141318", "tag": "2.2.0", "name": "2.2.0", "author": {"name": "fhieber", "type": "User"}, "description": "## [2.2.0]\r\n\r\n### Changed\r\n\r\n- Replaced multi-head attention with [interleaved_matmul_encdec](https://github.com/apache/incubator-mxnet/pull/16408) operators, which removes previously needed transposes and improves performance.\r\n\r\n- Beam search states and model layers now assume time-major format.\r\n\r\n## [2.1.26]\r\n\r\n### Fixed\r\n\r\n- Fixes a backwards incompatibility introduced in 2.1.17, which would prevent models trained with prior versions to be used for inference.\r\n\r\n## [2.1.25]\r\n\r\n### Changed\r\n\r\n- Reverting PR #772 as it causes issues with `amp`.\r\n\r\n## [2.1.24]\r\n\r\n### Changed\r\n\r\n- Make sure to write a final checkpoint when stopping with `--max-updates`, `--max-samples` or `--max-num-epochs`.\r\n\r\n## [2.1.23]\r\n\r\n### Changed\r\n\r\n- Updated to [MXNet 1.7.0](https://github.com/apache/incubator-mxnet/tree/1.7.0).\r\n- Re-introduced use of softmax with length parameter in DotAttentionCell (see PR #772).\r\n\r\n## [2.1.22]\r\n\r\n### Added\r\n\r\n- Re-introduced `--softmax-temperature` flag for `sockeye.score` and `sockeye.translate`.", "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/2.2.0", "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/2.2.0", "html_url": "https://github.com/awslabs/sockeye/releases/tag/2.2.0", "url": "https://api.github.com/repos/awslabs/sockeye/releases/32141318", "release_id": 32141318, "date_created": "2020-10-02T14:18:33Z", "date_published": "2020-10-04T17:22:34Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/awslabs/sockeye/releases/30220141", "tag": "2.1.21", "name": "2.1.21", "author": {"name": "fhieber", "type": "User"}, "description": "## [2.1.21]\r\n\r\n### Added\r\n\r\n- Added an optional ability to cache encoder outputs of model.\r\n\r\n## [2.1.20]\r\n\r\n### Fixed\r\n\r\n- Fixed a bug where the training state object was saved to disk before training metrics were added to it, leading to an inconsistency between the training state object and the metrics file (see #859).\r\n\r\n## [2.1.19]\r\n\r\n### Fixed\r\n\r\n- When loading a shard in Horovod mode, there is now a check that each non-empty bucket contains enough sentences to cover each worker's slice. If not, the bucket's sentences are replicated to guarantee coverage.\r\n\r\n## [2.1.18]\r\n\r\n### Fixed\r\n\r\n- Fixed a bug where sampling translation fails because an array is created in the wrong context.", "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/2.1.21", "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/2.1.21", "html_url": "https://github.com/awslabs/sockeye/releases/tag/2.1.21", "url": "https://api.github.com/repos/awslabs/sockeye/releases/30220141", "release_id": 30220141, "date_created": "2020-08-27T13:30:01Z", "date_published": "2020-08-27T13:31:17Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/awslabs/sockeye/releases/29943526", "tag": "2.1.17", "name": "2.1.17", "author": {"name": "fhieber", "type": "User"}, "description": "## [2.1.17]\r\n\r\n### Added\r\n\r\n- Added `layers.SSRU`, which implements a Simpler Simple Recurrent Unit as described in\r\nKim et al, \"From Research to Production and Back: Ludicrously Fast Neural Machine Translation\" WNGT 2019.\r\n\r\n- Added `ssru_transformer` option to `--decoder`, which enables the usage of SSRUs as a replacement for the decoder-side self-attention layers.\r\n\r\n### Changed\r\n\r\n- Reduced the number of arguments for `MultiHeadSelfAttention.hybrid_forward()`.\r\n `previous_keys` and `previous_values` should now be input together as `previous_states`, a list containing two symbols.", "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/2.1.17", "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/2.1.17", "html_url": "https://github.com/awslabs/sockeye/releases/tag/2.1.17", "url": "https://api.github.com/repos/awslabs/sockeye/releases/29943526", "release_id": 29943526, "date_created": "2020-08-20T18:20:06Z", "date_published": "2020-08-20T18:25:24Z"}, "confidence": 1, "technique": "GitHub_API"}], "readme_url": [{"result": {"value": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}], "contributing_guidelines": [{"result": {"value": "[CONTRIBUTING](docs/development.md)", "type": "File_dump"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/CONTRIBUTING.md"}], "has_script_file": [{"result": {"value": "https://raw.githubusercontent.com/awslabs/sockeye/main/style-check.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/awslabs/sockeye/main/docs/tutorials/multilingual/prepare-iwslt17-multilingual.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/awslabs/sockeye/main/sockeye_contrib/fast_align/build.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/awslabs/sockeye/main/sockeye_contrib/fast_align/lex_table.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}], "documentation": [{"result": {"value": "https://github.com/awslabs/sockeye/tree/main/docs", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "- For information on how to use Sockeye, please visit [our documentation](https://awslabs.github.io/sockeye/).\n- Developers may be interested in our [developer guidelines](https://awslabs.github.io/sockeye/development.html).\n", "type": "Text_excerpt", "original_header": "Documentation", "parent_header": ["Sockeye"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}, {"result": {"value": "- Sockeye 3.0, based on PyTorch & MXNet 2.x is available in the `sockeye_30` branch.\n- Sockeye 2.x, based on the MXNet Gluon API, is available in the `sockeye_2` branch.\n- Sockeye 1.x, based on the MXNet Module API, is available in the `sockeye_1` branch.\n", "type": "Text_excerpt", "original_header": "Older versions", "parent_header": ["Sockeye", "Documentation"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}, {"result": {"type": "Url", "value": "http://sockeye.readthedocs.io/", "format": "readthedocs"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}], "has_build_file": [{"result": {"value": "https://raw.githubusercontent.com/awslabs/sockeye/main/sockeye_contrib/fast_align/Dockerfile", "type": "Url", "format": "dockerfile"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/sockeye_contrib/fast_align/Dockerfile"}], "support": [{"result": {"value": "Starting with version 3.0.0, Sockeye is also based on PyTorch. We maintain backwards compatibility with\nMXNet models of version 2.3.x with 3.0.x. If MXNet 2.x is installed, Sockeye can run both with PyTorch or MXNet.\n\nAll models trained with 2.3.x (using MXNet)\ncan be converted to models running with PyTorch using the converter CLI (`sockeye.mx_to_pt`). This will\ncreate a PyTorch parameter file (`<model>/params.best`) and backup the existing MXNet parameter\nfile to `<model>/params.best.mx`. Note that this only applies to fully-trained models that are to be used\nfor inference. Continued training of an MXNet model with PyTorch is not supported\n(because we do not convert training and optimizer states).\n`sockeye.mx_to_pt` requires MXNet to be installed into the environment.\n\nAll CLIs of Version 3.0.0 now use PyTorch by default, e.g. `sockeye-{train,translate,score}`.\nMXNet-based CLIs/modules are still operational and accessible via `sockeye-{train,translate,score}-mx`.\n\nSockeye 3 can be installed and run without MXNet, but if installed, an extended test suite is executed to ensure\nequivalence between PyTorch and MXNet models. Note that running Sockeye 3.0.0 with MXNet requires MXNet 2.x to be\ninstalled (`pip install --pre -f https://dist.mxnet.io/python 'mxnet>=2.0.0b2021'`)\n", "type": "Text_excerpt", "original_header": "Version 3.0.0: Concurrent PyTorch and MXNet support", "parent_header": ["Sockeye"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}], "installation": [{"result": {"value": "Download the current version of Sockeye:\n```bash\ngit clone https://github.com/awslabs/sockeye.git\n```\n\nInstall the sockeye module and its dependencies:\n```bash\ncd sockeye && pip3 install --editable .\n```\n\nFor faster GPU training, install [NVIDIA Apex](https://github.com/NVIDIA/apex). NVIDIA also provides [PyTorch Docker containers](https://ngc.nvidia.com/catalog/containers/nvidia:pytorch) that include Apex.\n", "type": "Text_excerpt", "original_header": "Installation", "parent_header": ["Sockeye"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "With version 3.1.x, we remove support for MXNet 2.x. Models trained with PyTorch and Sockeye 3.0.x remain compatible\nwith Sockeye 3.1.x. Models trained with 2.3.x (using MXNet) and converted to PyTorch with Sockeye 3.0.x's conversion\ntool can NOT be used with Sockeye 3.1.x.\n \n", "original_header": "Version 3.1.x: PyTorch only"}, "confidence": 0.8399007860054933, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}], "citation": [{"result": {"value": "For more information about Sockeye, see our papers ([BibTeX](sockeye.bib)).\n", "type": "Text_excerpt", "original_header": "Citation", "parent_header": ["Sockeye"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}, {"result": {"value": "> Felix Hieber, Michael Denkowski, Tobias Domhan, Barbara Darques Barros, Celina Dong Ye, Xing Niu, Cuong Hoang, Ke Tran, Benjamin Hsu, Maria Nadejde, Surafel Lakew, Prashant Mathur, Anna Currey, Marcello Federico.\n> [Sockeye 3: Fast Neural Machine Translation with PyTorch](https://arxiv.org/abs/2207.05851). ArXiv e-prints.\n", "type": "Text_excerpt", "original_header": "Sockeye 3.x", "parent_header": ["Sockeye", "Citation"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}, {"result": {"value": "> Tobias Domhan, Michael Denkowski, David Vilar, Xing Niu, Felix Hieber, Kenneth Heafield.\n> [The Sockeye 2 Neural Machine Translation Toolkit at AMTA 2020](https://www.aclweb.org/anthology/2020.amta-research.10/). Proceedings of the 14th Conference of the Association for Machine Translation in the Americas (AMTA'20).\n\n> Felix Hieber, Tobias Domhan, Michael Denkowski, David Vilar.\n> [Sockeye 2: A Toolkit for Neural Machine Translation](https://www.amazon.science/publications/sockeye-2-a-toolkit-for-neural-machine-translation). Proceedings of the 22nd Annual Conference of the European Association for Machine Translation, Project Track (EAMT'20).\n", "type": "Text_excerpt", "original_header": "Sockeye 2.x", "parent_header": ["Sockeye", "Citation"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}, {"result": {"value": "> Felix Hieber, Tobias Domhan, Michael Denkowski, David Vilar, Artem Sokolov, Ann Clifton, Matt Post.\n> [The Sockeye Neural Machine Translation Toolkit at AMTA 2018](https://www.aclweb.org/anthology/W18-1820/). Proceedings of the 13th Conference of the Association for Machine Translation in the Americas  (AMTA'18).\n>\n> Felix Hieber, Tobias Domhan, Michael Denkowski, David Vilar, Artem Sokolov, Ann Clifton and Matt Post. 2017.\n> [Sockeye: A Toolkit for Neural Machine Translation](https://arxiv.org/abs/1712.05690). ArXiv e-prints.\n", "type": "Text_excerpt", "original_header": "Sockeye 1.x", "parent_header": ["Sockeye", "Citation"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}], "application_domain": [{"result": {"type": "String", "value": "Natural Language Processing"}, "confidence": 0.9689878407317334, "technique": "supervised_classification"}], "full_title": [{"result": {"type": "String", "value": "Sockeye"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}], "related_papers": [{"result": {"type": "Url", "value": "https://arxiv.org/abs/2101.10035 (2021)\n* Briakou, Eleftheria, Marine Carpuat. \"Beyond Noise: Mitigating the Impact of Fine-grained Semantic Divergences on Neural Machine Translation\". arXiv preprint https://arxiv.org/abs/2105.15087 (2021)\n* Hasler, Eva, Tobias Domhan, Sony Trenous, Ke Tran, Bill Byrne, Felix Hieber. \"Improving the Quality Trade-Off for Neural Machine Translation Multi-Domain Adaptation\". Proceedings of EMNLP (2021)\n* Tang, Gongbo, Philipp R\u00f6nchen, Rico Sennrich, Joakim Nivre. \"Revisiting Negation in Neural Machine Translation\". Transactions of the Association for Computation Linguistics 9 (2021)\n* Vu, Thuy, Alessandro Moschitti. \"Machine Translation Customization via Automatic Training Data Selection from the Web\". arXiv preprint https://arxiv.org/abs/2102.1024 (2021)\n* Xu, Weijia, Marine Carpuat. \"EDITOR: An Edit-Based Transformer with Repositioning for Neural Machine Translation with Soft Lexical Constraints.\" Transactions of the Association for Computation Linguistics 9 (2021)\n* M\u00fcller, Mathias, Rico Sennrich. \"Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation\". Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) (2021)\n* Popovi\u0107, Maja, Alberto Poncelas. \"On Machine Translation of User Reviews.\" Proceedings of RANLP (2021)\n* Popovi\u0107, Maja. \"On nature and causes of observed MT errors.\" Proceedings of the 18th MT Summit (Volume 1: Research Track) (2021)\n* Jain, Nishtha, Maja Popovi\u0107, Declan Groves, Eva Vanmassenhove. \"Generating Gender Augmented Data for NLP.\" Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing (2021)\n* Vilar, David, Marcello Federico. \"A Statistical Extension of Byte-Pair Encoding.\" Proceedings of IWSLT (2021)\n\n### 2020\n\n* Dinu, Georgiana, Prashant Mathur, Marcello Federico, Stanislas Lauly, Yaser Al-Onaizan. \"Joint translation and unit conversion for end-to-end localization.\" Proceedings of IWSLT (2020)\n* Exel, Miriam, Bianka Buschbeck, Lauritz Brandt, Simona Doneva. \"Terminology-Constrained Neural Machine Translation at SAP\". Proceedings of EAMT (2020).\n* Hisamoto, Sorami, Matt Post, Kevin Duh. \"Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data In Your Machine Translation System?\" Transactions of the Association for Computational Linguistics, Volume 8 (2020)\n* Naradowsky, Jason, Xuan Zhan, Kevin Duh. \"Machine Translation System Selection from Bandit Feedback.\" arXiv preprint https://arxiv.org/abs/2002.09646 (2020)\n* Niu, Xing, Prashant Mathur, Georgiana Dinu, Yaser Al-Onaizan. \"Evaluating Robustness to Input Perturbations for Neural Machine Translation\". arXiv preprint \thttps://arxiv.org/abs/2005.00580 (2020)\n* Niu, Xing, Marine Carpuat. \"Controlling Neural Machine Translation Formality with Synthetic Supervision.\" Proceedings of AAAI (2020)\n* Keung, Phillip, Julian Salazar, Yichao Liu, Noah A. Smith. \"Unsupervised Bitext Mining and Translation\nvia Self-Trained Contextual Embeddings.\" arXiv preprint https://arxiv.org/abs/2010.07761 (2020).\n* Sokolov, Alex, Tracy Rohlin, Ariya Rastrow. \"Neural Machine Translation for Multilingual Grapheme-to-Phoneme Conversion.\" arXiv preprint https://arxiv.org/abs/2006.14194 (2020)\n* Stafanovi\u010ds, Art\u016brs, Toms Bergmanis, M\u0101rcis Pinnis. \"Mitigating Gender Bias in Machine Translation with Target Gender\nAnnotations.\" arXiv preprint https://arxiv.org/abs/2010.06203 (2020)\n* Stojanovski, Dario, Alexander Fraser. \"Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation.\" arXiv preprint arXiv preprint https://arxiv.org/abs/2004.14927 (2020)\n* Stojanovski, Dario, Benno Krojer, Denis Peskov, Alexander Fraser. \"ContraCAT: Contrastive Coreference Analytical Templates for Machine Translation\". Proceedings of COLING (2020)\n* Zhang, Xuan, Kevin Duh. \"Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems.\" Transactions of the Association for Computational Linguistics, Volume 8 (2020)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant, Nandar Win Min, and Thepchai Supnithi, \"Unsupervised Neural Machine Translation between Myanmar Sign Language and Myanmar Language\", Journal of Intelligent Informatics and Smart Technology, April 1st Issue, 2020, pp. 53-61. (Submitted December 21, 2019; accepted March 6, 2020; revised March 16, 2020; published online April 30, 2020)\n* Thazin Myint Oo, Ye Kyaw Thu, Khin Mar Soe and Thepchai Supnithi, \"Neural Machine Translation between Myanmar (Burmese) and Dawei (Tavoyan)\", In Proceedings of the 18th International Conference on Computer Applications (ICCA 2020), Feb 27-28, 2020, Yangon, Myanmar, pp. 219-227\n* M\u00fcller, Mathias, Annette Rios, Rico Sennrich. \"Domain Robustness in Neural Machine Translation.\" Proceedings of AMTA (2020)\n* Rios, Annette, Mathias M\u00fcller, Rico Sennrich. \"Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Neural Machine Translation between similar South-Slavic languages.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Extracting correctly aligned segments from unclean parallel data using character n-gram matching.\" Proceedings of Conference on Language Technologies & Digital Humanities (JTDH 2020).\n* Popovi\u0107, Maja, Alberto Poncelas, Marija Brkic, Andy Way. \"Neural Machine Translation for translating into Croatian and Serbian.\" Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects (2020)\n\n### 2019\n\n* Agrawal, Sweta, Marine Carpuat. \"Controlling Text Complexity in Neural Machine Translation.\" Proceedings of EMNLP (2019)\n* Beck, Daniel, Trevor Cohn, Gholamreza Haffari. \"Neural Speech Translation using Lattice Transformations and Graph Networks.\" Proceedings of TextGraphs-13 (EMNLP 2019)\n* Currey, Anna, Kenneth Heafield. \"Zero-Resource Neural Machine Translation with Monolingual Pivot Data.\" Proceedings of EMNLP (2019)\n* Gupta, Prabhakar, Mayank Sharma. \"Unsupervised Translation Quality Estimation for Digital Entertainment Content Subtitles.\" IEEE International Journal of Semantic Computing (2019)\n* Hu, J. Edward, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt Post, and Benjamin Van Durme. \"Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting.\" Proceedings of NAACL-HLT (2019)\n* Rosendahl, Jan, Christian Herold, Yunsu Kim, Miguel Gra\u00e7a,Weiyue Wang, Parnia Bahar, Yingbo Gao and Hermann Ney \u201cThe RWTH Aachen University Machine Translation Systems for WMT 2019\u201d Proceedings of the 4th WMT: Research Papers (2019)\n* Thompson, Brian, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh, and Philipp Koehn. \"Overcoming catastrophic forgetting during domain adaptation of neural machine translation.\" Proceedings of NAACL-HLT 2019 (2019)\n* T\u00e4ttar, Andre, Elizaveta Korotkova, Mark Fishel \u201cUniversity of Tartu\u2019s Multilingual Multi-domain WMT19 News Translation Shared Task Submission\u201d Proceedings of 4th WMT: Research Papers (2019)\n* Thazin Myint Oo, Ye Kyaw Thu and Khin Mar Soe, \"Neural Machine Translation between Myanmar (Burmese) and Rakhine (Arakanese)\", In Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects, NAACL-2019, June 7th 2019, Minneapolis, United States, pp. 80-88\n\n### 2018\n\n* Domhan, Tobias. \"How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures\". Proceedings of 56th ACL (2018)\n* Kim, Yunsu, Yingbo Gao, and Hermann Ney. \"Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies.\" arXiv preprint https://arxiv.org/abs/1905.05475 (2019)\n* Korotkova, Elizaveta, Maksym Del, and Mark Fishel. \"Monolingual and Cross-lingual Zero-shot Style Transfer.\" arXiv preprint https://arxiv.org/abs/1808.00179 (2018)\n* Niu, Xing, Michael Denkowski, and Marine Carpuat. \"Bi-directional neural machine translation with synthetic parallel data.\" arXiv preprint https://arxiv.org/abs/1805.11213 (2018)\n* Niu, Xing, Sudha Rao, and Marine Carpuat. \"Multi-Task Neural Models for Translating Between Styles Within and Across Languages.\" COLING (2018)\n* Post, Matt and David Vilar. \"Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation.\" Proceedings of NAACL-HLT (2018)\n* Schamper, Julian, Jan Rosendahl, Parnia Bahar, Yunsu Kim, Arne Nix, and Hermann Ney. \"The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018.\" Proceedings of the 3rd WMT: Shared Task Papers (2018)\n* Schulz, Philip, Wilker Aziz, and Trevor Cohn. \"A stochastic decoder for neural machine translation.\" arXiv preprint https://arxiv.org/abs/1805.10844 (2018)\n* Tamer, Alkouli, Gabriel Bretschner, and Hermann Ney. \"On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation.\" Proceedings of the 3rd WMT: Research Papers (2018)\n* Tang, Gongbo, Rico Sennrich, and Joakim Nivre. \"An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation.\" Proceedings of 3rd WMT: Research Papers (2018)\n* Thompson, Brian, Huda Khayrallah, Antonios Anastasopoulos, Arya McCarthy, Kevin Duh, Rebecca Marvin, Paul McNamee, Jeremy Gwinnup, Tim Anderson, and Philipp Koehn. \"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1809.05218 (2018)\n* Vilar, David. \"Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.\" Proceedings of NAACL-HLT (2018)\n* Vyas, Yogarshi, Xing Niu and Marine Carpuat \u201cIdentifying Semantic Divergences in Parallel Text without Annotations\u201d. Proceedings of NAACL-HLT (2018)\n* Wang, Weiyue, Derui Zhu, Tamer Alkhouli, Zixuan Gan, and Hermann Ney. \"Neural Hidden Markov Model for Machine Translation\". Proceedings of 56th ACL (2018)\n* Zhang, Xuan, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. \"An Empirical Exploration of Curriculum Learning for Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017)."}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1809.05218 (2018)\n* Vilar, David. \"Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.\" Proceedings of NAACL-HLT (2018)\n* Vyas, Yogarshi, Xing Niu and Marine Carpuat \u201cIdentifying Semantic Divergences in Parallel Text without Annotations\u201d. Proceedings of NAACL-HLT (2018)\n* Wang, Weiyue, Derui Zhu, Tamer Alkhouli, Zixuan Gan, and Hermann Ney. \"Neural Hidden Markov Model for Machine Translation\". Proceedings of 56th ACL (2018)\n* Zhang, Xuan, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. \"An Empirical Exploration of Curriculum Learning for Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017)."}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2005.00580 (2020)\n* Niu, Xing, Marine Carpuat. \"Controlling Neural Machine Translation Formality with Synthetic Supervision.\" Proceedings of AAAI (2020)\n* Keung, Phillip, Julian Salazar, Yichao Liu, Noah A. Smith. \"Unsupervised Bitext Mining and Translation\nvia Self-Trained Contextual Embeddings.\" arXiv preprint https://arxiv.org/abs/2010.07761 (2020).\n* Sokolov, Alex, Tracy Rohlin, Ariya Rastrow. \"Neural Machine Translation for Multilingual Grapheme-to-Phoneme Conversion.\" arXiv preprint https://arxiv.org/abs/2006.14194 (2020)\n* Stafanovi\u010ds, Art\u016brs, Toms Bergmanis, M\u0101rcis Pinnis. \"Mitigating Gender Bias in Machine Translation with Target Gender\nAnnotations.\" arXiv preprint https://arxiv.org/abs/2010.06203 (2020)\n* Stojanovski, Dario, Alexander Fraser. \"Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation.\" arXiv preprint arXiv preprint https://arxiv.org/abs/2004.14927 (2020)\n* Stojanovski, Dario, Benno Krojer, Denis Peskov, Alexander Fraser. \"ContraCAT: Contrastive Coreference Analytical Templates for Machine Translation\". Proceedings of COLING (2020)\n* Zhang, Xuan, Kevin Duh. \"Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems.\" Transactions of the Association for Computational Linguistics, Volume 8 (2020)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant, Nandar Win Min, and Thepchai Supnithi, \"Unsupervised Neural Machine Translation between Myanmar Sign Language and Myanmar Language\", Journal of Intelligent Informatics and Smart Technology, April 1st Issue, 2020, pp. 53-61. (Submitted December 21, 2019; accepted March 6, 2020; revised March 16, 2020; published online April 30, 2020)\n* Thazin Myint Oo, Ye Kyaw Thu, Khin Mar Soe and Thepchai Supnithi, \"Neural Machine Translation between Myanmar (Burmese) and Dawei (Tavoyan)\", In Proceedings of the 18th International Conference on Computer Applications (ICCA 2020), Feb 27-28, 2020, Yangon, Myanmar, pp. 219-227\n* M\u00fcller, Mathias, Annette Rios, Rico Sennrich. \"Domain Robustness in Neural Machine Translation.\" Proceedings of AMTA (2020)\n* Rios, Annette, Mathias M\u00fcller, Rico Sennrich. \"Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Neural Machine Translation between similar South-Slavic languages.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Extracting correctly aligned segments from unclean parallel data using character n-gram matching.\" Proceedings of Conference on Language Technologies & Digital Humanities (JTDH 2020).\n* Popovi\u0107, Maja, Alberto Poncelas, Marija Brkic, Andy Way. \"Neural Machine Translation for translating into Croatian and Serbian.\" Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects (2020)\n\n### 2019\n\n* Agrawal, Sweta, Marine Carpuat. \"Controlling Text Complexity in Neural Machine Translation.\" Proceedings of EMNLP (2019)\n* Beck, Daniel, Trevor Cohn, Gholamreza Haffari. \"Neural Speech Translation using Lattice Transformations and Graph Networks.\" Proceedings of TextGraphs-13 (EMNLP 2019)\n* Currey, Anna, Kenneth Heafield. \"Zero-Resource Neural Machine Translation with Monolingual Pivot Data.\" Proceedings of EMNLP (2019)\n* Gupta, Prabhakar, Mayank Sharma. \"Unsupervised Translation Quality Estimation for Digital Entertainment Content Subtitles.\" IEEE International Journal of Semantic Computing (2019)\n* Hu, J. Edward, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt Post, and Benjamin Van Durme. \"Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting.\" Proceedings of NAACL-HLT (2019)\n* Rosendahl, Jan, Christian Herold, Yunsu Kim, Miguel Gra\u00e7a,Weiyue Wang, Parnia Bahar, Yingbo Gao and Hermann Ney \u201cThe RWTH Aachen University Machine Translation Systems for WMT 2019\u201d Proceedings of the 4th WMT: Research Papers (2019)\n* Thompson, Brian, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh, and Philipp Koehn. \"Overcoming catastrophic forgetting during domain adaptation of neural machine translation.\" Proceedings of NAACL-HLT 2019 (2019)\n* T\u00e4ttar, Andre, Elizaveta Korotkova, Mark Fishel \u201cUniversity of Tartu\u2019s Multilingual Multi-domain WMT19 News Translation Shared Task Submission\u201d Proceedings of 4th WMT: Research Papers (2019)\n* Thazin Myint Oo, Ye Kyaw Thu and Khin Mar Soe, \"Neural Machine Translation between Myanmar (Burmese) and Rakhine (Arakanese)\", In Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects, NAACL-2019, June 7th 2019, Minneapolis, United States, pp. 80-88\n\n### 2018\n\n* Domhan, Tobias. \"How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures\". Proceedings of 56th ACL (2018)\n* Kim, Yunsu, Yingbo Gao, and Hermann Ney. \"Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies.\" arXiv preprint https://arxiv.org/abs/1905.05475 (2019)\n* Korotkova, Elizaveta, Maksym Del, and Mark Fishel. \"Monolingual and Cross-lingual Zero-shot Style Transfer.\" arXiv preprint https://arxiv.org/abs/1808.00179 (2018)\n* Niu, Xing, Michael Denkowski, and Marine Carpuat. \"Bi-directional neural machine translation with synthetic parallel data.\" arXiv preprint https://arxiv.org/abs/1805.11213 (2018)\n* Niu, Xing, Sudha Rao, and Marine Carpuat. \"Multi-Task Neural Models for Translating Between Styles Within and Across Languages.\" COLING (2018)\n* Post, Matt and David Vilar. \"Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation.\" Proceedings of NAACL-HLT (2018)\n* Schamper, Julian, Jan Rosendahl, Parnia Bahar, Yunsu Kim, Arne Nix, and Hermann Ney. \"The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018.\" Proceedings of the 3rd WMT: Shared Task Papers (2018)\n* Schulz, Philip, Wilker Aziz, and Trevor Cohn. \"A stochastic decoder for neural machine translation.\" arXiv preprint https://arxiv.org/abs/1805.10844 (2018)\n* Tamer, Alkouli, Gabriel Bretschner, and Hermann Ney. \"On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation.\" Proceedings of the 3rd WMT: Research Papers (2018)\n* Tang, Gongbo, Rico Sennrich, and Joakim Nivre. \"An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation.\" Proceedings of 3rd WMT: Research Papers (2018)\n* Thompson, Brian, Huda Khayrallah, Antonios Anastasopoulos, Arya McCarthy, Kevin Duh, Rebecca Marvin, Paul McNamee, Jeremy Gwinnup, Tim Anderson, and Philipp Koehn. \"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1809.05218 (2018)\n* Vilar, David. \"Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.\" Proceedings of NAACL-HLT (2018)\n* Vyas, Yogarshi, Xing Niu and Marine Carpuat \u201cIdentifying Semantic Divergences in Parallel Text without Annotations\u201d. Proceedings of NAACL-HLT (2018)\n* Wang, Weiyue, Derui Zhu, Tamer Alkhouli, Zixuan Gan, and Hermann Ney. \"Neural Hidden Markov Model for Machine Translation\". Proceedings of 56th ACL (2018)\n* Zhang, Xuan, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. \"An Empirical Exploration of Curriculum Learning for Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017)."}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2010.07761 (2020).\n* Sokolov, Alex, Tracy Rohlin, Ariya Rastrow. \"Neural Machine Translation for Multilingual Grapheme-to-Phoneme Conversion.\" arXiv preprint https://arxiv.org/abs/2006.14194 (2020)\n* Stafanovi\u010ds, Art\u016brs, Toms Bergmanis, M\u0101rcis Pinnis. \"Mitigating Gender Bias in Machine Translation with Target Gender\nAnnotations.\" arXiv preprint https://arxiv.org/abs/2010.06203 (2020)\n* Stojanovski, Dario, Alexander Fraser. \"Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation.\" arXiv preprint arXiv preprint https://arxiv.org/abs/2004.14927 (2020)\n* Stojanovski, Dario, Benno Krojer, Denis Peskov, Alexander Fraser. \"ContraCAT: Contrastive Coreference Analytical Templates for Machine Translation\". Proceedings of COLING (2020)\n* Zhang, Xuan, Kevin Duh. \"Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems.\" Transactions of the Association for Computational Linguistics, Volume 8 (2020)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant, Nandar Win Min, and Thepchai Supnithi, \"Unsupervised Neural Machine Translation between Myanmar Sign Language and Myanmar Language\", Journal of Intelligent Informatics and Smart Technology, April 1st Issue, 2020, pp. 53-61. (Submitted December 21, 2019; accepted March 6, 2020; revised March 16, 2020; published online April 30, 2020)\n* Thazin Myint Oo, Ye Kyaw Thu, Khin Mar Soe and Thepchai Supnithi, \"Neural Machine Translation between Myanmar (Burmese) and Dawei (Tavoyan)\", In Proceedings of the 18th International Conference on Computer Applications (ICCA 2020), Feb 27-28, 2020, Yangon, Myanmar, pp. 219-227\n* M\u00fcller, Mathias, Annette Rios, Rico Sennrich. \"Domain Robustness in Neural Machine Translation.\" Proceedings of AMTA (2020)\n* Rios, Annette, Mathias M\u00fcller, Rico Sennrich. \"Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Neural Machine Translation between similar South-Slavic languages.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Extracting correctly aligned segments from unclean parallel data using character n-gram matching.\" Proceedings of Conference on Language Technologies & Digital Humanities (JTDH 2020).\n* Popovi\u0107, Maja, Alberto Poncelas, Marija Brkic, Andy Way. \"Neural Machine Translation for translating into Croatian and Serbian.\" Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects (2020)\n\n### 2019\n\n* Agrawal, Sweta, Marine Carpuat. \"Controlling Text Complexity in Neural Machine Translation.\" Proceedings of EMNLP (2019)\n* Beck, Daniel, Trevor Cohn, Gholamreza Haffari. \"Neural Speech Translation using Lattice Transformations and Graph Networks.\" Proceedings of TextGraphs-13 (EMNLP 2019)\n* Currey, Anna, Kenneth Heafield. \"Zero-Resource Neural Machine Translation with Monolingual Pivot Data.\" Proceedings of EMNLP (2019)\n* Gupta, Prabhakar, Mayank Sharma. \"Unsupervised Translation Quality Estimation for Digital Entertainment Content Subtitles.\" IEEE International Journal of Semantic Computing (2019)\n* Hu, J. Edward, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt Post, and Benjamin Van Durme. \"Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting.\" Proceedings of NAACL-HLT (2019)\n* Rosendahl, Jan, Christian Herold, Yunsu Kim, Miguel Gra\u00e7a,Weiyue Wang, Parnia Bahar, Yingbo Gao and Hermann Ney \u201cThe RWTH Aachen University Machine Translation Systems for WMT 2019\u201d Proceedings of the 4th WMT: Research Papers (2019)\n* Thompson, Brian, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh, and Philipp Koehn. \"Overcoming catastrophic forgetting during domain adaptation of neural machine translation.\" Proceedings of NAACL-HLT 2019 (2019)\n* T\u00e4ttar, Andre, Elizaveta Korotkova, Mark Fishel \u201cUniversity of Tartu\u2019s Multilingual Multi-domain WMT19 News Translation Shared Task Submission\u201d Proceedings of 4th WMT: Research Papers (2019)\n* Thazin Myint Oo, Ye Kyaw Thu and Khin Mar Soe, \"Neural Machine Translation between Myanmar (Burmese) and Rakhine (Arakanese)\", In Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects, NAACL-2019, June 7th 2019, Minneapolis, United States, pp. 80-88\n\n### 2018\n\n* Domhan, Tobias. \"How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures\". Proceedings of 56th ACL (2018)\n* Kim, Yunsu, Yingbo Gao, and Hermann Ney. \"Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies.\" arXiv preprint https://arxiv.org/abs/1905.05475 (2019)\n* Korotkova, Elizaveta, Maksym Del, and Mark Fishel. \"Monolingual and Cross-lingual Zero-shot Style Transfer.\" arXiv preprint https://arxiv.org/abs/1808.00179 (2018)\n* Niu, Xing, Michael Denkowski, and Marine Carpuat. \"Bi-directional neural machine translation with synthetic parallel data.\" arXiv preprint https://arxiv.org/abs/1805.11213 (2018)\n* Niu, Xing, Sudha Rao, and Marine Carpuat. \"Multi-Task Neural Models for Translating Between Styles Within and Across Languages.\" COLING (2018)\n* Post, Matt and David Vilar. \"Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation.\" Proceedings of NAACL-HLT (2018)\n* Schamper, Julian, Jan Rosendahl, Parnia Bahar, Yunsu Kim, Arne Nix, and Hermann Ney. \"The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018.\" Proceedings of the 3rd WMT: Shared Task Papers (2018)\n* Schulz, Philip, Wilker Aziz, and Trevor Cohn. \"A stochastic decoder for neural machine translation.\" arXiv preprint https://arxiv.org/abs/1805.10844 (2018)\n* Tamer, Alkouli, Gabriel Bretschner, and Hermann Ney. \"On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation.\" Proceedings of the 3rd WMT: Research Papers (2018)\n* Tang, Gongbo, Rico Sennrich, and Joakim Nivre. \"An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation.\" Proceedings of 3rd WMT: Research Papers (2018)\n* Thompson, Brian, Huda Khayrallah, Antonios Anastasopoulos, Arya McCarthy, Kevin Duh, Rebecca Marvin, Paul McNamee, Jeremy Gwinnup, Tim Anderson, and Philipp Koehn. \"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1809.05218 (2018)\n* Vilar, David. \"Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.\" Proceedings of NAACL-HLT (2018)\n* Vyas, Yogarshi, Xing Niu and Marine Carpuat \u201cIdentifying Semantic Divergences in Parallel Text without Annotations\u201d. Proceedings of NAACL-HLT (2018)\n* Wang, Weiyue, Derui Zhu, Tamer Alkhouli, Zixuan Gan, and Hermann Ney. \"Neural Hidden Markov Model for Machine Translation\". Proceedings of 56th ACL (2018)\n* Zhang, Xuan, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. \"An Empirical Exploration of Curriculum Learning for Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017)."}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2105.15087 (2021)\n* Hasler, Eva, Tobias Domhan, Sony Trenous, Ke Tran, Bill Byrne, Felix Hieber. \"Improving the Quality Trade-Off for Neural Machine Translation Multi-Domain Adaptation\". Proceedings of EMNLP (2021)\n* Tang, Gongbo, Philipp R\u00f6nchen, Rico Sennrich, Joakim Nivre. \"Revisiting Negation in Neural Machine Translation\". Transactions of the Association for Computation Linguistics 9 (2021)\n* Vu, Thuy, Alessandro Moschitti. \"Machine Translation Customization via Automatic Training Data Selection from the Web\". arXiv preprint https://arxiv.org/abs/2102.1024 (2021)\n* Xu, Weijia, Marine Carpuat. \"EDITOR: An Edit-Based Transformer with Repositioning for Neural Machine Translation with Soft Lexical Constraints.\" Transactions of the Association for Computation Linguistics 9 (2021)\n* M\u00fcller, Mathias, Rico Sennrich. \"Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation\". Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) (2021)\n* Popovi\u0107, Maja, Alberto Poncelas. \"On Machine Translation of User Reviews.\" Proceedings of RANLP (2021)\n* Popovi\u0107, Maja. \"On nature and causes of observed MT errors.\" Proceedings of the 18th MT Summit (Volume 1: Research Track) (2021)\n* Jain, Nishtha, Maja Popovi\u0107, Declan Groves, Eva Vanmassenhove. \"Generating Gender Augmented Data for NLP.\" Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing (2021)\n* Vilar, David, Marcello Federico. \"A Statistical Extension of Byte-Pair Encoding.\" Proceedings of IWSLT (2021)\n\n### 2020\n\n* Dinu, Georgiana, Prashant Mathur, Marcello Federico, Stanislas Lauly, Yaser Al-Onaizan. \"Joint translation and unit conversion for end-to-end localization.\" Proceedings of IWSLT (2020)\n* Exel, Miriam, Bianka Buschbeck, Lauritz Brandt, Simona Doneva. \"Terminology-Constrained Neural Machine Translation at SAP\". Proceedings of EAMT (2020).\n* Hisamoto, Sorami, Matt Post, Kevin Duh. \"Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data In Your Machine Translation System?\" Transactions of the Association for Computational Linguistics, Volume 8 (2020)\n* Naradowsky, Jason, Xuan Zhan, Kevin Duh. \"Machine Translation System Selection from Bandit Feedback.\" arXiv preprint https://arxiv.org/abs/2002.09646 (2020)\n* Niu, Xing, Prashant Mathur, Georgiana Dinu, Yaser Al-Onaizan. \"Evaluating Robustness to Input Perturbations for Neural Machine Translation\". arXiv preprint \thttps://arxiv.org/abs/2005.00580 (2020)\n* Niu, Xing, Marine Carpuat. \"Controlling Neural Machine Translation Formality with Synthetic Supervision.\" Proceedings of AAAI (2020)\n* Keung, Phillip, Julian Salazar, Yichao Liu, Noah A. Smith. \"Unsupervised Bitext Mining and Translation\nvia Self-Trained Contextual Embeddings.\" arXiv preprint https://arxiv.org/abs/2010.07761 (2020).\n* Sokolov, Alex, Tracy Rohlin, Ariya Rastrow. \"Neural Machine Translation for Multilingual Grapheme-to-Phoneme Conversion.\" arXiv preprint https://arxiv.org/abs/2006.14194 (2020)\n* Stafanovi\u010ds, Art\u016brs, Toms Bergmanis, M\u0101rcis Pinnis. \"Mitigating Gender Bias in Machine Translation with Target Gender\nAnnotations.\" arXiv preprint https://arxiv.org/abs/2010.06203 (2020)\n* Stojanovski, Dario, Alexander Fraser. \"Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation.\" arXiv preprint arXiv preprint https://arxiv.org/abs/2004.14927 (2020)\n* Stojanovski, Dario, Benno Krojer, Denis Peskov, Alexander Fraser. \"ContraCAT: Contrastive Coreference Analytical Templates for Machine Translation\". Proceedings of COLING (2020)\n* Zhang, Xuan, Kevin Duh. \"Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems.\" Transactions of the Association for Computational Linguistics, Volume 8 (2020)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant, Nandar Win Min, and Thepchai Supnithi, \"Unsupervised Neural Machine Translation between Myanmar Sign Language and Myanmar Language\", Journal of Intelligent Informatics and Smart Technology, April 1st Issue, 2020, pp. 53-61. (Submitted December 21, 2019; accepted March 6, 2020; revised March 16, 2020; published online April 30, 2020)\n* Thazin Myint Oo, Ye Kyaw Thu, Khin Mar Soe and Thepchai Supnithi, \"Neural Machine Translation between Myanmar (Burmese) and Dawei (Tavoyan)\", In Proceedings of the 18th International Conference on Computer Applications (ICCA 2020), Feb 27-28, 2020, Yangon, Myanmar, pp. 219-227\n* M\u00fcller, Mathias, Annette Rios, Rico Sennrich. \"Domain Robustness in Neural Machine Translation.\" Proceedings of AMTA (2020)\n* Rios, Annette, Mathias M\u00fcller, Rico Sennrich. \"Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Neural Machine Translation between similar South-Slavic languages.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Extracting correctly aligned segments from unclean parallel data using character n-gram matching.\" Proceedings of Conference on Language Technologies & Digital Humanities (JTDH 2020).\n* Popovi\u0107, Maja, Alberto Poncelas, Marija Brkic, Andy Way. \"Neural Machine Translation for translating into Croatian and Serbian.\" Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects (2020)\n\n### 2019\n\n* Agrawal, Sweta, Marine Carpuat. \"Controlling Text Complexity in Neural Machine Translation.\" Proceedings of EMNLP (2019)\n* Beck, Daniel, Trevor Cohn, Gholamreza Haffari. \"Neural Speech Translation using Lattice Transformations and Graph Networks.\" Proceedings of TextGraphs-13 (EMNLP 2019)\n* Currey, Anna, Kenneth Heafield. \"Zero-Resource Neural Machine Translation with Monolingual Pivot Data.\" Proceedings of EMNLP (2019)\n* Gupta, Prabhakar, Mayank Sharma. \"Unsupervised Translation Quality Estimation for Digital Entertainment Content Subtitles.\" IEEE International Journal of Semantic Computing (2019)\n* Hu, J. Edward, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt Post, and Benjamin Van Durme. \"Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting.\" Proceedings of NAACL-HLT (2019)\n* Rosendahl, Jan, Christian Herold, Yunsu Kim, Miguel Gra\u00e7a,Weiyue Wang, Parnia Bahar, Yingbo Gao and Hermann Ney \u201cThe RWTH Aachen University Machine Translation Systems for WMT 2019\u201d Proceedings of the 4th WMT: Research Papers (2019)\n* Thompson, Brian, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh, and Philipp Koehn. \"Overcoming catastrophic forgetting during domain adaptation of neural machine translation.\" Proceedings of NAACL-HLT 2019 (2019)\n* T\u00e4ttar, Andre, Elizaveta Korotkova, Mark Fishel \u201cUniversity of Tartu\u2019s Multilingual Multi-domain WMT19 News Translation Shared Task Submission\u201d Proceedings of 4th WMT: Research Papers (2019)\n* Thazin Myint Oo, Ye Kyaw Thu and Khin Mar Soe, \"Neural Machine Translation between Myanmar (Burmese) and Rakhine (Arakanese)\", In Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects, NAACL-2019, June 7th 2019, Minneapolis, United States, pp. 80-88\n\n### 2018\n\n* Domhan, Tobias. \"How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures\". Proceedings of 56th ACL (2018)\n* Kim, Yunsu, Yingbo Gao, and Hermann Ney. \"Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies.\" arXiv preprint https://arxiv.org/abs/1905.05475 (2019)\n* Korotkova, Elizaveta, Maksym Del, and Mark Fishel. \"Monolingual and Cross-lingual Zero-shot Style Transfer.\" arXiv preprint https://arxiv.org/abs/1808.00179 (2018)\n* Niu, Xing, Michael Denkowski, and Marine Carpuat. \"Bi-directional neural machine translation with synthetic parallel data.\" arXiv preprint https://arxiv.org/abs/1805.11213 (2018)\n* Niu, Xing, Sudha Rao, and Marine Carpuat. \"Multi-Task Neural Models for Translating Between Styles Within and Across Languages.\" COLING (2018)\n* Post, Matt and David Vilar. \"Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation.\" Proceedings of NAACL-HLT (2018)\n* Schamper, Julian, Jan Rosendahl, Parnia Bahar, Yunsu Kim, Arne Nix, and Hermann Ney. \"The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018.\" Proceedings of the 3rd WMT: Shared Task Papers (2018)\n* Schulz, Philip, Wilker Aziz, and Trevor Cohn. \"A stochastic decoder for neural machine translation.\" arXiv preprint https://arxiv.org/abs/1805.10844 (2018)\n* Tamer, Alkouli, Gabriel Bretschner, and Hermann Ney. \"On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation.\" Proceedings of the 3rd WMT: Research Papers (2018)\n* Tang, Gongbo, Rico Sennrich, and Joakim Nivre. \"An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation.\" Proceedings of 3rd WMT: Research Papers (2018)\n* Thompson, Brian, Huda Khayrallah, Antonios Anastasopoulos, Arya McCarthy, Kevin Duh, Rebecca Marvin, Paul McNamee, Jeremy Gwinnup, Tim Anderson, and Philipp Koehn. \"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1809.05218 (2018)\n* Vilar, David. \"Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.\" Proceedings of NAACL-HLT (2018)\n* Vyas, Yogarshi, Xing Niu and Marine Carpuat \u201cIdentifying Semantic Divergences in Parallel Text without Annotations\u201d. Proceedings of NAACL-HLT (2018)\n* Wang, Weiyue, Derui Zhu, Tamer Alkhouli, Zixuan Gan, and Hermann Ney. \"Neural Hidden Markov Model for Machine Translation\". Proceedings of 56th ACL (2018)\n* Zhang, Xuan, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. \"An Empirical Exploration of Curriculum Learning for Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017)."}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2203.13550 (2022)\n\n\n### 2021\n\n* Bergmanis, Toms, M\u0101rcis Pinnis. \"Facilitating Terminology Translation with Target Lemma Annotations\". arXiv preprint https://arxiv.org/abs/2101.10035 (2021)\n* Briakou, Eleftheria, Marine Carpuat. \"Beyond Noise: Mitigating the Impact of Fine-grained Semantic Divergences on Neural Machine Translation\". arXiv preprint https://arxiv.org/abs/2105.15087 (2021)\n* Hasler, Eva, Tobias Domhan, Sony Trenous, Ke Tran, Bill Byrne, Felix Hieber. \"Improving the Quality Trade-Off for Neural Machine Translation Multi-Domain Adaptation\". Proceedings of EMNLP (2021)\n* Tang, Gongbo, Philipp R\u00f6nchen, Rico Sennrich, Joakim Nivre. \"Revisiting Negation in Neural Machine Translation\". Transactions of the Association for Computation Linguistics 9 (2021)\n* Vu, Thuy, Alessandro Moschitti. \"Machine Translation Customization via Automatic Training Data Selection from the Web\". arXiv preprint https://arxiv.org/abs/2102.1024 (2021)\n* Xu, Weijia, Marine Carpuat. \"EDITOR: An Edit-Based Transformer with Repositioning for Neural Machine Translation with Soft Lexical Constraints.\" Transactions of the Association for Computation Linguistics 9 (2021)\n* M\u00fcller, Mathias, Rico Sennrich. \"Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation\". Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) (2021)\n* Popovi\u0107, Maja, Alberto Poncelas. \"On Machine Translation of User Reviews.\" Proceedings of RANLP (2021)\n* Popovi\u0107, Maja. \"On nature and causes of observed MT errors.\" Proceedings of the 18th MT Summit (Volume 1: Research Track) (2021)\n* Jain, Nishtha, Maja Popovi\u0107, Declan Groves, Eva Vanmassenhove. \"Generating Gender Augmented Data for NLP.\" Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing (2021)\n* Vilar, David, Marcello Federico. \"A Statistical Extension of Byte-Pair Encoding.\" Proceedings of IWSLT (2021)\n\n### 2020\n\n* Dinu, Georgiana, Prashant Mathur, Marcello Federico, Stanislas Lauly, Yaser Al-Onaizan. \"Joint translation and unit conversion for end-to-end localization.\" Proceedings of IWSLT (2020)\n* Exel, Miriam, Bianka Buschbeck, Lauritz Brandt, Simona Doneva. \"Terminology-Constrained Neural Machine Translation at SAP\". Proceedings of EAMT (2020).\n* Hisamoto, Sorami, Matt Post, Kevin Duh. \"Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data In Your Machine Translation System?\" Transactions of the Association for Computational Linguistics, Volume 8 (2020)\n* Naradowsky, Jason, Xuan Zhan, Kevin Duh. \"Machine Translation System Selection from Bandit Feedback.\" arXiv preprint https://arxiv.org/abs/2002.09646 (2020)\n* Niu, Xing, Prashant Mathur, Georgiana Dinu, Yaser Al-Onaizan. \"Evaluating Robustness to Input Perturbations for Neural Machine Translation\". arXiv preprint \thttps://arxiv.org/abs/2005.00580 (2020)\n* Niu, Xing, Marine Carpuat. \"Controlling Neural Machine Translation Formality with Synthetic Supervision.\" Proceedings of AAAI (2020)\n* Keung, Phillip, Julian Salazar, Yichao Liu, Noah A. Smith. \"Unsupervised Bitext Mining and Translation\nvia Self-Trained Contextual Embeddings.\" arXiv preprint https://arxiv.org/abs/2010.07761 (2020).\n* Sokolov, Alex, Tracy Rohlin, Ariya Rastrow. \"Neural Machine Translation for Multilingual Grapheme-to-Phoneme Conversion.\" arXiv preprint https://arxiv.org/abs/2006.14194 (2020)\n* Stafanovi\u010ds, Art\u016brs, Toms Bergmanis, M\u0101rcis Pinnis. \"Mitigating Gender Bias in Machine Translation with Target Gender\nAnnotations.\" arXiv preprint https://arxiv.org/abs/2010.06203 (2020)\n* Stojanovski, Dario, Alexander Fraser. \"Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation.\" arXiv preprint arXiv preprint https://arxiv.org/abs/2004.14927 (2020)\n* Stojanovski, Dario, Benno Krojer, Denis Peskov, Alexander Fraser. \"ContraCAT: Contrastive Coreference Analytical Templates for Machine Translation\". Proceedings of COLING (2020)\n* Zhang, Xuan, Kevin Duh. \"Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems.\" Transactions of the Association for Computational Linguistics, Volume 8 (2020)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant, Nandar Win Min, and Thepchai Supnithi, \"Unsupervised Neural Machine Translation between Myanmar Sign Language and Myanmar Language\", Journal of Intelligent Informatics and Smart Technology, April 1st Issue, 2020, pp. 53-61. (Submitted December 21, 2019; accepted March 6, 2020; revised March 16, 2020; published online April 30, 2020)\n* Thazin Myint Oo, Ye Kyaw Thu, Khin Mar Soe and Thepchai Supnithi, \"Neural Machine Translation between Myanmar (Burmese) and Dawei (Tavoyan)\", In Proceedings of the 18th International Conference on Computer Applications (ICCA 2020), Feb 27-28, 2020, Yangon, Myanmar, pp. 219-227\n* M\u00fcller, Mathias, Annette Rios, Rico Sennrich. \"Domain Robustness in Neural Machine Translation.\" Proceedings of AMTA (2020)\n* Rios, Annette, Mathias M\u00fcller, Rico Sennrich. \"Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Neural Machine Translation between similar South-Slavic languages.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Extracting correctly aligned segments from unclean parallel data using character n-gram matching.\" Proceedings of Conference on Language Technologies & Digital Humanities (JTDH 2020).\n* Popovi\u0107, Maja, Alberto Poncelas, Marija Brkic, Andy Way. \"Neural Machine Translation for translating into Croatian and Serbian.\" Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects (2020)\n\n### 2019\n\n* Agrawal, Sweta, Marine Carpuat. \"Controlling Text Complexity in Neural Machine Translation.\" Proceedings of EMNLP (2019)\n* Beck, Daniel, Trevor Cohn, Gholamreza Haffari. \"Neural Speech Translation using Lattice Transformations and Graph Networks.\" Proceedings of TextGraphs-13 (EMNLP 2019)\n* Currey, Anna, Kenneth Heafield. \"Zero-Resource Neural Machine Translation with Monolingual Pivot Data.\" Proceedings of EMNLP (2019)\n* Gupta, Prabhakar, Mayank Sharma. \"Unsupervised Translation Quality Estimation for Digital Entertainment Content Subtitles.\" IEEE International Journal of Semantic Computing (2019)\n* Hu, J. Edward, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt Post, and Benjamin Van Durme. \"Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting.\" Proceedings of NAACL-HLT (2019)\n* Rosendahl, Jan, Christian Herold, Yunsu Kim, Miguel Gra\u00e7a,Weiyue Wang, Parnia Bahar, Yingbo Gao and Hermann Ney \u201cThe RWTH Aachen University Machine Translation Systems for WMT 2019\u201d Proceedings of the 4th WMT: Research Papers (2019)\n* Thompson, Brian, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh, and Philipp Koehn. \"Overcoming catastrophic forgetting during domain adaptation of neural machine translation.\" Proceedings of NAACL-HLT 2019 (2019)\n* T\u00e4ttar, Andre, Elizaveta Korotkova, Mark Fishel \u201cUniversity of Tartu\u2019s Multilingual Multi-domain WMT19 News Translation Shared Task Submission\u201d Proceedings of 4th WMT: Research Papers (2019)\n* Thazin Myint Oo, Ye Kyaw Thu and Khin Mar Soe, \"Neural Machine Translation between Myanmar (Burmese) and Rakhine (Arakanese)\", In Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects, NAACL-2019, June 7th 2019, Minneapolis, United States, pp. 80-88\n\n### 2018\n\n* Domhan, Tobias. \"How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures\". Proceedings of 56th ACL (2018)\n* Kim, Yunsu, Yingbo Gao, and Hermann Ney. \"Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies.\" arXiv preprint https://arxiv.org/abs/1905.05475 (2019)\n* Korotkova, Elizaveta, Maksym Del, and Mark Fishel. \"Monolingual and Cross-lingual Zero-shot Style Transfer.\" arXiv preprint https://arxiv.org/abs/1808.00179 (2018)\n* Niu, Xing, Michael Denkowski, and Marine Carpuat. \"Bi-directional neural machine translation with synthetic parallel data.\" arXiv preprint https://arxiv.org/abs/1805.11213 (2018)\n* Niu, Xing, Sudha Rao, and Marine Carpuat. \"Multi-Task Neural Models for Translating Between Styles Within and Across Languages.\" COLING (2018)\n* Post, Matt and David Vilar. \"Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation.\" Proceedings of NAACL-HLT (2018)\n* Schamper, Julian, Jan Rosendahl, Parnia Bahar, Yunsu Kim, Arne Nix, and Hermann Ney. \"The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018.\" Proceedings of the 3rd WMT: Shared Task Papers (2018)\n* Schulz, Philip, Wilker Aziz, and Trevor Cohn. \"A stochastic decoder for neural machine translation.\" arXiv preprint https://arxiv.org/abs/1805.10844 (2018)\n* Tamer, Alkouli, Gabriel Bretschner, and Hermann Ney. \"On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation.\" Proceedings of the 3rd WMT: Research Papers (2018)\n* Tang, Gongbo, Rico Sennrich, and Joakim Nivre. \"An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation.\" Proceedings of 3rd WMT: Research Papers (2018)\n* Thompson, Brian, Huda Khayrallah, Antonios Anastasopoulos, Arya McCarthy, Kevin Duh, Rebecca Marvin, Paul McNamee, Jeremy Gwinnup, Tim Anderson, and Philipp Koehn. \"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1809.05218 (2018)\n* Vilar, David. \"Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.\" Proceedings of NAACL-HLT (2018)\n* Vyas, Yogarshi, Xing Niu and Marine Carpuat \u201cIdentifying Semantic Divergences in Parallel Text without Annotations\u201d. Proceedings of NAACL-HLT (2018)\n* Wang, Weiyue, Derui Zhu, Tamer Alkhouli, Zixuan Gan, and Hermann Ney. \"Neural Hidden Markov Model for Machine Translation\". Proceedings of 56th ACL (2018)\n* Zhang, Xuan, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. \"An Empirical Exploration of Curriculum Learning for Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017)."}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1808.00179 (2018)\n* Niu, Xing, Michael Denkowski, and Marine Carpuat. \"Bi-directional neural machine translation with synthetic parallel data.\" arXiv preprint https://arxiv.org/abs/1805.11213 (2018)\n* Niu, Xing, Sudha Rao, and Marine Carpuat. \"Multi-Task Neural Models for Translating Between Styles Within and Across Languages.\" COLING (2018)\n* Post, Matt and David Vilar. \"Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation.\" Proceedings of NAACL-HLT (2018)\n* Schamper, Julian, Jan Rosendahl, Parnia Bahar, Yunsu Kim, Arne Nix, and Hermann Ney. \"The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018.\" Proceedings of the 3rd WMT: Shared Task Papers (2018)\n* Schulz, Philip, Wilker Aziz, and Trevor Cohn. \"A stochastic decoder for neural machine translation.\" arXiv preprint https://arxiv.org/abs/1805.10844 (2018)\n* Tamer, Alkouli, Gabriel Bretschner, and Hermann Ney. \"On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation.\" Proceedings of the 3rd WMT: Research Papers (2018)\n* Tang, Gongbo, Rico Sennrich, and Joakim Nivre. \"An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation.\" Proceedings of 3rd WMT: Research Papers (2018)\n* Thompson, Brian, Huda Khayrallah, Antonios Anastasopoulos, Arya McCarthy, Kevin Duh, Rebecca Marvin, Paul McNamee, Jeremy Gwinnup, Tim Anderson, and Philipp Koehn. \"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1809.05218 (2018)\n* Vilar, David. \"Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.\" Proceedings of NAACL-HLT (2018)\n* Vyas, Yogarshi, Xing Niu and Marine Carpuat \u201cIdentifying Semantic Divergences in Parallel Text without Annotations\u201d. Proceedings of NAACL-HLT (2018)\n* Wang, Weiyue, Derui Zhu, Tamer Alkhouli, Zixuan Gan, and Hermann Ney. \"Neural Hidden Markov Model for Machine Translation\". Proceedings of 56th ACL (2018)\n* Zhang, Xuan, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. \"An Empirical Exploration of Curriculum Learning for Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017)."}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1805.10844 (2018)\n* Tamer, Alkouli, Gabriel Bretschner, and Hermann Ney. \"On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation.\" Proceedings of the 3rd WMT: Research Papers (2018)\n* Tang, Gongbo, Rico Sennrich, and Joakim Nivre. \"An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation.\" Proceedings of 3rd WMT: Research Papers (2018)\n* Thompson, Brian, Huda Khayrallah, Antonios Anastasopoulos, Arya McCarthy, Kevin Duh, Rebecca Marvin, Paul McNamee, Jeremy Gwinnup, Tim Anderson, and Philipp Koehn. \"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1809.05218 (2018)\n* Vilar, David. \"Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.\" Proceedings of NAACL-HLT (2018)\n* Vyas, Yogarshi, Xing Niu and Marine Carpuat \u201cIdentifying Semantic Divergences in Parallel Text without Annotations\u201d. Proceedings of NAACL-HLT (2018)\n* Wang, Weiyue, Derui Zhu, Tamer Alkhouli, Zixuan Gan, and Hermann Ney. \"Neural Hidden Markov Model for Machine Translation\". Proceedings of 56th ACL (2018)\n* Zhang, Xuan, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. \"An Empirical Exploration of Curriculum Learning for Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017)."}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2006.14194 (2020)\n* Stafanovi\u010ds, Art\u016brs, Toms Bergmanis, M\u0101rcis Pinnis. \"Mitigating Gender Bias in Machine Translation with Target Gender\nAnnotations.\" arXiv preprint https://arxiv.org/abs/2010.06203 (2020)\n* Stojanovski, Dario, Alexander Fraser. \"Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation.\" arXiv preprint arXiv preprint https://arxiv.org/abs/2004.14927 (2020)\n* Stojanovski, Dario, Benno Krojer, Denis Peskov, Alexander Fraser. \"ContraCAT: Contrastive Coreference Analytical Templates for Machine Translation\". Proceedings of COLING (2020)\n* Zhang, Xuan, Kevin Duh. \"Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems.\" Transactions of the Association for Computational Linguistics, Volume 8 (2020)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant, Nandar Win Min, and Thepchai Supnithi, \"Unsupervised Neural Machine Translation between Myanmar Sign Language and Myanmar Language\", Journal of Intelligent Informatics and Smart Technology, April 1st Issue, 2020, pp. 53-61. (Submitted December 21, 2019; accepted March 6, 2020; revised March 16, 2020; published online April 30, 2020)\n* Thazin Myint Oo, Ye Kyaw Thu, Khin Mar Soe and Thepchai Supnithi, \"Neural Machine Translation between Myanmar (Burmese) and Dawei (Tavoyan)\", In Proceedings of the 18th International Conference on Computer Applications (ICCA 2020), Feb 27-28, 2020, Yangon, Myanmar, pp. 219-227\n* M\u00fcller, Mathias, Annette Rios, Rico Sennrich. \"Domain Robustness in Neural Machine Translation.\" Proceedings of AMTA (2020)\n* Rios, Annette, Mathias M\u00fcller, Rico Sennrich. \"Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Neural Machine Translation between similar South-Slavic languages.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Extracting correctly aligned segments from unclean parallel data using character n-gram matching.\" Proceedings of Conference on Language Technologies & Digital Humanities (JTDH 2020).\n* Popovi\u0107, Maja, Alberto Poncelas, Marija Brkic, Andy Way. \"Neural Machine Translation for translating into Croatian and Serbian.\" Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects (2020)\n\n### 2019\n\n* Agrawal, Sweta, Marine Carpuat. \"Controlling Text Complexity in Neural Machine Translation.\" Proceedings of EMNLP (2019)\n* Beck, Daniel, Trevor Cohn, Gholamreza Haffari. \"Neural Speech Translation using Lattice Transformations and Graph Networks.\" Proceedings of TextGraphs-13 (EMNLP 2019)\n* Currey, Anna, Kenneth Heafield. \"Zero-Resource Neural Machine Translation with Monolingual Pivot Data.\" Proceedings of EMNLP (2019)\n* Gupta, Prabhakar, Mayank Sharma. \"Unsupervised Translation Quality Estimation for Digital Entertainment Content Subtitles.\" IEEE International Journal of Semantic Computing (2019)\n* Hu, J. Edward, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt Post, and Benjamin Van Durme. \"Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting.\" Proceedings of NAACL-HLT (2019)\n* Rosendahl, Jan, Christian Herold, Yunsu Kim, Miguel Gra\u00e7a,Weiyue Wang, Parnia Bahar, Yingbo Gao and Hermann Ney \u201cThe RWTH Aachen University Machine Translation Systems for WMT 2019\u201d Proceedings of the 4th WMT: Research Papers (2019)\n* Thompson, Brian, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh, and Philipp Koehn. \"Overcoming catastrophic forgetting during domain adaptation of neural machine translation.\" Proceedings of NAACL-HLT 2019 (2019)\n* T\u00e4ttar, Andre, Elizaveta Korotkova, Mark Fishel \u201cUniversity of Tartu\u2019s Multilingual Multi-domain WMT19 News Translation Shared Task Submission\u201d Proceedings of 4th WMT: Research Papers (2019)\n* Thazin Myint Oo, Ye Kyaw Thu and Khin Mar Soe, \"Neural Machine Translation between Myanmar (Burmese) and Rakhine (Arakanese)\", In Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects, NAACL-2019, June 7th 2019, Minneapolis, United States, pp. 80-88\n\n### 2018\n\n* Domhan, Tobias. \"How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures\". Proceedings of 56th ACL (2018)\n* Kim, Yunsu, Yingbo Gao, and Hermann Ney. \"Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies.\" arXiv preprint https://arxiv.org/abs/1905.05475 (2019)\n* Korotkova, Elizaveta, Maksym Del, and Mark Fishel. \"Monolingual and Cross-lingual Zero-shot Style Transfer.\" arXiv preprint https://arxiv.org/abs/1808.00179 (2018)\n* Niu, Xing, Michael Denkowski, and Marine Carpuat. \"Bi-directional neural machine translation with synthetic parallel data.\" arXiv preprint https://arxiv.org/abs/1805.11213 (2018)\n* Niu, Xing, Sudha Rao, and Marine Carpuat. \"Multi-Task Neural Models for Translating Between Styles Within and Across Languages.\" COLING (2018)\n* Post, Matt and David Vilar. \"Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation.\" Proceedings of NAACL-HLT (2018)\n* Schamper, Julian, Jan Rosendahl, Parnia Bahar, Yunsu Kim, Arne Nix, and Hermann Ney. \"The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018.\" Proceedings of the 3rd WMT: Shared Task Papers (2018)\n* Schulz, Philip, Wilker Aziz, and Trevor Cohn. \"A stochastic decoder for neural machine translation.\" arXiv preprint https://arxiv.org/abs/1805.10844 (2018)\n* Tamer, Alkouli, Gabriel Bretschner, and Hermann Ney. \"On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation.\" Proceedings of the 3rd WMT: Research Papers (2018)\n* Tang, Gongbo, Rico Sennrich, and Joakim Nivre. \"An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation.\" Proceedings of 3rd WMT: Research Papers (2018)\n* Thompson, Brian, Huda Khayrallah, Antonios Anastasopoulos, Arya McCarthy, Kevin Duh, Rebecca Marvin, Paul McNamee, Jeremy Gwinnup, Tim Anderson, and Philipp Koehn. \"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1809.05218 (2018)\n* Vilar, David. \"Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.\" Proceedings of NAACL-HLT (2018)\n* Vyas, Yogarshi, Xing Niu and Marine Carpuat \u201cIdentifying Semantic Divergences in Parallel Text without Annotations\u201d. Proceedings of NAACL-HLT (2018)\n* Wang, Weiyue, Derui Zhu, Tamer Alkhouli, Zixuan Gan, and Hermann Ney. \"Neural Hidden Markov Model for Machine Translation\". Proceedings of 56th ACL (2018)\n* Zhang, Xuan, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. \"An Empirical Exploration of Curriculum Learning for Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017)."}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2004.14927 (2020)\n* Stojanovski, Dario, Benno Krojer, Denis Peskov, Alexander Fraser. \"ContraCAT: Contrastive Coreference Analytical Templates for Machine Translation\". Proceedings of COLING (2020)\n* Zhang, Xuan, Kevin Duh. \"Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems.\" Transactions of the Association for Computational Linguistics, Volume 8 (2020)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant, Nandar Win Min, and Thepchai Supnithi, \"Unsupervised Neural Machine Translation between Myanmar Sign Language and Myanmar Language\", Journal of Intelligent Informatics and Smart Technology, April 1st Issue, 2020, pp. 53-61. (Submitted December 21, 2019; accepted March 6, 2020; revised March 16, 2020; published online April 30, 2020)\n* Thazin Myint Oo, Ye Kyaw Thu, Khin Mar Soe and Thepchai Supnithi, \"Neural Machine Translation between Myanmar (Burmese) and Dawei (Tavoyan)\", In Proceedings of the 18th International Conference on Computer Applications (ICCA 2020), Feb 27-28, 2020, Yangon, Myanmar, pp. 219-227\n* M\u00fcller, Mathias, Annette Rios, Rico Sennrich. \"Domain Robustness in Neural Machine Translation.\" Proceedings of AMTA (2020)\n* Rios, Annette, Mathias M\u00fcller, Rico Sennrich. \"Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Neural Machine Translation between similar South-Slavic languages.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Extracting correctly aligned segments from unclean parallel data using character n-gram matching.\" Proceedings of Conference on Language Technologies & Digital Humanities (JTDH 2020).\n* Popovi\u0107, Maja, Alberto Poncelas, Marija Brkic, Andy Way. \"Neural Machine Translation for translating into Croatian and Serbian.\" Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects (2020)\n\n### 2019\n\n* Agrawal, Sweta, Marine Carpuat. \"Controlling Text Complexity in Neural Machine Translation.\" Proceedings of EMNLP (2019)\n* Beck, Daniel, Trevor Cohn, Gholamreza Haffari. \"Neural Speech Translation using Lattice Transformations and Graph Networks.\" Proceedings of TextGraphs-13 (EMNLP 2019)\n* Currey, Anna, Kenneth Heafield. \"Zero-Resource Neural Machine Translation with Monolingual Pivot Data.\" Proceedings of EMNLP (2019)\n* Gupta, Prabhakar, Mayank Sharma. \"Unsupervised Translation Quality Estimation for Digital Entertainment Content Subtitles.\" IEEE International Journal of Semantic Computing (2019)\n* Hu, J. Edward, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt Post, and Benjamin Van Durme. \"Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting.\" Proceedings of NAACL-HLT (2019)\n* Rosendahl, Jan, Christian Herold, Yunsu Kim, Miguel Gra\u00e7a,Weiyue Wang, Parnia Bahar, Yingbo Gao and Hermann Ney \u201cThe RWTH Aachen University Machine Translation Systems for WMT 2019\u201d Proceedings of the 4th WMT: Research Papers (2019)\n* Thompson, Brian, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh, and Philipp Koehn. \"Overcoming catastrophic forgetting during domain adaptation of neural machine translation.\" Proceedings of NAACL-HLT 2019 (2019)\n* T\u00e4ttar, Andre, Elizaveta Korotkova, Mark Fishel \u201cUniversity of Tartu\u2019s Multilingual Multi-domain WMT19 News Translation Shared Task Submission\u201d Proceedings of 4th WMT: Research Papers (2019)\n* Thazin Myint Oo, Ye Kyaw Thu and Khin Mar Soe, \"Neural Machine Translation between Myanmar (Burmese) and Rakhine (Arakanese)\", In Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects, NAACL-2019, June 7th 2019, Minneapolis, United States, pp. 80-88\n\n### 2018\n\n* Domhan, Tobias. \"How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures\". Proceedings of 56th ACL (2018)\n* Kim, Yunsu, Yingbo Gao, and Hermann Ney. \"Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies.\" arXiv preprint https://arxiv.org/abs/1905.05475 (2019)\n* Korotkova, Elizaveta, Maksym Del, and Mark Fishel. \"Monolingual and Cross-lingual Zero-shot Style Transfer.\" arXiv preprint https://arxiv.org/abs/1808.00179 (2018)\n* Niu, Xing, Michael Denkowski, and Marine Carpuat. \"Bi-directional neural machine translation with synthetic parallel data.\" arXiv preprint https://arxiv.org/abs/1805.11213 (2018)\n* Niu, Xing, Sudha Rao, and Marine Carpuat. \"Multi-Task Neural Models for Translating Between Styles Within and Across Languages.\" COLING (2018)\n* Post, Matt and David Vilar. \"Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation.\" Proceedings of NAACL-HLT (2018)\n* Schamper, Julian, Jan Rosendahl, Parnia Bahar, Yunsu Kim, Arne Nix, and Hermann Ney. \"The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018.\" Proceedings of the 3rd WMT: Shared Task Papers (2018)\n* Schulz, Philip, Wilker Aziz, and Trevor Cohn. \"A stochastic decoder for neural machine translation.\" arXiv preprint https://arxiv.org/abs/1805.10844 (2018)\n* Tamer, Alkouli, Gabriel Bretschner, and Hermann Ney. \"On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation.\" Proceedings of the 3rd WMT: Research Papers (2018)\n* Tang, Gongbo, Rico Sennrich, and Joakim Nivre. \"An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation.\" Proceedings of 3rd WMT: Research Papers (2018)\n* Thompson, Brian, Huda Khayrallah, Antonios Anastasopoulos, Arya McCarthy, Kevin Duh, Rebecca Marvin, Paul McNamee, Jeremy Gwinnup, Tim Anderson, and Philipp Koehn. \"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1809.05218 (2018)\n* Vilar, David. \"Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.\" Proceedings of NAACL-HLT (2018)\n* Vyas, Yogarshi, Xing Niu and Marine Carpuat \u201cIdentifying Semantic Divergences in Parallel Text without Annotations\u201d. Proceedings of NAACL-HLT (2018)\n* Wang, Weiyue, Derui Zhu, Tamer Alkhouli, Zixuan Gan, and Hermann Ney. \"Neural Hidden Markov Model for Machine Translation\". Proceedings of 56th ACL (2018)\n* Zhang, Xuan, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. \"An Empirical Exploration of Curriculum Learning for Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017)."}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2207.05851"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2002.09646 (2020)\n* Niu, Xing, Prashant Mathur, Georgiana Dinu, Yaser Al-Onaizan. \"Evaluating Robustness to Input Perturbations for Neural Machine Translation\". arXiv preprint \thttps://arxiv.org/abs/2005.00580 (2020)\n* Niu, Xing, Marine Carpuat. \"Controlling Neural Machine Translation Formality with Synthetic Supervision.\" Proceedings of AAAI (2020)\n* Keung, Phillip, Julian Salazar, Yichao Liu, Noah A. Smith. \"Unsupervised Bitext Mining and Translation\nvia Self-Trained Contextual Embeddings.\" arXiv preprint https://arxiv.org/abs/2010.07761 (2020).\n* Sokolov, Alex, Tracy Rohlin, Ariya Rastrow. \"Neural Machine Translation for Multilingual Grapheme-to-Phoneme Conversion.\" arXiv preprint https://arxiv.org/abs/2006.14194 (2020)\n* Stafanovi\u010ds, Art\u016brs, Toms Bergmanis, M\u0101rcis Pinnis. \"Mitigating Gender Bias in Machine Translation with Target Gender\nAnnotations.\" arXiv preprint https://arxiv.org/abs/2010.06203 (2020)\n* Stojanovski, Dario, Alexander Fraser. \"Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation.\" arXiv preprint arXiv preprint https://arxiv.org/abs/2004.14927 (2020)\n* Stojanovski, Dario, Benno Krojer, Denis Peskov, Alexander Fraser. \"ContraCAT: Contrastive Coreference Analytical Templates for Machine Translation\". Proceedings of COLING (2020)\n* Zhang, Xuan, Kevin Duh. \"Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems.\" Transactions of the Association for Computational Linguistics, Volume 8 (2020)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant, Nandar Win Min, and Thepchai Supnithi, \"Unsupervised Neural Machine Translation between Myanmar Sign Language and Myanmar Language\", Journal of Intelligent Informatics and Smart Technology, April 1st Issue, 2020, pp. 53-61. (Submitted December 21, 2019; accepted March 6, 2020; revised March 16, 2020; published online April 30, 2020)\n* Thazin Myint Oo, Ye Kyaw Thu, Khin Mar Soe and Thepchai Supnithi, \"Neural Machine Translation between Myanmar (Burmese) and Dawei (Tavoyan)\", In Proceedings of the 18th International Conference on Computer Applications (ICCA 2020), Feb 27-28, 2020, Yangon, Myanmar, pp. 219-227\n* M\u00fcller, Mathias, Annette Rios, Rico Sennrich. \"Domain Robustness in Neural Machine Translation.\" Proceedings of AMTA (2020)\n* Rios, Annette, Mathias M\u00fcller, Rico Sennrich. \"Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Neural Machine Translation between similar South-Slavic languages.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Extracting correctly aligned segments from unclean parallel data using character n-gram matching.\" Proceedings of Conference on Language Technologies & Digital Humanities (JTDH 2020).\n* Popovi\u0107, Maja, Alberto Poncelas, Marija Brkic, Andy Way. \"Neural Machine Translation for translating into Croatian and Serbian.\" Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects (2020)\n\n### 2019\n\n* Agrawal, Sweta, Marine Carpuat. \"Controlling Text Complexity in Neural Machine Translation.\" Proceedings of EMNLP (2019)\n* Beck, Daniel, Trevor Cohn, Gholamreza Haffari. \"Neural Speech Translation using Lattice Transformations and Graph Networks.\" Proceedings of TextGraphs-13 (EMNLP 2019)\n* Currey, Anna, Kenneth Heafield. \"Zero-Resource Neural Machine Translation with Monolingual Pivot Data.\" Proceedings of EMNLP (2019)\n* Gupta, Prabhakar, Mayank Sharma. \"Unsupervised Translation Quality Estimation for Digital Entertainment Content Subtitles.\" IEEE International Journal of Semantic Computing (2019)\n* Hu, J. Edward, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt Post, and Benjamin Van Durme. \"Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting.\" Proceedings of NAACL-HLT (2019)\n* Rosendahl, Jan, Christian Herold, Yunsu Kim, Miguel Gra\u00e7a,Weiyue Wang, Parnia Bahar, Yingbo Gao and Hermann Ney \u201cThe RWTH Aachen University Machine Translation Systems for WMT 2019\u201d Proceedings of the 4th WMT: Research Papers (2019)\n* Thompson, Brian, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh, and Philipp Koehn. \"Overcoming catastrophic forgetting during domain adaptation of neural machine translation.\" Proceedings of NAACL-HLT 2019 (2019)\n* T\u00e4ttar, Andre, Elizaveta Korotkova, Mark Fishel \u201cUniversity of Tartu\u2019s Multilingual Multi-domain WMT19 News Translation Shared Task Submission\u201d Proceedings of 4th WMT: Research Papers (2019)\n* Thazin Myint Oo, Ye Kyaw Thu and Khin Mar Soe, \"Neural Machine Translation between Myanmar (Burmese) and Rakhine (Arakanese)\", In Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects, NAACL-2019, June 7th 2019, Minneapolis, United States, pp. 80-88\n\n### 2018\n\n* Domhan, Tobias. \"How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures\". Proceedings of 56th ACL (2018)\n* Kim, Yunsu, Yingbo Gao, and Hermann Ney. \"Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies.\" arXiv preprint https://arxiv.org/abs/1905.05475 (2019)\n* Korotkova, Elizaveta, Maksym Del, and Mark Fishel. \"Monolingual and Cross-lingual Zero-shot Style Transfer.\" arXiv preprint https://arxiv.org/abs/1808.00179 (2018)\n* Niu, Xing, Michael Denkowski, and Marine Carpuat. \"Bi-directional neural machine translation with synthetic parallel data.\" arXiv preprint https://arxiv.org/abs/1805.11213 (2018)\n* Niu, Xing, Sudha Rao, and Marine Carpuat. \"Multi-Task Neural Models for Translating Between Styles Within and Across Languages.\" COLING (2018)\n* Post, Matt and David Vilar. \"Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation.\" Proceedings of NAACL-HLT (2018)\n* Schamper, Julian, Jan Rosendahl, Parnia Bahar, Yunsu Kim, Arne Nix, and Hermann Ney. \"The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018.\" Proceedings of the 3rd WMT: Shared Task Papers (2018)\n* Schulz, Philip, Wilker Aziz, and Trevor Cohn. \"A stochastic decoder for neural machine translation.\" arXiv preprint https://arxiv.org/abs/1805.10844 (2018)\n* Tamer, Alkouli, Gabriel Bretschner, and Hermann Ney. \"On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation.\" Proceedings of the 3rd WMT: Research Papers (2018)\n* Tang, Gongbo, Rico Sennrich, and Joakim Nivre. \"An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation.\" Proceedings of 3rd WMT: Research Papers (2018)\n* Thompson, Brian, Huda Khayrallah, Antonios Anastasopoulos, Arya McCarthy, Kevin Duh, Rebecca Marvin, Paul McNamee, Jeremy Gwinnup, Tim Anderson, and Philipp Koehn. \"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1809.05218 (2018)\n* Vilar, David. \"Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.\" Proceedings of NAACL-HLT (2018)\n* Vyas, Yogarshi, Xing Niu and Marine Carpuat \u201cIdentifying Semantic Divergences in Parallel Text without Annotations\u201d. Proceedings of NAACL-HLT (2018)\n* Wang, Weiyue, Derui Zhu, Tamer Alkhouli, Zixuan Gan, and Hermann Ney. \"Neural Hidden Markov Model for Machine Translation\". Proceedings of 56th ACL (2018)\n* Zhang, Xuan, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. \"An Empirical Exploration of Curriculum Learning for Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017)."}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1805.11213 (2018)\n* Niu, Xing, Sudha Rao, and Marine Carpuat. \"Multi-Task Neural Models for Translating Between Styles Within and Across Languages.\" COLING (2018)\n* Post, Matt and David Vilar. \"Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation.\" Proceedings of NAACL-HLT (2018)\n* Schamper, Julian, Jan Rosendahl, Parnia Bahar, Yunsu Kim, Arne Nix, and Hermann Ney. \"The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018.\" Proceedings of the 3rd WMT: Shared Task Papers (2018)\n* Schulz, Philip, Wilker Aziz, and Trevor Cohn. \"A stochastic decoder for neural machine translation.\" arXiv preprint https://arxiv.org/abs/1805.10844 (2018)\n* Tamer, Alkouli, Gabriel Bretschner, and Hermann Ney. \"On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation.\" Proceedings of the 3rd WMT: Research Papers (2018)\n* Tang, Gongbo, Rico Sennrich, and Joakim Nivre. \"An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation.\" Proceedings of 3rd WMT: Research Papers (2018)\n* Thompson, Brian, Huda Khayrallah, Antonios Anastasopoulos, Arya McCarthy, Kevin Duh, Rebecca Marvin, Paul McNamee, Jeremy Gwinnup, Tim Anderson, and Philipp Koehn. \"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1809.05218 (2018)\n* Vilar, David. \"Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.\" Proceedings of NAACL-HLT (2018)\n* Vyas, Yogarshi, Xing Niu and Marine Carpuat \u201cIdentifying Semantic Divergences in Parallel Text without Annotations\u201d. Proceedings of NAACL-HLT (2018)\n* Wang, Weiyue, Derui Zhu, Tamer Alkhouli, Zixuan Gan, and Hermann Ney. \"Neural Hidden Markov Model for Machine Translation\". Proceedings of 56th ACL (2018)\n* Zhang, Xuan, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. \"An Empirical Exploration of Curriculum Learning for Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017)."}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2010.06203 (2020)\n* Stojanovski, Dario, Alexander Fraser. \"Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation.\" arXiv preprint arXiv preprint https://arxiv.org/abs/2004.14927 (2020)\n* Stojanovski, Dario, Benno Krojer, Denis Peskov, Alexander Fraser. \"ContraCAT: Contrastive Coreference Analytical Templates for Machine Translation\". Proceedings of COLING (2020)\n* Zhang, Xuan, Kevin Duh. \"Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems.\" Transactions of the Association for Computational Linguistics, Volume 8 (2020)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant, Nandar Win Min, and Thepchai Supnithi, \"Unsupervised Neural Machine Translation between Myanmar Sign Language and Myanmar Language\", Journal of Intelligent Informatics and Smart Technology, April 1st Issue, 2020, pp. 53-61. (Submitted December 21, 2019; accepted March 6, 2020; revised March 16, 2020; published online April 30, 2020)\n* Thazin Myint Oo, Ye Kyaw Thu, Khin Mar Soe and Thepchai Supnithi, \"Neural Machine Translation between Myanmar (Burmese) and Dawei (Tavoyan)\", In Proceedings of the 18th International Conference on Computer Applications (ICCA 2020), Feb 27-28, 2020, Yangon, Myanmar, pp. 219-227\n* M\u00fcller, Mathias, Annette Rios, Rico Sennrich. \"Domain Robustness in Neural Machine Translation.\" Proceedings of AMTA (2020)\n* Rios, Annette, Mathias M\u00fcller, Rico Sennrich. \"Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Neural Machine Translation between similar South-Slavic languages.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Extracting correctly aligned segments from unclean parallel data using character n-gram matching.\" Proceedings of Conference on Language Technologies & Digital Humanities (JTDH 2020).\n* Popovi\u0107, Maja, Alberto Poncelas, Marija Brkic, Andy Way. \"Neural Machine Translation for translating into Croatian and Serbian.\" Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects (2020)\n\n### 2019\n\n* Agrawal, Sweta, Marine Carpuat. \"Controlling Text Complexity in Neural Machine Translation.\" Proceedings of EMNLP (2019)\n* Beck, Daniel, Trevor Cohn, Gholamreza Haffari. \"Neural Speech Translation using Lattice Transformations and Graph Networks.\" Proceedings of TextGraphs-13 (EMNLP 2019)\n* Currey, Anna, Kenneth Heafield. \"Zero-Resource Neural Machine Translation with Monolingual Pivot Data.\" Proceedings of EMNLP (2019)\n* Gupta, Prabhakar, Mayank Sharma. \"Unsupervised Translation Quality Estimation for Digital Entertainment Content Subtitles.\" IEEE International Journal of Semantic Computing (2019)\n* Hu, J. Edward, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt Post, and Benjamin Van Durme. \"Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting.\" Proceedings of NAACL-HLT (2019)\n* Rosendahl, Jan, Christian Herold, Yunsu Kim, Miguel Gra\u00e7a,Weiyue Wang, Parnia Bahar, Yingbo Gao and Hermann Ney \u201cThe RWTH Aachen University Machine Translation Systems for WMT 2019\u201d Proceedings of the 4th WMT: Research Papers (2019)\n* Thompson, Brian, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh, and Philipp Koehn. \"Overcoming catastrophic forgetting during domain adaptation of neural machine translation.\" Proceedings of NAACL-HLT 2019 (2019)\n* T\u00e4ttar, Andre, Elizaveta Korotkova, Mark Fishel \u201cUniversity of Tartu\u2019s Multilingual Multi-domain WMT19 News Translation Shared Task Submission\u201d Proceedings of 4th WMT: Research Papers (2019)\n* Thazin Myint Oo, Ye Kyaw Thu and Khin Mar Soe, \"Neural Machine Translation between Myanmar (Burmese) and Rakhine (Arakanese)\", In Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects, NAACL-2019, June 7th 2019, Minneapolis, United States, pp. 80-88\n\n### 2018\n\n* Domhan, Tobias. \"How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures\". Proceedings of 56th ACL (2018)\n* Kim, Yunsu, Yingbo Gao, and Hermann Ney. \"Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies.\" arXiv preprint https://arxiv.org/abs/1905.05475 (2019)\n* Korotkova, Elizaveta, Maksym Del, and Mark Fishel. \"Monolingual and Cross-lingual Zero-shot Style Transfer.\" arXiv preprint https://arxiv.org/abs/1808.00179 (2018)\n* Niu, Xing, Michael Denkowski, and Marine Carpuat. \"Bi-directional neural machine translation with synthetic parallel data.\" arXiv preprint https://arxiv.org/abs/1805.11213 (2018)\n* Niu, Xing, Sudha Rao, and Marine Carpuat. \"Multi-Task Neural Models for Translating Between Styles Within and Across Languages.\" COLING (2018)\n* Post, Matt and David Vilar. \"Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation.\" Proceedings of NAACL-HLT (2018)\n* Schamper, Julian, Jan Rosendahl, Parnia Bahar, Yunsu Kim, Arne Nix, and Hermann Ney. \"The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018.\" Proceedings of the 3rd WMT: Shared Task Papers (2018)\n* Schulz, Philip, Wilker Aziz, and Trevor Cohn. \"A stochastic decoder for neural machine translation.\" arXiv preprint https://arxiv.org/abs/1805.10844 (2018)\n* Tamer, Alkouli, Gabriel Bretschner, and Hermann Ney. \"On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation.\" Proceedings of the 3rd WMT: Research Papers (2018)\n* Tang, Gongbo, Rico Sennrich, and Joakim Nivre. \"An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation.\" Proceedings of 3rd WMT: Research Papers (2018)\n* Thompson, Brian, Huda Khayrallah, Antonios Anastasopoulos, Arya McCarthy, Kevin Duh, Rebecca Marvin, Paul McNamee, Jeremy Gwinnup, Tim Anderson, and Philipp Koehn. \"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1809.05218 (2018)\n* Vilar, David. \"Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.\" Proceedings of NAACL-HLT (2018)\n* Vyas, Yogarshi, Xing Niu and Marine Carpuat \u201cIdentifying Semantic Divergences in Parallel Text without Annotations\u201d. Proceedings of NAACL-HLT (2018)\n* Wang, Weiyue, Derui Zhu, Tamer Alkhouli, Zixuan Gan, and Hermann Ney. \"Neural Hidden Markov Model for Machine Translation\". Proceedings of 56th ACL (2018)\n* Zhang, Xuan, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. \"An Empirical Exploration of Curriculum Learning for Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017)."}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1905.05475 (2019)\n* Korotkova, Elizaveta, Maksym Del, and Mark Fishel. \"Monolingual and Cross-lingual Zero-shot Style Transfer.\" arXiv preprint https://arxiv.org/abs/1808.00179 (2018)\n* Niu, Xing, Michael Denkowski, and Marine Carpuat. \"Bi-directional neural machine translation with synthetic parallel data.\" arXiv preprint https://arxiv.org/abs/1805.11213 (2018)\n* Niu, Xing, Sudha Rao, and Marine Carpuat. \"Multi-Task Neural Models for Translating Between Styles Within and Across Languages.\" COLING (2018)\n* Post, Matt and David Vilar. \"Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation.\" Proceedings of NAACL-HLT (2018)\n* Schamper, Julian, Jan Rosendahl, Parnia Bahar, Yunsu Kim, Arne Nix, and Hermann Ney. \"The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018.\" Proceedings of the 3rd WMT: Shared Task Papers (2018)\n* Schulz, Philip, Wilker Aziz, and Trevor Cohn. \"A stochastic decoder for neural machine translation.\" arXiv preprint https://arxiv.org/abs/1805.10844 (2018)\n* Tamer, Alkouli, Gabriel Bretschner, and Hermann Ney. \"On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation.\" Proceedings of the 3rd WMT: Research Papers (2018)\n* Tang, Gongbo, Rico Sennrich, and Joakim Nivre. \"An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation.\" Proceedings of 3rd WMT: Research Papers (2018)\n* Thompson, Brian, Huda Khayrallah, Antonios Anastasopoulos, Arya McCarthy, Kevin Duh, Rebecca Marvin, Paul McNamee, Jeremy Gwinnup, Tim Anderson, and Philipp Koehn. \"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1809.05218 (2018)\n* Vilar, David. \"Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.\" Proceedings of NAACL-HLT (2018)\n* Vyas, Yogarshi, Xing Niu and Marine Carpuat \u201cIdentifying Semantic Divergences in Parallel Text without Annotations\u201d. Proceedings of NAACL-HLT (2018)\n* Wang, Weiyue, Derui Zhu, Tamer Alkhouli, Zixuan Gan, and Hermann Ney. \"Neural Hidden Markov Model for Machine Translation\". Proceedings of 56th ACL (2018)\n* Zhang, Xuan, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. \"An Empirical Exploration of Curriculum Learning for Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017)."}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1712.05690"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2102.1024 (2021)\n* Xu, Weijia, Marine Carpuat. \"EDITOR: An Edit-Based Transformer with Repositioning for Neural Machine Translation with Soft Lexical Constraints.\" Transactions of the Association for Computation Linguistics 9 (2021)\n* M\u00fcller, Mathias, Rico Sennrich. \"Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation\". Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) (2021)\n* Popovi\u0107, Maja, Alberto Poncelas. \"On Machine Translation of User Reviews.\" Proceedings of RANLP (2021)\n* Popovi\u0107, Maja. \"On nature and causes of observed MT errors.\" Proceedings of the 18th MT Summit (Volume 1: Research Track) (2021)\n* Jain, Nishtha, Maja Popovi\u0107, Declan Groves, Eva Vanmassenhove. \"Generating Gender Augmented Data for NLP.\" Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing (2021)\n* Vilar, David, Marcello Federico. \"A Statistical Extension of Byte-Pair Encoding.\" Proceedings of IWSLT (2021)\n\n### 2020\n\n* Dinu, Georgiana, Prashant Mathur, Marcello Federico, Stanislas Lauly, Yaser Al-Onaizan. \"Joint translation and unit conversion for end-to-end localization.\" Proceedings of IWSLT (2020)\n* Exel, Miriam, Bianka Buschbeck, Lauritz Brandt, Simona Doneva. \"Terminology-Constrained Neural Machine Translation at SAP\". Proceedings of EAMT (2020).\n* Hisamoto, Sorami, Matt Post, Kevin Duh. \"Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data In Your Machine Translation System?\" Transactions of the Association for Computational Linguistics, Volume 8 (2020)\n* Naradowsky, Jason, Xuan Zhan, Kevin Duh. \"Machine Translation System Selection from Bandit Feedback.\" arXiv preprint https://arxiv.org/abs/2002.09646 (2020)\n* Niu, Xing, Prashant Mathur, Georgiana Dinu, Yaser Al-Onaizan. \"Evaluating Robustness to Input Perturbations for Neural Machine Translation\". arXiv preprint \thttps://arxiv.org/abs/2005.00580 (2020)\n* Niu, Xing, Marine Carpuat. \"Controlling Neural Machine Translation Formality with Synthetic Supervision.\" Proceedings of AAAI (2020)\n* Keung, Phillip, Julian Salazar, Yichao Liu, Noah A. Smith. \"Unsupervised Bitext Mining and Translation\nvia Self-Trained Contextual Embeddings.\" arXiv preprint https://arxiv.org/abs/2010.07761 (2020).\n* Sokolov, Alex, Tracy Rohlin, Ariya Rastrow. \"Neural Machine Translation for Multilingual Grapheme-to-Phoneme Conversion.\" arXiv preprint https://arxiv.org/abs/2006.14194 (2020)\n* Stafanovi\u010ds, Art\u016brs, Toms Bergmanis, M\u0101rcis Pinnis. \"Mitigating Gender Bias in Machine Translation with Target Gender\nAnnotations.\" arXiv preprint https://arxiv.org/abs/2010.06203 (2020)\n* Stojanovski, Dario, Alexander Fraser. \"Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation.\" arXiv preprint arXiv preprint https://arxiv.org/abs/2004.14927 (2020)\n* Stojanovski, Dario, Benno Krojer, Denis Peskov, Alexander Fraser. \"ContraCAT: Contrastive Coreference Analytical Templates for Machine Translation\". Proceedings of COLING (2020)\n* Zhang, Xuan, Kevin Duh. \"Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems.\" Transactions of the Association for Computational Linguistics, Volume 8 (2020)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant, Nandar Win Min, and Thepchai Supnithi, \"Unsupervised Neural Machine Translation between Myanmar Sign Language and Myanmar Language\", Journal of Intelligent Informatics and Smart Technology, April 1st Issue, 2020, pp. 53-61. (Submitted December 21, 2019; accepted March 6, 2020; revised March 16, 2020; published online April 30, 2020)\n* Thazin Myint Oo, Ye Kyaw Thu, Khin Mar Soe and Thepchai Supnithi, \"Neural Machine Translation between Myanmar (Burmese) and Dawei (Tavoyan)\", In Proceedings of the 18th International Conference on Computer Applications (ICCA 2020), Feb 27-28, 2020, Yangon, Myanmar, pp. 219-227\n* M\u00fcller, Mathias, Annette Rios, Rico Sennrich. \"Domain Robustness in Neural Machine Translation.\" Proceedings of AMTA (2020)\n* Rios, Annette, Mathias M\u00fcller, Rico Sennrich. \"Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Neural Machine Translation between similar South-Slavic languages.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Extracting correctly aligned segments from unclean parallel data using character n-gram matching.\" Proceedings of Conference on Language Technologies & Digital Humanities (JTDH 2020).\n* Popovi\u0107, Maja, Alberto Poncelas, Marija Brkic, Andy Way. \"Neural Machine Translation for translating into Croatian and Serbian.\" Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects (2020)\n\n### 2019\n\n* Agrawal, Sweta, Marine Carpuat. \"Controlling Text Complexity in Neural Machine Translation.\" Proceedings of EMNLP (2019)\n* Beck, Daniel, Trevor Cohn, Gholamreza Haffari. \"Neural Speech Translation using Lattice Transformations and Graph Networks.\" Proceedings of TextGraphs-13 (EMNLP 2019)\n* Currey, Anna, Kenneth Heafield. \"Zero-Resource Neural Machine Translation with Monolingual Pivot Data.\" Proceedings of EMNLP (2019)\n* Gupta, Prabhakar, Mayank Sharma. \"Unsupervised Translation Quality Estimation for Digital Entertainment Content Subtitles.\" IEEE International Journal of Semantic Computing (2019)\n* Hu, J. Edward, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt Post, and Benjamin Van Durme. \"Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting.\" Proceedings of NAACL-HLT (2019)\n* Rosendahl, Jan, Christian Herold, Yunsu Kim, Miguel Gra\u00e7a,Weiyue Wang, Parnia Bahar, Yingbo Gao and Hermann Ney \u201cThe RWTH Aachen University Machine Translation Systems for WMT 2019\u201d Proceedings of the 4th WMT: Research Papers (2019)\n* Thompson, Brian, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh, and Philipp Koehn. \"Overcoming catastrophic forgetting during domain adaptation of neural machine translation.\" Proceedings of NAACL-HLT 2019 (2019)\n* T\u00e4ttar, Andre, Elizaveta Korotkova, Mark Fishel \u201cUniversity of Tartu\u2019s Multilingual Multi-domain WMT19 News Translation Shared Task Submission\u201d Proceedings of 4th WMT: Research Papers (2019)\n* Thazin Myint Oo, Ye Kyaw Thu and Khin Mar Soe, \"Neural Machine Translation between Myanmar (Burmese) and Rakhine (Arakanese)\", In Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects, NAACL-2019, June 7th 2019, Minneapolis, United States, pp. 80-88\n\n### 2018\n\n* Domhan, Tobias. \"How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures\". Proceedings of 56th ACL (2018)\n* Kim, Yunsu, Yingbo Gao, and Hermann Ney. \"Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies.\" arXiv preprint https://arxiv.org/abs/1905.05475 (2019)\n* Korotkova, Elizaveta, Maksym Del, and Mark Fishel. \"Monolingual and Cross-lingual Zero-shot Style Transfer.\" arXiv preprint https://arxiv.org/abs/1808.00179 (2018)\n* Niu, Xing, Michael Denkowski, and Marine Carpuat. \"Bi-directional neural machine translation with synthetic parallel data.\" arXiv preprint https://arxiv.org/abs/1805.11213 (2018)\n* Niu, Xing, Sudha Rao, and Marine Carpuat. \"Multi-Task Neural Models for Translating Between Styles Within and Across Languages.\" COLING (2018)\n* Post, Matt and David Vilar. \"Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation.\" Proceedings of NAACL-HLT (2018)\n* Schamper, Julian, Jan Rosendahl, Parnia Bahar, Yunsu Kim, Arne Nix, and Hermann Ney. \"The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018.\" Proceedings of the 3rd WMT: Shared Task Papers (2018)\n* Schulz, Philip, Wilker Aziz, and Trevor Cohn. \"A stochastic decoder for neural machine translation.\" arXiv preprint https://arxiv.org/abs/1805.10844 (2018)\n* Tamer, Alkouli, Gabriel Bretschner, and Hermann Ney. \"On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation.\" Proceedings of the 3rd WMT: Research Papers (2018)\n* Tang, Gongbo, Rico Sennrich, and Joakim Nivre. \"An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation.\" Proceedings of 3rd WMT: Research Papers (2018)\n* Thompson, Brian, Huda Khayrallah, Antonios Anastasopoulos, Arya McCarthy, Kevin Duh, Rebecca Marvin, Paul McNamee, Jeremy Gwinnup, Tim Anderson, and Philipp Koehn. \"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1809.05218 (2018)\n* Vilar, David. \"Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.\" Proceedings of NAACL-HLT (2018)\n* Vyas, Yogarshi, Xing Niu and Marine Carpuat \u201cIdentifying Semantic Divergences in Parallel Text without Annotations\u201d. Proceedings of NAACL-HLT (2018)\n* Wang, Weiyue, Derui Zhu, Tamer Alkhouli, Zixuan Gan, and Hermann Ney. \"Neural Hidden Markov Model for Machine Translation\". Proceedings of 56th ACL (2018)\n* Zhang, Xuan, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. \"An Empirical Exploration of Curriculum Learning for Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017)."}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017)."}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/awslabs/sockeye/main/README.md"}]}