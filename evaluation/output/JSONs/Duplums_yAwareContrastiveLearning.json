{"somef_provenance": {"somef_version": "0.9.4", "somef_schema_version": "1.0.0", "date": "2023-12-21 19:16:27"}, "code_repository": [{"result": {"value": "https://github.com/Duplums/yAwareContrastiveLearning", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "owner": [{"result": {"value": "Duplums", "type": "User"}, "confidence": 1, "technique": "GitHub_API"}], "date_created": [{"result": {"value": "2021-02-05T14:03:14Z", "type": "Date"}, "confidence": 1, "technique": "GitHub_API"}], "date_updated": [{"result": {"value": "2023-10-13T16:18:50Z", "type": "Date"}, "confidence": 1, "technique": "GitHub_API"}], "description": [{"result": {"value": "Official Pytorch Implementation for y-Aware Contrastive Learning", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Text_excerpt", "value": "Official Pytorch Implementation for y-Aware Contrastive Learning (*MICCAI 2021*) [[paper]](https://hal.telecom-paris.fr/hal-03262256/document) \nWe propose an extension of the popular InfoNCE loss used in contrastive learning (SimCLR, MoCo, etc.) to the weakly supervised case where auxiliary information *y* is available for each image *x* (e.g subject's age or sex for medical images). We demonstrate a better data representation with our new loss, namely y-Aware InfoNCE.  \n", "original_header": "y-Aware Contrastive Learning"}, "confidence": 0.9553600988577831, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/Duplums/yAwareContrastiveLearning/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "In the paper, we aggregated 13 MRI datasets of healthy cohorts pre-processed with [CAT12](http://www.neuro.uni-jena.de/cat/). You can find the complete list below. \n", "original_header": "BHB-10K dataset for pre-training"}, "confidence": 0.8468868141270753, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/Duplums/yAwareContrastiveLearning/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "Originally, we have evaluated our approach on 3 classification target tasks with 2 public datasets (detailed below) and \n1 private one ([BIOBD](https://www.cambridge.org/core/journals/psychological-medicine/article/abs/lithium-prevents-grey-matter-atrophy-in-patients-with-bipolar-disorder-an-international-multicenter-study/6267A7E11F17EFDF5857F06E4C233D4F)).\nWe also pre-processed the T1-MRI scan with CAT12 toolbox and all the images passed a visual Quality Check (QC). \n", "original_header": "Datasets for evaluation/fine-tuning"}, "confidence": 0.9443451427159029, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/Duplums/yAwareContrastiveLearning/main/README.md"}], "name": [{"result": {"value": "yAwareContrastiveLearning", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}], "full_name": [{"result": {"value": "Duplums/yAwareContrastiveLearning", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}], "issue_tracker": [{"result": {"value": "https://api.github.com/repos/Duplums/yAwareContrastiveLearning/issues", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "forks_url": [{"result": {"value": "https://api.github.com/repos/Duplums/yAwareContrastiveLearning/forks", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "stargazers_count": [{"result": {"value": 47, "type": "Number"}, "confidence": 1, "technique": "GitHub_API"}], "keywords": [{"result": {"value": "", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}], "forks_count": [{"result": {"value": 9, "type": "Number"}, "confidence": 1, "technique": "GitHub_API"}], "download_url": [{"result": {"value": "https://github.com/Duplums/yAwareContrastiveLearning/releases", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "programming_languages": [{"result": {"value": "Python", "name": "Python", "type": "Programming_language", "size": 45863}, "confidence": 1, "technique": "GitHub_API"}], "readme_url": [{"result": {"value": "https://raw.githubusercontent.com/Duplums/yAwareContrastiveLearning/main/README.md", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}], "requirements": [{"result": {"value": "+ python >= 3.6\n+ pytorch >= 1.6\n+ numpy >= 1.17\n+ scikit-image=0.16.2\n+ pandas=0.25.2\n", "type": "Text_excerpt", "original_header": "Dependencies", "parent_header": ["y-Aware Contrastive Learning"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/Duplums/yAwareContrastiveLearning/main/README.md"}], "download": [{"result": {"value": "You can download our DenseNet121 model pre-trained on BHB-10K [here](https://drive.google.com/file/d/1BmDC4USdZmX0ZSi-jQyUVmKHaDFIJFo2/view?usp=drive_link). \nWe have used only random cutout during pre-training and we used the hyperparameters defined by default in `config.py`.\n", "type": "Text_excerpt", "original_header": "Download our pretrained model", "parent_header": ["y-Aware Contrastive Learning", "Pre-training"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/Duplums/yAwareContrastiveLearning/main/README.md"}], "run": [{"result": {"value": "Once you have filled `config.py` with the correct paths, you can simply run the DenseNet model with:\n``` bash\n$ python3 main.py --mode pretraining\n```\n\n", "type": "Text_excerpt", "original_header": "Running the model", "parent_header": ["y-Aware Contrastive Learning", "Pre-training", "Pretraining your own model"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/Duplums/yAwareContrastiveLearning/main/README.md"}], "installation": [{"result": {"type": "Text_excerpt", "value": "First, you can clone this repository with:\n``` bash \n$ git clone https://github.com/Duplums/yAwareContrastiveLearning.git\n$ cd yAwareContrastiveLearning\n``` \n", "original_header": "Pre-training"}, "confidence": 0.9957537170615283, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/Duplums/yAwareContrastiveLearning/main/README.md"}], "invocation": [{"result": {"type": "Text_excerpt", "value": "**Source**  | **# Subjects**  | **# Sessions** | **Age** | **Sex (\\%F)** | **# Sites**\n:---: | :---: | :---: | :---: | :---: | :---: | \n[HCP](https://www.humanconnectome.org/study/hcp-young-adult)  | 1113 | 1113 | 29 \u00b1 4 | 45 | 1\n[IXI](http://brain-development.org/ixi-dataset) | 559 | 559 | 48 \u00b1 16 | 55 | 3 \n[CoRR](https://www.nitrc.org/projects/fcon_1000) | 1371 | 2897 | 26 \u00b1 16 | 50 | 19\n[NPC](https://openneuro.org/datasets/ds002330/versions/1.1.0) | 65 | 65 | 26 \u00b1 4 | 55 | 1\n[NAR](https://openneuro.org/datasets/ds002345/versions/1.0.1) | 303 | 323 | 22 \u00b1 5 | 58 | 1\n[RBP](https://openneuro.org/datasets/ds002247/versions/1.0.0) | 40 | 40 | 23 \u00b1 5 | 52 | 1\n[OASIS 3](https://www.oasis-brains.org) | 597 | 1262 | 67 \u00b1 9 | 62 | 3\n[GSP](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/25833) | 1570 | 1639 | 21 \u00b1 3 | 58 | 1\n[ICBM](https://ida.loni.usc.edu) | 622 | 977 | 30 \u00b1 12 | 45 | 3\n[ABIDE 1](http://fcon_1000.projects.nitrc.org/indi/abide) | 567 | 567 | 17 \u00b1 8 | 17 | 20\n[ABIDE 2](http://fcon_1000.projects.nitrc.org/indi/abide) | 559 | 580 | 15 \u00b1 9 | 30 | 17\n[Localizer](http://brainomics.cea.fr/localizer/localizer) | 82 | 82 | 25 \u00b1 7 | 56 | 2\n[MPI-Leipzig](https://openneuro.org/datasets/ds000221/versions/00002) | 316 | 316 | 37 \u00b1 19 | 40 | 2\n**Total** | 7764 | **10420** | 32 \u00b1 19 | 50 | 74 \n \n", "original_header": "BHB-10K dataset for pre-training"}, "confidence": 0.8604584952392373, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/Duplums/yAwareContrastiveLearning/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "Then you can directly run the main script with your configuration in `config.py` including:\n- the paths to your training/validation data\n- the proxy label you want to use during training along with the hyperparameter sigma\n- the network (critic) including a base encoder and a projection head which is here a simple MLP(2)  \n``` python\nself.data_train = \"/path/to/your/training/data.npy\"\nself.label_train = \"/path/to/your/training/metadata.csv\"\n\nself.data_val = \"/path/to/your/validation/data.npy\" \nself.label_val = \"/path/to/your/validation/metadata.csv\" \n\nself.input_size = (C, H, W, D) # typically (1, 121, 145, 121) for sMRI \nself.label_name = \"age\" # asserts \"age\" in metadata.csv columns \n\nself.checkpoint_dir = \"/path/to/your/saving/directory/\"\nself.model = \"DenseNet\"\n``` \n", "original_header": "Configuration"}, "confidence": 0.8352473556862093, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/Duplums/yAwareContrastiveLearning/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "In order to fine-tune the model on your target task, do not forget to set the path to the downloaded file in `config.py`:\n``` python\nself.pretrained_path = \"/path/to/DenseNet121_BHB-10K_yAwareContrastive.pth\"\n```\nThen you can define your own Pytorch `Dataset` in  `main.py`:\nBASH2*  \nYou can finally fine-tune your model with:\n``` bash\n$ python3 main.py --mode finetuning\n``` \n \n", "original_header": "Fine-tuning your model"}, "confidence": 0.8224528809000635, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/Duplums/yAwareContrastiveLearning/main/README.md"}], "full_title": [{"result": {"type": "String", "value": "y-Aware Contrastive Learning"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/Duplums/yAwareContrastiveLearning/main/README.md"}], "image": [{"result": {"type": "Url", "value": "https://raw.githubusercontent.com/Duplums/yAwareContrastiveLearning/main/images/main.jpg"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/Duplums/yAwareContrastiveLearning/main/README.md"}, {"result": {"type": "Url", "value": "https://raw.githubusercontent.com/Duplums/yAwareContrastiveLearning/main/images/unsupervised_perf_scz_bip_ad_N10K.png"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/Duplums/yAwareContrastiveLearning/main/README.md"}, {"result": {"type": "Url", "value": "https://raw.githubusercontent.com/Duplums/yAwareContrastiveLearning/main/images/UMAP_contrastive-age-aware.png"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/Duplums/yAwareContrastiveLearning/main/README.md"}]}