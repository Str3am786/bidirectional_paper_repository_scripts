{"somef_provenance": {"somef_version": "0.9.4", "somef_schema_version": "1.0.0", "date": "2023-12-21 19:18:51"}, "code_repository": [{"result": {"value": "https://github.com/Audio-WestlakeU/NBSS", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "owner": [{"result": {"value": "Audio-WestlakeU", "type": "Organization"}, "confidence": 1, "technique": "GitHub_API"}], "date_created": [{"result": {"value": "2021-09-15T07:00:20Z", "type": "Date"}, "confidence": 1, "technique": "GitHub_API"}], "date_updated": [{"result": {"value": "2023-12-20T06:43:46Z", "type": "Date"}, "confidence": 1, "technique": "GitHub_API"}], "description": [{"result": {"value": "The official repo of NBC & SpatialNet for multichannel speech separation, denoising, and dereverberation", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Text_excerpt", "value": "This project is built on the `pytorch-lightning` package, in particular its [command line interface (CLI)](https://pytorch-lightning.readthedocs.io/en/latest/cli/lightning_cli_intermediate.html). Thus we recommond you to have some knowledge about the CLI in lightning. For Chinese user, you can learn CLI & lightning with this begining project [pytorch_lightning_template_for_beginners](https://github.com/Audio-WestlakeU/pytorch_lightning_template_for_beginners). \n", "original_header": "Train &amp; Test"}, "confidence": 0.9901944468004579, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/Audio-WestlakeU/NBSS/main/README.md"}], "name": [{"result": {"value": "NBSS", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}], "full_name": [{"result": {"value": "Audio-WestlakeU/NBSS", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}], "issue_tracker": [{"result": {"value": "https://api.github.com/repos/Audio-WestlakeU/NBSS/issues", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "forks_url": [{"result": {"value": "https://api.github.com/repos/Audio-WestlakeU/NBSS/forks", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "stargazers_count": [{"result": {"value": 118, "type": "Number"}, "confidence": 1, "technique": "GitHub_API"}], "keywords": [{"result": {"value": "full-band, multi-channel, narrow-band, pytorch, separation, speech", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}], "forks_count": [{"result": {"value": 13, "type": "Number"}, "confidence": 1, "technique": "GitHub_API"}], "download_url": [{"result": {"value": "https://github.com/Audio-WestlakeU/NBSS/releases", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "programming_languages": [{"result": {"value": "Python", "name": "Python", "type": "Programming_language", "size": 353442}, "confidence": 1, "technique": "GitHub_API"}], "readme_url": [{"result": {"value": "https://raw.githubusercontent.com/Audio-WestlakeU/NBSS/main/README.md", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}], "requirements": [{"result": {"value": "```bash\npip install -r requirements.txt\n\n# gpuRIR: check https://github.com/DavidDiazGuerra/gpuRIR\n```\n", "type": "Text_excerpt", "original_header": "Requirements", "parent_header": ["Multi-channel Speech Separation, Denoising and Dereverberation"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/Audio-WestlakeU/NBSS/main/README.md"}], "installation": [{"result": {"type": "Text_excerpt", "value": "where `version_x` should be replaced with the version you want to resume. \n", "original_header": "Train &amp; Test"}, "confidence": 0.8902685425644251, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/Audio-WestlakeU/NBSS/main/README.md"}], "invocation": [{"result": {"type": "Text_excerpt", "value": "Generate rirs for the dataset `SMS-WSJ_plus` used in `SpatialNet` ablation experiment.\n```bash\nCUDA_VISIBLE_DEVICES=0 python generate_rirs.py --rir_dir ~/datasets/SMS_WSJ_Plus_rirs --save_to configs/datasets/sms_wsj_rir_cfg.npz\ncp configs/datasets/sms_wsj_plus_diffuse.npz ~/datasets/SMS_WSJ_Plus_rirs/diffuse.npz # copy diffuse parameters\n```\n \n", "original_header": "Generate Dataset SMS-WSJ-Plus"}, "confidence": 0.8467087238082225, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/Audio-WestlakeU/NBSS/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "**Train** SpatialNet on the 0-th GPU with network config file `configs/SpatialNet.yaml` and dataset config file `configs/datasets/sms_wsj_plus.yaml` (replace the rir & clean speech dir before training).\n```bash\npython SharedTrainer.py fit \\\n --config=configs/SpatialNet.yaml \\ # network config\n --config=configs/datasets/sms_wsj_plus.yaml \\ # dataset config\n --model.channels=[0,1,2,3,4,5] \\ # the channels used\n --model.arch.dim_input=12 \\ # input dim per T-F point, i.e. 2 * the number of channels\n --model.arch.dim_output=4 \\ # output dim per T-F point, i.e. 2 * the number of sources\n --model.arch.num_freqs=129 \\ # the number of frequencies, related to model.stft.n_fft\n --trainer.precision=bf16-mixed \\ # mixed precision training, can also be 16-mixed or 32\n --model.compile=True \\ # compile the network, requires torch>=2.0. the compiled model is trained much faster\n --data.batch_size=[2,4] \\ # batch size for train and val\n --trainer.devices=0, \\\n --trainer.max_epochs=100\n```\n \n**Resume** training from a checkpoint:\n```bash\npython SharedTrainer.py fit --config=logs/SpatialNet/version_x/config.yaml \\\n --data.batch_size=[2,2] \\\n --trainer.devices=0, \\ \n --ckpt_path=logs/SpatialNet/version_x/checkpoints/last.ckpt\n```\n \n**Test** the model trained:\n```bash\npython SharedTrainer.py test --config=logs/SpatialNet/version_x/config.yaml \\ \n --ckpt_path=logs/SpatialNet/version_x/checkpoints/epochY_neg_si_sdrZ.ckpt \\ \n --trainer.devices=0,\n```\n \n", "original_header": "Train &amp; Test"}, "confidence": 0.9201525205938997, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/Audio-WestlakeU/NBSS/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "| network | file |\n|:---|:---|\n| SpatialNet [4] |models/arch/SpatialNet.py|\n| NB-BLSTM [1] | models/arch/NBSS.py |\n| NBC [2] | models/arch/NBSS.py |\n| NBC2 [3] | models/arch/NBSS.py |\n \n", "original_header": "Module Version"}, "confidence": 0.9160567362278167, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/Audio-WestlakeU/NBSS/main/README.md"}], "full_title": [{"result": {"type": "String", "value": "Multi-channel Speech Separation, Denoising and Dereverberation"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/Audio-WestlakeU/NBSS/main/README.md"}], "related_documentation": [{"result": {"type": "Url", "value": "https://pytorch-lightning.readthedocs.io/", "format": "readthedocs"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/Audio-WestlakeU/NBSS/main/README.md"}], "related_papers": [{"result": {"type": "Url", "value": "https://arxiv.org/abs/2212.02076"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/Audio-WestlakeU/NBSS/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2307.16516.\n\nAudio examples can be found at [https://audio.westlake.edu.cn/Research/nbss.htm](https://audio.westlake.edu.cn/Research/nbss.htm) and [https://audio.westlake.edu.cn/Research/SpatialNet.htm](https://audio.westlake.edu.cn/Research/SpatialNet.htm).\nMore information about our group can be found at [https://audio.westlake.edu.cn](https://audio.westlake.edu.cn/Publications.htm).\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n\n# gpuRIR: check https://github.com/DavidDiazGuerra/gpuRIR\n```\n\n## Generate Dataset SMS-WSJ-Plus\n\nGenerate rirs for the dataset `SMS-WSJ_plus` used in `SpatialNet` ablation experiment.\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python generate_rirs.py --rir_dir ~/datasets/SMS_WSJ_Plus_rirs --save_to configs/datasets/sms_wsj_rir_cfg.npz\ncp configs/datasets/sms_wsj_plus_diffuse.npz ~/datasets/SMS_WSJ_Plus_rirs/diffuse.npz # copy diffuse parameters\n```\n\nFor SMS-WSJ, please see https://github.com/fgnt/sms_wsj\n\n## Train & Test\n\nThis project is built on the `pytorch-lightning` package, in particular its [command line interface (CLI)](https://pytorch-lightning.readthedocs.io/en/latest/cli/lightning_cli_intermediate.html). Thus we recommond you to have some knowledge about the CLI in lightning. For Chinese user, you can learn CLI & lightning with this begining project [pytorch_lightning_template_for_beginners](https://github.com/Audio-WestlakeU/pytorch_lightning_template_for_beginners).\n\n**Train** SpatialNet on the 0-th GPU with network config file `configs/SpatialNet.yaml` and dataset config file `configs/datasets/sms_wsj_plus.yaml` (replace the rir & clean speech dir before training).\n\n```bash\npython SharedTrainer.py fit \\\n --config=configs/SpatialNet.yaml \\ # network config\n --config=configs/datasets/sms_wsj_plus.yaml \\ # dataset config\n --model.channels=[0,1,2,3,4,5] \\ # the channels used\n --model.arch.dim_input=12 \\ # input dim per T-F point, i.e. 2 * the number of channels\n --model.arch.dim_output=4 \\ # output dim per T-F point, i.e. 2 * the number of sources\n --model.arch.num_freqs=129 \\ # the number of frequencies, related to model.stft.n_fft\n --trainer.precision=bf16-mixed \\ # mixed precision training, can also be 16-mixed or 32\n --model.compile=True \\ # compile the network, requires torch>=2.0. the compiled model is trained much faster\n --data.batch_size=[2,4] \\ # batch size for train and val\n --trainer.devices=0, \\\n --trainer.max_epochs=100\n```\n\nMore gpus can be used by appending the gpu indexes to `trainer.devices`, e.g. `--trainer.devices=0,1,2,3,`.\n\n**Resume** training from a checkpoint:\n\n```bash\npython SharedTrainer.py fit --config=logs/SpatialNet/version_x/config.yaml \\\n --data.batch_size=[2,2] \\\n --trainer.devices=0, \\ \n --ckpt_path=logs/SpatialNet/version_x/checkpoints/last.ckpt\n```\n\nwhere `version_x` should be replaced with the version you want to resume.\n\n**Test** the model trained:\n\n```bash\npython SharedTrainer.py test --config=logs/SpatialNet/version_x/config.yaml \\ \n --ckpt_path=logs/SpatialNet/version_x/checkpoints/epochY_neg_si_sdrZ.ckpt \\ \n --trainer.devices=0,\n```\n\n## Module Version\n\n| network | file |\n|:---|:---|\n| SpatialNet [4] |models/arch/SpatialNet.py|\n| NB-BLSTM [1] | models/arch/NBSS.py |\n| NBC [2] | models/arch/NBSS.py |\n| NBC2 [3] | models/arch/NBSS.py |\n\n## Note\nThe dataset generation & training commands for the `NB-BLSTM`/`NBC`/`NBC2` are available in the `NBSS` branch."}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/Audio-WestlakeU/NBSS/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2307.16516"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/Audio-WestlakeU/NBSS/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2204.04464"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/Audio-WestlakeU/NBSS/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2212.02076.  \n[4] Changsheng Quan, Xiaofei Li. [SpatialNet: Extensively Learning Spatial Information for Multichannel Joint Speech Separation, Denoising and Dereverberation](https://arxiv.org/abs/2307.16516). https://arxiv.org/abs/2307.16516.\n\nAudio examples can be found at [https://audio.westlake.edu.cn/Research/nbss.htm](https://audio.westlake.edu.cn/Research/nbss.htm) and [https://audio.westlake.edu.cn/Research/SpatialNet.htm](https://audio.westlake.edu.cn/Research/SpatialNet.htm).\nMore information about our group can be found at [https://audio.westlake.edu.cn](https://audio.westlake.edu.cn/Publications.htm).\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n\n# gpuRIR: check https://github.com/DavidDiazGuerra/gpuRIR\n```\n\n## Generate Dataset SMS-WSJ-Plus\n\nGenerate rirs for the dataset `SMS-WSJ_plus` used in `SpatialNet` ablation experiment.\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python generate_rirs.py --rir_dir ~/datasets/SMS_WSJ_Plus_rirs --save_to configs/datasets/sms_wsj_rir_cfg.npz\ncp configs/datasets/sms_wsj_plus_diffuse.npz ~/datasets/SMS_WSJ_Plus_rirs/diffuse.npz # copy diffuse parameters\n```\n\nFor SMS-WSJ, please see https://github.com/fgnt/sms_wsj\n\n## Train & Test\n\nThis project is built on the `pytorch-lightning` package, in particular its [command line interface (CLI)](https://pytorch-lightning.readthedocs.io/en/latest/cli/lightning_cli_intermediate.html). Thus we recommond you to have some knowledge about the CLI in lightning. For Chinese user, you can learn CLI & lightning with this begining project [pytorch_lightning_template_for_beginners](https://github.com/Audio-WestlakeU/pytorch_lightning_template_for_beginners).\n\n**Train** SpatialNet on the 0-th GPU with network config file `configs/SpatialNet.yaml` and dataset config file `configs/datasets/sms_wsj_plus.yaml` (replace the rir & clean speech dir before training).\n\n```bash\npython SharedTrainer.py fit \\\n --config=configs/SpatialNet.yaml \\ # network config\n --config=configs/datasets/sms_wsj_plus.yaml \\ # dataset config\n --model.channels=[0,1,2,3,4,5] \\ # the channels used\n --model.arch.dim_input=12 \\ # input dim per T-F point, i.e. 2 * the number of channels\n --model.arch.dim_output=4 \\ # output dim per T-F point, i.e. 2 * the number of sources\n --model.arch.num_freqs=129 \\ # the number of frequencies, related to model.stft.n_fft\n --trainer.precision=bf16-mixed \\ # mixed precision training, can also be 16-mixed or 32\n --model.compile=True \\ # compile the network, requires torch>=2.0. the compiled model is trained much faster\n --data.batch_size=[2,4] \\ # batch size for train and val\n --trainer.devices=0, \\\n --trainer.max_epochs=100\n```\n\nMore gpus can be used by appending the gpu indexes to `trainer.devices`, e.g. `--trainer.devices=0,1,2,3,`.\n\n**Resume** training from a checkpoint:\n\n```bash\npython SharedTrainer.py fit --config=logs/SpatialNet/version_x/config.yaml \\\n --data.batch_size=[2,2] \\\n --trainer.devices=0, \\ \n --ckpt_path=logs/SpatialNet/version_x/checkpoints/last.ckpt\n```\n\nwhere `version_x` should be replaced with the version you want to resume.\n\n**Test** the model trained:\n\n```bash\npython SharedTrainer.py test --config=logs/SpatialNet/version_x/config.yaml \\ \n --ckpt_path=logs/SpatialNet/version_x/checkpoints/epochY_neg_si_sdrZ.ckpt \\ \n --trainer.devices=0,\n```\n\n## Module Version\n\n| network | file |\n|:---|:---|\n| SpatialNet [4] |models/arch/SpatialNet.py|\n| NB-BLSTM [1] | models/arch/NBSS.py |\n| NBC [2] | models/arch/NBSS.py |\n| NBC2 [3] | models/arch/NBSS.py |\n\n## Note\nThe dataset generation & training commands for the `NB-BLSTM`/`NBC`/`NBC2` are available in the `NBSS` branch."}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/Audio-WestlakeU/NBSS/main/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/2110.05966"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/Audio-WestlakeU/NBSS/main/README.md"}]}