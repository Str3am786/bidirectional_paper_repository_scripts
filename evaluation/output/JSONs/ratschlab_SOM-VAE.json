{"somef_provenance": {"somef_version": "0.9.4", "somef_schema_version": "1.0.0", "date": "2023-12-21 18:48:29"}, "code_repository": [{"result": {"value": "https://github.com/ratschlab/SOM-VAE", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "owner": [{"result": {"value": "ratschlab", "type": "Organization"}, "confidence": 1, "technique": "GitHub_API"}], "date_created": [{"result": {"value": "2018-06-26T16:43:26Z", "type": "Date"}, "confidence": 1, "technique": "GitHub_API"}], "date_updated": [{"result": {"value": "2023-12-20T03:32:52Z", "type": "Date"}, "confidence": 1, "technique": "GitHub_API"}], "license": [{"result": {"value": "https://api.github.com/licenses/mit", "type": "License", "name": "MIT License", "url": "https://api.github.com/licenses/mit", "spdx_id": "MIT"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "Copyright (c) 2018 Vincent Fortuin, Biomedical Informatics group, ETH Zurich\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n", "type": "File_dump"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/ratschlab/SOM-VAE/master/LICENSE"}, {"result": {"value": "This project is licensed under the MIT License - see the [LICENSE](LICENSE.md) file for details\n\n", "type": "Text_excerpt", "original_header": "License", "parent_header": ["SOM-VAE model"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/ratschlab/SOM-VAE/master/README.md"}], "description": [{"result": {"value": "TensorFlow implementation of the SOM-VAE model as described in https://arxiv.org/abs/1806.02199", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Text_excerpt", "value": "This repository contains a TensorFlow implementation of the self-organizing map variational autoencoder as described in the paper [SOM-VAE: Interpretable Discrete Representation Learning on Time Series](https://arxiv.org/abs/1806.02199). \n", "original_header": "SOM-VAE model"}, "confidence": 0.9908341326686753, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/ratschlab/SOM-VAE/master/README.md"}, {"result": {"type": "Text_excerpt", "value": "See also the list of [contributors](https://github.com/ratschlab/SOM-VAE/contributors) who participated in this project.\n \n", "original_header": "Authors"}, "confidence": 0.9945052979934714, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/ratschlab/SOM-VAE/master/README.md"}], "name": [{"result": {"value": "SOM-VAE", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}], "full_name": [{"result": {"value": "ratschlab/SOM-VAE", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}], "issue_tracker": [{"result": {"value": "https://api.github.com/repos/ratschlab/SOM-VAE/issues", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "forks_url": [{"result": {"value": "https://api.github.com/repos/ratschlab/SOM-VAE/forks", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "stargazers_count": [{"result": {"value": 183, "type": "Number"}, "confidence": 1, "technique": "GitHub_API"}], "keywords": [{"result": {"value": "", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}], "forks_count": [{"result": {"value": 31, "type": "Number"}, "confidence": 1, "technique": "GitHub_API"}], "download_url": [{"result": {"value": "https://github.com/ratschlab/SOM-VAE/releases", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "programming_languages": [{"result": {"value": "Python", "name": "Python", "type": "Programming_language", "size": 35544}, "confidence": 1, "technique": "GitHub_API"}], "readme_url": [{"result": {"value": "https://raw.githubusercontent.com/ratschlab/SOM-VAE/master/README.md", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}], "usage": [{"result": {"value": "These instructions will get you a copy of the project up and running on your local machine for development and testing purposes.\n", "type": "Text_excerpt", "original_header": "Getting Started", "parent_header": ["SOM-VAE model"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/ratschlab/SOM-VAE/master/README.md"}, {"result": {"value": "The SOM-VAE model is defined in [somvae_model.py](som_vae/somvae_model.py).\nThe training script is [somvae_train.py](som_vae/somvae_train.py).\n\nIf you just want to train the model with default parameter settings, you can run\n\n```\npython somvae_train.py\n```\n\nThis will download the MNIST data set into `data/MNIST_data/` and train on it. Afterwards, it will evaluate the trained model in terms of different clustering performance measures.\n\nThe parameters are handled using [sacred](https://github.com/IDSIA/sacred).\nThat means that if you want to run the model with a different parameter setting, e.g. a latent space dimensionality of 32, you can just call the training script like\n\n```\npython somvae_train.py with latent_dim=32\n```\n\nPer default, the script will generate time courses of linearly interpolated MNIST digits.\nTo train on normal MNIST instead, run\n\n```\npython somvae_train.py with time_series=False\n```\n\nNote that for non-time-series training, you should also set the loss parameters `gamma` and `tau` to 0.\nIf you want to save the model for later use, run\n\n```\npython somvae_train.py with save_model=True\n```\n\nIf you want to train on [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) istead of normal MNIST, download the data set into `data/fashion/` and run\n\n```\npython somvae_train.py with data_set=\"fashion\"\n```\n\nFor more details regarding the different model parameters and how to set them, please look at the documentation in the [code](som_vae/somvae_train.py) and at the sacred documentation.\n", "type": "Text_excerpt", "original_header": "Training the model", "parent_header": ["SOM-VAE model", "Getting Started"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/ratschlab/SOM-VAE/master/README.md"}, {"result": {"value": "If you want to optimize the models hyperparameters, you have to additionally install [labwatch](https://github.com/automl/labwatch) and [SMAC](https://github.com/automl/SMAC3) and comment the commented out lines in [somvae_train.py](som_vae/somvae_train.py) in.\nNote that you also have to run a local distribution of the [MongoDB](ihttps://www.mongodb.com/).\n", "type": "Text_excerpt", "original_header": "Hyperparameter optimization", "parent_header": ["SOM-VAE model", "Getting Started"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/ratschlab/SOM-VAE/master/README.md"}, {"result": {"value": "If you want to train on other types of data, you have to run the training with\n\n```\npython somvae_train.py with mnist=False\n```\n\nMoreover, you have to define the correct dimensionality in the respective `input_length` and `input_channels` parameters of the model, provide a suitable data generator in [somvae_train.py](som_vae/somvae_train.py) and potentially change the dimensionality of the layers in [somvae_model.py](som_vae/somvae_model.py).\n\nTo reproduce the experiments on eICU data, please use the preprocessing pipeline from this repository: https://github.com/ratschlab/variational-psom\n", "type": "Text_excerpt", "original_header": "Train on other kinds of data", "parent_header": ["SOM-VAE model", "Getting Started"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/ratschlab/SOM-VAE/master/README.md"}], "requirements": [{"result": {"value": "In order to install and run the model, you will need a working Python 3 distribution as well as a NVIDIA GPU with CUDA and cuDNN installed.\n", "type": "Text_excerpt", "original_header": "Prerequisites", "parent_header": ["SOM-VAE model", "Getting Started"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/ratschlab/SOM-VAE/master/README.md"}], "installation": [{"result": {"value": "In order to install the model and run it, you have to follow these steps:\n\n* Clone the repository, i.e. run `git clone https://github.com/ratschlab/SOM-VAE`\n* Change into the directory, i.e. run `cd SOM-VAE`\n* Install the requirements, i.e. run `pip install -r requirements.txt`\n* Install the package itself, i.e. run `pip install .`\n* Change into the code directory, i.e. `cd som_vae`\n\nNow you should be able to run the code, e.g. do `python somvae_train.py`.\n", "type": "Text_excerpt", "original_header": "Installing", "parent_header": ["SOM-VAE model", "Getting Started"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/ratschlab/SOM-VAE/master/README.md"}], "application_domain": [{"result": {"type": "String", "value": "Computer Vision"}, "confidence": 0.9138615227147391, "technique": "supervised_classification"}], "full_title": [{"result": {"type": "String", "value": "SOM-VAE model"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/ratschlab/SOM-VAE/master/README.md"}], "related_papers": [{"result": {"type": "Url", "value": "https://arxiv.org/abs/1806.02199"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/ratschlab/SOM-VAE/master/README.md"}, {"result": {"type": "Url", "value": "https://arxiv.org/abs/1910.01590"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/ratschlab/SOM-VAE/master/README.md"}]}